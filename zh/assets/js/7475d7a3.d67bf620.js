"use strict";(self.webpackChunkdoc=self.webpackChunkdoc||[]).push([[64633],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>c});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function l(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?l(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):l(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},l=Object.keys(e);for(a=0;a<l.length;a++)n=l[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(a=0;a<l.length;a++)n=l[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var p=a.createContext({}),s=function(e){var t=a.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},d=function(e){var t=s(e.components);return a.createElement(p.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,l=e.originalType,p=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),u=s(n),c=r,g=u["".concat(p,".").concat(c)]||u[c]||m[c]||l;return n?a.createElement(g,i(i({ref:t},d),{},{components:n})):a.createElement(g,i({ref:t},d))}));function c(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var l=n.length,i=new Array(l);i[0]=u;var o={};for(var p in t)hasOwnProperty.call(t,p)&&(o[p]=t[p]);o.originalType=e,o.mdxType="string"==typeof e?e:r,i[1]=o;for(var s=2;s<l;s++)i[s]=n[s];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},96456:(e,t,n)=>{n.r(t),n.d(t,{contentTitle:()=>i,default:()=>d,frontMatter:()=>l,metadata:()=>o,toc:()=>p});var a=n(87462),r=(n(67294),n(3905));const l={title:"ai-proxy",keywords:["Apache APISIX","API Gateway","Plugin","ai-proxy","AI","LLM"],description:"The ai-proxy Plugin simplifies access to LLM and embedding models providers by converting Plugin configurations into the required request format for OpenAI, DeepSeek, Azure, AIMLAPI, and other OpenAI-compatible APIs."},i=void 0,o={unversionedId:"plugins/ai-proxy",id:"version-3.14/plugins/ai-proxy",isDocsHomePage:!1,title:"ai-proxy",description:"The ai-proxy Plugin simplifies access to LLM and embedding models providers by converting Plugin configurations into the required request format for OpenAI, DeepSeek, Azure, AIMLAPI, and other OpenAI-compatible APIs.",source:"@site/docs-apisix_versioned_docs/version-3.14/plugins/ai-proxy.md",sourceDirName:"plugins",slug:"/plugins/ai-proxy",permalink:"/zh/docs/apisix/3.14/plugins/ai-proxy",editUrl:"/zh/edit#https://github.com/apache/apisix/edit/release/3.14/docs/zh/latest/plugins/ai-proxy.md",tags:[],version:"3.14",frontMatter:{title:"ai-proxy",keywords:["Apache APISIX","API Gateway","Plugin","ai-proxy","AI","LLM"],description:"The ai-proxy Plugin simplifies access to LLM and embedding models providers by converting Plugin configurations into the required request format for OpenAI, DeepSeek, Azure, AIMLAPI, and other OpenAI-compatible APIs."},sidebar:"version-3.14/docs",previous:{title:"Secret",permalink:"/zh/docs/apisix/3.14/terminology/secret"},next:{title:"ai-proxy-multi",permalink:"/zh/docs/apisix/3.14/plugins/ai-proxy-multi"}},p=[{value:"Description",id:"description",children:[]},{value:"Request Format",id:"request-format",children:[]},{value:"Attributes",id:"attributes",children:[]},{value:"Examples",id:"examples",children:[{value:"Proxy to OpenAI",id:"proxy-to-openai",children:[]},{value:"Proxy to DeepSeek",id:"proxy-to-deepseek",children:[]},{value:"Proxy to Azure OpenAI",id:"proxy-to-azure-openai",children:[]},{value:"Proxy to Embedding Models",id:"proxy-to-embedding-models",children:[]},{value:"Include LLM Information in Access Log",id:"include-llm-information-in-access-log",children:[]}]}],s={toc:p};function d(e){let{components:t,...n}=e;return(0,r.kt)("wrapper",(0,a.Z)({},s,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("head",null,(0,r.kt)("link",{rel:"canonical",href:"https://docs.api7.ai/hub/ai-proxy"})),(0,r.kt)("h2",{id:"description"},"Description"),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"ai-proxy")," Plugin simplifies access to LLM and embedding models by transforming Plugin configurations into the designated request format. It supports the integration with OpenAI, DeepSeek, Azure, AIMLAPI, and other OpenAI-compatible APIs."),(0,r.kt)("p",null,"In addition, the Plugin also supports logging LLM request information in the access log, such as token usage, model, time to the first response, and more."),(0,r.kt)("h2",{id:"request-format"},"Request Format"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Type"),(0,r.kt)("th",{parentName:"tr",align:null},"Required"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"messages")),(0,r.kt)("td",{parentName:"tr",align:null},"Array"),(0,r.kt)("td",{parentName:"tr",align:null},"True"),(0,r.kt)("td",{parentName:"tr",align:null},"An array of message objects.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"messages.role")),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"True"),(0,r.kt)("td",{parentName:"tr",align:null},"Role of the message (",(0,r.kt)("inlineCode",{parentName:"td"},"system"),", ",(0,r.kt)("inlineCode",{parentName:"td"},"user"),", ",(0,r.kt)("inlineCode",{parentName:"td"},"assistant"),").")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"messages.content")),(0,r.kt)("td",{parentName:"tr",align:null},"String"),(0,r.kt)("td",{parentName:"tr",align:null},"True"),(0,r.kt)("td",{parentName:"tr",align:null},"Content of the message.")))),(0,r.kt)("h2",{id:"attributes"},"Attributes"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Type"),(0,r.kt)("th",{parentName:"tr",align:null},"Required"),(0,r.kt)("th",{parentName:"tr",align:null},"Default"),(0,r.kt)("th",{parentName:"tr",align:null},"Valid values"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"provider"),(0,r.kt)("td",{parentName:"tr",align:null},"string"),(0,r.kt)("td",{parentName:"tr",align:null},"True"),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"[openai, deepseek, azure-openai, aimlapi, openai-compatible]"),(0,r.kt)("td",{parentName:"tr",align:null},"LLM service provider. When set to ",(0,r.kt)("inlineCode",{parentName:"td"},"openai"),", the Plugin will proxy the request to ",(0,r.kt)("inlineCode",{parentName:"td"},"https://api.openai.com/chat/completions"),". When set to ",(0,r.kt)("inlineCode",{parentName:"td"},"deepseek"),", the Plugin will proxy the request to ",(0,r.kt)("inlineCode",{parentName:"td"},"https://api.deepseek.com/chat/completions"),". When set to ",(0,r.kt)("inlineCode",{parentName:"td"},"aimlapi"),", the Plugin uses the OpenAI-compatible driver and proxies the request to ",(0,r.kt)("inlineCode",{parentName:"td"},"https://api.aimlapi.com/v1/chat/completions")," by default. When set to ",(0,r.kt)("inlineCode",{parentName:"td"},"openai-compatible"),", the Plugin will proxy the request to the custom endpoint configured in ",(0,r.kt)("inlineCode",{parentName:"td"},"override"),".")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"auth"),(0,r.kt)("td",{parentName:"tr",align:null},"object"),(0,r.kt)("td",{parentName:"tr",align:null},"True"),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"Authentication configurations.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"auth.header"),(0,r.kt)("td",{parentName:"tr",align:null},"object"),(0,r.kt)("td",{parentName:"tr",align:null},"False"),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"Authentication headers. At least one of ",(0,r.kt)("inlineCode",{parentName:"td"},"header")," or ",(0,r.kt)("inlineCode",{parentName:"td"},"query")," must be configured.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"auth.query"),(0,r.kt)("td",{parentName:"tr",align:null},"object"),(0,r.kt)("td",{parentName:"tr",align:null},"False"),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"Authentication query parameters. At least one of ",(0,r.kt)("inlineCode",{parentName:"td"},"header")," or ",(0,r.kt)("inlineCode",{parentName:"td"},"query")," must be configured.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"options"),(0,r.kt)("td",{parentName:"tr",align:null},"object"),(0,r.kt)("td",{parentName:"tr",align:null},"False"),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"Model configurations. In addition to ",(0,r.kt)("inlineCode",{parentName:"td"},"model"),", you can configure additional parameters and they will be forwarded to the upstream LLM service in the request body. For instance, if you are working with OpenAI, you can configure additional parameters such as ",(0,r.kt)("inlineCode",{parentName:"td"},"temperature"),", ",(0,r.kt)("inlineCode",{parentName:"td"},"top_p"),", and ",(0,r.kt)("inlineCode",{parentName:"td"},"stream"),". See your LLM provider's API documentation for more available options.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"options.model"),(0,r.kt)("td",{parentName:"tr",align:null},"string"),(0,r.kt)("td",{parentName:"tr",align:null},"False"),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"Name of the LLM model, such as ",(0,r.kt)("inlineCode",{parentName:"td"},"gpt-4")," or ",(0,r.kt)("inlineCode",{parentName:"td"},"gpt-3.5"),". Refer to the LLM provider's API documentation for available models.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"override"),(0,r.kt)("td",{parentName:"tr",align:null},"object"),(0,r.kt)("td",{parentName:"tr",align:null},"False"),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"Override setting.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"override.endpoint"),(0,r.kt)("td",{parentName:"tr",align:null},"string"),(0,r.kt)("td",{parentName:"tr",align:null},"False"),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"Custom LLM provider endpoint, required when ",(0,r.kt)("inlineCode",{parentName:"td"},"provider")," is ",(0,r.kt)("inlineCode",{parentName:"td"},"openai-compatible"),".")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"logging"),(0,r.kt)("td",{parentName:"tr",align:null},"object"),(0,r.kt)("td",{parentName:"tr",align:null},"False"),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"Logging configurations.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"logging.summaries"),(0,r.kt)("td",{parentName:"tr",align:null},"boolean"),(0,r.kt)("td",{parentName:"tr",align:null},"False"),(0,r.kt)("td",{parentName:"tr",align:null},"false"),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"If true, logs request LLM model, duration, request, and response tokens.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"logging.payloads"),(0,r.kt)("td",{parentName:"tr",align:null},"boolean"),(0,r.kt)("td",{parentName:"tr",align:null},"False"),(0,r.kt)("td",{parentName:"tr",align:null},"false"),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"If true, logs request and response payload.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"timeout"),(0,r.kt)("td",{parentName:"tr",align:null},"integer"),(0,r.kt)("td",{parentName:"tr",align:null},"False"),(0,r.kt)("td",{parentName:"tr",align:null},"30000"),(0,r.kt)("td",{parentName:"tr",align:null},"\u2265 1"),(0,r.kt)("td",{parentName:"tr",align:null},"Request timeout in milliseconds when requesting the LLM service.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"keepalive"),(0,r.kt)("td",{parentName:"tr",align:null},"boolean"),(0,r.kt)("td",{parentName:"tr",align:null},"False"),(0,r.kt)("td",{parentName:"tr",align:null},"true"),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"If true, keeps the connection alive when requesting the LLM service.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"keepalive_timeout"),(0,r.kt)("td",{parentName:"tr",align:null},"integer"),(0,r.kt)("td",{parentName:"tr",align:null},"False"),(0,r.kt)("td",{parentName:"tr",align:null},"60000"),(0,r.kt)("td",{parentName:"tr",align:null},"\u2265 1000"),(0,r.kt)("td",{parentName:"tr",align:null},"Keepalive timeout in milliseconds when connecting to the LLM service.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"keepalive_pool"),(0,r.kt)("td",{parentName:"tr",align:null},"integer"),(0,r.kt)("td",{parentName:"tr",align:null},"False"),(0,r.kt)("td",{parentName:"tr",align:null},"30"),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"Keepalive pool size for the LLM service connection.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"ssl_verify"),(0,r.kt)("td",{parentName:"tr",align:null},"boolean"),(0,r.kt)("td",{parentName:"tr",align:null},"False"),(0,r.kt)("td",{parentName:"tr",align:null},"true"),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"If true, verifies the LLM service's certificate.")))),(0,r.kt)("h2",{id:"examples"},"Examples"),(0,r.kt)("p",null,"The examples below demonstrate how you can configure ",(0,r.kt)("inlineCode",{parentName:"p"},"ai-proxy")," for different scenarios."),(0,r.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},"You can fetch the ",(0,r.kt)("inlineCode",{parentName:"p"},"admin_key")," from ",(0,r.kt)("inlineCode",{parentName:"p"},"config.yaml")," and save to an environment variable with the following command:"),(0,r.kt)("pre",{parentName:"div"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"admin_key=$(yq '.deployment.admin.admin_key[0].key' conf/config.yaml | sed 's/\"//g')\n")))),(0,r.kt)("h3",{id:"proxy-to-openai"},"Proxy to OpenAI"),(0,r.kt)("p",null,"The following example demonstrates how you can configure the API key, model, and other parameters in the ",(0,r.kt)("inlineCode",{parentName:"p"},"ai-proxy")," Plugin and configure the Plugin on a Route to proxy user prompts to OpenAI."),(0,r.kt)("p",null,"Obtain the OpenAI ",(0,r.kt)("a",{parentName:"p",href:"https://openai.com/blog/openai-api"},"API key")," and save it to an environment variable:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"export OPENAI_API_KEY=<your-api-key>\n")),(0,r.kt)("p",null,"Create a Route and configure the ",(0,r.kt)("inlineCode",{parentName:"p"},"ai-proxy")," Plugin as such:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9180/apisix/admin/routes" -X PUT \\\n  -H "X-API-KEY: ${admin_key}" \\\n  -d \'{\n    "id": "ai-proxy-route",\n    "uri": "/anything",\n    "methods": ["POST"],\n    "plugins": {\n      "ai-proxy": {\n        "provider": "openai",\n        "auth": {\n          "header": {\n            "Authorization": "Bearer \'"$OPENAI_API_KEY"\'"\n          }\n        },\n        "options":{\n          "model": "gpt-4"\n        }\n      }\n    }\n  }\'\n')),(0,r.kt)("p",null,"Send a POST request to the Route with a system prompt and a sample user question in the request body:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9080/anything" -X POST \\\n  -H "Content-Type: application/json" \\\n  -H "Host: api.openai.com" \\\n  -d \'{\n    "messages": [\n      { "role": "system", "content": "You are a mathematician" },\n      { "role": "user", "content": "What is 1+1?" }\n    ]\n  }\'\n')),(0,r.kt)("p",null,"You should receive a response similar to the following:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  ...,\n  "model": "gpt-4-0613",\n  "choices": [\n    {\n      "index": 0,\n      "message": {\n        "role": "assistant",\n        "content": "1+1 equals 2.",\n        "refusal": null\n      },\n      "logprobs": null,\n      "finish_reason": "stop"\n    }\n  ],\n  ...\n}\n')),(0,r.kt)("h3",{id:"proxy-to-deepseek"},"Proxy to DeepSeek"),(0,r.kt)("p",null,"The following example demonstrates how you can configure the ",(0,r.kt)("inlineCode",{parentName:"p"},"ai-proxy")," Plugin to proxy requests to DeekSeek."),(0,r.kt)("p",null,"Obtain the DeekSeek API key and save it to an environment variable:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"export DEEPSEEK_API_KEY=<your-api-key>\n")),(0,r.kt)("p",null,"Create a Route and configure the ",(0,r.kt)("inlineCode",{parentName:"p"},"ai-proxy")," Plugin as such:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9180/apisix/admin/routes" -X PUT \\\n  -H "X-API-KEY: ${admin_key}" \\\n  -d \'{\n    "id": "ai-proxy-route",\n    "uri": "/anything",\n    "methods": ["POST"],\n    "plugins": {\n      "ai-proxy": {\n        "provider": "deepseek",\n        "auth": {\n          "header": {\n            "Authorization": "Bearer \'"$DEEPSEEK_API_KEY"\'"\n          }\n        },\n        "options": {\n          "model": "deepseek-chat"\n        }\n      }\n    }\n  }\'\n')),(0,r.kt)("p",null,"Send a POST request to the Route with a sample question in the request body:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9080/anything" -X POST \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "messages": [\n      {\n        "role": "system",\n        "content": "You are an AI assistant that helps people find information."\n      },\n      {\n        "role": "user",\n        "content": "Write me a 50-word introduction for Apache APISIX."\n      }\n    ]\n  }\'\n')),(0,r.kt)("p",null,"You should receive a response similar to the following:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  ...\n  "choices": [\n    {\n      "index": 0,\n      "message": {\n        "role": "assistant",\n        "content": "Apache APISIX is a dynamic, real-time, high-performance API gateway and cloud-native platform. It provides rich traffic management features like load balancing, dynamic upstream, canary release, circuit breaking, authentication, observability, and more. Designed for microservices and serverless architectures, APISIX ensures scalability, security, and seamless integration with modern DevOps workflows."\n      },\n      "logprobs": null,\n      "finish_reason": "stop"\n    }\n  ],\n  ...\n}\n')),(0,r.kt)("h3",{id:"proxy-to-azure-openai"},"Proxy to Azure OpenAI"),(0,r.kt)("p",null,"The following example demonstrates how you can configure the ",(0,r.kt)("inlineCode",{parentName:"p"},"ai-proxy")," Plugin to proxy requests to other LLM services, such as Azure OpenAI."),(0,r.kt)("p",null,"Obtain the Azure OpenAI API key and save it to an environment variable:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"export AZ_OPENAI_API_KEY=<your-api-key>\n")),(0,r.kt)("p",null,"Create a Route and configure the ",(0,r.kt)("inlineCode",{parentName:"p"},"ai-proxy")," Plugin as such:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9180/apisix/admin/routes" -X PUT \\\n  -H "X-API-KEY: ${admin_key}" \\\n  -d \'{\n    "id": "ai-proxy-route",\n    "uri": "/anything",\n    "methods": ["POST"],\n    "plugins": {\n      "ai-proxy": {\n        "provider": "openai-compatible",\n        "auth": {\n          "header": {\n            "api-key": "\'"$AZ_OPENAI_API_KEY"\'"\n          }\n        },\n        "options":{\n          "model": "gpt-4"\n        },\n        "override": {\n          "endpoint": "https://api7-auzre-openai.openai.azure.com/openai/deployments/gpt-4/chat/completions?api-version=2024-02-15-preview"\n        }\n      }\n    }\n  }\'\n')),(0,r.kt)("p",null,"Send a POST request to the Route with a sample question in the request body:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9080/anything" -X POST \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "messages": [\n      {\n        "role": "system",\n        "content": "You are an AI assistant that helps people find information."\n      },\n      {\n        "role": "user",\n        "content": "Write me a 50-word introduction for Apache APISIX."\n      }\n    ],\n    "max_tokens": 800,\n    "temperature": 0.7,\n    "frequency_penalty": 0,\n    "presence_penalty": 0,\n    "top_p": 0.95,\n    "stop": null\n  }\'\n')),(0,r.kt)("p",null,"You should receive a response similar to the following:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "choices": [\n    {\n      ...,\n      "message": {\n        "content": "Apache APISIX is a modern, cloud-native API gateway built to handle high-performance and low-latency use cases. It offers a wide range of features, including load balancing, rate limiting, authentication, and dynamic routing, making it an ideal choice for microservices and cloud-native architectures.",\n        "role": "assistant"\n      }\n    }\n  ],\n  ...\n}\n')),(0,r.kt)("h3",{id:"proxy-to-embedding-models"},"Proxy to Embedding Models"),(0,r.kt)("p",null,"The following example demonstrates how you can configure the ",(0,r.kt)("inlineCode",{parentName:"p"},"ai-proxy")," Plugin to proxy requests to embedding models. This example will use the OpenAI embedding model endpoint."),(0,r.kt)("p",null,"Obtain the OpenAI ",(0,r.kt)("a",{parentName:"p",href:"https://openai.com/blog/openai-api"},"API key")," and save it to an environment variable:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"export OPENAI_API_KEY=<your-api-key>\n")),(0,r.kt)("p",null,"Create a Route and configure the ",(0,r.kt)("inlineCode",{parentName:"p"},"ai-proxy")," Plugin as such:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9180/apisix/admin/routes" -X PUT \\\n  -H "X-API-KEY: ${admin_key}" \\\n  -d \'{\n    "id": "ai-proxy-route",\n    "uri": "/embeddings",\n    "methods": ["POST"],\n    "plugins": {\n      "ai-proxy": {\n        "provider": "openai",\n        "auth": {\n          "header": {\n            "Authorization": "Bearer \'"$OPENAI_API_KEY"\'"\n          }\n        },\n        "options":{\n          "model": "text-embedding-3-small",\n          "encoding_format": "float"\n        },\n        "override": {\n          "endpoint": "https://api.openai.com/v1/embeddings"\n        }\n      }\n    }\n  }\'\n')),(0,r.kt)("p",null,"Send a POST request to the Route with an input string:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9080/embeddings" -X POST \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "input": "hello world"\n  }\'\n')),(0,r.kt)("p",null,"You should receive a response similar to the following:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "object": "list",\n  "data": [\n    {\n      "object": "embedding",\n      "index": 0,\n      "embedding": [\n        -0.0067144386,\n        -0.039197803,\n        0.034177095,\n        0.028763203,\n        -0.024785956,\n        -0.04201061,\n        ...\n      ],\n    }\n  ],\n  "model": "text-embedding-3-small",\n  "usage": {\n    "prompt_tokens": 2,\n    "total_tokens": 2\n  }\n}\n')),(0,r.kt)("h3",{id:"include-llm-information-in-access-log"},"Include LLM Information in Access Log"),(0,r.kt)("p",null,"The following example demonstrates how you can log LLM request related information in the gateway's access log to improve analytics and audit. The following variables are available:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"request_llm_model"),": LLM model name specified in the request."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"apisix_upstream_response_time"),": Time taken for APISIX to send the request to the upstream service and receive the full response"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"request_type"),": Type of request, where the value could be ",(0,r.kt)("inlineCode",{parentName:"li"},"traditional_http"),", ",(0,r.kt)("inlineCode",{parentName:"li"},"ai_chat"),", or ",(0,r.kt)("inlineCode",{parentName:"li"},"ai_stream"),"."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"llm_time_to_first_token"),": Duration from request sending to the first token received from the LLM service, in milliseconds."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"llm_model"),": LLM model."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"llm_prompt_tokens"),": Number of tokens in the prompt."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"llm_completion_tokens"),": Number of chat completion tokens in the prompt.")),(0,r.kt)("p",null,"Update the access log format in your configuration file to include additional LLM related variables:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml",metastring:'title="conf/config.yaml"',title:'"conf/config.yaml"'},'nginx_config:\n  http:\n    access_log_format: "$remote_addr - $remote_user [$time_local] $http_host \\"$request_line\\" $status $body_bytes_sent $request_time \\"$http_referer\\" \\"$http_user_agent\\" $upstream_addr $upstream_status $apisix_upstream_response_time \\"$upstream_scheme://$upstream_host$upstream_uri\\" \\"$apisix_request_id\\" \\"$request_type\\" \\"$llm_time_to_first_token\\" \\"$llm_model\\" \\"$request_llm_model\\"  \\"$llm_prompt_tokens\\" \\"$llm_completion_tokens\\""\n')),(0,r.kt)("p",null,"Reload APISIX for configuration changes to take effect."),(0,r.kt)("p",null,"Now if you create a Route and send a request following the ",(0,r.kt)("a",{parentName:"p",href:"#proxy-to-openai"},"Proxy to OpenAI example"),", you should receive a response similar to the following:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  ...,\n  "model": "gpt-4-0613",\n  "choices": [\n    {\n      "index": 0,\n      "message": {\n        "role": "assistant",\n        "content": "1+1 equals 2.",\n        "refusal": null,\n        "annotations": []\n      },\n      "logprobs": null,\n      "finish_reason": "stop"\n    }\n  ],\n  "usage": {\n    "prompt_tokens": 23,\n    "completion_tokens": 8,\n    "total_tokens": 31,\n    "prompt_tokens_details": {\n      "cached_tokens": 0,\n      "audio_tokens": 0\n    },\n    ...\n  },\n  "service_tier": "default",\n  "system_fingerprint": null\n}\n')),(0,r.kt)("p",null,"In the gateway's access log, you should see a log entry similar to the following:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},'192.168.215.1 - - [21/Mar/2025:04:28:03 +0000] api.openai.com "POST /anything HTTP/1.1" 200 804 2.858 "-" "curl/8.6.0" - - - 5765 "http://api.openai.com" "5c5e0b95f8d303cb81e4dc456a4b12d9" "ai_chat" "2858" "gpt-4" "gpt-4" "23" "8"\n')),(0,r.kt)("p",null,"The access log entry shows the request type is ",(0,r.kt)("inlineCode",{parentName:"p"},"ai_chat"),", Apisix upstream response time is ",(0,r.kt)("inlineCode",{parentName:"p"},"5765")," milliseconds, time to first token is ",(0,r.kt)("inlineCode",{parentName:"p"},"2858")," milliseconds, Requested LLM model is ",(0,r.kt)("inlineCode",{parentName:"p"},"gpt-4"),". LLM model is ",(0,r.kt)("inlineCode",{parentName:"p"},"gpt-4"),", prompt token usage is ",(0,r.kt)("inlineCode",{parentName:"p"},"23"),", and completion token usage is ",(0,r.kt)("inlineCode",{parentName:"p"},"8"),"."))}d.isMDXComponent=!0}}]);