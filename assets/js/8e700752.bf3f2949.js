"use strict";(self.webpackChunkdoc=self.webpackChunkdoc||[]).push([[42110],{3905:(e,n,t)=>{t.d(n,{Zo:()=>d,kt:()=>c});var a=t(67294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function l(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?l(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):l(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function o(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},l=Object.keys(e);for(a=0;a<l.length;a++)t=l[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(a=0;a<l.length;a++)t=l[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var s=a.createContext({}),p=function(e){var n=a.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},d=function(e){var n=p(e.components);return a.createElement(s.Provider,{value:n},e.children)},m={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},u=a.forwardRef((function(e,n){var t=e.components,i=e.mdxType,l=e.originalType,s=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),u=p(t),c=i,h=u["".concat(s,".").concat(c)]||u[c]||m[c]||l;return t?a.createElement(h,r(r({ref:n},d),{},{components:t})):a.createElement(h,r({ref:n},d))}));function c(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var l=t.length,r=new Array(l);r[0]=u;var o={};for(var s in n)hasOwnProperty.call(n,s)&&(o[s]=n[s]);o.originalType=e,o.mdxType="string"==typeof e?e:i,r[1]=o;for(var p=2;p<l;p++)r[p]=t[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,t)}u.displayName="MDXCreateElement"},36869:(e,n,t)=>{t.r(n),t.d(n,{contentTitle:()=>r,default:()=>d,frontMatter:()=>l,metadata:()=>o,toc:()=>s});var a=t(87462),i=(t(67294),t(3905));const l={title:"ai-proxy-multi",keywords:["Apache APISIX","API Gateway","Plugin","ai-proxy-multi","AI","LLM"],description:"The ai-proxy-multi Plugin extends the capabilities of ai-proxy with load balancing, retries, fallbacks, and health chekcs, simplying the integration with OpenAI, DeepSeek, and other OpenAI-compatible APIs."},r=void 0,o={unversionedId:"plugins/ai-proxy-multi",id:"version-3.13/plugins/ai-proxy-multi",isDocsHomePage:!1,title:"ai-proxy-multi",description:"The ai-proxy-multi Plugin extends the capabilities of ai-proxy with load balancing, retries, fallbacks, and health chekcs, simplying the integration with OpenAI, DeepSeek, and other OpenAI-compatible APIs.",source:"@site/docs-apisix_versioned_docs/version-3.13/plugins/ai-proxy-multi.md",sourceDirName:"plugins",slug:"/plugins/ai-proxy-multi",permalink:"/docs/apisix/3.13/plugins/ai-proxy-multi",editUrl:"/edit#https://github.com/apache/apisix/edit/release/3.13/docs/en/latest/plugins/ai-proxy-multi.md",tags:[],version:"3.13",frontMatter:{title:"ai-proxy-multi",keywords:["Apache APISIX","API Gateway","Plugin","ai-proxy-multi","AI","LLM"],description:"The ai-proxy-multi Plugin extends the capabilities of ai-proxy with load balancing, retries, fallbacks, and health chekcs, simplying the integration with OpenAI, DeepSeek, and other OpenAI-compatible APIs."},sidebar:"version-3.13/docs",previous:{title:"ai-proxy",permalink:"/docs/apisix/3.13/plugins/ai-proxy"},next:{title:"ai-rate-limiting",permalink:"/docs/apisix/3.13/plugins/ai-rate-limiting"}},s=[{value:"Description",id:"description",children:[]},{value:"Request Format",id:"request-format",children:[]},{value:"Attributes",id:"attributes",children:[]},{value:"Examples",id:"examples",children:[{value:"Load Balance between Instances",id:"load-balance-between-instances",children:[]},{value:"Configure Instance Priority and Rate Limiting",id:"configure-instance-priority-and-rate-limiting",children:[]},{value:"Load Balance and Rate Limit by Consumers",id:"load-balance-and-rate-limit-by-consumers",children:[]},{value:"Restrict Maximum Number of Completion Tokens",id:"restrict-maximum-number-of-completion-tokens",children:[]},{value:"Proxy to Embedding Models",id:"proxy-to-embedding-models",children:[]},{value:"Enable Active Health Checks",id:"enable-active-health-checks",children:[]}]}],p={toc:s};function d(e){let{components:n,...t}=e;return(0,i.kt)("wrapper",(0,a.Z)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("head",null,(0,i.kt)("link",{rel:"canonical",href:"https://docs.api7.ai/hub/ai-proxy-multi"})),(0,i.kt)("h2",{id:"description"},"Description"),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"ai-proxy-multi")," Plugin simplifies access to LLM and embedding models by transforming Plugin configurations into the designated request format for OpenAI, DeepSeek, and other OpenAI-compatible APIs. It extends the capabilities of ",(0,i.kt)("a",{parentName:"p",href:"/docs/apisix/3.13/plugins/ai-proxy"},(0,i.kt)("inlineCode",{parentName:"a"},"ai-proxy-multi"))," with load balancing, retries, fallbacks, and health checks."),(0,i.kt)("p",null,"In addition, the Plugin also supports logging LLM request information in the access log, such as token usage, model, time to the first response, and more."),(0,i.kt)("h2",{id:"request-format"},"Request Format"),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Name"),(0,i.kt)("th",{parentName:"tr",align:null},"Type"),(0,i.kt)("th",{parentName:"tr",align:null},"Required"),(0,i.kt)("th",{parentName:"tr",align:null},"Description"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"messages")),(0,i.kt)("td",{parentName:"tr",align:null},"Array"),(0,i.kt)("td",{parentName:"tr",align:null},"True"),(0,i.kt)("td",{parentName:"tr",align:null},"An array of message objects.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"messages.role")),(0,i.kt)("td",{parentName:"tr",align:null},"String"),(0,i.kt)("td",{parentName:"tr",align:null},"True"),(0,i.kt)("td",{parentName:"tr",align:null},"Role of the message (",(0,i.kt)("inlineCode",{parentName:"td"},"system"),", ",(0,i.kt)("inlineCode",{parentName:"td"},"user"),", ",(0,i.kt)("inlineCode",{parentName:"td"},"assistant"),").")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},(0,i.kt)("inlineCode",{parentName:"td"},"messages.content")),(0,i.kt)("td",{parentName:"tr",align:null},"String"),(0,i.kt)("td",{parentName:"tr",align:null},"True"),(0,i.kt)("td",{parentName:"tr",align:null},"Content of the message.")))),(0,i.kt)("h2",{id:"attributes"},"Attributes"),(0,i.kt)("table",null,(0,i.kt)("thead",{parentName:"table"},(0,i.kt)("tr",{parentName:"thead"},(0,i.kt)("th",{parentName:"tr",align:null},"Name"),(0,i.kt)("th",{parentName:"tr",align:null},"Type"),(0,i.kt)("th",{parentName:"tr",align:null},"Required"),(0,i.kt)("th",{parentName:"tr",align:null},"Default"),(0,i.kt)("th",{parentName:"tr",align:null},"Valid Values"),(0,i.kt)("th",{parentName:"tr",align:null},"Description"))),(0,i.kt)("tbody",{parentName:"table"},(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"fallback_strategy"),(0,i.kt)("td",{parentName:"tr",align:null},"string"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null},"instance_health_and_rate_limiting"),(0,i.kt)("td",{parentName:"tr",align:null},"instance_health_and_rate_limiting"),(0,i.kt)("td",{parentName:"tr",align:null},"Fallback strategy. When set, the Plugin will check whether the specified instance\u2019s token has been exhausted when a request is forwarded. If so, forward the request to the next instance regardless of the instance priority. When not set, the Plugin will not forward the request to low priority instances when token of the high priority instance is exhausted.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"balancer"),(0,i.kt)("td",{parentName:"tr",align:null},"object"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"Load balancing configurations.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"balancer.algorithm"),(0,i.kt)("td",{parentName:"tr",align:null},"string"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null},"roundrobin"),(0,i.kt)("td",{parentName:"tr",align:null},"[roundrobin, chash]"),(0,i.kt)("td",{parentName:"tr",align:null},"Load balancing algorithm. When set to ",(0,i.kt)("inlineCode",{parentName:"td"},"roundrobin"),", weighted round robin algorithm is used. When set to ",(0,i.kt)("inlineCode",{parentName:"td"},"chash"),", consistent hashing algorithm is used.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"balancer.hash_on"),(0,i.kt)("td",{parentName:"tr",align:null},"string"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"[vars, headers, cookie, consumer, vars_combinations]"),(0,i.kt)("td",{parentName:"tr",align:null},"Used when ",(0,i.kt)("inlineCode",{parentName:"td"},"type")," is ",(0,i.kt)("inlineCode",{parentName:"td"},"chash"),". Support hashing on ",(0,i.kt)("a",{parentName:"td",href:"https://nginx.org/en/docs/varindex.html"},"NGINX variables"),", headers, cookie, consumer, or a combination of ",(0,i.kt)("a",{parentName:"td",href:"https://nginx.org/en/docs/varindex.html"},"NGINX variables"),".")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"balancer.key"),(0,i.kt)("td",{parentName:"tr",align:null},"string"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"Used when ",(0,i.kt)("inlineCode",{parentName:"td"},"type")," is ",(0,i.kt)("inlineCode",{parentName:"td"},"chash"),". When ",(0,i.kt)("inlineCode",{parentName:"td"},"hash_on")," is set to ",(0,i.kt)("inlineCode",{parentName:"td"},"header")," or ",(0,i.kt)("inlineCode",{parentName:"td"},"cookie"),", ",(0,i.kt)("inlineCode",{parentName:"td"},"key")," is required. When ",(0,i.kt)("inlineCode",{parentName:"td"},"hash_on")," is set to ",(0,i.kt)("inlineCode",{parentName:"td"},"consumer"),", ",(0,i.kt)("inlineCode",{parentName:"td"},"key")," is not required as the consumer name will be used as the key automatically.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"instances"),(0,i.kt)("td",{parentName:"tr",align:null},"array","[object]"),(0,i.kt)("td",{parentName:"tr",align:null},"True"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"LLM instance configurations.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"instances.name"),(0,i.kt)("td",{parentName:"tr",align:null},"string"),(0,i.kt)("td",{parentName:"tr",align:null},"True"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"Name of the LLM service instance.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"instances.provider"),(0,i.kt)("td",{parentName:"tr",align:null},"string"),(0,i.kt)("td",{parentName:"tr",align:null},"True"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"[openai, deepseek, openai-compatible]"),(0,i.kt)("td",{parentName:"tr",align:null},"LLM service provider. When set to ",(0,i.kt)("inlineCode",{parentName:"td"},"openai"),", the Plugin will proxy the request to ",(0,i.kt)("inlineCode",{parentName:"td"},"api.openai.com"),". When set to ",(0,i.kt)("inlineCode",{parentName:"td"},"deepseek"),", the Plugin will proxy the request to ",(0,i.kt)("inlineCode",{parentName:"td"},"api.deepseek.com"),". When set to ",(0,i.kt)("inlineCode",{parentName:"td"},"openai-compatible"),", the Plugin will proxy the request to the custom endpoint configured in ",(0,i.kt)("inlineCode",{parentName:"td"},"override"),".")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"instances.priority"),(0,i.kt)("td",{parentName:"tr",align:null},"integer"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null},"0"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"Priority of the LLM instance in load balancing. ",(0,i.kt)("inlineCode",{parentName:"td"},"priority")," takes precedence over ",(0,i.kt)("inlineCode",{parentName:"td"},"weight"),".")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"instances.weight"),(0,i.kt)("td",{parentName:"tr",align:null},"string"),(0,i.kt)("td",{parentName:"tr",align:null},"True"),(0,i.kt)("td",{parentName:"tr",align:null},"0"),(0,i.kt)("td",{parentName:"tr",align:null},"greater or equal to 0"),(0,i.kt)("td",{parentName:"tr",align:null},"Weight of the LLM instance in load balancing.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"instances.auth"),(0,i.kt)("td",{parentName:"tr",align:null},"object"),(0,i.kt)("td",{parentName:"tr",align:null},"True"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"Authentication configurations.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"instances.auth.header"),(0,i.kt)("td",{parentName:"tr",align:null},"object"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"Authentication headers. At least one of the ",(0,i.kt)("inlineCode",{parentName:"td"},"header")," and ",(0,i.kt)("inlineCode",{parentName:"td"},"query")," should be configured.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"instances.auth.query"),(0,i.kt)("td",{parentName:"tr",align:null},"object"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"Authentication query parameters. At least one of the ",(0,i.kt)("inlineCode",{parentName:"td"},"header")," and ",(0,i.kt)("inlineCode",{parentName:"td"},"query")," should be configured.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"instances.options"),(0,i.kt)("td",{parentName:"tr",align:null},"object"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"Model configurations. In addition to ",(0,i.kt)("inlineCode",{parentName:"td"},"model"),", you can configure additional parameters and they will be forwarded to the upstream LLM service in the request body. For instance, if you are working with OpenAI or DeepSeek, you can configure additional parameters such as ",(0,i.kt)("inlineCode",{parentName:"td"},"max_tokens"),", ",(0,i.kt)("inlineCode",{parentName:"td"},"temperature"),", ",(0,i.kt)("inlineCode",{parentName:"td"},"top_p"),", and ",(0,i.kt)("inlineCode",{parentName:"td"},"stream"),". See your LLM provider's API documentation for more available options.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"instances.options.model"),(0,i.kt)("td",{parentName:"tr",align:null},"string"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"Name of the LLM model, such as ",(0,i.kt)("inlineCode",{parentName:"td"},"gpt-4")," or ",(0,i.kt)("inlineCode",{parentName:"td"},"gpt-3.5"),". See your LLM provider's API documentation for more available models.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"logging"),(0,i.kt)("td",{parentName:"tr",align:null},"object"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"Logging configurations.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"logging.summaries"),(0,i.kt)("td",{parentName:"tr",align:null},"boolean"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null},"false"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"If true, log request LLM model, duration, request, and response tokens.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"logging.payloads"),(0,i.kt)("td",{parentName:"tr",align:null},"boolean"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null},"false"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"If true, log request and response payload.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"logging.override"),(0,i.kt)("td",{parentName:"tr",align:null},"object"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"Override setting.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"logging.override.endpoint"),(0,i.kt)("td",{parentName:"tr",align:null},"string"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"LLM provider endpoint to replace the default endpoint with. If not configured, the Plugin uses the default OpenAI endpoint ",(0,i.kt)("inlineCode",{parentName:"td"},"https://api.openai.com/v1/chat/completions"),".")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"checks"),(0,i.kt)("td",{parentName:"tr",align:null},"object"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"Health check configurations. Note that at the moment, OpenAI and DeepSeek do not provide an official health check endpoint. Other LLM services that you can configure under ",(0,i.kt)("inlineCode",{parentName:"td"},"openai-compatible")," provider may have available health check endpoints.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"checks.active"),(0,i.kt)("td",{parentName:"tr",align:null},"object"),(0,i.kt)("td",{parentName:"tr",align:null},"True"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"Active health check configurations.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"checks.active.type"),(0,i.kt)("td",{parentName:"tr",align:null},"string"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null},"http"),(0,i.kt)("td",{parentName:"tr",align:null},"[http, https, tcp]"),(0,i.kt)("td",{parentName:"tr",align:null},"Type of health check connection.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"checks.active.timeout"),(0,i.kt)("td",{parentName:"tr",align:null},"number"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null},"1"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"Health check timeout in seconds.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"checks.active.concurrency"),(0,i.kt)("td",{parentName:"tr",align:null},"integer"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null},"10"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"Number of upstream nodes to be checked at the same time.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"checks.active.host"),(0,i.kt)("td",{parentName:"tr",align:null},"string"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"HTTP host.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"checks.active.port"),(0,i.kt)("td",{parentName:"tr",align:null},"integer"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"between 1 and 65535 inclusive"),(0,i.kt)("td",{parentName:"tr",align:null},"HTTP port.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"checks.active.http_path"),(0,i.kt)("td",{parentName:"tr",align:null},"string"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null},"/"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"Path for HTTP probing requests.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"checks.active.https_verify_certificate"),(0,i.kt)("td",{parentName:"tr",align:null},"boolean"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null},"true"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"If true, verify the node's TLS certificate.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"timeout"),(0,i.kt)("td",{parentName:"tr",align:null},"integer"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null},"30000"),(0,i.kt)("td",{parentName:"tr",align:null},"greater than or equal to 1"),(0,i.kt)("td",{parentName:"tr",align:null},"Request timeout in milliseconds when requesting the LLM service.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"keepalive"),(0,i.kt)("td",{parentName:"tr",align:null},"boolean"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null},"true"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"If true, keep the connection alive when requesting the LLM service.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"keepalive_timeout"),(0,i.kt)("td",{parentName:"tr",align:null},"integer"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null},"60000"),(0,i.kt)("td",{parentName:"tr",align:null},"greater than or equal to 1000"),(0,i.kt)("td",{parentName:"tr",align:null},"Request timeout in milliseconds when requesting the LLM service.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"keepalive_pool"),(0,i.kt)("td",{parentName:"tr",align:null},"integer"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null},"30"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"Keepalive pool size for when connecting with the LLM service.")),(0,i.kt)("tr",{parentName:"tbody"},(0,i.kt)("td",{parentName:"tr",align:null},"ssl_verify"),(0,i.kt)("td",{parentName:"tr",align:null},"boolean"),(0,i.kt)("td",{parentName:"tr",align:null},"False"),(0,i.kt)("td",{parentName:"tr",align:null},"true"),(0,i.kt)("td",{parentName:"tr",align:null}),(0,i.kt)("td",{parentName:"tr",align:null},"If true, verify the LLM service's certificate.")))),(0,i.kt)("h2",{id:"examples"},"Examples"),(0,i.kt)("p",null,"The examples below demonstrate how you can configure ",(0,i.kt)("inlineCode",{parentName:"p"},"ai-proxy-multi")," for different scenarios."),(0,i.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"note")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"You can fetch the ",(0,i.kt)("inlineCode",{parentName:"p"},"admin_key")," from ",(0,i.kt)("inlineCode",{parentName:"p"},"config.yaml")," and save to an environment variable with the following command:"),(0,i.kt)("pre",{parentName:"div"},(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"admin_key=$(yq '.deployment.admin.admin_key[0].key' conf/config.yaml | sed 's/\"//g')\n")))),(0,i.kt)("h3",{id:"load-balance-between-instances"},"Load Balance between Instances"),(0,i.kt)("p",null,"The following example demonstrates how you can configure two models for load balancing, forwarding 80% of the traffic to one instance and 20% to the other."),(0,i.kt)("p",null,"For demonstration and easier differentiation, you will be configuring one OpenAI instance and one DeepSeek instance as the upstream LLM services."),(0,i.kt)("p",null,"Create a Route as such and update with your LLM providers, models, API keys, and endpoints if applicable:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9180/apisix/admin/routes" -X PUT \\\n  -H "X-API-KEY: ${admin_key}" \\\n  -d \'{\n    "id": "ai-proxy-multi-route",\n    "uri": "/anything",\n    "methods": ["POST"],\n    "plugins": {\n      "ai-proxy-multi": {\n        "instances": [\n          {\n            "name": "openai-instance",\n            "provider": "openai",\n            "weight": 8,\n            "auth": {\n              "header": {\n                "Authorization": "Bearer \'"$OPENAI_API_KEY"\'"\n              }\n            },\n            "options": {\n              "model": "gpt-4"\n            }\n          },\n          {\n            "name": "deepseek-instance",\n            "provider": "deepseek",\n            "weight": 2,\n            "auth": {\n              "header": {\n                "Authorization": "Bearer \'"$DEEPSEEK_API_KEY"\'"\n              }\n            },\n            "options": {\n              "model": "deepseek-chat"\n            }\n          }\n        ]\n      }\n    }\n  }\'\n')),(0,i.kt)("p",null,"Send 10 POST requests to the Route with a system prompt and a sample user question in the request body, to see the number of requests forwarded to OpenAI and DeepSeek:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'openai_count=0\ndeepseek_count=0\n\nfor i in {1..10}; do\n  model=$(curl -s "http://127.0.0.1:9080/anything" -X POST \\\n    -H "Content-Type: application/json" \\\n    -d \'{\n      "messages": [\n        { "role": "system", "content": "You are a mathematician" },\n        { "role": "user", "content": "What is 1+1?" }\n      ]\n    }\' | jq -r \'.model\')\n\n  if [[ "$model" == *"gpt-4"* ]]; then\n    ((openai_count++))\n  elif [[ "$model" == "deepseek-chat" ]]; then\n    ((deepseek_count++))\n  fi\ndone\n\necho "OpenAI responses: $openai_count"\necho "DeepSeek responses: $deepseek_count"\n')),(0,i.kt)("p",null,"You should see a response similar to the following:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-text"},"OpenAI responses: 8\nDeepSeek responses: 2\n")),(0,i.kt)("h3",{id:"configure-instance-priority-and-rate-limiting"},"Configure Instance Priority and Rate Limiting"),(0,i.kt)("p",null,"The following example demonstrates how you can configure two models with different priorities and apply rate limiting on the instance with a higher priority. In the case where ",(0,i.kt)("inlineCode",{parentName:"p"},"fallback_strategy")," is set to ",(0,i.kt)("inlineCode",{parentName:"p"},"instance_health_and_rate_limiting"),", the Plugin should continue to forward requests to the low priority instance once the high priority instance's rate limiting quota is fully consumed."),(0,i.kt)("p",null,"Create a Route as such and update with your LLM providers, models, API keys, and endpoints if applicable:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9180/apisix/admin/routes" -X PUT \\\n  -H "X-API-KEY: ${admin_key}" \\\n  -d \'{\n    "id": "ai-proxy-multi-route",\n    "uri": "/anything",\n    "methods": ["POST"],\n    "plugins": {\n      "ai-proxy-multi": {\n        "fallback_strategy: "instance_health_and_rate_limiting",\n        "instances": [\n          {\n            "name": "openai-instance",\n            "provider": "openai",\n            "priority": 1,\n            "weight": 0,\n            "auth": {\n              "header": {\n                "Authorization": "Bearer \'"$OPENAI_API_KEY"\'"\n              }\n            },\n            "options": {\n              "model": "gpt-4"\n            }\n          },\n          {\n            "name": "deepseek-instance",\n            "provider": "deepseek",\n            "priority": 0,\n            "weight": 0,\n            "auth": {\n              "header": {\n                "Authorization": "Bearer \'"$DEEPSEEK_API_KEY"\'"\n              }\n            },\n            "options": {\n              "model": "deepseek-chat"\n            }\n          }\n        ]\n      },\n      "ai-rate-limiting": {\n        "instances": [\n          {\n            "name": "openai-instance",\n            "limit": 10,\n            "time_window": 60\n          }\n        ],\n        "limit_strategy": "total_tokens"\n      }\n    }\n  }\'\n')),(0,i.kt)("p",null,"Send a POST request to the Route with a system prompt and a sample user question in the request body:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9080/anything" -X POST \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "messages": [\n      { "role": "system", "content": "You are a mathematician" },\n      { "role": "user", "content": "What is 1+1?" }\n    ]\n  }\'\n')),(0,i.kt)("p",null,"You should receive a response similar to the following:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'{\n  ...,\n  "model": "gpt-4-0613",\n  "choices": [\n    {\n      "index": 0,\n      "message": {\n        "role": "assistant",\n        "content": "1+1 equals 2.",\n        "refusal": null\n      },\n      "logprobs": null,\n      "finish_reason": "stop"\n    }\n  ],\n  "usage": {\n    "prompt_tokens": 23,\n    "completion_tokens": 8,\n    "total_tokens": 31,\n    "prompt_tokens_details": {\n      "cached_tokens": 0,\n      "audio_tokens": 0\n    },\n    "completion_tokens_details": {\n      "reasoning_tokens": 0,\n      "audio_tokens": 0,\n      "accepted_prediction_tokens": 0,\n      "rejected_prediction_tokens": 0\n    }\n  },\n  "service_tier": "default",\n  "system_fingerprint": null\n}\n')),(0,i.kt)("p",null,"Since the ",(0,i.kt)("inlineCode",{parentName:"p"},"total_tokens")," value exceeds the configured quota of ",(0,i.kt)("inlineCode",{parentName:"p"},"10"),", the next request within the 60-second window is expected to be forwarded to the other instance."),(0,i.kt)("p",null,"Within the same 60-second window, send another POST request to the route:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9080/anything" -X POST \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "messages": [\n      { "role": "system", "content": "You are a mathematician" },\n      { "role": "user", "content": "Explain Newton law" }\n    ]\n  }\'\n')),(0,i.kt)("p",null,"You should see a response similar to the following:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'{\n  ...,\n  "model": "deepseek-chat",\n  "choices": [\n    {\n      "index": 0,\n      "message": {\n        "role": "assistant",\n        "content": "Certainly! Newton\'s laws of motion are three fundamental principles that describe the relationship between the motion of an object and the forces acting on it. They were formulated by Sir Isaac Newton in the late 17th century and are foundational to classical mechanics.\\n\\n---\\n\\n### **1. Newton\'s First Law (Law of Inertia):**\\n- **Statement:** An object at rest will remain at rest, and an object in motion will continue moving at a constant velocity (in a straight line at a constant speed), unless acted upon by an external force.\\n- **Key Idea:** This law introduces the concept of **inertia**, which is the tendency of an object to resist changes in its state of motion.\\n- **Example:** If you slide a book across a table, it eventually stops because of the force of friction acting on it. Without friction, the book would keep moving indefinitely.\\n\\n---\\n\\n### **2. Newton\'s Second Law (Law of Acceleration):**\\n- **Statement:** The acceleration of an object is directly proportional to the net force acting on it and inversely proportional to its mass. Mathematically, this is expressed as:\\n  \\\\[\\n  F = ma\\n  \\\\]\\n  where:\\n  - \\\\( F \\\\) = net force applied (in Newtons),\\n  -"\n      },\n      ...\n    }\n  ],\n  ...\n}\n')),(0,i.kt)("h3",{id:"load-balance-and-rate-limit-by-consumers"},"Load Balance and Rate Limit by Consumers"),(0,i.kt)("p",null,"The following example demonstrates how you can configure two models for load balancing and apply rate limiting by consumer."),(0,i.kt)("p",null,"Create a Consumer ",(0,i.kt)("inlineCode",{parentName:"p"},"johndoe")," and a rate limiting quota of 10 tokens in a 60-second window on ",(0,i.kt)("inlineCode",{parentName:"p"},"openai-instance")," instance:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9180/apisix/admin/consumers" -X PUT \\\n  -H "X-API-KEY: ${admin_key}" \\\n  -d \'{\n    "username": "johndoe",\n    "plugins": {\n      "ai-rate-limiting": {\n        "instances": [\n          {\n            "name": "openai-instance",\n            "limit": 10,\n            "time_window": 60\n          }\n        ],\n        "rejected_code": 429,\n        "limit_strategy": "total_tokens"\n      }\n    }\n  }\'\n')),(0,i.kt)("p",null,"Configure ",(0,i.kt)("inlineCode",{parentName:"p"},"key-auth")," credential for ",(0,i.kt)("inlineCode",{parentName:"p"},"johndoe"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9180/apisix/admin/consumers/johndoe/credentials" -X PUT \\\n  -H "X-API-KEY: ${admin_key}" \\\n  -d \'{\n    "id": "cred-john-key-auth",\n    "plugins": {\n      "key-auth": {\n        "key": "john-key"\n      }\n    }\n  }\'\n')),(0,i.kt)("p",null,"Create another Consumer ",(0,i.kt)("inlineCode",{parentName:"p"},"janedoe")," and a rate limiting quota of 10 tokens in a 60-second window on ",(0,i.kt)("inlineCode",{parentName:"p"},"deepseek-instance")," instance:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9180/apisix/admin/consumers" -X PUT \\\n  -H "X-API-KEY: ${admin_key}" \\\n  -d \'{\n    "username": "johndoe",\n    "plugins": {\n      "ai-rate-limiting": {\n        "instances": [\n          {\n            "name": "deepseek-instance",\n            "limit": 10,\n            "time_window": 60\n          }\n        ],\n        "rejected_code": 429,\n        "limit_strategy": "total_tokens"\n      }\n    }\n  }\'\n')),(0,i.kt)("p",null,"Configure ",(0,i.kt)("inlineCode",{parentName:"p"},"key-auth")," credential for ",(0,i.kt)("inlineCode",{parentName:"p"},"janedoe"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9180/apisix/admin/consumers/janedoe/credentials" -X PUT \\\n  -H "X-API-KEY: ${admin_key}" \\\n  -d \'{\n    "id": "cred-jane-key-auth",\n    "plugins": {\n      "key-auth": {\n        "key": "jane-key"\n      }\n    }\n  }\'\n')),(0,i.kt)("p",null,"Create a Route as such and update with your LLM providers, models, API keys, and endpoints if applicable:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9180/apisix/admin/routes" -X PUT \\\n  -H "X-API-KEY: ${admin_key}" \\\n  -d \'{\n    "id": "ai-proxy-multi-route",\n    "uri": "/anything",\n    "methods": ["POST"],\n    "plugins": {\n      "key-auth": {},\n      "ai-proxy-multi": {\n        "fallback_strategy: "instance_health_and_rate_limiting",\n        "instances": [\n          {\n            "name": "openai-instance",\n            "provider": "openai",\n            "weight": 0,\n            "auth": {\n              "header": {\n                "Authorization": "Bearer \'"$OPENAI_API_KEY"\'"\n              }\n            },\n            "options": {\n              "model": "gpt-4"\n            }\n          },\n          {\n            "name": "deepseek-instance",\n            "provider": "deepseek",\n            "weight": 0,\n            "auth": {\n              "header": {\n                "Authorization": "Bearer \'"$DEEPSEEK_API_KEY"\'"\n              }\n            },\n            "options": {\n              "model": "deepseek-chat"\n            }\n          }\n        ]\n      }\n    }\n  }\'\n')),(0,i.kt)("p",null,"Send a POST request to the Route without any consumer key:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'curl -i "http://127.0.0.1:9080/anything" -X POST \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "messages": [\n      { "role": "system", "content": "You are a mathematician" },\n      { "role": "user", "content": "What is 1+1?" }\n    ]\n  }\'\n')),(0,i.kt)("p",null,"You should receive an ",(0,i.kt)("inlineCode",{parentName:"p"},"HTTP/1.1 401 Unauthorized")," response."),(0,i.kt)("p",null,"Send a POST request to the Route with ",(0,i.kt)("inlineCode",{parentName:"p"},"johndoe"),"'s key:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9080/anything" -X POST \\\n  -H "Content-Type: application/json" \\\n  -H \'apikey: john-key\' \\\n  -d \'{\n    "messages": [\n      { "role": "system", "content": "You are a mathematician" },\n      { "role": "user", "content": "What is 1+1?" }\n    ]\n  }\'\n')),(0,i.kt)("p",null,"You should receive a response similar to the following:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'{\n  ...,\n  "model": "gpt-4-0613",\n  "choices": [\n    {\n      "index": 0,\n      "message": {\n        "role": "assistant",\n        "content": "1+1 equals 2.",\n        "refusal": null\n      },\n      "logprobs": null,\n      "finish_reason": "stop"\n    }\n  ],\n  "usage": {\n    "prompt_tokens": 23,\n    "completion_tokens": 8,\n    "total_tokens": 31,\n    "prompt_tokens_details": {\n      "cached_tokens": 0,\n      "audio_tokens": 0\n    },\n    "completion_tokens_details": {\n      "reasoning_tokens": 0,\n      "audio_tokens": 0,\n      "accepted_prediction_tokens": 0,\n      "rejected_prediction_tokens": 0\n    }\n  },\n  "service_tier": "default",\n  "system_fingerprint": null\n}\n')),(0,i.kt)("p",null,"Since the ",(0,i.kt)("inlineCode",{parentName:"p"},"total_tokens")," value exceeds the configured quota of the ",(0,i.kt)("inlineCode",{parentName:"p"},"openai")," instance for ",(0,i.kt)("inlineCode",{parentName:"p"},"johndoe"),", the next request within the 60-second window from ",(0,i.kt)("inlineCode",{parentName:"p"},"johndoe")," is expected to be forwarded to the ",(0,i.kt)("inlineCode",{parentName:"p"},"deepseek")," instance."),(0,i.kt)("p",null,"Within the same 60-second window, send another POST request to the Route with ",(0,i.kt)("inlineCode",{parentName:"p"},"johndoe"),"'s key:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9080/anything" -X POST \\\n  -H "Content-Type: application/json" \\\n  -H \'apikey: john-key\' \\\n  -d \'{\n    "messages": [\n      { "role": "system", "content": "You are a mathematician" },\n      { "role": "user", "content": "Explain Newtons laws to me" }\n    ]\n  }\'\n')),(0,i.kt)("p",null,"You should see a response similar to the following:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'{\n  ...,\n  "model": "deepseek-chat",\n  "choices": [\n    {\n      "index": 0,\n      "message": {\n        "role": "assistant",\n        "content": "Certainly! Newton\'s laws of motion are three fundamental principles that describe the relationship between the motion of an object and the forces acting on it. They were formulated by Sir Isaac Newton in the late 17th century and are foundational to classical mechanics.\\n\\n---\\n\\n### **1. Newton\'s First Law (Law of Inertia):**\\n- **Statement:** An object at rest will remain at rest, and an object in motion will continue moving at a constant velocity (in a straight line at a constant speed), unless acted upon by an external force.\\n- **Key Idea:** This law introduces the concept of **inertia**, which is the tendency of an object to resist changes in its state of motion.\\n- **Example:** If you slide a book across a table, it eventually stops because of the force of friction acting on it. Without friction, the book would keep moving indefinitely.\\n\\n---\\n\\n### **2. Newton\'s Second Law (Law of Acceleration):**\\n- **Statement:** The acceleration of an object is directly proportional to the net force acting on it and inversely proportional to its mass. Mathematically, this is expressed as:\\n  \\\\[\\n  F = ma\\n  \\\\]\\n  where:\\n  - \\\\( F \\\\) = net force applied (in Newtons),\\n  -"\n      },\n      ...\n    }\n  ],\n  ...\n}\n')),(0,i.kt)("p",null,"Send a POST request to the Route with ",(0,i.kt)("inlineCode",{parentName:"p"},"janedoe"),"'s key:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9080/anything" -X POST \\\n  -H "Content-Type: application/json" \\\n  -H \'apikey: jane-key\' \\\n  -d \'{\n    "messages": [\n      { "role": "system", "content": "You are a mathematician" },\n      { "role": "user", "content": "What is 1+1?" }\n    ]\n  }\'\n')),(0,i.kt)("p",null,"You should receive a response similar to the following:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'{\n  ...,\n  "model": "deepseek-chat",\n  "choices": [\n    {\n      "index": 0,\n      "message": {\n        "role": "assistant",\n        "content": "The sum of 1 and 1 is 2. This is a basic arithmetic operation where you combine two units to get a total of two units."\n      },\n      "logprobs": null,\n      "finish_reason": "stop"\n    }\n  ],\n  "usage": {\n    "prompt_tokens": 14,\n    "completion_tokens": 31,\n    "total_tokens": 45,\n    "prompt_tokens_details": {\n      "cached_tokens": 0\n    },\n    "prompt_cache_hit_tokens": 0,\n    "prompt_cache_miss_tokens": 14\n  },\n  "system_fingerprint": "fp_3a5770e1b4_prod0225"\n}\n')),(0,i.kt)("p",null,"Since the ",(0,i.kt)("inlineCode",{parentName:"p"},"total_tokens")," value exceeds the configured quota of the ",(0,i.kt)("inlineCode",{parentName:"p"},"deepseek")," instance for ",(0,i.kt)("inlineCode",{parentName:"p"},"janedoe"),", the next request within the 60-second window from ",(0,i.kt)("inlineCode",{parentName:"p"},"janedoe")," is expected to be forwarded to the ",(0,i.kt)("inlineCode",{parentName:"p"},"openai")," instance."),(0,i.kt)("p",null,"Within the same 60-second window, send another POST request to the Route with ",(0,i.kt)("inlineCode",{parentName:"p"},"janedoe"),"'s key:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9080/anything" -X POST \\\n  -H "Content-Type: application/json" \\\n  -H \'apikey: jane-key\' \\\n  -d \'{\n    "messages": [\n      { "role": "system", "content": "You are a mathematician" },\n      { "role": "user", "content": "Explain Newtons laws to me" }\n    ]\n  }\'\n')),(0,i.kt)("p",null,"You should see a response similar to the following:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'{\n  ...,\n  "model": "gpt-4-0613",\n  "choices": [\n    {\n      "index": 0,\n      "message": {\n        "role": "assistant",\n        "content": "Sure, here are Newton\'s three laws of motion:\\n\\n1) Newton\'s First Law, also known as the Law of Inertia, states that an object at rest will stay at rest, and an object in motion will stay in motion, unless acted on by an external force. In simple words, this law suggests that an object will keep doing whatever it is doing until something causes it to do otherwise. \\n\\n2) Newton\'s Second Law states that the force acting on an object is equal to the mass of that object times its acceleration (F=ma). This means that force is directly proportional to mass and acceleration. The heavier the object and the faster it accelerates, the greater the force.\\n\\n3) Newton\'s Third Law, also known as the law of action and reaction, states that for every action, there is an equal and opposite reaction. Essentially, any force exerted onto a body will create a force of equal magnitude but in the opposite direction on the object that exerted the first force.\\n\\nRemember, these laws become less accurate when considering speeds near the speed of light (where Einstein\'s theory of relativity becomes more appropriate) or objects very small or very large. However, for everyday situations, they provide a good model of how things move.",\n        "refusal": null\n      },\n      "logprobs": null,\n      "finish_reason": "stop"\n    }\n  ],\n  ...\n}\n')),(0,i.kt)("p",null,"This shows ",(0,i.kt)("inlineCode",{parentName:"p"},"ai-proxy-multi")," load balance the traffic with respect to the rate limiting rules in ",(0,i.kt)("inlineCode",{parentName:"p"},"ai-rate-limiting")," by consumers."),(0,i.kt)("h3",{id:"restrict-maximum-number-of-completion-tokens"},"Restrict Maximum Number of Completion Tokens"),(0,i.kt)("p",null,"The following example demonstrates how you can restrict the number of ",(0,i.kt)("inlineCode",{parentName:"p"},"completion_tokens")," used when generating the chat completion."),(0,i.kt)("p",null,"For demonstration and easier differentiation, you will be configuring one OpenAI instance and one DeepSeek instance as the upstream LLM services."),(0,i.kt)("p",null,"Create a Route as such and update with your LLM providers, models, API keys, and endpoints if applicable:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9180/apisix/admin/routes" -X PUT \\\n  -H "X-API-KEY: ${admin_key}" \\\n  -d \'{\n    "id": "ai-proxy-multi-route",\n    "uri": "/anything",\n    "methods": ["POST"],\n    "plugins": {\n      "ai-proxy-multi": {\n        "instances": [\n          {\n            "name": "openai-instance",\n            "provider": "openai",\n            "weight": 0,\n            "auth": {\n              "header": {\n                "Authorization": "Bearer \'"$OPENAI_API_KEY"\'"\n              }\n            },\n            "options": {\n              "model": "gpt-4",\n              "max_tokens": 50\n            }\n          },\n          {\n            "name": "deepseek-instance",\n            "provider": "deepseek",\n            "weight": 0,\n            "auth": {\n              "header": {\n                "Authorization": "Bearer \'"$DEEPSEEK_API_KEY"\'"\n              }\n            },\n            "options": {\n              "model": "deepseek-chat",\n              "max_tokens": 100\n            }\n          }\n        ]\n      }\n    }\n  }\'\n')),(0,i.kt)("p",null,"Send a POST request to the Route with a system prompt and a sample user question in the request body:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9080/anything" -X POST \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "messages": [\n      { "role": "system", "content": "You are a mathematician" },\n      { "role": "user", "content": "Explain Newtons law" }\n    ]\n  }\'\n')),(0,i.kt)("p",null,"If the request is proxied to OpenAI, you should see a response similar to the following, where the content is truncated per 50 ",(0,i.kt)("inlineCode",{parentName:"p"},"max_tokens")," threshold:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'{\n  ...,\n  "model": "gpt-4-0613",\n  "choices": [\n    {\n      "index": 0,\n      "message": {\n        "role": "assistant",\n        "content": "Newton\'s Laws of Motion are three physical laws that form the bedrock for classical mechanics. They describe the relationship between a body and the forces acting upon it, and the body\'s motion in response to those forces. \\n\\n1. Newton\'s First Law",\n        "refusal": null\n      },\n      "logprobs": null,\n      "finish_reason": "length"\n    }\n  ],\n  "usage": {\n    "prompt_tokens": 20,\n    "completion_tokens": 50,\n    "total_tokens": 70,\n    "prompt_tokens_details": {\n      "cached_tokens": 0,\n      "audio_tokens": 0\n    },\n    "completion_tokens_details": {\n      "reasoning_tokens": 0,\n      "audio_tokens": 0,\n      "accepted_prediction_tokens": 0,\n      "rejected_prediction_tokens": 0\n    }\n  },\n  "service_tier": "default",\n  "system_fingerprint": null\n}\n')),(0,i.kt)("p",null,"If the request is proxied to DeepSeek, you should see a response similar to the following, where the content is truncated per 100 ",(0,i.kt)("inlineCode",{parentName:"p"},"max_tokens")," threshold:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'{\n  ...,\n  "model": "deepseek-chat",\n  "choices": [\n    {\n      "index": 0,\n      "message": {\n        "role": "assistant",\n        "content": "Newton\'s Laws of Motion are three fundamental principles that form the foundation of classical mechanics. They describe the relationship between a body and the forces acting upon it, and the body\'s motion in response to those forces. Here\'s a brief explanation of each law:\\n\\n1. **Newton\'s First Law (Law of Inertia):**\\n   - **Statement:** An object will remain at rest or in uniform motion in a straight line unless acted upon by an external force.\\n   - **Explanation:** This law"\n      },\n      "logprobs": null,\n      "finish_reason": "length"\n    }\n  ],\n  "usage": {\n    "prompt_tokens": 10,\n    "completion_tokens": 100,\n    "total_tokens": 110,\n    "prompt_tokens_details": {\n      "cached_tokens": 0\n    },\n    "prompt_cache_hit_tokens": 0,\n    "prompt_cache_miss_tokens": 10\n  },\n  "system_fingerprint": "fp_3a5770e1b4_prod0225"\n}\n')),(0,i.kt)("h3",{id:"proxy-to-embedding-models"},"Proxy to Embedding Models"),(0,i.kt)("p",null,"The following example demonstrates how you can configure the ",(0,i.kt)("inlineCode",{parentName:"p"},"ai-proxy-multi")," Plugin to proxy requests and load balance between embedding models."),(0,i.kt)("p",null,"Create a Route as such and update with your LLM providers, embedding models, API keys, and endpoints:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9180/apisix/admin/routes" -X PUT \\\n  -H "X-API-KEY: ${admin_key}" \\\n  -d \'{\n    "id": "ai-proxy-multi-route",\n    "uri": "/anything",\n    "methods": ["POST"],\n    "plugins": {\n      "ai-proxy-multi": {\n        "instances": [\n          {\n            "name": "openai-instance",\n            "provider": "openai",\n            "weight": 0,\n            "auth": {\n              "header": {\n                "Authorization": "Bearer \'"$OPENAI_API_KEY"\'"\n              }\n            },\n            "options": {\n              "model": "text-embedding-3-small"\n            },\n            "override": {\n              "endpoint": "https://api.openai.com/v1/embeddings"\n            }\n          },\n          {\n            "name": "az-openai-instance",\n            "provider": "openai-compatible",\n            "weight": 0,\n            "auth": {\n              "header": {\n                "Authorization": "Bearer \'"$AZ_OPENAI_API_KEY"\'"\n              }\n            },\n            "options": {\n              "model": "text-embedding-3-small"\n            },\n            "override": {\n              "endpoint": "https://ai-plugin-developer.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2023-05-15"\n            }\n          }\n        ]\n      }\n    }\n  }\'\n')),(0,i.kt)("p",null,"Send a POST request to the Route with an input string:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9080/embeddings" -X POST \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "input": "hello world"\n  }\'\n')),(0,i.kt)("p",null,"You should receive a response similar to the following:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "object": "list",\n  "data": [\n    {\n      "object": "embedding",\n      "index": 0,\n      "embedding": [\n        -0.0067144386,\n        -0.039197803,\n        0.034177095,\n        0.028763203,\n        -0.024785956,\n        -0.04201061,\n        ...\n      ],\n    }\n  ],\n  "model": "text-embedding-3-small",\n  "usage": {\n    "prompt_tokens": 2,\n    "total_tokens": 2\n  }\n}\n')),(0,i.kt)("h3",{id:"enable-active-health-checks"},"Enable Active Health Checks"),(0,i.kt)("p",null,"The following example demonstrates how you can configure the ",(0,i.kt)("inlineCode",{parentName:"p"},"ai-proxy-multi")," Plugin to proxy requests and load balance between models, and enable active health check to improve service availability. You can enable health check on one or multiple instances."),(0,i.kt)("p",null,"Create a Route as such and update the LLM providers, embedding models, API keys, and health check related configurations:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},'curl "http://127.0.0.1:9180/apisix/admin/routes" -X PUT \\\n  -H "X-API-KEY: ${admin_key}" \\\n  -d \'{\n    "id": "ai-proxy-multi-route",\n    "uri": "/anything",\n    "methods": ["POST"],\n    "plugins": {\n      "ai-proxy-multi": {\n        "instances": [\n          {\n            "name": "llm-instance-1",\n            "provider": "openai-compatible",\n            "weight": 0,\n            "auth": {\n              "header": {\n                "Authorization": "Bearer \'"$YOUR_LLM_API_KEY"\'"\n              }\n            },\n            "options": {\n              "model": "\'"$YOUR_LLM_MODEL"\'"\n            }\n          },\n          {\n            "name": "llm-instance-2",\n            "provider": "openai-compatible",\n            "weight": 0,\n            "auth": {\n              "header": {\n                "Authorization": "Bearer \'"$YOUR_LLM_API_KEY"\'"\n              }\n            },\n            "options": {\n              "model": "\'"$YOUR_LLM_MODEL"\'"\n            },\n            "checks": {\n              "active": {\n                "type": "https",\n                "host": "yourhost.com",\n                "http_path": "/your/probe/path",\n                "healthy": {\n                  "interval": 2,\n                  "successes": 1\n                },\n                "unhealthy": {\n                  "interval": 1,\n                  "http_failures": 3\n                }\n              }\n            }\n          }\n        ]\n      }\n    }\n  }\'\n')),(0,i.kt)("p",null,"For verification, the behaviours should be consistent with the verification in ",(0,i.kt)("a",{parentName:"p",href:"/docs/apisix/3.13/tutorials/health-check"},"active health checks"),"."))}d.isMDXComponent=!0}}]);