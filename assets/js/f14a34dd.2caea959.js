"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[32587],{35318:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>d});var a=n(27378);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},g=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),g=p(n),d=r,h=g["".concat(l,".").concat(d)]||g[d]||c[d]||o;return n?a.createElement(h,i(i({ref:t},u),{},{components:n})):a.createElement(h,i({ref:t},u))}));function d(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=g;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var p=2;p<o;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}g.displayName="MDXCreateElement"},37349:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>c,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var a=n(25773),r=(n(27378),n(35318));const o={title:"APISIX integrate with Kafka for real-time log monitoring",authors:[{name:"Zeping Bai",title:"Author",url:"https://github.com/bzp2010",image_url:"https://avatars.githubusercontent.com/u/8078418?v=4"},{name:"Sylvia",title:"Technical Writer",url:"https://github.com/SylviaBABY",image_url:"https://avatars.githubusercontent.com/u/39793568?v=4"}],keywords:["Apache APISIX","Apache Kafka","Kafka","logging","monitoring"],description:"This article describes how to use the kafka-logger plugin with APISIX. Wiht enhancements, the plugin now has mature and complete functions.",tags:["Ecosystem","Plugins"],image:"https://static.apiseven.com/2022/blog/0818/plugins/kafka.png"},i=void 0,s={permalink:"/blog/2022/01/17/apisix-kafka-integration",source:"@site/blog/2022/01/17/apisix-kafka-integration.md",title:"APISIX integrate with Kafka for real-time log monitoring",description:"This article describes how to use the kafka-logger plugin with APISIX. Wiht enhancements, the plugin now has mature and complete functions.",date:"2022-01-17T00:00:00.000Z",formattedDate:"January 17, 2022",tags:[{label:"Ecosystem",permalink:"/blog/tags/ecosystem"},{label:"Plugins",permalink:"/blog/tags/plugins"}],readingTime:4.45,truncated:!0,authors:[{name:"Zeping Bai",title:"Author",url:"https://github.com/bzp2010",image_url:"https://avatars.githubusercontent.com/u/8078418?v=4",imageURL:"https://avatars.githubusercontent.com/u/8078418?v=4"},{name:"Sylvia",title:"Technical Writer",url:"https://github.com/SylviaBABY",image_url:"https://avatars.githubusercontent.com/u/39793568?v=4",imageURL:"https://avatars.githubusercontent.com/u/39793568?v=4"}],prevItem:{title:"Biweekly Report (Jan 1 - Jan 16)",permalink:"/blog/2022/01/19/weekly-report-0116"},nextItem:{title:"Makes Convenienter to Proxy Dubbo in Apache APISIX",permalink:"/blog/2022/01/13/how-to-proxy-dubbo-in-apache-apisix"}},l={authorsImageUrls:[void 0,void 0]},p=[{value:"Implementation: kafka-logger",id:"implementation-kafka-logger",children:[],level:2},{value:"How to use",id:"how-to-use",children:[{value:"Step 1: Start Kafka Cluster",id:"step-1-start-kafka-cluster",children:[],level:3},{value:"Step 2: Create Topic",id:"step-2-create-topic",children:[],level:3},{value:"Step 3: Create Routing and Enable Plugin",id:"step-3-create-routing-and-enable-plugin",children:[],level:3},{value:"Step 4: Send Requests",id:"step-4-send-requests",children:[{value:"Customize the logging structure",id:"customize-the-logging-structure",children:[],level:4}],level:3},{value:"Turn off the plugin",id:"turn-off-the-plugin",children:[],level:3}],level:2},{value:"Summary",id:"summary",children:[],level:2}],u={toc:p};function c(e){let{components:t,...n}=e;return(0,r.kt)("wrapper",(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"This article describes how to use the kafka-logger plugin with APISIX. Wiht enhancements, the plugin now has mature and complete functions.")),(0,r.kt)("p",null,"Apache Kafka is an open source stream processing platform managed by Apache, written in Scala and Java. Apache Kafka provides uniformed, high-throughput, low-latency functionality for real-time data processing."),(0,r.kt)("p",null,'Apache Kafka\'s persistence layer is essentially a "massive publish/subscribe message queue following a distributed transaction logging architecture," making it valuable as an enterprise-class infrastructure for processing streaming data. It is used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.'),(0,r.kt)("h2",{id:"implementation-kafka-logger"},"Implementation: kafka-logger"),(0,r.kt)("p",null,"Apache APISIX has been providing support for Apache Kafka since version 1.2 with the ",(0,r.kt)("inlineCode",{parentName:"p"},"kafka-logger")," plugin release. ",(0,r.kt)("inlineCode",{parentName:"p"},"kafka-logger")," has been enhanced several times since then to provide very mature and complete functionality. It supports pushing API request logs, request bodies, and response bodies, to a Kafka cluster in JSON format."),(0,r.kt)("p",null,"When using ",(0,r.kt)("inlineCode",{parentName:"p"},"kafka-logger"),", users can send a wide range of data and customize the format of the logs sent. ",(0,r.kt)("inlineCode",{parentName:"p"},"kafka-logger")," supports sending logs in a packaged manner in a batch or for automatic retries."),(0,r.kt)("h2",{id:"how-to-use"},"How to use"),(0,r.kt)("h3",{id:"step-1-start-kafka-cluster"},"Step 1: Start Kafka Cluster"),(0,r.kt)("p",null,"This article only demonstrates one way to start the cluster. Please refer to the official documentation for details of other ways to start the cluster."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'# Start a cluster with 1 ZooKeeper node and 3 kafka nodes using docker-compose\n# At the same time, an EFAK is started for data monitoring.\nversion: \'3\'\n\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:6.2.1\n    hostname: zookeeper\n    ports:\n      - "2181:2181"\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n      ZOOKEEPER_SERVER_ID: 1\n      ZOOKEEPER_SERVERS: zookeeper:2888:3888\n\n  kafka1:\n    image: confluentinc/cp-kafka:6.2.1\n    hostname: kafka1\n    ports:\n      - "9092:9092"\n    environment:\n      KAFKA_ADVERTISED_LISTENERS: LISTENER_DOCKER_INTERNAL://kafka1:19092,LISTENER_DOCKER_EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT\n      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_DOCKER_INTERNAL\n      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"\n      KAFKA_BROKER_ID: 1\n      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"\n    depends_on:\n      - zookeeper\n\n  kafka2:\n    image: confluentinc/cp-kafka:6.2.1\n    hostname: kafka2\n    ports:\n      - "9093:9093"\n    environment:\n      KAFKA_ADVERTISED_LISTENERS: LISTENER_DOCKER_INTERNAL://kafka2:19093,LISTENER_DOCKER_EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9093\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT\n      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_DOCKER_INTERNAL\n      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"\n      KAFKA_BROKER_ID: 2\n      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"\n    depends_on:\n      - zookeeper\n\n\n  kafka3:\n    image: confluentinc/cp-kafka:6.2.1\n    hostname: kafka3\n    ports:\n      - "9094:9094"\n    environment:\n      KAFKA_ADVERTISED_LISTENERS: LISTENER_DOCKER_INTERNAL://kafka3:19094,LISTENER_DOCKER_EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9094\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT\n      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_DOCKER_INTERNAL\n      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"\n      KAFKA_BROKER_ID: 3\n      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"\n    depends_on:\n      - zookeeper\n\n  efak:\n    image: nickzurich/kafka-eagle:2.0.9\n    hostname: efak\n    ports:\n      - "8048:8048"\n    depends_on:\n      - kafka1\n      - kafka2\n      - kafka3\n')),(0,r.kt)("h3",{id:"step-2-create-topic"},"Step 2: Create Topic"),(0,r.kt)("p",null,"Next we create the ",(0,r.kt)("inlineCode",{parentName:"p"},"test Topic")," to collect logs."),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://static.apiseven.com/202108/1642390784736-562187ed-ade9-4a2f-96e1-c79556f9dd7d.png",alt:"test Topic"})),(0,r.kt)("h3",{id:"step-3-create-routing-and-enable-plugin"},"Step 3: Create Routing and Enable Plugin"),(0,r.kt)("p",null,"The following commands allow you to create routes and enable the ",(0,r.kt)("inlineCode",{parentName:"p"},"kafka-logger")," plugin."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'curl -XPUT \'http://127.0.0.1:9080/apisix/admin/routes/r1\' \\\n--header \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "uri": "/*",\n    "plugins": {\n        "kafka-logger": {\n            "batch_max_size": 1,\n            "broker_list": {\n                "127.0.0.1": 9092\n            },\n            "disable": false,\n            "kafka_topic": "test",\n            "producer_type": "sync"\n        }\n    },\n    "upstream": {\n        "nodes": {\n            "httpbin.org:80": 1\n        },\n        "type": "roundrobin"\n    }\n}\'\n')),(0,r.kt)("p",null,"The above code configures the ",(0,r.kt)("inlineCode",{parentName:"p"},"kafka broker")," address, the target Topic, the production mode for synchronization, and the maximum number of logs to be included in each batch. Here we can start by setting ",(0,r.kt)("inlineCode",{parentName:"p"},"batch_max_size")," to 1, i.e. write one message to Kafka for every log produced."),(0,r.kt)("p",null,"With the above settings, it is possible to send the logs of API requests under the ",(0,r.kt)("inlineCode",{parentName:"p"},"/*")," path to Kafka."),(0,r.kt)("h3",{id:"step-4-send-requests"},"Step 4: Send Requests"),(0,r.kt)("p",null,"Next, we send some requests through the API and record the number of requests."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"# Send 10 requests to API\ncurl http://127.0.0.1:9080/get\n")),(0,r.kt)("p",null,"As you can see in the figure below, some log messages have been written to the ",(0,r.kt)("inlineCode",{parentName:"p"},"test topic")," we created. If you click to view the log content, you can see that the logs of the API requests made above have been written."),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://static.apiseven.com/202108/1642390828394-721eccfa-ab02-4f8f-a0d8-8039e0eaabc1.png",alt:"Send Requests-1"})),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://static.apiseven.com/202108/1642390874028-89683dfb-ab16-48cd-92de-496cc60df3b5.png",alt:"Send Requests-2"})),(0,r.kt)("h4",{id:"customize-the-logging-structure"},"Customize the logging structure"),(0,r.kt)("p",null,"Of course, we can also set the structure of the log data sent to Kafka during use through the metadata configuration provided by the ",(0,r.kt)("inlineCode",{parentName:"p"},"kafka-logger")," plugin. By setting the ",(0,r.kt)("inlineCode",{parentName:"p"},"log_format")," data, you can control the type of data sent."),(0,r.kt)("p",null,"For example, ",(0,r.kt)("inlineCode",{parentName:"p"},"$host")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"$time_iso8601")," in the following data are built-in variables provided by Nginx; APISIX variables such as ",(0,r.kt)("inlineCode",{parentName:"p"},"$route_id")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"$service_id")," are also supported."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'curl -XPUT \'http://127.0.0.1:9080/apisix/admin/plugin_metadata/kafka-logger\' \\\n--header \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' \\\n--header \'Content-Type: application/json\' \\\n--data-raw \'{\n    "log_format": {\n        "host": "$host",\n        "@timestamp": "$time_iso8601",\n        "client_ip": "$remote_addr",\n        "route_id": "$route_id"\n    }\n}\'\n')),(0,r.kt)("p",null,"A simple test by sending a request shows that the above logging structure settings have taken effect. Currently, Apache APISIX provides a variety of log format templates, which provides great flexibility in configuration, and more details on log format can be found in ",(0,r.kt)("a",{parentName:"p",href:"https://apisix.apache.org/docs/apisix/plugins/kafka-logger#metadata"},"Apache APISIX documentation"),"."),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://static.apiseven.com/202108/1642390899127-d1eb560a-499e-4a9f-9227-4063ba711e2d.png",alt:"customize the logging structure"})),(0,r.kt)("h3",{id:"turn-off-the-plugin"},"Turn off the plugin"),(0,r.kt)("p",null,"If you are done using the plugin, simply remove the kafka-logger plugin-related configuration from the route configuration and save it to turn off the plugin on the route. Thanks to the dynamic advantage of Apache APISIX, the process of turning on and off the plugin does not require restarting Apache APISIX, which is very convenient."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'curl http://127.0.0.1:9080/apisix/admin/routes/1  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\n{\n    "methods": ["GET"],\n    "uri": "/hello",\n    "plugins": {},\n    "upstream": {\n        "type": "roundrobin",\n        "nodes": {\n            "127.0.0.1:1980": 1\n        }\n    }\n}\'\n')),(0,r.kt)("h2",{id:"summary"},"Summary"),(0,r.kt)("p",null,"In this article, we have introduced the feature preview and usage steps of the kafka-logger plugin. For more information about the kafka-logger plugin and the full configuration list, you can refer to the official documentation."),(0,r.kt)("p",null,"We are also currently working on other logging plugins to integrate with more related services. If you're interested in such integration projects, feel free to start a discussion in ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/apache/apisix/discussions"},"GitHub Discussions")," or communicate via the ",(0,r.kt)("a",{parentName:"p",href:"https://apisix.apache.org/zh/docs/general/join"},"mailing list"),"."))}c.isMDXComponent=!0}}]);